{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":230200333,"sourceType":"kernelVersion"}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Pipeline Corrosion_protein_classification \non Sequence Analysis and Functional Prediction ","metadata":{}},{"cell_type":"code","source":"'''import os\nos.path.exists('/content/drive/MyDrive')'''","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-28T13:27:08.859814Z","iopub.execute_input":"2025-03-28T13:27:08.860102Z","iopub.status.idle":"2025-03-28T13:27:08.870718Z","shell.execute_reply.started":"2025-03-28T13:27:08.860075Z","shell.execute_reply":"2025-03-28T13:27:08.869390Z"}},"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"False"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"'''# Colab specific\nfrom google.colab import drive\nfrom google.colab import files\nimport os\n\ndrive.mount('/content/drive')\n\n#change the path\nos.chdir('/content/drive/MyDrive/MIC/data_picrust')'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T13:27:08.872742Z","iopub.execute_input":"2025-03-28T13:27:08.873293Z","iopub.status.idle":"2025-03-28T13:27:08.996056Z","shell.execute_reply.started":"2025-03-28T13:27:08.873251Z","shell.execute_reply":"2025-03-28T13:27:08.994428Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-d3f63d2d2b5e>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#change the path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    116\u001b[0m   \u001b[0;34m\"\"\"Internal helper to mount Google Drive.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/var/colab/hostname'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m     raise NotImplementedError(\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0;34m'Mounting drive is unsupported in this environment. Use PyDrive'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;34m' instead. See examples at'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNotImplementedError\u001b[0m: Mounting drive is unsupported in this environment. Use PyDrive instead. See examples at https://colab.research.google.com/notebooks/io.ipynb#scrollTo=7taylj9wpsA2."],"ename":"NotImplementedError","evalue":"Mounting drive is unsupported in this environment. Use PyDrive instead. See examples at https://colab.research.google.com/notebooks/io.ipynb#scrollTo=7taylj9wpsA2.","output_type":"error"}],"execution_count":2},{"cell_type":"code","source":"# Making sure to use same python version for compatibility\n!sudo apt-get update -y\n!sudo apt-get install python3.10\n!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.10 1\n!python --version","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T13:31:57.946204Z","iopub.execute_input":"2025-03-28T13:31:57.946532Z","iopub.status.idle":"2025-03-28T13:32:30.395315Z","shell.execute_reply.started":"2025-03-28T13:31:57.946504Z","shell.execute_reply":"2025-03-28T13:32:30.394064Z"}},"outputs":[{"name":"stdout","text":"Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\nGet:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\nGet:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [69.9 kB]\nHit:4 http://archive.ubuntu.com/ubuntu jammy InRelease                         \nGet:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,381 kB]\nGet:6 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]      \nGet:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]        \nGet:8 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]           \nGet:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\nGet:10 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,796 kB] \nGet:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]     \nGet:12 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\nGet:13 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,239 kB]\nHit:14 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease   \nGet:15 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [55.7 kB]\nGet:16 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [33.6 kB]\nGet:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,538 kB]\nGet:18 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [46.8 kB]\nGet:19 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [3,892 kB]\nGet:20 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [4,049 kB]\nGet:21 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [47.7 kB]\nGet:22 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,737 kB]\nGet:23 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,045 kB]\nGet:24 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [35.2 kB]\nGet:25 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [82.7 kB]\nGet:26 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,691 kB]\nFetched 30.2 MB in 5s (6,700 kB/s)                           \nReading package lists... Done\nW: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following additional packages will be installed:\n  libpython3.10 libpython3.10-dev libpython3.10-minimal libpython3.10-stdlib\n  python3.10-dev python3.10-minimal\nSuggested packages:\n  python3.10-venv python3.10-doc binfmt-support\nThe following packages will be upgraded:\n  libpython3.10 libpython3.10-dev libpython3.10-minimal libpython3.10-stdlib\n  python3.10 python3.10-dev python3.10-minimal\n7 upgraded, 0 newly installed, 0 to remove and 161 not upgraded.\nNeed to get 12.7 MB of archives.\nAfter this operation, 1,024 B of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 python3.10-dev amd64 3.10.12-1~22.04.9 [508 kB]\nGet:2 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpython3.10-dev amd64 3.10.12-1~22.04.9 [4,763 kB]\nGet:3 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpython3.10 amd64 3.10.12-1~22.04.9 [1,949 kB]\nGet:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 python3.10 amd64 3.10.12-1~22.04.9 [508 kB]\nGet:5 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpython3.10-stdlib amd64 3.10.12-1~22.04.9 [1,850 kB]\nGet:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 python3.10-minimal amd64 3.10.12-1~22.04.9 [2,263 kB]\nGet:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpython3.10-minimal amd64 3.10.12-1~22.04.9 [815 kB]\nFetched 12.7 MB in 3s (4,066 kB/s)             \ndebconf: unable to initialize frontend: Dialog\ndebconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 7.)\ndebconf: falling back to frontend: Readline\n(Reading database ... 127400 files and directories currently installed.)\nPreparing to unpack .../0-python3.10-dev_3.10.12-1~22.04.9_amd64.deb ...\nUnpacking python3.10-dev (3.10.12-1~22.04.9) over (3.10.12-1~22.04.7) ...\nPreparing to unpack .../1-libpython3.10-dev_3.10.12-1~22.04.9_amd64.deb ...\nUnpacking libpython3.10-dev:amd64 (3.10.12-1~22.04.9) over (3.10.12-1~22.04.7) ...\nPreparing to unpack .../2-libpython3.10_3.10.12-1~22.04.9_amd64.deb ...\nUnpacking libpython3.10:amd64 (3.10.12-1~22.04.9) over (3.10.12-1~22.04.7) ...\nPreparing to unpack .../3-python3.10_3.10.12-1~22.04.9_amd64.deb ...\nUnpacking python3.10 (3.10.12-1~22.04.9) over (3.10.12-1~22.04.7) ...\nPreparing to unpack .../4-libpython3.10-stdlib_3.10.12-1~22.04.9_amd64.deb ...\nUnpacking libpython3.10-stdlib:amd64 (3.10.12-1~22.04.9) over (3.10.12-1~22.04.7) ...\nPreparing to unpack .../5-python3.10-minimal_3.10.12-1~22.04.9_amd64.deb ...\nUnpacking python3.10-minimal (3.10.12-1~22.04.9) over (3.10.12-1~22.04.7) ...\nPreparing to unpack .../6-libpython3.10-minimal_3.10.12-1~22.04.9_amd64.deb ...\nUnpacking libpython3.10-minimal:amd64 (3.10.12-1~22.04.9) over (3.10.12-1~22.04.7) ...\nSetting up libpython3.10-minimal:amd64 (3.10.12-1~22.04.9) ...\nSetting up python3.10-minimal (3.10.12-1~22.04.9) ...\nSetting up libpython3.10-stdlib:amd64 (3.10.12-1~22.04.9) ...\nSetting up libpython3.10:amd64 (3.10.12-1~22.04.9) ...\nSetting up python3.10 (3.10.12-1~22.04.9) ...\nSetting up libpython3.10-dev:amd64 (3.10.12-1~22.04.9) ...\nSetting up python3.10-dev (3.10.12-1~22.04.9) ...\nProcessing triggers for man-db (2.10.2-1) ...\nProcessing triggers for libc-bin (2.35-0ubuntu3.4) ...\n/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n\nupdate-alternatives: using /usr/bin/python3.10 to provide /usr/bin/python3 (python3) in auto mode\nPython 3.10.12\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Set up memory footprint support libraries\n!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n!pip install gputil\n!pip install psutil\n%pip install humanize\n%pip install memory_profiler\nimport psutil\nimport humanize\nimport os\nimport GPUtil as GPU\nGPUs = GPU.getGPUs()\nfrom psutil import virtual_memory\nram_gb = virtual_memory().total / 1e9\nprint('Runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n\nif ram_gb < 20:\n  print('Not using a high-RAM runtime')\nelse:\n  print('Using a high-RAM runtime!')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T13:32:36.128513Z","iopub.execute_input":"2025-03-28T13:32:36.128904Z","iopub.status.idle":"2025-03-28T13:32:58.007809Z","shell.execute_reply.started":"2025-03-28T13:32:36.128873Z","shell.execute_reply":"2025-03-28T13:32:58.006473Z"}},"outputs":[{"name":"stdout","text":"Collecting gputil\n  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nBuilding wheels for collected packages: gputil\n  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for gputil: filename=GPUtil-1.4.0-py3-none-any.whl size=7392 sha256=30960d50bf973ccec549edaa813a9076545c34044eb4b40ad549247fa117ea37\n  Stored in directory: /root/.cache/pip/wheels/a9/8a/bd/81082387151853ab8b6b3ef33426e98f5cbfebc3c397a9d4d0\nSuccessfully built gputil\nInstalling collected packages: gputil\nSuccessfully installed gputil-1.4.0\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (5.9.5)\nRequirement already satisfied: humanize in /usr/local/lib/python3.10/dist-packages (4.11.0)\nNote: you may need to restart the kernel to use updated packages.\nCollecting memory_profiler\n  Downloading memory_profiler-0.61.0-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from memory_profiler) (5.9.5)\nDownloading memory_profiler-0.61.0-py3-none-any.whl (31 kB)\nInstalling collected packages: memory_profiler\nSuccessfully installed memory_profiler-0.61.0\nNote: you may need to restart the kernel to use updated packages.\nRuntime has 33.7 gigabytes of available RAM\n\nUsing a high-RAM runtime!\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"!pip install psutil\nimport psutil\n!pip install biopython\nfrom IPython import get_ipython\nfrom IPython.display import display\n!pip install biom-format\n%pip install umap-learn\n!pip install lxml pandas\n!pip install pyarrow\n!pip install kneed\n!pip install scipy\n%pip install \"dask[complete]\"\n!pip install fuzzywuzzy\n!pip install python-Levenshtein","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T13:32:58.009116Z","iopub.execute_input":"2025-03-28T13:32:58.009427Z","iopub.status.idle":"2025-03-28T13:34:19.313693Z","shell.execute_reply.started":"2025-03-28T13:32:58.009397Z","shell.execute_reply":"2025-03-28T13:34:19.312140Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (5.9.5)\nCollecting biopython\n  Downloading biopython-1.85-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from biopython) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->biopython) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->biopython) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->biopython) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->biopython) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->biopython) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->biopython) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->biopython) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->biopython) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->biopython) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->biopython) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->biopython) (2024.2.0)\nDownloading biopython-1.85-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: biopython\nSuccessfully installed biopython-1.85\nCollecting biom-format\n  Downloading biom-format-2.1.16.tar.gz (11.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from biom-format) (8.1.7)\nRequirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.10/dist-packages (from biom-format) (1.26.4)\nRequirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from biom-format) (1.13.1)\nRequirement already satisfied: pandas>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from biom-format) (2.2.3)\nRequirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from biom-format) (3.12.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.9.2->biom-format) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.9.2->biom-format) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.9.2->biom-format) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.9.2->biom-format) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.9.2->biom-format) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.9.2->biom-format) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.20.0->biom-format) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.20.0->biom-format) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.20.0->biom-format) (2025.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=0.20.0->biom-format) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.9.2->biom-format) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.9.2->biom-format) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.9.2->biom-format) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.9.2->biom-format) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.9.2->biom-format) (2024.2.0)\nBuilding wheels for collected packages: biom-format\n  Building wheel for biom-format (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for biom-format: filename=biom_format-2.1.16-cp310-cp310-linux_x86_64.whl size=12159045 sha256=3fe3df957d7b4af264e38c811f51828908e6e92cbfd92bbcc2d67e296d6d7f5d\n  Stored in directory: /root/.cache/pip/wheels/8e/a9/f9/197fd5a0e5bbab5f2e03c89194f6c194bed7af5d7a8c8759f3\nSuccessfully built biom-format\nInstalling collected packages: biom-format\nSuccessfully installed biom-format-2.1.16\nCollecting umap-learn\n  Downloading umap_learn-0.5.7-py3-none-any.whl.metadata (21 kB)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.26.4)\nRequirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.13.1)\nRequirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.2.2)\nRequirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (0.60.0)\nCollecting pynndescent>=0.5 (from umap-learn)\n  Downloading pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from umap-learn) (4.67.1)\nRequirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn) (0.43.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->umap-learn) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->umap-learn) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->umap-learn) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->umap-learn) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->umap-learn) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->umap-learn) (2.4.1)\nRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from pynndescent>=0.5->umap-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22->umap-learn) (3.5.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->umap-learn) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->umap-learn) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->umap-learn) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->umap-learn) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->umap-learn) (2024.2.0)\nDownloading umap_learn-0.5.7-py3-none-any.whl (88 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.8/88.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: pynndescent, umap-learn\nSuccessfully installed pynndescent-0.5.13 umap-learn-0.5.7\nNote: you may need to restart the kernel to use updated packages.\nRequirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (5.3.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.3)\nRequirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.22.4->pandas) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.22.4->pandas) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.22.4->pandas) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.22.4->pandas) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.22.4->pandas) (2024.2.0)\nRequirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (19.0.1)\nCollecting kneed\n  Downloading kneed-0.8.5-py3-none-any.whl.metadata (5.5 kB)\nRequirement already satisfied: numpy>=1.14.2 in /usr/local/lib/python3.10/dist-packages (from kneed) (1.26.4)\nRequirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from kneed) (1.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.2->kneed) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.2->kneed) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.2->kneed) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.2->kneed) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.2->kneed) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.2->kneed) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.14.2->kneed) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.14.2->kneed) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.14.2->kneed) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.14.2->kneed) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.14.2->kneed) (2024.2.0)\nDownloading kneed-0.8.5-py3-none-any.whl (10 kB)\nInstalling collected packages: kneed\nSuccessfully installed kneed-0.8.5\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.13.1)\nRequirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from scipy) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<2.3,>=1.22.4->scipy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<2.3,>=1.22.4->scipy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<2.3,>=1.22.4->scipy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<2.3,>=1.22.4->scipy) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<2.3,>=1.22.4->scipy) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<2.3,>=1.22.4->scipy) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2.3,>=1.22.4->scipy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2.3,>=1.22.4->scipy) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<2.3,>=1.22.4->scipy) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<2.3,>=1.22.4->scipy) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<2.3,>=1.22.4->scipy) (2024.2.0)\nRequirement already satisfied: dask[complete] in /usr/local/lib/python3.10/dist-packages (2024.12.1)\nRequirement already satisfied: click>=8.1 in /usr/local/lib/python3.10/dist-packages (from dask[complete]) (8.1.7)\nRequirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from dask[complete]) (3.1.0)\nRequirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.10/dist-packages (from dask[complete]) (2024.12.0)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from dask[complete]) (24.2)\nRequirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from dask[complete]) (1.4.2)\nRequirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from dask[complete]) (6.0.2)\nRequirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from dask[complete]) (0.12.1)\nRequirement already satisfied: importlib_metadata>=4.13.0 in /usr/local/lib/python3.10/dist-packages (from dask[complete]) (8.5.0)\nRequirement already satisfied: pyarrow>=14.0.1 in /usr/local/lib/python3.10/dist-packages (from dask[complete]) (19.0.1)\nCollecting lz4>=4.3.2 (from dask[complete])\n  Downloading lz4-4.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\nRequirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib_metadata>=4.13.0->dask[complete]) (3.21.0)\nRequirement already satisfied: locket in /usr/local/lib/python3.10/dist-packages (from partd>=1.4.0->dask[complete]) (1.0.0)\nRequirement already satisfied: distributed==2024.12.1 in /usr/local/lib/python3.10/dist-packages (from dask[complete]) (2024.12.1)\nRequirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.10/dist-packages (from dask[complete]) (1.26.4)\nRequirement already satisfied: bokeh>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from dask[complete]) (3.6.2)\nRequirement already satisfied: jinja2>=2.10.3 in /usr/local/lib/python3.10/dist-packages (from dask[complete]) (3.1.4)\nRequirement already satisfied: pandas>=2.0 in /usr/local/lib/python3.10/dist-packages (from dask[complete]) (2.2.3)\nRequirement already satisfied: dask-expr<1.2,>=1.1 in /usr/local/lib/python3.10/dist-packages (from dask[complete]) (1.1.21)\nRequirement already satisfied: msgpack>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from distributed==2024.12.1->dask[complete]) (1.1.0)\nRequirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from distributed==2024.12.1->dask[complete]) (5.9.5)\nRequirement already satisfied: sortedcontainers>=2.0.5 in /usr/local/lib/python3.10/dist-packages (from distributed==2024.12.1->dask[complete]) (2.4.0)\nRequirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from distributed==2024.12.1->dask[complete]) (3.0.0)\nRequirement already satisfied: tornado>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from distributed==2024.12.1->dask[complete]) (6.3.3)\nRequirement already satisfied: urllib3>=1.26.5 in /usr/local/lib/python3.10/dist-packages (from distributed==2024.12.1->dask[complete]) (2.3.0)\nRequirement already satisfied: zict>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from distributed==2024.12.1->dask[complete]) (3.0.0)\nRequirement already satisfied: contourpy>=1.2 in /usr/local/lib/python3.10/dist-packages (from bokeh>=3.1.0->dask[complete]) (1.3.1)\nRequirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from bokeh>=3.1.0->dask[complete]) (11.0.0)\nRequirement already satisfied: xyzservices>=2021.09.1 in /usr/local/lib/python3.10/dist-packages (from bokeh>=3.1.0->dask[complete]) (2024.9.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.10.3->dask[complete]) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->dask[complete]) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->dask[complete]) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->dask[complete]) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->dask[complete]) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->dask[complete]) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->dask[complete]) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0->dask[complete]) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0->dask[complete]) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0->dask[complete]) (2025.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0->dask[complete]) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.24->dask[complete]) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.24->dask[complete]) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.24->dask[complete]) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.24->dask[complete]) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.24->dask[complete]) (2024.2.0)\nDownloading lz4-4.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: lz4\nSuccessfully installed lz4-4.4.3\nNote: you may need to restart the kernel to use updated packages.\nRequirement already satisfied: fuzzywuzzy in /usr/local/lib/python3.10/dist-packages (0.18.0)\nCollecting python-Levenshtein\n  Downloading python_levenshtein-0.27.1-py3-none-any.whl.metadata (3.7 kB)\nCollecting Levenshtein==0.27.1 (from python-Levenshtein)\n  Downloading levenshtein-0.27.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\nCollecting rapidfuzz<4.0.0,>=3.9.0 (from Levenshtein==0.27.1->python-Levenshtein)\n  Downloading rapidfuzz-3.12.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\nDownloading python_levenshtein-0.27.1-py3-none-any.whl (9.4 kB)\nDownloading levenshtein-0.27.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.5/161.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading rapidfuzz-3.12.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein, python-Levenshtein\nSuccessfully installed Levenshtein-0.27.1 python-Levenshtein-0.27.1 rapidfuzz-3.12.2\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Standard library imports\nimport os\nimport sys\nimport ast\nimport subprocess\nimport logging\nimport time\nfrom datetime import datetime\nimport shutil\nfrom io import StringIO\nfrom pathlib import Path\nimport re\nimport json\n# Data processing and analysis\nimport pandas as pd\nimport numpy as np\nimport openpyxl\nimport seaborn as sns\nimport networkx as nx\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport plotly.io as pio\nimport matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom matplotlib.colors import to_rgba, LinearSegmentedColormap\nimport matplotlib.patches as mpatches\n# Machine learning and statistical analysis\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\nfrom sklearn.decomposition import PCA, NMF\nfrom sklearn.cluster import AgglomerativeClustering, KMeans\nfrom sklearn.manifold import TSNE\nimport umap\nimport scipy\nfrom scipy import stats\nfrom scipy.cluster import hierarchy\nimport scipy.cluster.hierarchy as sch\nfrom statsmodels.stats.multitest import multipletests\nfrom scipy.spatial.distance import pdist\nfrom scipy.stats import spearmanr, kruskal, mannwhitneyu\nfrom kneed import KneeLocator\nfrom scipy.signal import savgol_filter\nfrom joblib import Parallel, delayed\n\n# Bioinformatics\nfrom Bio import SeqIO\nfrom Bio.Seq import Seq\nfrom Bio.SeqRecord import SeqRecord\nfrom biom import Table, load_table\nfrom biom.util import biom_open\n\n# Web and data retrieval\nimport requests\nimport xml.etree.ElementTree as ET\nfrom lxml import etree\n\n# Utility libraries\nimport gzip\nimport random\nfrom natsort import natsorted\nfrom typing import Dict, List, Tuple, Set, Optional\nimport pickle\nimport gc\nimport joblib\nimport h5py\nimport os\nos.environ['DISPLAY'] = ':0'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T13:34:19.316120Z","iopub.execute_input":"2025-03-28T13:34:19.316559Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set up logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s'\n)\n\n# Directory Structure Definitions\nSIMPLE_BASE = {\n    'known': 'simple_known_mic',\n    'other': 'simple_candidate_mic'\n}\n\nDETAILED_BASE = {\n    'known': 'detailed_known_mic',\n    'pure_checked': 'detailed_pure_checked_mic',\n    'pure_core': 'detailed_pure_core_mic',\n    'checked_core': 'detailed_checked_core_mic'\n}\n\nSUBDIRS = [\n    'EC_predictions',\n    'pathway_predictions',\n    'KO_predictions',\n    'other_picrust_files'\n]\n\n'''\n# Base Paths\nif \"google.colab\" in sys.modules:\n    base_dir = Path(\"/content/drive/MyDrive/MIC/data_picrust\")\nelse:\n    base_dir = Path(\"/home/beatriz/MIC/2_Micro/data_picrust\")\n\n#base dir for small files to git\nbase_dir = Path(\"/home/beatriz/MIC/2_Micro/data_picrust\")\n\nabundance_excel= Path(\"/home/beatriz/MIC/2_Micro/data_Ref/merged_to_sequence.xlsx\")\nfasta_file_final = Path(\"/home/beatriz/MIC/2_Micro/data_qiime/results_match_gg/final_sequences_gg.fasta\")\naligned_fasta = Path(\"/home/beatriz/MIC/2_Micro/data_qiime/results_match_gg/aligned-dna-sequences_gg.fasta\")\n\n# Create output directory if it doesn't exist\noutput_base = base_dir / \"output_base\"\noutput_base.mkdir(parents=True, exist_ok=True)\n# large galaxies input and output #large size dir for large files hosted instead in kaggle\nlarge_dir = Path(\"/home/beatriz/MIC\")\nlarge_dir.mkdir(parents=True, exist_ok=True)\n# databases\ndb_dir = large_dir / \"Databases\"\n# input galaxies and uniprots\ninput_galaxy = large_dir / \"data_galaxies\"\n# Directory to output large files # eccontris, compilated db\noutput_large = large_dir / \"output_large\"\noutput_large.mkdir(parents=True, exist_ok=True)\n\n# for colab\n# Create output directory if it doesn't exist\nbase_dir = Path(\"/content/drive/MyDrive/MIC/data_picrust/\")\nbase_dir.mkdir(parents=True, exist_ok=True)\nabundance_excel= Path(\"/content/drive/MyDrive/MIC/data_picrust/merged_to_sequence.xlsx\")\nfasta_file_final = Path(\"/content/drive/MyDrive/MIC/data_picrust/final_sequences_gg.fasta\")\naligned_fasta = Path(\"/content/drive/MyDrive/MIC/data_picrust/aligned-dna-sequences_gg.fasta\")\n\noutput_base = base_dir  # Separate output directory\noutput_base.mkdir(parents=True, exist_ok=True)\nlarge_dir = Path(\"/content/drive/MyDrive/MIC/\")\nlarge_dir.mkdir(parents=True, exist_ok=True)\ndb_dir = Path(\"/content/drive/MyDrive/MIC/Databases\")\ndb_dir.mkdir(parents=True, exist_ok=True)\ninput_galaxy = large_dir / \"data_galaxies\"\n# Directory to output large files # eccontris, compilated db\noutput_large = large_dir / \"output_large\"\noutput_large.mkdir(parents=True, exist_ok=True)\n'''\n# For Kaggle work\n# Input datasets (read-only in Kaggle)\nbase_dir = Path(\"/kaggle/input/new-picrust\") #base dir for small files to git /kaggle/input/new-picrust\n\n# Files in small input directory\nabundance_excel= base_dir / \"merged_to_sequence.xlsx\" # inside input small sizes input\nfasta_file_final = base_dir  / \"final_sequences_gg.fasta\" # inside input small sizes\n\n# Output for small files has to be changed for vscode no to push it to git\noutput_base = Path(\"/kaggle/working/output_base\")\noutput_base.mkdir(parents=True, exist_ok=True)\n#datasets large galaxies and databases\ndb_dir = Path(\"/kaggle/input/databases/Databases\")\ninput_galaxy = Path(\"/kaggle/input/data-galaxies\")\n\n# Directory to output large files # eccontris, compilated db\nlarge_dir =  Path(\"/kaggle/working/\")\n\n# Directory to output large files # eccontris, compilated db\noutput_large = large_dir / \"output_large\"\noutput_large.mkdir(parents=True, exist_ok=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Integrated taxa from origin genus as headers with levels 6 for the genera, 7 for the GID, muss be cleaned\nIntegrated_T = pd.read_excel(abundance_excel, sheet_name='core_check_usual_taxa', header=[0,1,2,3,4,5,6,7], engine ='openpyxl')\n# Drop first row (index 0) and first column in one chain\nIntegrated_T = Integrated_T.drop(index=0).drop(Integrated_T.columns[0], axis=1)\nIntegrated_T= Integrated_T.astype({'Sites': str})\nIntegrated_T['Sites'] = Integrated_T['Sites'].fillna('Source')\n# Remove 'Unnamed' level names\nIntegrated_T.columns = Integrated_T.columns.map(lambda x: tuple('' if 'Unnamed' in str(level) else level for level in x))\n# Changing dtypes to category whiles respecting structure\nIntegrated_T[\"Category\"] = Integrated_T[\"Category\"].astype(\"Int64\")\nIntegrated_T= Integrated_T.set_index(\"Sites\")\npre_Integrated = Integrated_T.T","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load colab already has corrected the name and put identifiers at this point and category included\nECcontri_Uniprot_enriched = pd.read_parquet(output_large / 'ECcontri_Uniprot_enriched.parquet')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define category dict outside\ncategory_dict = Integrated_T.T.iloc[0, 0:-1].to_dict()\n\n# Define colors and categories\ncategory_colors = {1: '#008800',  # Dark green\n                   2: '#FF8C00',  # Dark orange\n                   3: '#FF0000'}   # Red\n\ncategories_labels = {1: 'Normal Operation',\n              2: 'Early Warning',\n              3: 'System Failure'}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 9.5. Filtering pairs Bacteria-Protein by significance to the risk category","metadata":{}},{"cell_type":"code","source":"sample_contri = ECcontri_Uniprot_enriched.sample(n=5000, random_state=42) # row == zeile","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def prepare_base_matrix_stats(eccontri_df):\n    # Ensure 'idx' is the index if it's not already\n    if 'idx' not in eccontri_df.index.names:\n        eccontri_df = eccontri_df.set_index('idx')\n\n    # Group by 'Category', 'Genus', and 'protein_name', and sum 'norm_abund_contri'\n    base_matrix = eccontri_df.groupby(['Genus', 'protein_name', 'Category'], observed=True, as_index=True)['norm_abund_contri'].sum()\n\n    base_matrix = base_matrix.reset_index(drop=False)\n    return base_matrix","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"base_matrix = prepare_base_matrix_stats(sample_contri)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def perform_abundance_analysis(base_matrix, eccontri_df, alpha=0.05, n_jobs=-1):\n    \"\"\"\n    Performs statistical analysis on protein-genus pairs focusing on abundance patterns.\n\n    This function:\n    1. Identifies patterns based on category presence/absence AND abundance\n    2. Performs statistical tests where possible (pairs in multiple categories)\n    3. Identifies category-specific and abundant proteins\n\n    Parameters:  base_matrix :  DataFrame with columns 'Genus', 'protein_name', 'Category', 'norm_abund_contri'\n    alpha : float, default=0.05   Significance level for statistical tests\n    n_jobs : int, default=-1   Number of parallel jobs to run\n\n    Returns:\n    --------\n    tuple: (results_df, category_stats)\n        results_df: DataFrame with statistical results and patterns\n        category_stats: Dictionary with summary statistics\n    \"\"\"\n    # Start timer\n    start_time = time.time()\n\n    # Get unique categories and genus-protein pairs\n    categories = sorted(base_matrix[\"Category\"].unique())\n    print(f\"Found {len(categories)} categories: {categories}\")\n\n    pairs = base_matrix[['Genus', 'protein_name']].drop_duplicates()\n    print(f\"Total unique genus-protein pairs unique: {len(pairs)}\")\n\n    # Pre-compute statistics for all genus-protein-category combinations\n    print(\"Computing statistics for all genus-protein-category combinations...\")\n    grouped_stats = base_matrix.groupby(['Genus', 'protein_name', 'Category'])['norm_abund_contri'].agg([\n        ('mean', 'mean'),\n        ('median', 'median'),\n        ('std', 'std'),\n        ('count', 'count'),\n        ('sum', 'sum'),\n        ('values', lambda x: list(x))\n    ]).reset_index()\n\n    # Convert to dictionary for faster lookups and Create a list of tuples for keys and a list of dictionaries for values\n    keys = list(zip(grouped_stats['Genus'], grouped_stats['protein_name'], grouped_stats['Category'])) #generator\n    values = grouped_stats[['mean', 'median', 'std', 'count', 'sum', 'values']].to_dict('records')\n    grouped_dict = dict(zip(keys, values))\n\n    # Create a pivot table to easily see category presence\n    category_pivot = grouped_stats.pivot_table(\n        index=['Genus', 'protein_name'],\n        columns='Category',\n        values='count',\n        fill_value=0\n    ).reset_index()\n\n    # Add columns for category presence\n    for cat in categories:\n        category_pivot[f'present_in_cat{cat}'] = category_pivot[cat] > 0\n\n    def cliffs_delta(x, y):\n        \"\"\"\n        Calculate Cliff's Delta - a non-parametric effect size measure.\n        Returns value between -1 and 1, where:\n        -1: all values in y > values in x\n        +1: all values in x > values in y\n        0: distributions overlap completely\n        \"\"\"\n        # Count comparisons where x > y and y > x\n        greater = 0\n        less = 0\n        \n        for i in x:\n            for j in y:\n                if i > j:\n                    greater += 1\n                elif i < j:\n                    less += 1\n        \n        # Calculate delta\n        delta = (greater - less) / (len(x) * len(y))\n        return delta\n\n    # Define function to process each pair\n    def process_pair(row_data):\n        # Get category presence for this pair\n        genus = row_data['Genus']\n        protein = row_data['protein_name']\n\n        pivot_row = category_pivot[\n            (category_pivot['Genus'] == genus) &\n            (category_pivot['protein_name'] == protein)\n        ]\n\n        if pivot_row.empty:\n            return None\n\n        pivot_row = pivot_row.iloc[0]\n\n        # Check which categories have data\n        present_cats = []\n        for cat in categories:\n            if pivot_row[f'present_in_cat{cat}']:\n                present_cats.append(cat)\n\n        # Skip if no categories have data (shouldn't happen)\n        if not present_cats:\n            return None\n\n        # Compile stats for each category\n        cat_stats = {}\n        for cat in categories:\n            key = (genus, protein, cat)\n            if key in grouped_dict:\n                cat_stats[f\"mean_cat{cat}\"] = grouped_dict[key]['mean']\n                cat_stats[f\"median_cat{cat}\"] = grouped_dict[key]['median']\n                cat_stats[f\"std_cat{cat}\"] = grouped_dict[key]['std']\n                cat_stats[f\"count_cat{cat}\"] = grouped_dict[key]['count']\n                cat_stats[f\"sum_cat{cat}\"] = grouped_dict[key]['sum']\n            else:\n                cat_stats[f\"mean_cat{cat}\"] = np.nan\n                cat_stats[f\"median_cat{cat}\"] = np.nan\n                cat_stats[f\"std_cat{cat}\"] = np.nan\n                cat_stats[f\"count_cat{cat}\"] = np.nan\n                cat_stats[f\"sum_cat{cat}\"] = np.nan\n\n        # First determine presence pattern\n        pattern = \"unknown\"\n        category_str = ''.join(str(c) for c in present_cats)\n                               \n        if category_str == '123':\n            presence_pattern = 'all_categories'\n        elif category_str == '12':\n            presence_pattern = 'cat1_and_cat2'\n        elif category_str == '13':\n            presence_pattern = 'cat1_and_cat3'\n        elif category_str == '23':\n            presence_pattern = 'cat2_and_cat3'\n        elif category_str == '1':\n            presence_pattern = 'only_cat1'\n        elif category_str == '2':\n            presence_pattern = 'only_cat2'\n        elif category_str == '3':\n            presence_pattern = 'only_cat3'\n        else:\n            presence_pattern = 'unknown'\n\n        # Determine pattern based on statistical criteria:\n        if presence_pattern == 'all_categories':\n            mean1, mean2, mean3 = cat_stats[\"mean_cat1\"], cat_stats[\"mean_cat2\"], cat_stats[\"mean_cat3\"]\n            \n            # Calculate percent changes between categories\n            pct_change_1to2 = (mean2 - mean1) / mean1 if mean1 > 0 else np.inf\n            pct_change_2to3 = (mean3 - mean2) / mean2 if mean2 > 0 else np.inf\n            \n            # Set minimum threshold for meaningful change (e.g., 20%)\n            significant_change_threshold = 0.2\n            \n            # Classify patterns with statistical criteria\n            if pct_change_1to2 > significant_change_threshold and pct_change_2to3 > significant_change_threshold:\n                pattern = \"increasing_abundance\"\n            elif pct_change_1to2 < -significant_change_threshold and pct_change_2to3 < -significant_change_threshold:\n                pattern = \"decreasing_abundance\"\n            elif pct_change_1to2 > significant_change_threshold and pct_change_2to3 < -significant_change_threshold:\n                pattern = \"peak_at_cat2\"\n            elif pct_change_1to2 < -significant_change_threshold and pct_change_2to3 > significant_change_threshold:\n                pattern = \"valley_at_cat2\"\n            else:\n                pattern = \"mixed_abundance\"\n        else:\n            pattern = presence_pattern\n            \n       # Calculate effect sizes between consecutive categories\n        effect_sizes = {}\n        for j in range(len(categories)-1):\n            cat1, cat2 = categories[j], categories[j+1]\n            \n            if cat_stats[f\"count_cat{cat1}\"] > 0 and cat_stats[f\"count_cat{cat2}\"] > 0:\n                key1 = (genus, protein, cat1)\n                key2 = (genus, protein, cat2)\n                \n                if key1 in grouped_dict and key2 in grouped_dict:\n                    values1 = grouped_dict[key1]['values']\n                    values2 = grouped_dict[key2]['values']\n                    \n                    if len(values1) > 0 and len(values2) > 0:\n                        # Calculate Cliff's delta\n                        effect_sizes[f\"effect_size_{cat1}_to_{cat2}\"] = cliffs_delta(values1, values2)\n                    else:\n                        effect_sizes[f\"effect_size_{cat1}_to_{cat2}\"] = np.nan\n                else:\n                    effect_sizes[f\"effect_size_{cat1}_to_{cat2}\"] = np.nan\n            else:\n                effect_sizes[f\"effect_size_{cat1}_to_{cat2}\"] = np.nan\n           \n        # Statistical tests for pairs in multiple categories\n        if len(present_cats) >= 2:\n            # Get values for each category\n            cat_values = []\n            sample_sizes = []\n            for cat in present_cats:\n                key = (genus, protein, cat)\n                if key in grouped_dict and len(grouped_dict[key]['values']) > 0:\n                    cat_values.append(grouped_dict[key]['values'])\n                    sample_sizes.append(len(grouped_dict[key]['values']))\n            \n            # Check if we have adequate samples for testing\n            if len(cat_values) >= 2 and min(sample_sizes) >= 3:  # Minimum 3 samples per group\n                try:\n                    if len(cat_values) == 2:\n                        # Mann-Whitney U test for two categories\n                        # Check for ties and use exact method when appropriate\n                        if max(sample_sizes) > 20:  # Large sample approximation\n                            stat, p_val = mannwhitneyu(cat_values[0], cat_values[1], alternative='two-sided')\n                        else:\n                            # Use exact method for small samples\n                            stat, p_val = mannwhitneyu(cat_values[0], cat_values[1], alternative='two-sided', method='exact')\n                    else:\n                        # Kruskal-Wallis test for more than two categories\n                        h_stat, p_val = kruskal(*cat_values, nan_policy='omit')\n                        \n                    # Record the sample sizes for context\n                    effect_sizes['n_samples'] = sum(sample_sizes)\n                except Exception as e:\n                    print(f\"Statistical test failed: {e}\")\n                    h_stat, p_val = np.nan, np.nan\n\n            # Calculate correlation using all individual measurements\n            corr_value = 0.0\n            if len(present_cats) >= 2:\n                try:\n                    # Collect all individual measurements\n                    all_values = []\n                    all_categories = []\n                    \n                    for cat in present_cats:\n                        key = (genus, protein, cat)\n                        if key in grouped_dict and len(grouped_dict[key]['values']) > 0:\n                            all_values.extend(grouped_dict[key]['values'])\n                            all_categories.extend([cat] * len(grouped_dict[key]['values']))\n                    \n                    # Convert to numpy arrays\n                    all_values = np.array(all_values)\n                    all_categories = np.array(all_categories)\n                    \n                    # Use Spearman correlation (robust, non-parametric)\n                    from scipy.stats import spearmanr\n                    if len(all_values) >= 5:  # Minimum points for reliable correlation\n                        corr, p = spearmanr(all_categories, all_values)\n                        corr_value = corr\n                    else:\n                        # For few points, calculate trend direction\n                        trend = np.polyfit(all_categories, all_values, 1)[0]\n                        corr_value = np.sign(trend) * 0.5  # Scale to (-0.5, 0.5) for limited data\n                except Exception as e:\n                    print(f\"Correlation calculation error: {e}\")\n                    corr_value = 0.0\n       \n        # Create result dictionary\n        result = {\n            'Genus': genus,\n            'protein_name': protein,\n            'h_statistic': h_stat,\n            'p_value': p_val,\n            'pattern': pattern,\n            'presence_pattern': presence_pattern,\n            'category_count': len(present_cats),\n            'categories': category_str,\n            'corr': corr_value, \n            **cat_stats,\n            **effect_sizes\n        }\n\n        return result\n\n    # Setup progress reporting\n    total_pairs = len(pairs)\n    start_time = time.time()\n    last_print_time = start_time\n\n    # Process in batches with reporting every 10 minutes\n    batch_size = 10000\n    results = []\n\n    for i in range(0, len(pairs), batch_size):\n        batch_end = min(i + batch_size, len(pairs))\n        batch_pairs = pairs.iloc[i:batch_end]\n\n        batch_results = Parallel(n_jobs=n_jobs)(\n            delayed(process_pair)(row)\n            for _, row in batch_pairs.iterrows())\n\n        results.extend([r for r in batch_results if r is not None])\n\n        # Print progress every 10 minutes\n        current_time = time.time()\n        if current_time - last_print_time >= 600:  # 600 seconds = 10 minutes\n            elapsed = current_time - start_time\n            progress = batch_end / total_pairs * 100\n            print(f\"Progress: {batch_end}/{total_pairs} pairs ({progress:.1f}%) - Elapsed time: {elapsed:.1f} seconds\")\n            last_print_time = current_time\n\n    # Remove None results\n    results = [r for r in results if r is not None]\n    print(f\"Generated {len(results)} valid results\")\n\n    # Create results DataFrame\n    if not results:\n        print(\"Warning: No valid statistical test results were produced.\")\n        return pd.DataFrame(), {}\n\n    results_df = pd.DataFrame(results)\n\n    # Perform multiple testing correction with FDR\n    if 'p_value' in results_df.columns:\n        valid_pvals = ~results_df['p_value'].isna()\n        if sum(valid_pvals) > 0:\n            # Create columns for adjusted p-values and significance\n            results_df['p_adjusted'] = np.nan\n            results_df['significant'] = False\n            \n            # Apply Benjamini-Hochberg correction\n            from statsmodels.stats.multitest import multipletests\n            reject, pvals_corrected, _, _ = multipletests(\n                results_df.loc[valid_pvals, 'p_value'].values,\n                alpha=alpha,\n                method='fdr_bh'  # Benjamini-Hochberg FDR correction\n            )\n            \n            # Store results\n            results_df.loc[valid_pvals, 'p_adjusted'] = pvals_corrected\n            results_df.loc[valid_pvals, 'significant'] = reject\n            \n            print(f\"After FDR correction: {sum(reject)}/{sum(valid_pvals)} tests significant at alpha={alpha}\")\n\n    # Add total abundance column (sum across all categories)\n    results_df['total_abundance'] = 0\n    for cat in categories:\n        results_df['total_abundance'] += results_df[f'sum_cat{cat}'].fillna(0)\n\n    # Define safe_log2fc function\n    def safe_log2fc(value1, value2, min_value=1e-10):\n        \"\"\"\n        Calculate log2 fold change with safety checks for zero/NaN values.\n        \"\"\"\n        # Handle NaN values - keep as NaN rather than converting to 0\n        if pd.isna(value1) or pd.isna(value2):\n            return np.nan\n        \n        # Handle zeros\n        v1 = max(float(value1), min_value)\n        v2 = max(float(value2), min_value)\n        \n        # Calculate fold change\n        try:\n            return np.log2(v1 / v2)\n        except:\n            return np.nan\n\n    # Create masks for valid comparisons\n    mask_cat1_cat2 = (results_df['count_cat1'] > 0) & (results_df['count_cat2'] > 0)\n    mask_cat1_cat3 = (results_df['count_cat1'] > 0) & (results_df['count_cat3'] > 0)\n    mask_cat2_cat3 = (results_df['count_cat2'] > 0) & (results_df['count_cat3'] > 0)\n\n    # Initialize with NaN\n    results_df['log2fc_cat2_vs_cat1'] = np.nan\n    results_df['log2fc_cat3_vs_cat1'] = np.nan\n    results_df['log2fc_cat3_vs_cat2'] = np.nan\n\n    # Only calculate for valid pairs using vectorized operations\n    if sum(mask_cat1_cat2) > 0:\n        cat1_values = results_df.loc[mask_cat1_cat2, 'mean_cat1'].values\n        cat2_values = results_df.loc[mask_cat1_cat2, 'mean_cat2'].values\n        cat1_values = np.maximum(cat1_values, 1e-10)\n        cat2_values = np.maximum(cat2_values, 1e-10)\n        results_df.loc[mask_cat1_cat2, 'log2fc_cat2_vs_cat1'] = np.log2(cat2_values / cat1_values)\n\n    if sum(mask_cat1_cat3) > 0:\n        cat1_values = results_df.loc[mask_cat1_cat3, 'mean_cat1'].values\n        cat3_values = results_df.loc[mask_cat1_cat3, 'mean_cat3'].values\n        cat1_values = np.maximum(cat1_values, 1e-10)\n        cat3_values = np.maximum(cat3_values, 1e-10)\n        results_df.loc[mask_cat1_cat3, 'log2fc_cat3_vs_cat1'] = np.log2(cat3_values / cat1_values)\n\n    if sum(mask_cat2_cat3) > 0:\n        cat2_values = results_df.loc[mask_cat2_cat3, 'mean_cat2'].values\n        cat3_values = results_df.loc[mask_cat2_cat3, 'mean_cat3'].values\n        cat2_values = np.maximum(cat2_values, 1e-10)\n        cat3_values = np.maximum(cat3_values, 1e-10)\n        results_df.loc[mask_cat2_cat3, 'log2fc_cat3_vs_cat2'] = np.log2(cat3_values / cat2_values)\n\n    # Print a progress update\n    print(f\"Added log2 fold change calculations for {sum(mask_cat1_cat2)} cat1-cat2 pairs, {sum(mask_cat1_cat3)} cat1-cat3 pairs, {sum(mask_cat2_cat3)} cat2-cat3 pairs\")\n    # Create a dictionary with category statistics\n    category_stats = {\n        'total_pairs': len(results_df),\n        'tested_pairs': sum(~results_df['p_value'].isna()),\n        'significant_pairs': sum(results_df['significant']) if 'significant' in results_df.columns else 0\n    }\n\n    # Add pattern counts to stats\n    if 'pattern' in results_df.columns:\n        for pattern in results_df['pattern'].unique():\n            category_stats[f'pattern_{pattern}'] = sum(results_df['pattern'] == pattern)\n   \n    results_df = results_df.reset_index(drop=False)\n\n    return results_df, category_stats","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def process_in_resumable_chunks(base_matrix, output_dir, eccontri_df, chunk_size=50000):\n    \"\"\"\n    Process data in chunks and save intermediate results to allow resuming.\n    \n    Parameters:\n    base_matrix : DataFrame to process\n    output_dir : Directory to save chunk results\n    eccontri_df : Original dataframe with idx information\n    chunk_size : Number of rows per chunk\n    \n    Returns:\n    DataFrame with combined results\n    \"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Get total chunks needed\n    total_rows = len(base_matrix)\n    total_chunks = (total_rows + chunk_size - 1) // chunk_size\n    \n    # Check for existing chunks\n    existing_chunks = [f for f in os.listdir(output_dir) if f.startswith('pattern_chunk_') and f.endswith('.csv')]\n    existing_nums = sorted([int(f.split('_')[-1].split('.')[0]) for f in existing_chunks])\n    \n    print(f\"Found {len(existing_nums)} existing chunks out of {total_chunks} total needed\")\n    \n    # Process remaining chunks\n    for chunk_num in range(1, total_chunks + 1):\n        # Skip if already processed\n        if chunk_num in existing_nums:\n            print(f\"Skipping chunk {chunk_num}/{total_chunks} - already processed\")\n            continue\n        \n        # Process this chunk\n        start_idx = (chunk_num - 1) * chunk_size\n        end_idx = min(start_idx + chunk_size, total_rows)\n        \n        print(f\"Processing chunk {chunk_num}/{total_chunks}: rows {start_idx} to {end_idx}\")\n        chunk_data = base_matrix.iloc[start_idx:end_idx]\n        \n        # Run analysis on this chunk\n        chunk_results, _ = perform_abundance_analysis(chunk_data,eccontri_df, alpha=0.05, n_jobs=-1)\n        \n        # Save results\n        output_file = os.path.join(output_dir, f\"pattern_chunk_{chunk_num}.parquet\")\n        chunk_results.to_parquet(output_file, index=False)\n        print(f\"Saved results for chunk {chunk_num} to {output_file}\")\n    \n    # Combine all chunks once complete\n    print(\"Combining all chunks...\")\n    all_chunks = []\n    for chunk_num in range(1, total_chunks + 1):\n        file_path = os.path.join(output_dir, f\"pattern_chunk_{chunk_num}.parquet\")\n        if os.path.exists(file_path):\n            chunk_df = pd.read_parquet(file_path)\n            all_chunks.append(chunk_df)\n    \n    if all_chunks:\n        combined_results = pd.concat(all_chunks, ignore_index=True)\n        print(f\"Combined {len(all_chunks)} chunks into final result with {len(combined_results)} rows\")\n        return combined_results\n    else:\n        print(\"No chunks were processed\")\n        return pd.DataFrame()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"    # Reshape data for analysis and calculate abundance matrix\n    print(\"calculating abundancy pattern...\")\n    output_dir = \"pattern_analysis_results\"\n    pattern_df = process_in_resumable_chunks(base_matrix,  output_dir, sample_contri, chunk_size=50000)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.hist(pattern_df['pattern'].dropna(), bins=20)\nplt.title('Distribution of pattern')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Reset indexes to ensure proper merging\npattern_df = pattern_df.reset_index(drop=True)\nsample_contri = sample_contri.reset_index(drop=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Perform a single merge to get idx and all other needed columns\nintegrated_results = pd.merge(\n    pattern_df,\n    sample_contri[['idx', 'Genus', 'protein_name', 'enzyme_names', 'enzyme_class', 'pathways', 'hierarchy',\n                 'metals_involved', 'metals_consolidated', 'corrosion_mechanisms', \n                 'corrosion_relevance_score', 'corrosion_relevance', 'has_metal']],\n    on=['Genus', 'protein_name'],\n    how='left'\n)\n\n# Find protein-genus pairs with multiple idx values\nmulti_idx = sample_contri.groupby(['Genus', 'protein_name'])['idx'].nunique()\nproblematic_pairs = multi_idx[multi_idx > 1].index.tolist()\nprint(f\"Found {len(problematic_pairs)} protein-genus pairs with multiple idx values\")\n\n# If there are duplicate idx columns, fix them\nif 'idx_x' in integrated_results.columns and 'idx_y' in integrated_results.columns:\n    integrated_results['idx'] = integrated_results['idx_y']\n    integrated_results = integrated_results.drop(['idx_x', 'idx_y'], axis=1)\nintegrated_results.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check how many rows matched during merge\nbefore_count = len(pattern_df)\nafter_count = len(integrated_results)\nprint(f\"Rows before merge: {before_count}, after merge: {after_count}\")\n\n# Verify idx values are matching\nprint(f\"Unique idx values in pattern_df: {pattern_df['idx'].nunique()}\")\nprint(f\"Unique idx values in ECcontri_Uniprot_enriched: {sample_contri['idx'].nunique()}\")\nprint(f\"Rows with matching idx: {sum(pattern_df['idx'].isin(sample_contri['idx']))}\")\n# Check if these columns exist in pattern_df before the merge\nprint(\"Columns in pattern_df:\", pattern_df.columns.tolist())\nprint(\"NaN counts in pattern_df:\")\nprint(pattern_df.isna().sum())\n\n# Check if these columns exist in eccontri_df before the merge\nprint(\"Columns in ECcontri_Uniprot_enriched:\", sample_contri.columns.tolist())\nprint(\"NaN counts in ECcontri_Uniprot_enriched:\")\nprint(sample_contri.isna().sum())","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}