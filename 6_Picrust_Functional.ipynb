{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDED5j4zCqR_"
      },
      "source": [
        "# Sequence Analysis and Functional Prediction Pipeline\n",
        "\n",
        "## 1. Introduction\n",
        "This notebook analyzes the functional and sequence relationships between newly identified bacteria and known corrosion-influencing microorganisms. The analysis builds upon previous findings where:\n",
        "- Statistical significance was established between the selected bacteria and corrosion risk (Notebook 3)\n",
        "- Literature validation confirmed corrosion influence for many bacteria (Notebook 4)\n",
        "- Evolutionary relationships were mapped through phylogenetic analysis (Notebook 5)\n",
        "\n",
        "The study focuses on bacteria from operational heating and cooling water systems, primarily in Germany. Using 16S rRNA data (bootstrap-validated from Notebook 5), this analysis employs PICRUSt2 to predict metabolic functions and compare functional profiles between different bacterial groups.\n",
        "\n",
        "### Analysis Approaches\n",
        "We implement two classification strategies:\n",
        "\n",
        "1. Simple Classification:\n",
        "   - Known corrosion-causing bacteria (usual_taxa)\n",
        "   - Other bacteria (combining checked_taxa and core_taxa)\n",
        "\n",
        "2. Detailed Classification:\n",
        "   - Known corrosion-causing bacteria (usual_taxa)\n",
        "   - Pure checked bacteria (exclusive to checked_taxa)\n",
        "   - Pure core bacteria (exclusive to core_taxa)\n",
        "   - Checked-core bacteria (overlap between checked and core taxa)\n",
        "\n",
        "This detailed approach allows for more nuanced analysis of functional profiles and better understanding of potential corrosion mechanisms across different bacterial groups.\n",
        "\n",
        "### Analysis Goals:\n",
        "- Predict metabolic functions from 16S sequences\n",
        "- Focus on corrosion-relevant pathways (sulfur/iron metabolism)\n",
        "- Compare functional profiles between known corrosion-causing bacteria and newly identified candidates\n",
        "- Validate whether statistical correlations reflect genuine metabolic capabilities associated with corrosion processes\n",
        "\n",
        "### Directory Structure:\n",
        " Following is the structure of the notebook data named data_picrus  \n",
        "data_tree  \n",
        " ├── sequences/  \n",
        " │   ├── known.fasta : sequences of known corrosion-causing bacteria  \n",
        " │   ├── candidate.fasta : sequences of potential new corrosion-causing bacteria  \n",
        " |   └── other files  \n",
        " data_picrus  \n",
        " └── picrust_results/  \n",
        "      ├── known_bacteria/  \n",
        "      |               ├── EC_predictions/       : enzyme predictions  \n",
        "      |               ├── pathway_predictions/  : metabolic pathway abundance  \n",
        "      |               ├── KO_predictions/       : KEGG ortholog predictions  \n",
        "      |               └── other_picrust_files/  \n",
        "      ├── candidate_bacteria/  \n",
        "      |               ├── EC_predictions/       : enzyme predictions  \n",
        "      |               ├── pathway_predictions/  : metabolic pathway abundance  \n",
        "      |               ├── KO_predictions/       : KEGG ortholog predictions  \n",
        "      |               └── other_picrust_files/  : final comparison summary\n",
        "      ├── core_bacteria/\n",
        "      |               ├── EC_predictions/       : enzyme predictions  \n",
        "      |               ├── pathway_predictions/  : metabolic pathway abundance  \n",
        "      |               ├── KO_predictions/       : KEGG ortholog predictions  \n",
        "      |               └── other_picrust_files/  \n",
        "      │      \n",
        "      └── functional_comparison.xlsx  \n",
        "\n",
        "Picrust2 works using its reference database that was installed with the package   \n",
        "~/miniconda3/envs/picrust2/lib/python3.9/site-packages/picrust2/default_files/prokaryotic/pro_ref\n",
        "\n",
        "About picrust2  \n",
        "https://evomics.org/wp-content/uploads/2015/01/presentation_evomics-05-picrust_01-18-15.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeIn7RIBCqSD"
      },
      "source": [
        "# 2. Loading and Preparing the Data\n",
        "\n",
        "## 2.1 Colab Initialisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-02-19T09:40:11.357108Z",
          "iopub.status.busy": "2025-02-19T09:40:11.356719Z",
          "iopub.status.idle": "2025-02-19T09:40:11.366864Z",
          "shell.execute_reply": "2025-02-19T09:40:11.365324Z",
          "shell.execute_reply.started": "2025-02-19T09:40:11.357071Z"
        },
        "id": "5U6gm_R_JQjt",
        "outputId": "b7f6613b-d152-40ce-d816-3e2bd17bef4a",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "'''# Colab specific\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#change the path\n",
        "os.chdir('/content/drive/MyDrive/MIC/data_picrust')'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWqOI6IDD1qW"
      },
      "source": [
        "__Importing PICRUST IN COLAB__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "execution": {
          "iopub.execute_input": "2025-02-19T09:40:11.368681Z",
          "iopub.status.busy": "2025-02-19T09:40:11.368247Z",
          "iopub.status.idle": "2025-02-19T09:40:11.395272Z",
          "shell.execute_reply": "2025-02-19T09:40:11.394026Z",
          "shell.execute_reply.started": "2025-02-19T09:40:11.368644Z"
        },
        "id": "uKimriI3hmTq",
        "outputId": "77fab0fd-be3e-41b6-b4a0-88e9a7fd57ce",
        "trusted": true,
        "vscode": {
          "languageId": "javascript"
        }
      },
      "outputs": [],
      "source": [
        "'''# Install miniconda and initialize\n",
        "!wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
        "!bash Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local/miniconda3\n",
        "!conda config --add channels defaults\n",
        "!conda config --add channels bioconda\n",
        "!conda config --add channels conda-forge\n",
        "# Imports for colab\n",
        "import condacolab\n",
        "import sys\n",
        "sys.path.append('/usr/local/miniconda3/lib/python3.7/site-packages/')\n",
        "\n",
        "# Install PICRUSt2 and its dependencies\n",
        "%conda install -c bioconda -c conda-forge picrust2=2.4.1 -y\n",
        "# Verify installations%\n",
        "%conda list | grep picrust2'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzaNXN9suvgG"
      },
      "source": [
        "### Using Pro colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "execution": {
          "iopub.execute_input": "2025-02-19T09:40:11.397167Z",
          "iopub.status.busy": "2025-02-19T09:40:11.396676Z",
          "iopub.status.idle": "2025-02-19T09:40:11.419557Z",
          "shell.execute_reply": "2025-02-19T09:40:11.418358Z",
          "shell.execute_reply.started": "2025-02-19T09:40:11.397109Z"
        },
        "id": "qF94FGxn2iUL",
        "outputId": "313cb93a-d291-4e80-d16f-c0b90c878989",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "'''import sys\n",
        "print([module for module in sys.modules if 'tensorflow' in module])'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-02-19T09:43:44.41279Z",
          "iopub.status.busy": "2025-02-19T09:43:44.412414Z",
          "iopub.status.idle": "2025-02-19T09:44:02.324275Z",
          "shell.execute_reply": "2025-02-19T09:44:02.322737Z",
          "shell.execute_reply.started": "2025-02-19T09:43:44.41276Z"
        },
        "id": "3gWJfdx3Ni1f",
        "outputId": "bf357fcd-64d9-4fab-9252-3ac2c0aa6ca0",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "'''# Set up memory footprint support libraries\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "!pip install memory_profiler\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTKk2e3eYelt"
      },
      "source": [
        "### Kaggle / Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-02-19T09:44:02.326561Z",
          "iopub.status.busy": "2025-02-19T09:44:02.326066Z",
          "iopub.status.idle": "2025-02-19T09:44:11.408729Z",
          "shell.execute_reply": "2025-02-19T09:44:11.407173Z",
          "shell.execute_reply.started": "2025-02-19T09:44:02.326507Z"
        },
        "id": "KC8v0oJRuvgH",
        "outputId": "cc242778-6451-48ae-da6c-77422493d597",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "'''!pip install biopython\n",
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "!pip install biom-format\n",
        "%pip install umap-learn'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiHOVZiIuvgH"
      },
      "source": [
        "# 2.2. Importing Libraries,  Making Directories and Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T09:41:13.071035Z",
          "iopub.status.busy": "2025-02-19T09:41:13.070714Z",
          "iopub.status.idle": "2025-02-19T09:41:14.601408Z",
          "shell.execute_reply": "2025-02-19T09:41:14.6003Z",
          "shell.execute_reply.started": "2025-02-19T09:41:13.071005Z"
        },
        "id": "l92DnCZ3CqSD",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Standard library imports\n",
        "import os\n",
        "import sys\n",
        "import ast\n",
        "import subprocess\n",
        "import logging\n",
        "from datetime import datetime\n",
        "import shutil\n",
        "from io import StringIO\n",
        "from pathlib import Path\n",
        "# Data processing imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import openpyxl\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "#%matplotlib inline\n",
        "#plt.style.use('seaborn')\n",
        "import seaborn as sns\n",
        "from natsort import natsorted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T10:36:09.575981Z",
          "iopub.status.busy": "2025-02-19T10:36:09.575602Z",
          "iopub.status.idle": "2025-02-19T10:36:09.582325Z",
          "shell.execute_reply": "2025-02-19T10:36:09.58091Z",
          "shell.execute_reply.started": "2025-02-19T10:36:09.575949Z"
        },
        "id": "cLkUWNNbwYbV",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import umap\n",
        "# datascience libraries\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.cluster import hierarchy\n",
        "from sklearn.decomposition import PCA, NMF\n",
        "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
        "from scipy.stats import spearmanr\n",
        "from scipy.spatial.distance import pdist\n",
        "import networkx as nx\n",
        "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
        "\n",
        "# from Bio\n",
        "from Bio import SeqIO\n",
        "from Bio.Seq import Seq\n",
        "from Bio.SeqRecord import SeqRecord\n",
        "\n",
        "# BIOM handling\n",
        "from biom import Table\n",
        "from biom.util import biom_open\n",
        "from biom import load_table\n",
        "\n",
        "#imports retrieval\n",
        "import requests\n",
        "import time\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "execution": {
          "iopub.execute_input": "2025-02-19T09:42:02.982923Z",
          "iopub.status.busy": "2025-02-19T09:42:02.982171Z",
          "iopub.status.idle": "2025-02-19T09:42:02.989557Z",
          "shell.execute_reply": "2025-02-19T09:42:02.988248Z",
          "shell.execute_reply.started": "2025-02-19T09:42:02.98286Z"
        },
        "id": "Jhg73Rb9CqSF",
        "outputId": "f76ca834-11cd-4b55-951e-0f05863e4afc",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Set up logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "\n",
        "# Directory Structure Definitions\n",
        "SIMPLE_BASE = {\n",
        "    'known': 'simple_known_mic',\n",
        "    'other': 'simple_candidate_mic'\n",
        "}\n",
        "\n",
        "DETAILED_BASE = {\n",
        "    'known': 'detailed_known_mic',\n",
        "    'pure_checked': 'detailed_pure_checked_mic',\n",
        "    'pure_core': 'detailed_pure_core_mic',\n",
        "    'checked_core': 'detailed_checked_core_mic'\n",
        "}\n",
        "\n",
        "SUBDIRS = [\n",
        "    'EC_predictions',\n",
        "    'pathway_predictions',\n",
        "    'KO_predictions',\n",
        "    'other_picrust_files'\n",
        "]\n",
        "\n",
        "# Base Paths\n",
        "if \"google.colab\" in sys.modules:\n",
        "    base_dir = Path(\"/content/drive/MyDrive/MIC/data_picrust\")\n",
        "else:\n",
        "    base_dir = Path(\"/home/beatriz/MIC/2_Micro/data_picrust\")\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "base_dir = Path(\"/home/beatriz/MIC/2_Micro/data_picrust\")\n",
        "base_dir.mkdir(parents=True, exist_ok=True)\n",
        "abundance_excel= Path(\"/home/beatriz/MIC/2_Micro/data_Ref/merged_to_sequence.xlsx\")\n",
        "fasta_file_final = Path(\"/home/beatriz/MIC/2_Micro/data_qiime/results_match_gg/final_sequences_gg.fasta\")\n",
        "aligned_fasta = Path(\"/home/beatriz/MIC/2_Micro/data_qiime/results_match_gg/aligned-dna-sequences_gg.fasta\")\n",
        "large_dir = Path(\"/home/beatriz/MIC/MIC_large\") \n",
        "output_dir = base_dir\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "'''\n",
        "# for colab\n",
        "# Create output directory if it doesn't exist\n",
        "base_dir = Path(\"/content/drive/MyDrive/MIC/data_picrust/\")\n",
        "base_dir.mkdir(parents=True, exist_ok=True)\n",
        "abundance_excel= Path(\"/content/drive/MyDrive/MIC/data_picrust/merged_to_sequence.xlsx\")\n",
        "fasta_file_final = Path(\"/content/drive/MyDrive/MIC/data_picrust/final_sequences_gg.fasta\")\n",
        "aligned_fasta = Path(\"/content/drive/MyDrive/MIC/data_picrust/aligned-dna-sequences_gg.fasta\")\n",
        "results_file = base_dir / \"/content/drive/MyDrive/MIC/data_picrust/functional_comparison.xlsx\"\n",
        "output_dir = base_dir  # Separate output directory\n",
        "\n",
        "# For Kaggle work\n",
        "base_dir = Path(\"/kaggle/input/data-picrust\")\n",
        "abundance_excel= base_dir / \"merged_to_sequence.xlsx\"\n",
        "fasta_file_final = base_dir / \"final_sequences_gg.fasta\"\n",
        "output_dir = Path(\"/kaggle/working/\") '''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COW1kGFZCqSG"
      },
      "source": [
        "The fasta file come from the Alternative Sequences finding from the Greenes Genes Database, from the taxonomy in this study made in section 7 in the 5_Sequences_qiime notebook: final_sequences_gg.fasta. Abundance dataframe come from the data from notebook 4 merged_to_sequence.xlsx sheet=core_check_usual_taxa which is a unified df between 3 different groups explained previously: cora_taxa (>20% 60 abundance features), usual_taxa (17 high literature ranking bacteria influencing corrosion) and checked_taxa (30 statistically significant to the corrosion risk label) in total 85 features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T09:50:11.074331Z",
          "iopub.status.busy": "2025-02-19T09:50:11.073894Z",
          "iopub.status.idle": "2025-02-19T09:50:11.267133Z",
          "shell.execute_reply": "2025-02-19T09:50:11.265805Z",
          "shell.execute_reply.started": "2025-02-19T09:50:11.074292Z"
        },
        "id": "qzMCSdlPuvgI",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Integrated taxa from origin genus as headers with levels 6 for the genera, 7 for the GID, muss be cleaned\n",
        "Integrated_T = pd.read_excel(abundance_excel, sheet_name='core_check_usual_taxa', header=[0,1,2,3,4,5,6,7], engine ='openpyxl')\n",
        "# Drop first row (index 0) and first column in one chain\n",
        "Integrated_T = Integrated_T.drop(index=0).drop(Integrated_T.columns[0], axis=1)\n",
        "Integrated_T= Integrated_T.astype({'Sites': str})\n",
        "Integrated_T['Sites'] = Integrated_T['Sites'].fillna('Source')\n",
        "# Remove 'Unnamed' level names\n",
        "Integrated_T.columns = Integrated_T.columns.map(lambda x: tuple('' if 'Unnamed' in str(level) else level for level in x))\n",
        "# Changing dtypes to category whiles respecting structure\n",
        "Integrated_T[\"Category\"] = Integrated_T[\"Category\"].astype(\"Int64\")\n",
        "Integrated_T= Integrated_T.set_index(\"Sites\")\n",
        "pre_Integrated = Integrated_T.T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "execution": {
          "iopub.execute_input": "2025-02-19T09:50:15.084302Z",
          "iopub.status.busy": "2025-02-19T09:50:15.083951Z",
          "iopub.status.idle": "2025-02-19T09:50:15.122637Z",
          "shell.execute_reply": "2025-02-19T09:50:15.121449Z",
          "shell.execute_reply.started": "2025-02-19T09:50:15.084276Z"
        },
        "id": "aZSzaNSQuvgI",
        "outputId": "1f73ab91-7072-424a-e74e-6060f03924f7",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "Integrated_T.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdOLY5SPuvgI"
      },
      "source": [
        "## 2.3. Making Sequences for Picrust fasta file\n",
        "\n",
        "Picrust Functional Analyiss requires a biom table with otus as index, samples as headers and abundance as values. The present biom has genus names but is needs instead Otus instead. The other input file for picrust is the representative sequences table that consist of the sequences per genera followed by the frequency of that genera on the whole sample, this is done directly by the software. The fasta file requires the otus instead of the genera names and the sequences non aligned coming from notebook 5. The following scrips will formate the data to picrust."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-02-19T09:42:03.38396Z",
          "iopub.status.busy": "2025-02-19T09:42:03.383656Z",
          "iopub.status.idle": "2025-02-19T09:42:03.422952Z",
          "shell.execute_reply": "2025-02-19T09:42:03.421779Z",
          "shell.execute_reply.started": "2025-02-19T09:42:03.383933Z"
        },
        "id": "cizMvGCGuvgI",
        "outputId": "99bca377-e9ad-4ca4-d89f-3f168124503b",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Read and modify sequences\n",
        "new_records = []\n",
        "for record in SeqIO.parse(fasta_file_final, \"fasta\"):\n",
        "    match = re.search(r\"\\s(\\d+)\\s\", record.description)  # Look for digits surrounded by spaces\n",
        "    if match:\n",
        "        otu_id = match.group(1)\n",
        "    else:\n",
        "        print(f\"Warning: Could not extract OTU ID from description: {record.description}\")\n",
        "        continue  # Skip this record if OTU ID not found\n",
        "\n",
        "    # Create new record with only OTU as ID\n",
        "    new_record = SeqRecord(\n",
        "        record.seq,\n",
        "        id=otu_id,\n",
        "        description=\"\"  # Empty description to keep only ID\n",
        "    )\n",
        "    new_records.append(new_record)\n",
        "\n",
        "# Write modified FASTA\n",
        "output_fasta_path = Path(output_dir / \"sequences_for_picrust.fasta\")\n",
        "\n",
        "SeqIO.write(new_records, output_fasta_path, \"fasta\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-02-19T09:42:03.424698Z",
          "iopub.status.busy": "2025-02-19T09:42:03.424272Z",
          "iopub.status.idle": "2025-02-19T09:42:03.430677Z",
          "shell.execute_reply": "2025-02-19T09:42:03.429613Z",
          "shell.execute_reply.started": "2025-02-19T09:42:03.424654Z"
        },
        "id": "2W_9inz2Yelu",
        "outputId": "b7cdf043-392d-48ad-d35e-266c40a85b71",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "new_record"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RN-pFp1cuvgI"
      },
      "source": [
        "## 2.4. Making of Dataframes for 2 Different Pipelines\n",
        "The following script is the path to the biom file but also to the Integrate dataframe which create dataframes that discriminate its origin in order to pass then through picrust different pipelines, to know: Simple_Base that compares the known bacteria namely usual_taxa against the other features to understand their relationships on the function of their metabolism, an additional group is put forward as simply_candidate_mic which corresponds to the bacteria no previously linked to corrosion but showing an statistical significance with the risk label, those come from the checked_taxa and in this study are: genera(GID): Bulleida (154); Mycoplana (471), Oxobacter (512) and Oerskovia (). Also as showing an favor behaviour against corrosion are presented: Phenylobacterium (549), Gelria(334), Porphyrobacteria (564) and Tepidimonas (712)\n",
        "SIMPLE_BASE = {'known': 'simple_known_mic', 'other': 'simple_candidate_mic'}\n",
        "The second pipeline comprises a more detailed separation of the bacteria and that is: The Known bacteria as previously, pure_checked corresponding to the statistical significant genera, pure_core correspondent to the core taxa on the systems and the combination of the core and checked taxa.\n",
        "DETAILED_BASE = {'known': 'detailed_known_mic','pure_checked': 'detailed_pure_checked_mic',\n",
        "    'pure_core': 'detailed_pure_core_mic', 'checked_core': 'detailed_checked_core_mic'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akl8MkZjuvgI"
      },
      "source": [
        "__Making the Integrated dataframe__\n",
        "The original dataframe has a column for source, indicating from which df  came from (core, usual, checked), this script proceses that datadrame into individual dfs and the combined preserving the source for further analysis. The Integrated dataframe continues to be process on the next step to become the biom abundance df."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-02-19T09:42:03.431944Z",
          "iopub.status.busy": "2025-02-19T09:42:03.43168Z",
          "iopub.status.idle": "2025-02-19T09:42:03.497242Z",
          "shell.execute_reply": "2025-02-19T09:42:03.496104Z",
          "shell.execute_reply.started": "2025-02-19T09:42:03.431915Z"
        },
        "id": "MWukq8fUCqSH",
        "outputId": "301d040e-a748-4d32-d5d9-e362b3ff40fb",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def process_integrated_data(df):\n",
        "    \"\"\"\n",
        "    Process the integrated DataFrame to create a new DataFrame with clear column names\n",
        "    and preserve all values including source information.\n",
        "\n",
        "    Parameters:\n",
        "    df (pandas.DataFrame): Input DataFrame with MultiIndex index and site columns\n",
        "\n",
        "    Returns:\n",
        "    pandas.DataFrame: Processed DataFrame with clear structure\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract genera and GIDs from the index MultiIndex\n",
        "    genera = df.index.get_level_values(6)[1:]  # Skip first row\n",
        "    gids = pd.to_numeric(df.index.get_level_values(7)[1:], errors='coerce')\n",
        "\n",
        "    # Create a new DataFrame with the extracted information\n",
        "    result_df = pd.DataFrame({\n",
        "        'Genus': genera,\n",
        "        'GID': gids\n",
        "    })\n",
        "\n",
        "    # Add the site values from the original DataFrame\n",
        "    for col in df.columns:\n",
        "        result_df[col] = df.iloc[1:][col].values\n",
        "\n",
        "    # Clean up the DataFrame\n",
        "    result_df['GID'] = pd.to_numeric(result_df['GID'], errors='coerce')\n",
        "    result_df = result_df.dropna(subset=['GID'])\n",
        "    result_df['GID'] = result_df['GID'].astype(int)\n",
        "\n",
        "    return result_df\n",
        "\n",
        "def get_taxa_groups(df):\n",
        "    \"\"\"\n",
        "    Separate the processed DataFrame into different taxa groups based on Source column\n",
        "\n",
        "    Parameters:\n",
        "    df (pandas.DataFrame): Processed DataFrame from process_integrated_data()\n",
        "\n",
        "    Returns:\n",
        "    dict: Dictionary containing DataFrames for different taxa groups\n",
        "    \"\"\"\n",
        "    # Split the data into groups based on 'Source' column patterns\n",
        "\n",
        "    # Known corrosion bacteria (any pattern with 'us')\n",
        "    known_bacteria = df[df['Source'].str.contains('us', case=False, na=False)]\n",
        "\n",
        "    # Pure checked bacteria (only 'chk' without 'core' or 'us')\n",
        "    pure_checked = df[\n",
        "        df['Source'].str.contains('chk', case=False, na=False) &\n",
        "        ~df['Source'].str.contains('core|us', case=False, na=False)\n",
        "    ]\n",
        "\n",
        "    # Pure core bacteria (only 'core' without 'chk' or 'us')\n",
        "    pure_core = df[\n",
        "        df['Source'].str.contains('core', case=False, na=False) &\n",
        "        ~df['Source'].str.contains('chk|us', case=False, na=False)\n",
        "    ]\n",
        "\n",
        "    # Checked-core bacteria (contains both 'core' and 'chk' but no 'us')\n",
        "    checked_core = df[\n",
        "        df['Source'].str.contains('chk.*core|core.*chk', case=False, na=False) &\n",
        "        ~df['Source'].str.contains('us', case=False, na=False)\n",
        "    ]\n",
        "\n",
        "    # Create groups dictionary\n",
        "    taxa_groups = {\n",
        "        'known_bacteria': known_bacteria,\n",
        "        'pure_checked': pure_checked,\n",
        "        'pure_core': pure_core,\n",
        "        'checked_core': checked_core\n",
        "    }\n",
        "\n",
        "    # Print summary statistics\n",
        "    print(\"\\nDetailed Classification Results:\")\n",
        "    print(f\"Known corrosion bacteria: {len(known_bacteria)}\")\n",
        "    print(f\"Pure checked bacteria: {len(pure_checked)}\")\n",
        "    print(f\"Pure core bacteria: {len(pure_core)}\")\n",
        "    print(f\"Checked-core bacteria: {len(checked_core)}\")\n",
        "\n",
        "    # Verify total matches expected\n",
        "    total_classified = len(known_bacteria) + len(pure_checked) + len(pure_core) + len(checked_core)\n",
        "    print(f\"\\nTotal classified taxa: {total_classified}\")\n",
        "    print(f\"Total in dataset: {len(df)}\")\n",
        "\n",
        "    return taxa_groups\n",
        "\n",
        "# Usage example:\n",
        "Integrated = process_integrated_data(pre_Integrated)\n",
        "\n",
        "# Get the groups\n",
        "taxa_groups = get_taxa_groups(Integrated)\n",
        "\n",
        "# Access individual groups -\n",
        "known_bacteria = taxa_groups['known_bacteria']\n",
        "pure_core = taxa_groups['pure_core']\n",
        "pure_checked = taxa_groups['pure_checked']\n",
        "checked_core = taxa_groups['checked_core']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "LkfsNjfki4Ap",
        "outputId": "36ed19eb-4505-480f-b0fa-34c80b5e4072"
      },
      "outputs": [],
      "source": [
        "checked_core"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g56zbUtiuvgJ"
      },
      "source": [
        "## 2.5. Making the Abundanc Biom dataframe for Picrust\n",
        "\n",
        "The final biom should have as index the Otus numbers no the genera names and a clean formate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T09:42:03.498862Z",
          "iopub.status.busy": "2025-02-19T09:42:03.498435Z",
          "iopub.status.idle": "2025-02-19T09:42:03.533685Z",
          "shell.execute_reply": "2025-02-19T09:42:03.532594Z",
          "shell.execute_reply.started": "2025-02-19T09:42:03.498824Z"
        },
        "id": "Qn6xPmvfuvgJ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# droping source and genus and putting GID as index\n",
        "pre_biom= Integrated.drop(columns=[\"Source\", \"GID\"])\n",
        "pre_biom= pre_biom.set_index(\"Genus\").astype(str)\n",
        "# Ensure all data values are float\n",
        "pre_biom = pre_biom.astype(float)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mI5nSqwXuvgJ"
      },
      "source": [
        "__changing genera to otus__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-02-19T09:42:03.535198Z",
          "iopub.status.busy": "2025-02-19T09:42:03.534792Z",
          "iopub.status.idle": "2025-02-19T09:42:03.555967Z",
          "shell.execute_reply": "2025-02-19T09:42:03.554671Z",
          "shell.execute_reply.started": "2025-02-19T09:42:03.535169Z"
        },
        "id": "amKUleGLuvgJ",
        "outputId": "bdb770e8-73ac-4fd9-bed1-7435ec085011",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Create genus to OTU mapping from FASTA headers\n",
        "genus_to_otu = {}\n",
        "for record in SeqIO.parse(fasta_file_final, \"fasta\"):\n",
        "    parts = record.description.split()\n",
        "    if len(parts) >= 3:\n",
        "        genus = parts[0]\n",
        "        otu = parts[1]  # We'll use the first OTU number\n",
        "        genus_to_otu[genus] = otu\n",
        "\n",
        "# Print a few mappings to verify\n",
        "print(\"Sample genus to OTU mappings:\")\n",
        "for i, (genus, otu) in enumerate(list(genus_to_otu.items())[:5]):\n",
        "    print(f\"{genus} -> {otu}\")\n",
        "\n",
        "# Replace genus with OTU in the index\n",
        "pre_biom.index = pre_biom.index.map(lambda x: genus_to_otu.get(x, x))\n",
        "\n",
        "# Remove the 'Genus' name from the index\n",
        "pre_biom.index.name = \"OTU\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFYJS2KuuvgJ"
      },
      "source": [
        "__Calculation counts for picrust2__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 475
        },
        "execution": {
          "iopub.execute_input": "2025-02-19T09:42:03.557141Z",
          "iopub.status.busy": "2025-02-19T09:42:03.55673Z",
          "iopub.status.idle": "2025-02-19T09:42:03.597345Z",
          "shell.execute_reply": "2025-02-19T09:42:03.596224Z",
          "shell.execute_reply.started": "2025-02-19T09:42:03.557111Z"
        },
        "id": "bwLLzgWLuvgJ",
        "outputId": "72823c19-764a-4bba-acb6-bc748f385947",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "scaling_factor = 10000\n",
        "# Multiply by scaling factor and round to nearest integer\n",
        "count_pre_biom = np.round(pre_biom * scaling_factor).astype(int)\n",
        "count_pre_biom"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gu2uuKLHuvgJ"
      },
      "source": [
        "__Creating the biom table formate__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T09:42:03.59893Z",
          "iopub.status.busy": "2025-02-19T09:42:03.598532Z",
          "iopub.status.idle": "2025-02-19T09:42:03.622266Z",
          "shell.execute_reply": "2025-02-19T09:42:03.621214Z",
          "shell.execute_reply.started": "2025-02-19T09:42:03.598882Z"
        },
        "id": "PsCMsci7w8R7",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "'''# Create BIOM table with type specification\n",
        "biom_table = Table(data=count_pre_biom.values,\n",
        "                  observation_ids=count_pre_biom.index.astype(str),\n",
        "                  sample_ids=count_pre_biom.columns.astype(str),\n",
        "                  type=\"OTU table\",\n",
        "                  create_date=datetime.now().isoformat(),\n",
        "                  generated_by=\"BIOM-Format\",\n",
        "                  matrix_type=\"sparse\",\n",
        "                  matrix_element_type=\"float\")\n",
        "\n",
        "# Save with explicit format\n",
        "output_path = output_dir / \"count_abundance_85.biom\"\n",
        "\n",
        "with biom_open(output_path, 'w') as f:\n",
        "    biom_table.to_hdf5(f, generated_by=\"BIOM-Format\")'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-02-19T09:42:03.623693Z",
          "iopub.status.busy": "2025-02-19T09:42:03.623367Z",
          "iopub.status.idle": "2025-02-19T09:42:06.698815Z",
          "shell.execute_reply": "2025-02-19T09:42:06.697254Z",
          "shell.execute_reply.started": "2025-02-19T09:42:03.623666Z"
        },
        "id": "gV5uEFS_uvgJ",
        "outputId": "0c7b28f2-fde1-44bb-e2d7-baccd30249cd",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "'''# Validate the table structure\n",
        "print(\"\\nValidating table...\")\n",
        "!biom validate-table -i {output_path}\n",
        "#/home/beatriz/MIC/2_Micro/data_picrust/count_abundance_85.biom\n",
        "\n",
        "# Show table info\n",
        "!biom summarize-table -i {output_path}'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Validating table...\n",
        "\n",
        "The input file is a valid BIOM-formatted file.\n",
        "Num samples: 70\n",
        "Num observations: 85\n",
        "Total count: 56747993\n",
        "Table density (fraction of non-zero values): 0.405\n",
        "\n",
        "Counts/sample summary:\n",
        " Min: 181800.000\n",
        " Max: 990578.000\n",
        " Median: 851078.500\n",
        " Mean: 810685.614\n",
        " Std. dev.: 157876.192\n",
        " Sample Metadata Categories: None provided\n",
        " Observation Metadata Categories: None provided\n",
        "\n",
        "Counts/sample detail:\n",
        "site_69: 181800.000\n",
        "site_67: 217903.000\n",
        "site_70: 270600.000\n",
        "site_26: 582999.000\n",
        "site_21: 589725.000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmMvQVofuvgK"
      },
      "source": [
        "# 3. Making the representative sequences\n",
        "\n",
        "__Convert Abundance Biom table and the Sequences into a QIIME2 artifact__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-02-19T09:42:06.700877Z",
          "iopub.status.busy": "2025-02-19T09:42:06.700455Z",
          "iopub.status.idle": "2025-02-19T09:42:06.72345Z",
          "shell.execute_reply": "2025-02-19T09:42:06.722298Z",
          "shell.execute_reply.started": "2025-02-19T09:42:06.700816Z"
        },
        "id": "yXcOOPN7uvgK",
        "outputId": "c99f3ca2-d57f-4b7b-d54e-aa73dc61c883",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def create_rep_seqs_with_freq(sequence_file, pre_biom_df, output_fasta):\n",
        "    \"\"\"\n",
        "    Create representative sequences with frequencies written to output\n",
        "\n",
        "    Args:\n",
        "        sequence_file: Path to FASTA file with OTU sequences\n",
        "        pre_biom_df: DataFrame with abundance data\n",
        "        output_fasta: Path to save sequences with frequencies\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Calculate total frequency for each OTU\n",
        "        total_frequencies = round(pre_biom_df.sum(axis=1), 2)\n",
        "\n",
        "        with open(output_fasta, 'w') as out:\n",
        "            for record in SeqIO.parse(sequence_file, \"fasta\"):\n",
        "                otu_id = record.id\n",
        "\n",
        "                if otu_id in total_frequencies.index:\n",
        "                    freq = total_frequencies[otu_id]\n",
        "                    sequence = str(record.seq)\n",
        "\n",
        "                    # Write sequence with frequency to FASTA\n",
        "                    out.write(f\">{otu_id} {sequence} {freq}\\n\")\n",
        "\n",
        "        # First lines of the file\n",
        "        print(\"Representative Sequences head:\")\n",
        "        with open(output_fasta, 'r') as f:\n",
        "            for i, line in enumerate(f):\n",
        "                if i < 1:  # Show first 3 sequences (header + sequence lines)\n",
        "                    print(line.strip())\n",
        "        return output_fasta\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred: {e}\")\n",
        "    return None\n",
        "\n",
        "\n",
        "# Representative sequences\n",
        "sequences_for_picrust = output_dir / \"sequences_for_picrust.fasta\"\n",
        "\n",
        "output_fasta = output_dir/ \"representative_sequences\"\n",
        "\n",
        "repres_sequ = create_rep_seqs_with_freq(sequences_for_picrust, pre_biom, output_fasta)\n",
        "repres_sequ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ku8lUve9uvgK"
      },
      "source": [
        "__Disclamer:__ These notebook was mean to do the analysis of the functional mechanisms of bacteria using picrust2, however the capacity of the laptop was no sufficient to run it, nor colab on public library, nor a virtual machine, that is the reason why the analysis was undertaken in the galaxy website, where the data resides.\n",
        "https://usegalaxy.eu/  \n",
        "username= magicalex238"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zv-yUkfmCqSN"
      },
      "source": [
        "## 3.1. Classifying Bacteria by their Source DataFrame\n",
        "Two distinct classification approaches are implemented to categorize bacteria. The simple approach (get_bacteria_sources_simple) divides bacteria into known corrosion-causers (usual_taxa) and candidates (all others). The detailed approach (get_bacteria_sources_detailed) provides finer categorization by separating bacteria into known corrosion-causers, pure checked taxa, pure core taxa, and those present in both checked and core datasets. Please notice that this function uses df Integrated for source clasification and no abundance.biom which will be used for the picrust2 pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T09:42:06.731117Z",
          "iopub.status.busy": "2025-02-19T09:42:06.730712Z",
          "iopub.status.idle": "2025-02-19T09:42:06.745406Z",
          "shell.execute_reply": "2025-02-19T09:42:06.7441Z",
          "shell.execute_reply.started": "2025-02-19T09:42:06.73108Z"
        },
        "id": "5bDVrPwWCqSR",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def get_bacteria_sources_simple(Integrated_df):\n",
        "    \"\"\"\n",
        "    Simple classification:\n",
        "    1. Known (anything with 'us')\n",
        "    2. All others (combined chk, core, chk-core)\n",
        "    \"\"\"\n",
        "    # Get genera and gids from column levels 6 and 7\n",
        "    genera = Integrated_df[\"Genus\"]\n",
        "    gids = Integrated_df[\"GID\"]\n",
        "\n",
        "    # Look for Source in the data, not index\n",
        "    sources = Integrated_df['Source'] if 'Source' in Integrated_df.columns else None\n",
        "\n",
        "    known_bacteria = {}     # usual_taxa\n",
        "    other_bacteria = {}     # everything else\n",
        "\n",
        "    sources_found = set()\n",
        "    source ={}\n",
        "    patterns = ['us', 'core-us', 'chk-us', 'chk-core-us']\n",
        "\n",
        "    for i, (genus, gid) in enumerate (zip(genera, gids)):\n",
        "        if source is not None:  # Check if source exists for this genus\n",
        "            source = str(sources.iloc[i]).strip().lower()\n",
        "            sources_found.add(source)\n",
        "\n",
        "            if source in patterns:\n",
        "                known_bacteria[genus] = int(gid) if str(gid).isdigit() else gid\n",
        "            else:\n",
        "                other_bacteria[genus] = int(gid) if str(gid).isdigit() else gid\n",
        "\n",
        "    print(\"\\nSimple Classification Results:\")\n",
        "    print(f\"Known corrosion bacteria: {len(known_bacteria)}\")\n",
        "    print(f\"Other bacteria: {len(other_bacteria)}\")\n",
        "    print(\"\\nSources found:\", sources_found)\n",
        "\n",
        "    return {\n",
        "        'known_bacteria': known_bacteria,\n",
        "        'other_bacteria': other_bacteria\n",
        "    }\n",
        "\n",
        "def get_bacteria_sources_detailed(Integrated_df):\n",
        "    \"\"\"\n",
        "    Detailed classification with all possible combinations:\n",
        "    1. Known (usual_taxa)\n",
        "    2. Pure checked (only 'chk')\n",
        "    3. Pure core (only 'core')\n",
        "    4. Checked-core (overlap 'chk-core')\n",
        "    \"\"\"\n",
        "\n",
        "    genera = Integrated_df[\"Genus\"]\n",
        "    gids = Integrated_df[\"GID\"]\n",
        "\n",
        "    sources = Integrated_df['Source'] if 'Source' in Integrated_df.columns else None\n",
        "\n",
        "    known_bacteria = {}      # usual_taxa\n",
        "    pure_checked = {}        # only 'chk' checked_taxa\n",
        "    pure_core = {}          # only 'core' core_taxa\n",
        "    checked_core = {}       # 'chk-core' checked and core taxa\n",
        "    source ={}\n",
        "    sources_found = set()\n",
        "    patterns = ['us', 'core-us', 'chk-us', 'chk-core-us']\n",
        "\n",
        "    for i, (genus, gid) in enumerate (zip(genera, gids)):\n",
        "        if source is not None:  # Check if source exists for this genus\n",
        "            source = str(sources.iloc[i]).strip().lower()\n",
        "            sources_found.add(source)\n",
        "\n",
        "            if source in patterns:\n",
        "                known_bacteria[genus] = int(gid) if str(gid).isdigit() else gid\n",
        "                continue\n",
        "\n",
        "            # Then handle other combinations\n",
        "            if source == 'chk':\n",
        "                pure_checked[genus] = gid\n",
        "            elif source == 'core':\n",
        "                pure_core[genus] = gid\n",
        "            elif 'chk-core' in source:\n",
        "                checked_core[genus] = gid\n",
        "\n",
        "    print(\"\\nDetailed Classification Results:\")\n",
        "    print(f\"Known corrosion bacteria: {len(known_bacteria)}\")\n",
        "    print(f\"Pure checked bacteria: {len(pure_checked)}\")\n",
        "    print(f\"Pure core bacteria: {len(pure_core)}\")\n",
        "    print(f\"Checked-core bacteria: {len(checked_core)}\")\n",
        "    print(\"\\nSources found:\", sources_found)\n",
        "\n",
        "    return {\n",
        "        'known_bacteria': known_bacteria,\n",
        "        'pure_checked': pure_checked,\n",
        "        'pure_core': pure_core,\n",
        "        'checked_core': checked_core\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-02-19T09:42:06.748865Z",
          "iopub.status.busy": "2025-02-19T09:42:06.748512Z",
          "iopub.status.idle": "2025-02-19T09:42:06.766435Z",
          "shell.execute_reply": "2025-02-19T09:42:06.765138Z",
          "shell.execute_reply.started": "2025-02-19T09:42:06.748825Z"
        },
        "id": "LSxCpNd8Yelz",
        "outputId": "ecc67df6-bcdf-4c6a-89ac-caad0b4c8e5f",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "sources_simple = get_bacteria_sources_simple(Integrated)\n",
        "\n",
        "sources_detail = get_bacteria_sources_detailed(Integrated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T09:42:06.767806Z",
          "iopub.status.busy": "2025-02-19T09:42:06.767438Z",
          "iopub.status.idle": "2025-02-19T09:42:06.785756Z",
          "shell.execute_reply": "2025-02-19T09:42:06.784698Z",
          "shell.execute_reply.started": "2025-02-19T09:42:06.767762Z"
        },
        "id": "QtD3K9OIYelz",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Extracting the genus lists for each group:\n",
        "known_bacteria_list = list(sources_detail['known_bacteria'].keys())\n",
        "pure_checked_list = list(sources_detail['pure_checked'].keys())\n",
        "pure_core_list = list(sources_detail['pure_core'].keys())\n",
        "checked_core_list = list(sources_detail['checked_core'].keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCA_PPxRYel0"
      },
      "source": [
        "The lists will be utilised later in order to groupby this list int he analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23P2k1QwCqSR"
      },
      "source": [
        "## 3.2. Prepare picrust data and Creating Directories for PICRUSt2 Input\n",
        "The check_missing_genera function processes the integrated data and handles data quality control. Known problematic genera (e.g., 'Clostridium_sensu_stricto_12', 'Oxalobacteraceae_unclassified') are flagged for exclusion to prevent analysis errors. The function also creates an organized directory structure as outlined in the introduction, with separate paths for different bacterial classifications (known_mic, candidate_mic, etc.) and their respective analysis outputs (EC_predictions, pathway_predictions, KO_predictions). Following function prepares the data for picrust analysis but both dataframes the abundance.biom and Integrated have some bacteria that were no sequenciated mostly cause are no known specimens. So it is necesary to do same procedure to both dfs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "execution": {
          "iopub.execute_input": "2025-02-19T09:42:06.787249Z",
          "iopub.status.busy": "2025-02-19T09:42:06.78684Z",
          "iopub.status.idle": "2025-02-19T09:42:06.800994Z",
          "shell.execute_reply": "2025-02-19T09:42:06.799794Z",
          "shell.execute_reply.started": "2025-02-19T09:42:06.787219Z"
        },
        "id": "bNfnbXfKCqSS",
        "outputId": "7d709fc3-8cd6-4502-8da9-de7834d87d8a",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def prepare_picrust_data(Integrated_df, aligned_file, function_type='simple'):\n",
        "    \"\"\"\n",
        "    Prepare data for PICRUSt analysis with choice of  function_type method\n",
        "\n",
        "    Args:\n",
        "        Integrated_df: Input DataFrame\n",
        "        aligned_file: Path to aligned sequences\n",
        "        function_type: 'simple' or 'detailed'\n",
        "    \"\"\"\n",
        "    # Get bacteria source_groups based on chosen  function_type\n",
        "    if  function_type == 'simple':\n",
        "        source_groups = get_bacteria_sources_simple(Integrated_df)\n",
        "    else:\n",
        "        source_groups= get_bacteria_sources_detailed(Integrated_df)\n",
        "\n",
        "    # Create appropriate directory structure\n",
        "    create_directory_structure(function_type)\n",
        "\n",
        "    return source_groups\n",
        "\n",
        "def create_directory_structure(function_type='simple'):\n",
        "    \"\"\"Create directory structure for PICRUSt analysis\"\"\"\n",
        "    base_dir = Path(\"/home/beatriz/MIC/2_Micro/data_picrust\")\n",
        "    base_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    if function_type == 'simple':\n",
        "        directories = SIMPLE_BASE\n",
        "    else:\n",
        "        directories = DETAILED_BASE\n",
        "\n",
        "    # Create all required directories\n",
        "    for dir_name in directories.values():\n",
        "        for subdir in SUBDIRS:\n",
        "            (base_dir / dir_name / subdir).mkdir(parents=True, exist_ok=True)\n",
        "    logging.info(\"Directory structure created successfully\")\n",
        "\n",
        "    return True\n",
        "\n",
        "'''  except Exception as e:\n",
        "    logging.error(f\"Error creating directory structure: {str(e)}\")\n",
        "    return False'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T09:42:06.802491Z",
          "iopub.status.busy": "2025-02-19T09:42:06.802091Z",
          "iopub.status.idle": "2025-02-19T09:42:06.825255Z",
          "shell.execute_reply": "2025-02-19T09:42:06.823564Z",
          "shell.execute_reply.started": "2025-02-19T09:42:06.802433Z"
        },
        "id": "q4v7JVU8uvgK",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def verify_input_files():\n",
        "    \"\"\"Verify that input files exist and are readable\"\"\"\n",
        "    missing_files = []\n",
        "\n",
        "    if not fasta_file.exists():\n",
        "        missing_files.append(str(fasta_file))\n",
        "    if not biom_table.exists():\n",
        "        missing_files.append(str(biom_table))\n",
        "\n",
        "    if missing_files:\n",
        "        logging.error(f\"Missing input files: {', '.join(missing_files)}\")\n",
        "        return False\n",
        "\n",
        "    logging.info(\"All input files found\")\n",
        "    return True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUwADMKZCqSS"
      },
      "source": [
        "# 4. PICRUSt Pipeline Definition\n",
        "The pipeline processes the aligned sequence data from notebook 5 that has or not undergo cleaning of the sequences as previously done on section 2. Also processes the biom_table in order to account on this anylsis on abundance. It queries the PICRUSt database to predict potential metabolic pathways for each genus. This prediction is based on evolutionary relationships and known genomic capabilities of related organisms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T09:42:06.826915Z",
          "iopub.status.busy": "2025-02-19T09:42:06.826589Z",
          "iopub.status.idle": "2025-02-19T09:42:06.844098Z",
          "shell.execute_reply": "2025-02-19T09:42:06.842947Z",
          "shell.execute_reply.started": "2025-02-19T09:42:06.826883Z"
        },
        "id": "srMpS5DkCqSS",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def run_picrust2_pipeline(fasta_file, biom_file, output_dir):\n",
        "    \"\"\"\n",
        "    Run the main PICRUSt2 pipeline on input sequences and BIOM table.\n",
        "\n",
        "    Args:\n",
        "        fasta_file: Path to the aligned sequences FASTA file.\n",
        "        biom_file: Path to the BIOM table (without extra columns).\n",
        "        output_dir: Directory for PICRUSt2 output.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Run main PICRUSt2 pipeline\n",
        "        cmd = [\n",
        "            'picrust2_pipeline.py',\n",
        "            '-s', fasta_file,        # Input FASTA file with aligned sequences\n",
        "            '-i', biom_file,         # BIOM table with abundance data\n",
        "            '-o', output_dir,        # Output directory\n",
        "            '--processes', '4',      # Parallel processes\n",
        "            '--verbose',\n",
        "            '--min_align', '0.25'    # Note the split here\n",
        "        ]\n",
        "        subprocess.run(cmd, check=True)\n",
        "\n",
        "        # Add pathway descriptions if the pathway file exists\n",
        "        pathway_file = os.path.join(output_dir, 'pathways_out/path_abun_unstrat.tsv.gz')\n",
        "        if os.path.exists(pathway_file):\n",
        "            cmd_desc = [\n",
        "                'add_descriptions.py',\n",
        "                '-i', pathway_file,\n",
        "                '-m', 'PATHWAY',\n",
        "                '-o', os.path.join(output_dir, 'pathways_with_descriptions.tsv')\n",
        "            ]\n",
        "            subprocess.run(cmd_desc, check=True)\n",
        "\n",
        "        print(f\"PICRUSt2 pipeline completed successfully for {output_dir}\")\n",
        "        return True\n",
        "\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Error running PICRUSt2: {e}\")\n",
        "        return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rx_DyzHbCqSS"
      },
      "source": [
        "# 5. Analysis of Pathways\n",
        "The analysis focuses on metabolic pathways known to be involved in microbially influenced corrosion, including sulfur metabolism, organic acid production, iron metabolism, and biofilm formation. These pathways were selected based on documented mechanisms of known corrosion-inducing bacteria. Separate pipeline runs for simple and detailed classifications ensure proper pathway analysis for each bacterial group."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T09:42:06.845667Z",
          "iopub.status.busy": "2025-02-19T09:42:06.845294Z",
          "iopub.status.idle": "2025-02-19T09:42:06.867161Z",
          "shell.execute_reply": "2025-02-19T09:42:06.865768Z",
          "shell.execute_reply.started": "2025-02-19T09:42:06.845641Z"
        },
        "id": "8eP8MAidCqSS",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def analyze_functional_profiles(picrust_output_dir, bacteria_list):\n",
        "    \"\"\"\n",
        "    Analyze functional profiles with focus on corrosion-relevant pathways\n",
        "\n",
        "    Parameters:\n",
        "    picrust_output_dir: directory containing PICRUSt2 output\n",
        "    bacteria_list: list of bacteria names to analyze\n",
        "    \"\"\"\n",
        "    # Define corrosion-relevant pathways\n",
        "    relevant_pathways = [\n",
        "        'Sulfur metabolism',\n",
        "        'Iron metabolism',\n",
        "        'Energy metabolism',\n",
        "        'Biofilm formation',\n",
        "        'Metal transport',\n",
        "        'ochre formation',\n",
        "        'iron oxide deposits',\n",
        "        'iron precipitation',\n",
        "        'rust formation',\n",
        "        'organic acid production',\n",
        "        'acetate production',\n",
        "        'lactate metabolism',\n",
        "        'formate production',\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        # Read PICRUSt2 output\n",
        "        pathway_file = os.path.join(picrust_output_dir, 'pathways_with_descriptions.tsv')\n",
        "        pathways_df = pd.read_csv(pathway_file, sep='\\t')\n",
        "\n",
        "        # Filter for relevant pathways\n",
        "        filtered_pathways = pathways_df[\n",
        "            pathways_df['description'].str.contains('|'.join(relevant_pathways),\n",
        "                                                  case=False,\n",
        "                                                  na=False)]\n",
        "\n",
        "        # Calculate pathway abundances per bacteria\n",
        "        pathway_abundances = filtered_pathways.groupby('description').sum()\n",
        "\n",
        "        # Calculate pathway similarities between bacteria\n",
        "        pathway_similarities = {}\n",
        "        for bacteria in bacteria_list:\n",
        "            if bacteria in pathways_df.columns:\n",
        "                similarities = pathways_df[bacteria].corr(pathways_df[list(bacteria_list)])\n",
        "                pathway_similarities[bacteria] = similarities\n",
        "\n",
        "        # Predict functional potential\n",
        "        functional_predictions = {}\n",
        "        for pathway in relevant_pathways:\n",
        "            pathway_presence = filtered_pathways[\n",
        "                filtered_pathways['description'].str.contains(pathway, case=False)\n",
        "            ]\n",
        "            if not pathway_presence.empty:\n",
        "                functional_predictions[pathway] = {\n",
        "                    'presence': len(pathway_presence),\n",
        "                    'mean_abundance': pathway_presence.mean().mean(),\n",
        "                    'max_abundance': pathway_presence.max().max()\n",
        "                }\n",
        "\n",
        "        # Calculate correlation scores\n",
        "        correlation_scores = {}\n",
        "        for bacteria in bacteria_list:\n",
        "            if bacteria in pathways_df.columns:\n",
        "                correlations = pathways_df[bacteria].corr(\n",
        "                    pathways_df[filtered_pathways.index]\n",
        "                )\n",
        "                correlation_scores[bacteria] = {\n",
        "                    'mean_correlation': correlations.mean(),\n",
        "                    'max_correlation': correlations.max(),\n",
        "                    'key_pathways': correlations.nlargest(5).index.tolist()\n",
        "                }\n",
        "\n",
        "        comparison_results = {\n",
        "            'pathway_similarities': pathway_similarities,\n",
        "            'functional_predictions': functional_predictions,\n",
        "            'correlation_scores': correlation_scores,\n",
        "            'pathway_abundances': pathway_abundances.to_dict()\n",
        "        }\n",
        "\n",
        "        return filtered_pathways, comparison_results\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in pathway analysis: {str(e)}\")\n",
        "        return None, None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-EEla9jCqSS"
      },
      "source": [
        "## 5.2. Testing the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "execution": {
          "iopub.execute_input": "2025-02-19T09:42:06.868761Z",
          "iopub.status.busy": "2025-02-19T09:42:06.868347Z",
          "iopub.status.idle": "2025-02-19T09:42:06.891494Z",
          "shell.execute_reply": "2025-02-19T09:42:06.890301Z",
          "shell.execute_reply.started": "2025-02-19T09:42:06.868719Z"
        },
        "id": "mnwckS6sCqST",
        "outputId": "dd95eb17-aace-40b0-9617-63c4bb9b9c2e",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "'''# ---- RUNNING THE PIPELINE ----\n",
        "\n",
        "# Set paths\n",
        "fasta_file = Path('/home/beatriz/MIC/2_Micro/data_tree/accession_sequences.fasta')\n",
        "abundance_biom_file =  Path('/home/beatriz/MIC/2_Micro/data_picrust/abundance_accession.biom')\n",
        "output_dir = 'picrust_output'\n",
        "\n",
        "# List of bacteria to analyze\n",
        "bacteria_of_interest = ['Azospira', 'Brachybacterium', 'Bulleidia']\n",
        "\n",
        "# Run PICRUSt2\n",
        "if run_picrust2_pipeline(aligned_fasta_file,\n",
        "                         abundance_biom_file,\n",
        "                         output_dir\n",
        "                        ):\n",
        "    # Analyze functional profiles if the pipeline completes successfully\n",
        "    filtered_pathways, abundances = analyze_functional_profiles(output_dir, bacteria_of_interest)'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03KIf3UaCqST"
      },
      "source": [
        "# 6. Functional Analysis\n",
        "## 6.1 Running picrust full pipeline 1\n",
        "The analysis workflow begins by categorizing bacteria into source groups using the classification functions. These categorized data are then processed through the PICRUSt pipeline to predict metabolic capabilities. The functional analysis examines pathway presence, abundance, and correlations between different bacterial groups to identify potential corrosion-related metabolic patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T09:42:06.893107Z",
          "iopub.status.busy": "2025-02-19T09:42:06.892812Z",
          "iopub.status.idle": "2025-02-19T09:42:06.910241Z",
          "shell.execute_reply": "2025-02-19T09:42:06.909114Z",
          "shell.execute_reply.started": "2025-02-19T09:42:06.893083Z"
        },
        "id": "uHpbek-BCqST",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def run_functional_analysis(df, Integrated_df, aligned_file, analysis_type='simple'):\n",
        "    \"\"\"\n",
        "    Run complete functional analysis pipeline for either simple or detailed classification\n",
        "\n",
        "    Parameters:\n",
        "    df: Input DataFrame\n",
        "    aligned_file: Path to aligned sequences file\n",
        "    analysis_type: 'simple' or 'detailed'\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"Starting {analysis_type} classification analysis\")\n",
        "        print(f\"{'='*50}\")\n",
        "\n",
        "        # Prepare data and get source groups\n",
        "        print(\"\\nStep 1: Preparing data...\")\n",
        "\n",
        "        source_groups = prepare_picrust_data(Integrated_df, aligned_file, function_type=analysis_type)\n",
        "\n",
        "        if not source_groups:\n",
        "            raise ValueError(\"Failed to prepare data: No source groups returned\")\n",
        "\n",
        "        # Base directory for PICRUSt output\n",
        "        base_dir = Path(\"~MIC/2_Micro/data_picrust\")\n",
        "\n",
        "        results = {}\n",
        "\n",
        "        if analysis_type == 'simple':\n",
        "            # Run analysis for simple classification\n",
        "            # Known bacteria\n",
        "            known_output_dir = base_dir /SIMPLE_BASE['known']\n",
        "            success_known = run_picrust2_pipeline(aligned_file, df, str(known_output_dir))\n",
        "            if success_known:\n",
        "                results_known = analyze_functional_profiles(str(known_output_dir),\n",
        "                                                        source_groups['known_bacteria'].keys())\n",
        "\n",
        "            # Other bacteria\n",
        "            other_output_dir = base_dir / SIMPLE_BASE['other']\n",
        "            success_other = run_picrust2_pipeline(aligned_file, str(other_output_dir))\n",
        "            if success_other:\n",
        "                results_other = analyze_functional_profiles(str(other_output_dir),\n",
        "                                                        source_groups['other_bacteria'].keys())\n",
        "\n",
        "        else:\n",
        "            # Run analysis for detailed classification\n",
        "            for group, dir_name in DETAILED_BASE.items():\n",
        "\n",
        "                # Known bacteria\n",
        "                known_output_dir = base_dir / DETAILED_BASE['known']\n",
        "                success_known = run_picrust2_pipeline(aligned_file, str(known_output_dir))\n",
        "                if success_known:\n",
        "                    results_known = analyze_functional_profiles(str(known_output_dir),\n",
        "                                                            source_groups['known_bacteria'].keys())\n",
        "\n",
        "                # Pure checked bacteria\n",
        "                checked_output_dir = base_dir /  DETAILED_BASE['pure_checked']\n",
        "                success_checked = run_picrust2_pipeline(aligned_file, str(checked_output_dir))\n",
        "                if success_checked:\n",
        "                    results_checked = analyze_functional_profiles(str(checked_output_dir),\n",
        "                                                            source_groups['pure_checked'].keys())\n",
        "\n",
        "                # Pure core bacteria\n",
        "                core_output_dir = base_dir /DETAILED_BASE['pure_core']\n",
        "                success_core = run_picrust2_pipeline(aligned_file, str(core_output_dir))\n",
        "                if success_core:\n",
        "                    results_core = analyze_functional_profiles(str(core_output_dir),\n",
        "                                                            source_groups['pure_core'].keys())\n",
        "\n",
        "                # Checked-core bacteria\n",
        "                checked_core_output_dir = base_dir /DETAILED_BASE['checked_core']\n",
        "                success_checked_core = run_picrust2_pipeline(aligned_file, str(checked_core_output_dir))\n",
        "                if success_checked_core:\n",
        "                    results_checked_core = analyze_functional_profiles(str(checked_core_output_dir),\n",
        "                                                                    source_groups['checked_core'].keys())\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Error running PICRUSt2: {e}\")\n",
        "\n",
        "        return \"Analysis completed successfully\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "execution": {
          "iopub.execute_input": "2025-02-19T09:42:06.911832Z",
          "iopub.status.busy": "2025-02-19T09:42:06.911563Z",
          "iopub.status.idle": "2025-02-19T09:42:06.934059Z",
          "shell.execute_reply": "2025-02-19T09:42:06.932768Z",
          "shell.execute_reply.started": "2025-02-19T09:42:06.91181Z"
        },
        "id": "3NyEekbBCqST",
        "outputId": "cc8afa97-a116-4452-d878-4bc7c51c6d9f",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "'''# Run the analysis for both types\n",
        "# Simple source classification\n",
        "simple_results = run_functional_analysis(biom_table, aligned_file, analysis_type='simple') # output_biom\n",
        "\n",
        "# Detailed source classification\n",
        "detailed_results = run_functional_analysis(biom_table, aligned_file, analysis_type='detailed')'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4rP1kdUCqST"
      },
      "source": [
        "## 6.2 Running picrust full pipeline 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T09:42:06.935833Z",
          "iopub.status.busy": "2025-02-19T09:42:06.935418Z",
          "iopub.status.idle": "2025-02-19T09:42:06.951967Z",
          "shell.execute_reply": "2025-02-19T09:42:06.950733Z",
          "shell.execute_reply.started": "2025-02-19T09:42:06.935796Z"
        },
        "id": "5A2a1CNLCqSW",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def run_picrust2_pipeline(fasta_file, output_dir, min_align =0.5):\n",
        "    \"\"\"\n",
        "    Run PICRUSt2 pipeline with improved error handling and path management\n",
        "\n",
        "    Args:\n",
        "        fasta_file: Path to aligned sequences fasta file (str or Path)\n",
        "        output_dir: Directory for PICRUSt2 output (str or Path)\n",
        "    \"\"\"\n",
        "    import subprocess\n",
        "    import os\n",
        "    from pathlib import Path\n",
        "\n",
        "    # Convert paths to strings\n",
        "    fasta_file = str(fasta_file)\n",
        "    output_dir = str(output_dir)\n",
        "\n",
        "    try:\n",
        "        # Verify picrust2 is available\n",
        "        picrust_check = subprocess.run(['which', 'picrust2_pipeline.py'],\n",
        "                                     capture_output=True,\n",
        "                                     text=True)\n",
        "        if picrust_check.returncode != 0:\n",
        "            raise RuntimeError(\"picrust2_pipeline.py not found. Please ensure PICRUSt2 is properly installed.\")\n",
        "\n",
        "        # Create output directory\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # Construct command as a single string\n",
        "        cmd = f\"picrust2_pipeline.py -s {fasta_file} -i {fasta_file} -o {output_dir} --processes 1 --verbose\"\n",
        "\n",
        "        # Run pipeline\n",
        "        print(f\"Running command: {cmd}\")\n",
        "        process = subprocess.run(cmd,\n",
        "                               shell=True,  # Use shell to handle command string\n",
        "                               check=True,\n",
        "                               capture_output=True,\n",
        "                               text=True)\n",
        "\n",
        "        print(\"PICRUSt2 Output:\")\n",
        "        print(process.stdout)\n",
        "\n",
        "        if process.stderr:\n",
        "            print(\"Warnings/Errors:\")\n",
        "            print(process.stderr)\n",
        "\n",
        "        # Add descriptions if pathway file exists\n",
        "        pathway_file = os.path.join(output_dir, 'pathways_out/path_abun_unstrat.tsv.gz')\n",
        "        if os.path.exists(pathway_file):\n",
        "            desc_cmd = f\"add_descriptions.py -i {pathway_file} -m PATHWAY -o {os.path.join(output_dir, 'pathways_with_descriptions.tsv')}\"\n",
        "            subprocess.run(desc_cmd, shell=True, check=True)\n",
        "\n",
        "        print(f\"PICRUSt2 pipeline completed successfully for {output_dir}\")\n",
        "        return True\n",
        "\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Error running PICRUSt2 command: {e}\")\n",
        "        print(f\"Command output: {e.output}\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"Error in pipeline: {str(e)}\")\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "execution": {
          "iopub.execute_input": "2025-02-19T09:42:06.953585Z",
          "iopub.status.busy": "2025-02-19T09:42:06.953173Z",
          "iopub.status.idle": "2025-02-19T09:42:06.976246Z",
          "shell.execute_reply": "2025-02-19T09:42:06.975194Z",
          "shell.execute_reply.started": "2025-02-19T09:42:06.953546Z"
        },
        "id": "Hbhd5NNkCqSX",
        "outputId": "52763e43-52c6-4bfb-df68-c790f58b65d8",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "'''# For original sequences\n",
        "aligned_file = aligned_fasta\n",
        "output_dir = Path(\"~MIC/2_Micro/data_picrust/original_results\")\n",
        "success = run_picrust2_pipeline(aligned_file, output_dir)\n",
        "\n",
        "# For improved sequences\n",
        "optimized_file = Path(\"~/MIC/2_Micro/data_picrust/picrust_optimized_sequences.fasta\")\n",
        "optimized_output = Path(\"~/MIC/2_Micro/data_picrust/optimized_results\")\n",
        "success_opt = run_picrust2_pipeline(optimized_file, optimized_output)'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EDOctt8uvgP"
      },
      "source": [
        "# 7. Findings and Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXu7ACOAuvgP"
      },
      "source": [
        "The PICRUSt2 pipeline generated a series of interconnected files revealing the functional potential of the microbial community. These files collectively map metabolic pathways, enzymatic functions, and taxonomic relationships, providing a multi-layered view of microbial functional capabilities across samples. Detailed view of the files found in the folder ~data_picrust are located in the manuscript."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Se5YK4UfuvgP"
      },
      "source": [
        "Picrust_Result_SEPP and Picrust_Result_EPA contain the descriptions, pathways and abundance of the full pipeline of picrust."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "execution": {
          "iopub.execute_input": "2025-02-19T09:42:06.977754Z",
          "iopub.status.busy": "2025-02-19T09:42:06.977416Z",
          "iopub.status.idle": "2025-02-19T09:42:07.065439Z",
          "shell.execute_reply": "2025-02-19T09:42:07.064304Z",
          "shell.execute_reply.started": "2025-02-19T09:42:06.977725Z"
        },
        "id": "BZ8njtbRuvgP",
        "outputId": "482b820c-83c2-4c44-c93c-dd3677b78ba0",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "MetaCyc_EPA_path = base_dir / \"Galaxy19_PICRUSt2_Add_descriptions_on_data_8.tabular\"\n",
        "Picrust_Result= pd.read_csv(MetaCyc_EPA_path, sep = \"\\t\")\n",
        "Picrust_Result_EPA= pd.read_csv(MetaCyc_EPA_path, sep = \"\\t\")\n",
        "Picrust_Result_EPA.set_index(\"description\", inplace=True)\n",
        "Picrust_Result_EPA = Picrust_Result_EPA.drop(\"pathway\", axis=1)\n",
        "Picrust_Result_EPA.index.name = \"pathway\"\n",
        "MetaCyc_SEPP_path = Path(base_dir / \"Galaxy35_Add_descriptions_SEPP.tabular\")\n",
        "Picrust_Result_SEPP= pd.read_csv(MetaCyc_SEPP_path, sep = \"\\t\")\n",
        "Picrust_Result_SEPP.set_index(\"description\", inplace=True)\n",
        "Picrust_Result_SEPP = Picrust_Result_SEPP.drop(\"pathway\", axis=1)\n",
        "Picrust_Result_SEPP.index.name = \"pathway\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H39IXM7-uvgP"
      },
      "source": [
        "## 7.1. Placement Algorithm EPA vs SEPP\n",
        "nsti_SEPP and nsti_EPA Corresponds to a sample-wide measure of how closely related the microbial taxa in that sample are to known reference genomes with two different placement algoritms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "execution": {
          "iopub.execute_input": "2025-02-19T09:42:07.067061Z",
          "iopub.status.busy": "2025-02-19T09:42:07.066653Z",
          "iopub.status.idle": "2025-02-19T09:42:07.094101Z",
          "shell.execute_reply": "2025-02-19T09:42:07.092827Z",
          "shell.execute_reply.started": "2025-02-19T09:42:07.067031Z"
        },
        "id": "TW5Je0oouvgP",
        "outputId": "a3ea2b45-a0f7-437c-bf7b-03716674d1cd",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "nsti_path_EPA = Path(base_dir  / \"Galaxy13_EC_weighted_nsti.tabular\")\n",
        "nsti_EPA= pd.read_csv(nsti_path_EPA, sep = \"\\t\")\n",
        "nsti_path_SEPP = Path(base_dir  / \"Galaxy20_EC_weighted_nsti_SEPP.tabular\")\n",
        "nsti_SEPP= pd.read_csv(nsti_path_SEPP, sep = \"\\t\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T09:42:07.095532Z",
          "iopub.status.busy": "2025-02-19T09:42:07.095203Z",
          "iopub.status.idle": "2025-02-19T09:42:07.861889Z",
          "shell.execute_reply": "2025-02-19T09:42:07.860659Z",
          "shell.execute_reply.started": "2025-02-19T09:42:07.095491Z"
        },
        "id": "ESTXja6nuvgP",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Create the scatter plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(nsti_EPA['sample'], nsti_EPA['weighted_NSTI'], alpha=0.5, label= \"EPA\", color=\"blue\")\n",
        "plt.scatter(nsti_SEPP['sample'], nsti_SEPP['weighted_NSTI'], alpha=0.5, label= \"SEPP\", color=\"gray\")\n",
        "\n",
        "# Add the threshold line\n",
        "plt.axhline(y=0.15, color='black', linestyle='--', label='Threshold (0.15)')\n",
        "\n",
        "# Customize the plot\n",
        "plt.title('NSTI Values by Site')\n",
        "plt.xlabel('Site')\n",
        "plt.ylabel('NSTI Value')\n",
        "plt.legend()\n",
        "\n",
        "# Rotate x-axis labels if there are many samples\n",
        "plt.xticks(rotation=90)\n",
        "\n",
        "# Adjust layout and display the plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_fb468duvgP"
      },
      "source": [
        "Interestingly, the results are no as expected, it was though that the algorithm for placing the sequences more convenient for the present samples was SEPP because it is design specially for 16sRNA samples and diverse microbios communities, however the samples show another story. I fail to realise that the present data has been validated with the greenes genes database with the purpose of finding more compatibility with the picrust2 database, and therefore the EPA algoritm is performing much better on the all of samples using EPA placement algoritm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUPl-r2huvgQ"
      },
      "source": [
        "## 7.2. Explore Pathway Patterns\n",
        "The pathway analysis strategy is to do a preliminar exploration before diving into specific hypotheses about organic matter metabolism and corrosion. It was chosen to start with unbiased exploratory data analysis of the PICRUSt pathways. The aim is to let the data reveal natural patterns without preconceptions. That helps to identify unexpected relationships between pathways, providing a baseline understanding of pathway distributions and relationships. This will guide subsequent targeted analyses of corrosion-relevant pathways.\n",
        "The following script takes multiple perspectives in order to visualise the data without bias and let it reveal itself. We do PCA for linear patterns, NMF for modular organization, UMAP for non-linear relationships and take different clustering approaches. The aim being to look for natural Patterns without predefined categories, so that strong strong correlations can be identified regardless of pathway type. It is visualised the distribution of pathway abundances, correlation structure, hierarchical relationships and non-linear patterns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfLGAWTXuvgQ"
      },
      "source": [
        "__Category Dict__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T09:42:07.863417Z",
          "iopub.status.busy": "2025-02-19T09:42:07.863112Z",
          "iopub.status.idle": "2025-02-19T09:42:07.870061Z",
          "shell.execute_reply": "2025-02-19T09:42:07.868875Z",
          "shell.execute_reply.started": "2025-02-19T09:42:07.863391Z"
        },
        "id": "bSQwysg7uvgQ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Define category dict outside so that all charts can use same dict\n",
        "\n",
        "category_dict = Integrated_T.T.iloc[0, 0:-1].to_dict()\n",
        "\n",
        "# Define colors and categories\n",
        "category_colors = {1: '#008800',  # Dark green\n",
        "                   2: '#FF8C00',  # Dark orange\n",
        "                   3: '#FF0000'}   # Red\n",
        "\n",
        "categories_labels = {1: 'Normal Operation',\n",
        "              2: 'Early Warning',\n",
        "              3: 'System Failure'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T09:42:07.871445Z",
          "iopub.status.busy": "2025-02-19T09:42:07.871137Z",
          "iopub.status.idle": "2025-02-19T09:42:07.89862Z",
          "shell.execute_reply": "2025-02-19T09:42:07.897406Z",
          "shell.execute_reply.started": "2025-02-19T09:42:07.871405Z"
        },
        "id": "TvoGFdS5uvgQ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def explore_pathway_patterns(df):\n",
        "    \"\"\"\n",
        "    Explore pathway patterns using multiple analytical approaches\n",
        "    \"\"\"\n",
        "    # Standardize data\n",
        "    scaler = StandardScaler()\n",
        "    scaled_data = scaler.fit_transform(df)\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    def plot_exploration_results(df, results, category_dict, category_colors, categories_labels):\n",
        "        \"\"\"\n",
        "        Create visualizations for the exploratory analysis with consistent category colors\n",
        "        \"\"\"\n",
        "        # 1. Distribution of pathway abundances with category colors - side by side\n",
        "        plt.figure(figsize=(12, 6))\n",
        "\n",
        "        # Create dictionary to store abundances by category\n",
        "        category_abundances = {cat_id: [] for cat_id in categories_labels.keys()}\n",
        "\n",
        "        # Group abundances by category\n",
        "        for site_col in df.columns:\n",
        "            if site_col.startswith('site_'):\n",
        "                site_num = int(site_col.split('_')[1])\n",
        "                category = category_dict.get(f'site_{site_num}', 0)\n",
        "                if category in category_abundances:\n",
        "                    category_abundances[category].extend(df[site_col].values)\n",
        "\n",
        "        # Plot distribution for each category side by side\n",
        "        for category_id in categories_labels.keys():\n",
        "            sns.histplot(data=category_abundances[category_id],\n",
        "                        bins=50,\n",
        "                        color=category_colors[category_id],\n",
        "                        label=categories_labels[category_id],\n",
        "                        alpha=0.6,\n",
        "                        multiple=\"layer\")\n",
        "\n",
        "        plt.title('Distribution of Pathway Abundances by Category')\n",
        "        plt.xlabel('Abundance')\n",
        "        plt.yscale('log')\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    # 2. PCA Dimensionality Reduction\n",
        "    pca = PCA(n_components=5)\n",
        "    X_pca = pca.fit_transform(scaled_data)\n",
        "    results['pca'] = {\n",
        "        'components': X_pca,\n",
        "        'explained_variance': pca.explained_variance_ratio_,\n",
        "        'loadings': pd.DataFrame(\n",
        "            pca.components_.T,\n",
        "            index=df.columns,\n",
        "            columns=[f'PC{i+1}' for i in range(5)])}\n",
        "\n",
        "    # 3 NMF for pathway modules\n",
        "    nmf = NMF(n_components=5, init='random', random_state=0, max_iter=400)\n",
        "    W = nmf.fit_transform(df.clip(lower=0))\n",
        "    H = nmf.components_\n",
        "    results['nmf'] = {\n",
        "        'W': pd.DataFrame(W, index=df.index, columns=[f'NMF{i+1}' for i in range(5)]), # Pathway contributions\n",
        "        'H': pd.DataFrame(H, columns=df.columns, index=[f'NMF{i+1}' for i in range(5)]), # Sample patterns\n",
        "        'reconstruction_err': nmf.reconstruction_err_ }\n",
        "\n",
        "    # 4 UMAP for non-linear patterns\n",
        "    umap_reducer = umap.UMAP(random_state=0)\n",
        "    umap_result = umap_reducer.fit_transform(scaled_data)\n",
        "    results['umap'] = pd.DataFrame(umap_result, index=df.index, columns=['UMAP1', 'UMAP2'])\n",
        "\n",
        "    # 5. Multiple Clustering Approaches / Hierarchical clustering\n",
        "    linkage_matrix = hierarchy.linkage(scaled_data, method='ward')\n",
        "\n",
        "    # Try different numbers of clusters\n",
        "    cluster_results = {}\n",
        "    for n_clusters in [5, 10, 15]:\n",
        "        # Hierarchical\n",
        "        hc = AgglomerativeClustering(n_clusters=n_clusters)\n",
        "        hc_labels = hc.fit_predict(scaled_data)\n",
        "\n",
        "        # K-means\n",
        "        km = KMeans(n_clusters=n_clusters, random_state=0)\n",
        "        km_labels = km.fit_predict(scaled_data)\n",
        "\n",
        "        cluster_results[n_clusters] = {'hierarchical': pd.Series(hc_labels, index=df.index, name='cluster'),\n",
        "            'kmeans': pd.Series(km_labels, index=df.index, name='cluster')}\n",
        "\n",
        "    results['clustering'] = cluster_results\n",
        "    results['linkage'] = linkage_matrix\n",
        "\n",
        "    # 4. Correlation Analysis/Spearman correlation for non-linear relationships\n",
        "    corr_matrix = spearmanr(df.T)[0]\n",
        "    results['correlation'] = pd.DataFrame(corr_matrix, index=df.index, columns=df.index)\n",
        "\n",
        "    return results, X_pca\n",
        "\n",
        "def plot_exploration_results(df, results, category_dict, category_colors, categories_labels):\n",
        "    \"\"\"\n",
        "    Create visualizations for the exploratory analysis with colored categories_labels\n",
        "    \"\"\"\n",
        "    # Modified PCA visualization with categories\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # Create subplots\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(range(1, 6), results['pca']['explained_variance'], 'bo-')\n",
        "    plt.title('PCA Explained Variance')\n",
        "    plt.xlabel('Component')\n",
        "    plt.ylabel('Explained Variance Ratio')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "\n",
        "    # Get PCA components\n",
        "    pca_data = results['pca']['components']\n",
        "\n",
        "    # Plot each category separately to create the legend\n",
        "    for category_id in categories_labels.keys():\n",
        "        # Get indices for current category\n",
        "        category_mask = [category_dict.get(f'site_{i+1}', 0) == category_id\n",
        "                        for i in range(len(pca_data))]\n",
        "\n",
        "        # Plot points for current category\n",
        "        plt.scatter(pca_data[category_mask, 0],\n",
        "                   pca_data[category_mask, 1],\n",
        "                   c=category_colors[category_id],\n",
        "                   label=categories_labels[category_id],\n",
        "                   alpha=0.6)\n",
        "\n",
        "    plt.title('PCA First Two Components')\n",
        "    plt.xlabel('PC1')\n",
        "    plt.ylabel('PC2')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    # 3. UMAP visualization with categories\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    umap_df = results['umap']\n",
        "\n",
        "    for category_id in categories_labels.keys():\n",
        "        category_mask = [category_dict.get(f'site_{i+1}', 0) == category_id\n",
        "                        for i in range(len(umap_df))]\n",
        "        category_data = umap_df[category_mask]\n",
        "\n",
        "        plt.scatter(category_data['UMAP1'],\n",
        "                   category_data['UMAP2'],\n",
        "                   c=category_colors[category_id],\n",
        "                   label=categories_labels[category_id],\n",
        "                   alpha=0.6)\n",
        "\n",
        "    plt.title('UMAP Projection of Pathways by Category')\n",
        "    plt.xlabel('UMAP1')\n",
        "    plt.ylabel('UMAP2')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # 4. Hierarchical clustering dendrogram - simplified version\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    dendrogram = hierarchy.dendrogram(\n",
        "        results['linkage'],\n",
        "        labels=df.index,  # Use index , columns instead of index\n",
        "        leaf_rotation=90,\n",
        "        leaf_font_size=8\n",
        "    )\n",
        "    plt.title('Pathway Clustering Dendrogram')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # 5. Correlation heatmap\n",
        "    plt.figure(figsize=(12, 12))\n",
        "    mask = np.triu(np.ones_like(results['correlation']))\n",
        "\n",
        "    # Create a custom colormap that uses your category colors\n",
        "    custom_cmap = sns.diverging_palette(220, 20, as_cmap=True)\n",
        "\n",
        "    sns.heatmap(results['correlation'],\n",
        "                mask=mask,\n",
        "                cmap=custom_cmap,\n",
        "                center=0,\n",
        "                vmin=-1,\n",
        "                vmax=1)\n",
        "    plt.title('Pathway Correlation Heatmap')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def identify_key_patterns(df, results):\n",
        "    \"\"\"\n",
        "    Identify and summarize key patterns in the data\n",
        "    \"\"\"\n",
        "    patterns = {}\n",
        "\n",
        "    # Find highly correlated pathway groups\n",
        "    corr = results['correlation']\n",
        "    high_corr = pd.DataFrame(\n",
        "        [(i, j, corr.loc[i,j])\n",
        "         for i in corr.index\n",
        "         for j in corr.index\n",
        "         if i < j and abs(corr.loc[i,j]) > 0.8],\n",
        "        columns=['pathway1', 'pathway2', 'correlation']\n",
        "    ).sort_values('correlation', ascending=False)\n",
        "\n",
        "    # Find pathways with strong PCA loadings\n",
        "    loadings = results['pca']['loadings']\n",
        "    strong_loadings = pd.DataFrame({\n",
        "        'PC1_contribution': abs(loadings['PC1']),\n",
        "        'PC2_contribution': abs(loadings['PC2'])\n",
        "    }).sort_values('PC1_contribution', ascending=False)\n",
        "\n",
        "    patterns['high_correlations'] = high_corr\n",
        "    patterns['strong_loadings'] = strong_loadings\n",
        "\n",
        "    return patterns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T09:42:07.900108Z",
          "iopub.status.busy": "2025-02-19T09:42:07.899753Z",
          "iopub.status.idle": "2025-02-19T09:42:26.644907Z",
          "shell.execute_reply": "2025-02-19T09:42:26.643394Z",
          "shell.execute_reply.started": "2025-02-19T09:42:07.900067Z"
        },
        "id": "TrS9Ii_GuvgQ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Calling the function for the pipeline using EPA algoritm\n",
        "results_SEPP, X_pca_SEPP = explore_pathway_patterns(Picrust_Result_SEPP)\n",
        "plot_exploration_results(Picrust_Result_SEPP, results_SEPP, category_dict, category_colors, categories_labels)\n",
        "patterns_SEPP = identify_key_patterns(Picrust_Result_SEPP, results_SEPP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T09:42:26.646674Z",
          "iopub.status.busy": "2025-02-19T09:42:26.646265Z"
        },
        "id": "EDmJPqM7uvgQ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Calling the function for the pipeline using EPA algoritm\n",
        "results_EPA, X_pca_EPA = explore_pathway_patterns(Picrust_Result_EPA)\n",
        "plot_exploration_results(Picrust_Result_EPA, results_EPA, category_dict, category_colors, categories_labels)\n",
        "patterns_EPA = identify_key_patterns(Picrust_Result_EPA, results_EPA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R59lKZ60uvgQ"
      },
      "source": [
        "__Discussing first results__\n",
        "\n",
        "The distribution of pathway abundances shows a typical microbial community pattern with few dominant pathways, suggesting key metabolic processes are essential across samples. PCA analysis reveals that only two components explain over 80% of the variance, indicating that metabolism in these systems might be driven by two major functional groups. The UMAP visualization confirms this binary pattern through two distinct clusters, demonstrating the robustness of this separation across different dimensional reduction techniques. The hierarchical clustering dendrogram further validates this division by showing two major branches, which notably align with previously observed physicochemical patterns in our Pourbaix plot analysis. The correlation heatmap exhibits strong relationships between specific pathway groups, suggesting coordinated metabolic activities that require detailed pathway mapping for full biological interpretation. EPA sequence placement shows much better differenciation on the pc plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T09:50:52.239156Z",
          "iopub.status.busy": "2025-02-19T09:50:52.238781Z",
          "iopub.status.idle": "2025-02-19T09:50:52.264494Z",
          "shell.execute_reply": "2025-02-19T09:50:52.263075Z",
          "shell.execute_reply.started": "2025-02-19T09:50:52.23913Z"
        },
        "id": "C9qoJdFfYel2",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "Picrust_Result.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RI1NL6_uuvgQ"
      },
      "source": [
        "## 7.3. Distribution of pathway abundances and Heatmap Hierarchies\n",
        "In the following script we map the column pathway on the dataframe Picrust_Result_raw to the actual names provided by the Galaxy website that corresponds to the MetaCyc pathways. We will end up with the original Picrust_Results df with disernible names.After the 20 most abundant pathways will be plotted and the heatmap with the hierarchichal pathways drawn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T09:50:57.989611Z",
          "iopub.status.busy": "2025-02-19T09:50:57.98922Z",
          "iopub.status.idle": "2025-02-19T09:50:57.996428Z",
          "shell.execute_reply": "2025-02-19T09:50:57.995153Z",
          "shell.execute_reply.started": "2025-02-19T09:50:57.98958Z"
        },
        "id": "Zczh30NruvgQ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Define category dict outside so that all charts can use same dict\n",
        "\n",
        "category_dict = Integrated_T.T.iloc[0, 0:-1].to_dict()\n",
        "\n",
        "# Define colors and categories\n",
        "category_colors = {1: '#008800',  # Dark green\n",
        "                   2: '#FF8C00',  # Dark orange\n",
        "                   3: '#FF0000'}   # Red\n",
        "\n",
        "categories_labels = {1: 'Normal Operation',\n",
        "              2: 'Early Warning',\n",
        "              3: 'System Failure'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T09:51:10.614855Z",
          "iopub.status.busy": "2025-02-19T09:51:10.614412Z",
          "iopub.status.idle": "2025-02-19T09:51:12.397866Z",
          "shell.execute_reply": "2025-02-19T09:51:12.39672Z",
          "shell.execute_reply.started": "2025-02-19T09:51:10.614824Z"
        },
        "id": "praqb_f6uvgQ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def analyze_pathway_patterns(df, mean_abundances, category_dict, top_n=20):\n",
        "    \"\"\"\n",
        "    Create two separate visualizations for pathway analysis:\n",
        "    1. Stacked bar chart of top pathways by system state\n",
        "    2. Correlation heatmap of top pathways\n",
        "\n",
        "    Parameters:\n",
        "    df: DataFrame with pathway data\n",
        "    mean_abundances: Series with pre-calculated mean abundances\n",
        "    category_dict: Dictionary mapping sites to risk categories\n",
        "    top_n: Number of top pathways to display\n",
        "    \"\"\"\n",
        "    # Get top pathways\n",
        "    top_pathways = mean_abundances.nlargest(top_n)\n",
        "\n",
        "    # 1. Stacked Bar Chart\n",
        "    plt.figure(figsize=(15, 8))\n",
        "\n",
        "    # Prepare data for stacking\n",
        "    pathway_data = []\n",
        "    for pathway in top_pathways.index:\n",
        "        cat_means = {}\n",
        "        for cat in [1, 2, 3]:\n",
        "            cat_sites = [site for site, c in category_dict.items() if c == cat]\n",
        "            if cat_sites:\n",
        "                cat_means[cat] = df.loc[pathway, cat_sites].mean()\n",
        "            else:\n",
        "                cat_means[cat] = 0\n",
        "        pathway_data.append((pathway, cat_means))\n",
        "\n",
        "    # Create stacked bars\n",
        "    bottoms = np.zeros(len(top_pathways))\n",
        "    for cat in [1, 2, 3]:\n",
        "        values = [d[1][cat] for d in pathway_data]\n",
        "        plt.bar(range(len(top_pathways)), values, bottom=bottoms,\n",
        "                label=categories_labels[cat], color=category_colors[cat], alpha=0.7)\n",
        "        bottoms += values\n",
        "\n",
        "    plt.title('Top 20 Most Abundant Pathways by System State', fontsize=14, pad=20)\n",
        "    plt.xlabel('Pathway', fontsize=12)\n",
        "    plt.ylabel('Mean Abundance', fontsize=12)\n",
        "    plt.xticks(range(len(top_pathways)), top_pathways.index,\n",
        "               rotation=45, ha='right', fontsize=10)\n",
        "    plt.legend(title='System State', title_fontsize=12, fontsize=10)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # 2. Correlation Heatmap (separate figure)\n",
        "    plt.figure(figsize=(15, 12))\n",
        "    top_data = df.loc[top_pathways.index]\n",
        "    corr = top_data.T.corr()\n",
        "\n",
        "    # Create mask for upper triangle\n",
        "    mask = np.triu(np.ones_like(corr), k=1)\n",
        "\n",
        "    # Create heatmap with improved readability\n",
        "    sns.heatmap(corr,\n",
        "                mask=mask,\n",
        "                cmap='coolwarm',\n",
        "                center=0,\n",
        "                annot=True,\n",
        "                fmt='.2f',\n",
        "                square=True,\n",
        "                cbar_kws={'label': 'Correlation Coefficient'},\n",
        "                annot_kws={'size': 8})\n",
        "\n",
        "    plt.title('Pathway Correlation Heatmap\\n(Top 20 Most Abundant)',\n",
        "              fontsize=14,\n",
        "              pad=20)\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return corr, top_data\n",
        "\n",
        "#Calculate mean abundances and run analysis\n",
        "mean_abundances_epa = Picrust_Result_EPA.mean(axis=1)\n",
        "corr_epa, top_data = analyze_pathway_patterns(Picrust_Result_EPA, mean_abundances_epa, category_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meleGILNuvgR"
      },
      "source": [
        "__Discussing the 20 biggest metabolisms and their Hierarchical Heatmap__\n",
        "The metabolic pathway analysis reveals aerobic respiration as the dominant metabolism, showing approximately 75% higher abundance than other pathways across all systems. Correlation analysis highlights strong relationships between aerobic respiration and key metabolic processes, including TCA cycles and amino acid biosynthesis pathways, particularly those involved in biofilm formation. While these patterns provide insights into the overall metabolic landscape, a more detailed analysis separating corroded and non-corroded systems, along with integration of physicochemical variables and risk labels, would be necessary for actionable conclusions about corrosion processes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LmHp51DYel2"
      },
      "source": [
        "## 7.4. Distribution of Reactions abundances and Heatmap Hierarchies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T09:58:35.11704Z",
          "iopub.status.busy": "2025-02-19T09:58:35.116608Z",
          "iopub.status.idle": "2025-02-19T09:58:35.177891Z",
          "shell.execute_reply": "2025-02-19T09:58:35.176717Z",
          "shell.execute_reply.started": "2025-02-19T09:58:35.117011Z"
        },
        "id": "PPFMKGrgYel2",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#parsing pathways (PWY) to the reactions (RXN), parce has a single column with 575 rows, that will mean that the patways can be more than once with different reactions\n",
        "parce_path = Path(base_dir / \"Galaxy17_parsed_mapfile.tabular\")\n",
        "parce= pd.read_csv(parce_path, sep = \"\\t\")\n",
        "# reaction is a regroup file comprises the list of reactions in the index and the sites with abundances, similar to the pathways with abundances master file\n",
        "# whiles pathways has 366 rows (pathway), react has 2956 rows(reactions)\n",
        "react_path =Path(base_dir / \"Galaxy18_regrouped_infile.tabular\")\n",
        "react= pd.read_csv(react_path, sep = \"\\t\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDPL2I7v65sa"
      },
      "outputs": [],
      "source": [
        "# 1. Set 'function' as index and convert to string\n",
        "react = react.set_index(\"function\")\n",
        "react.index = react.index.astype(str)\n",
        "# 1. Parse 'parce' (Corrected - Handle multi-line entries, hyphens, and missing matches)\n",
        "parce_mapping = {}\n",
        "current_entry = \"\"\n",
        "for line in parce.iloc[:, 0]:  # Iterate through the first column (handling multi-line entries)\n",
        "    if line.strip():  # Check if the line is not empty or just whitespace\n",
        "        current_entry += line.strip() + \" \"\n",
        "    else:  # Empty line indicates the end of an entry\n",
        "        if current_entry:  # Check if we have accumulated an entry\n",
        "            reactions = current_entry.split(\"-\")  # Split by hyphens\n",
        "            for reaction in reactions:\n",
        "                match = parce.index[parce.iloc[:, 0] == current_entry.strip()].tolist()\n",
        "                if match:  # Check if a match was found\n",
        "                    parce_mapping[reaction.strip()] = match[0]  # Get the first match\n",
        "            current_entry = \"\"\n",
        "if current_entry: #for the last entry\n",
        "    reactions = current_entry.split(\"-\")  # Split by hyphens\n",
        "    for reaction in reactions:\n",
        "        match = parce.index[parce.iloc[:, 0] == current_entry.strip()].tolist()\n",
        "        if match:  # Check if a match was found\n",
        "            parce_mapping[reaction.strip()] = match[0]  # Get the first match\n",
        "    current_entry = \"\"\n",
        "\n",
        "# 2. Function to selectively replace names (Corrected and Robust)\n",
        "def replace_name(name):\n",
        "    if name in parce_mapping:\n",
        "        return parce_mapping[name]\n",
        "\n",
        "    if isinstance(name, str):\n",
        "        for key, value in parce_mapping.items():\n",
        "            if isinstance(key, str) and key in name:\n",
        "                return value\n",
        "    return name\n",
        "\n",
        "# 3. Apply the function to the index of 'react'\n",
        "react.index = react.index.map(replace_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q82BFL_--F_D",
        "outputId": "b6595344-3892-41fa-e038-defcd21c234c"
      },
      "outputs": [],
      "source": [
        "react.index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "execution": {
          "iopub.execute_input": "2025-02-19T09:59:04.293272Z",
          "iopub.status.busy": "2025-02-19T09:59:04.292801Z",
          "iopub.status.idle": "2025-02-19T09:59:04.382736Z",
          "shell.execute_reply": "2025-02-19T09:59:04.380879Z",
          "shell.execute_reply.started": "2025-02-19T09:59:04.293235Z"
        },
        "id": "A7kaHJFkYel2",
        "outputId": "0798f869-75d6-40e1-e1b0-83162784791b",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def analyze_pathway_patterns(df, mean_abundances, category_dict, top_n=20):\n",
        "    \"\"\"\n",
        "    Analyzes pathway patterns for a DataFrame with 'function' as index and 'Sites' as columns.\n",
        "    This is the FINAL, CORRECTED implementation.\n",
        "    \"\"\"\n",
        "    top_functions = mean_abundances.nlargest(top_n)\n",
        "    df.index = df.index.astype(str)\n",
        "\n",
        "    # 1. Stacked Bar Chart\n",
        "    plt.figure(figsize=(15, 8))\n",
        "\n",
        "    function_data = []\n",
        "    for function in top_functions.index:\n",
        "        cat_means = {}\n",
        "        for cat in [1, 2, 3]:\n",
        "            cat_sites = [site for site, c in category_dict.items() if c == cat]\n",
        "            # Optimized site selection:\n",
        "            relevant_sites = list(df.columns.intersection(cat_sites)) # More efficient intersection\n",
        "            if relevant_sites:\n",
        "                cat_means[cat] = df.loc[function, relevant_sites].mean()\n",
        "            else:\n",
        "                cat_means[cat] = 0\n",
        "        function_data.append((function, cat_means))\n",
        "\n",
        "    bottoms = np.zeros(len(top_functions))\n",
        "    for cat in [1, 2, 3]:\n",
        "        values = [d[1][cat] for d in function_data]\n",
        "        plt.bar(range(len(top_functions)), values, bottom=bottoms,\n",
        "                label=categories_labels[cat], color=category_colors[cat], alpha=0.7)\n",
        "        bottoms += values\n",
        "\n",
        "    plt.title('Top 20 Most Abundant Functions by System State', fontsize=14, pad=20)\n",
        "    plt.xlabel('Function', fontsize=12)\n",
        "    plt.ylabel('Mean Abundance', fontsize=12)\n",
        "    plt.xticks(range(len(top_functions)), top_functions.index, rotation=45, ha='right', fontsize=10)\n",
        "    plt.legend(title='System State', title_fontsize=12, fontsize=10)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # 2. Correlation Heatmap\n",
        "    plt.figure(figsize=(15, 12))\n",
        "    top_data = df.loc[top_functions.index]\n",
        "    # Convert all columns of top_data to numeric, coercing errors to NaN\n",
        "    top_data = top_data.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "    # Drop rows with any NaN values to ensure only numeric data is used for correlation\n",
        "    top_data = top_data.dropna(axis=1, how='all')\n",
        "    # Check if there are any columns left after dropping NaNs\n",
        "    if top_data.empty:\n",
        "        print(\"Warning: DataFrame is empty after dropping NaN columns. Skipping correlation heatmap.\")\n",
        "        return None  # Or return an empty DataFrame or a placeholder\n",
        "\n",
        "    corr = top_data.T.corr()  # Transpose for function correlation\n",
        "\n",
        "    mask = np.triu(np.ones_like(corr), k=1)  # Mask for upper triangle\n",
        "    sns.heatmap(corr, mask=mask, cmap='coolwarm', center=0, annot=True, fmt='.2f',\n",
        "                square=True, cbar_kws={'label': 'Correlation Coefficient'}, annot_kws={'size': 8})\n",
        "\n",
        "    plt.title('Function Correlation Heatmap\\n(Top 20 Most Abundant)', fontsize=14, pad=20)\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return corr, top_data\n",
        "\n",
        "# Convert numeric columns to the correct data type\n",
        "for col in react.columns[1:]:  # Exclude 'function' column\n",
        "    try:\n",
        "        react[col] = pd.to_numeric(react[col], errors='coerce') # Skip errors but convert rest\n",
        "    except ValueError:\n",
        "        print(f\"Could not convert column '{col}' to numeric. Check its contents.\")\n",
        "        # Handle the error or investigate the column for non-numeric values\n",
        "\n",
        "# Calculate the mean after type conversion\n",
        "mean_abundances_react = react.mean(axis=1, numeric_only=True) # Specify only numeric in case strings remain\n",
        "corr_react, top_data = analyze_pathway_patterns(react, mean_abundances_react, category_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0Sbd6A7uvgR"
      },
      "source": [
        "# 8. Mapping the Pathways back to the Genera\n",
        "\n",
        "The result we obtained from the picrust pipeline contain the following dataframes, here described so it would be possible to parse. Following are the files description with the shape\n",
        "| Picrust_Result | Picrust_Result | Picrust_Result | parce | parce | parce | ECcontri | ECcontri | ECcontri | ECcontri | React | React |\n",
        "|---|---|---|---|---|---|---|---|---|---|---|---|\n",
        "| pathway | description | Sites/abund | pathway | RXN | EC number | EC number | varios abundances | Sites | OTU | Sites/abund | Reactions |\n",
        "|366,72|366,72|366,72|574,1|574,1|574,1|1491288, 9|1491288, 9|1491288, 9|1491288, 9| (2955, 71)|(2955, 71)|\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T09:51:24.070313Z",
          "iopub.status.busy": "2025-02-19T09:51:24.06992Z",
          "iopub.status.idle": "2025-02-19T09:51:26.166719Z",
          "shell.execute_reply": "2025-02-19T09:51:26.165435Z",
          "shell.execute_reply.started": "2025-02-19T09:51:24.070284Z"
        },
        "id": "RGMCpOrRuvgR",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "EC_path = Path(base_dir / \"Galaxy9-[EC_T].tabular\")\n",
        "EC= pd.read_csv(EC_path, sep = \"\\t\")\n",
        "# ECcontri and KOcontri files contain sample, function (EC/KO number), taxon (genus/OTU ID), and abundance metrics.\n",
        "ECcontri_path =  Path(large_dir / \"Galaxy26-[EC_pred_metagenome_contrib].tabular\") # for VSCODE\n",
        "\n",
        "#ECcontri_path =  Path(base_dir / \"Galaxy26_contrib.tabular\") # for Kaggle\n",
        "ECcontri= pd.read_csv(ECcontri_path, sep = \"\\t\")\n",
        "#KOcontri_path = Path(large_dir / \"Galaxy30-[KO_pred_metagenome_contrib].tabular\")\n",
        "#KOcontri= pd.read_csv(KOcontri_path, sep = \"\\t\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HB57hdTiuvgR"
      },
      "source": [
        "## 8.1. Mapping Genera to Otu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T09:51:27.806009Z",
          "iopub.status.busy": "2025-02-19T09:51:27.80564Z",
          "iopub.status.idle": "2025-02-19T09:51:27.817542Z",
          "shell.execute_reply": "2025-02-19T09:51:27.81602Z",
          "shell.execute_reply.started": "2025-02-19T09:51:27.805982Z"
        },
        "id": "C_7z90_iuvgR",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Mapping the Genera to Otu for the Taxonomy assigment requeriment\n",
        "def create_otu_mapping(fasta_file_final):\n",
        "    \"\"\"Creates a DataFrame mapping OTUs to genera from a FASTA file\n",
        "    Args: fasta_file (str): Path to FASTA file\n",
        "    Returns: pd.DataFrame: DataFrame with columns ['Genus', 'OTU']\n",
        "    \"\"\"\n",
        "    mapping_data = []\n",
        "\n",
        "    for record in SeqIO.parse(fasta_file_final, \"fasta\"):\n",
        "        # Split description to get genus and OTU\n",
        "        parts = record.description.split()\n",
        "        genus = parts[0]\n",
        "        otu = parts[1]  # Take first OTU number\n",
        "\n",
        "        mapping_data.append({'Genus': genus,'OTU': otu})\n",
        "\n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame(mapping_data).sort_values('Genus')\n",
        "\n",
        "    return df\n",
        "\n",
        "otu_mapping = create_otu_mapping(fasta_file_final)\n",
        "# Change the name of the Otus since they using taxon\n",
        "otu_mapping = otu_mapping.rename(columns={\"OTU\" : \"taxon\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_HSFhUmuvgR"
      },
      "source": [
        "Now with the parce file where there is info for pathway, reactions and also it is the EC numbers, the EC numbers will be extracted because they are encoded inside the parce file. So that we can link the EC with the pathways in the ECcontri df. Precisely the stratified Pathway Abundance contributions (KO/EC + taxon + Taxon abundance +  others ) = KOcontri, ECcontri will be join by the taxon (which is same as the otus) with the file where is located the taxonomy Assigment = Otus + Genera Falta. Ultimately, the pathway descriptions file = Full pipipeline results(pathways + abundances) + description(human readable pathway) = Picrust_Result will also be join in order to make the visualisations, we doing that in steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SWsDEMsuvgR"
      },
      "source": [
        "### Map pathways to ECcontri\n",
        "\n",
        "Map pathways (Picrust_Result) to Parce → Ensures accurate EC-pathway links.   \n",
        "Map reactions (React) to Parce → Ensures correct RXN-pathway links.   \n",
        "Use these mappings to update ECcontri → Assign pathways to EC numbers.  \n",
        "Handle unmapped EC numbers → Keep them separate for review.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjaxRTlluvgR"
      },
      "source": [
        "## 8.2. Decomvoluting Parce :\n",
        "separating the contents of the parsing file parce df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-02-19T09:51:38.05516Z",
          "iopub.status.busy": "2025-02-19T09:51:38.054805Z",
          "iopub.status.idle": "2025-02-19T09:51:38.082039Z",
          "shell.execute_reply": "2025-02-19T09:51:38.080827Z",
          "shell.execute_reply.started": "2025-02-19T09:51:38.055132Z"
        },
        "id": "r7mUFRVcuvgR",
        "outputId": "74b50ebb-6525-41c1-d71f-2e7158c6f470",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Initialize lists\n",
        "pathways = []\n",
        "all_ec_numbers = []\n",
        "reactions = []\n",
        "\n",
        "# Parse parce file\n",
        "for line in parce.iloc[:, 0]:\n",
        "    parts = line.split(' ', 1)\n",
        "    if len(parts) == 2:\n",
        "        pathway = parts[0]\n",
        "        reaction_list = parts[1].strip().split()\n",
        "\n",
        "        # Find EC numbers\n",
        "        ec_nums = []\n",
        "        rxns = []\n",
        "\n",
        "        for rxn in reaction_list:\n",
        "            if rxn.count('.') == 3:\n",
        "                ec_part = rxn.split('-RXN')[0]\n",
        "                if all(p.replace('-','').isdigit() for p in ec_part.split('.')):\n",
        "                    ec_nums.append(ec_part)\n",
        "                    continue\n",
        "            rxns.append(rxn)\n",
        "\n",
        "        # Pad ec_nums list to always have 7 elements\n",
        "        ec_nums.extend([None] * (7 - len(ec_nums)))\n",
        "\n",
        "        # Add to lists\n",
        "        pathways.append(pathway)\n",
        "        all_ec_numbers.append(ec_nums)\n",
        "        reactions.append(' '.join(rxns))\n",
        "\n",
        "# Create DataFrame\n",
        "parce_reference = pd.DataFrame({\n",
        "    'pathway': pathways,\n",
        "    'EC1': [ecs[0] for ecs in all_ec_numbers],\n",
        "    'EC2': [ecs[1] for ecs in all_ec_numbers],\n",
        "    'EC3': [ecs[2] for ecs in all_ec_numbers],\n",
        "    'EC4': [ecs[3] for ecs in all_ec_numbers],\n",
        "    'EC5': [ecs[4] for ecs in all_ec_numbers],\n",
        "    'EC6': [ecs[5] for ecs in all_ec_numbers],\n",
        "    'EC7': [ecs[6] for ecs in all_ec_numbers],\n",
        "    'other_reactions': reactions\n",
        "})\n",
        "\n",
        "# Display first few rows\n",
        "print(\"First 5 rows of parce data with all EC numbers:\")\n",
        "print(parce_reference.head().to_string())\n",
        "\n",
        "# Print example of a pathway with many EC numbers\n",
        "print(\"\\nExample of pathway with many EC numbers:\")\n",
        "print(parce_reference[parce_reference['EC7'].notna()].iloc[0].to_string())\n",
        "\n",
        "#parce_reference.to_csv(\"~/MIC/2_Micro/data_picrust/EC_path_parce.csv\", index=False, sep=\"\\t\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhNsmPeouvgR"
      },
      "source": [
        "enriched_picrust"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-02-19T09:51:53.678217Z",
          "iopub.status.busy": "2025-02-19T09:51:53.677836Z",
          "iopub.status.idle": "2025-02-19T09:51:53.693849Z",
          "shell.execute_reply": "2025-02-19T09:51:53.692763Z",
          "shell.execute_reply.started": "2025-02-19T09:51:53.678186Z"
        },
        "id": "3FsuN4hYuvgR",
        "outputId": "ea3dec85-cc59-4637-bffc-7ea150c762ad",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Get list of pathways we need from Picrust_Result\n",
        "picrust_pathways = set(Picrust_Result['pathway'])\n",
        "\n",
        "# Filter parce_reference to only include needed pathways\n",
        "parced_picrust = parce_reference[parce_reference['pathway'].isin(picrust_pathways)]\n",
        "\n",
        "# Print statistics\n",
        "print(\"Matching Statistics:\")\n",
        "print(f\"Total pathways in Picrust_Result: {len(picrust_pathways)}\")\n",
        "print(f\"Matched pathways from parced_picrust: {len(parced_picrust)}\")\n",
        "\n",
        "# Show some examples of the matched data\n",
        "print(\"\\nFirst few matched pathways:\")\n",
        "print(parced_picrust.head().to_string())\n",
        "\n",
        "# Check if we missed any pathways\n",
        "missing_pathways = picrust_pathways - set(parced_picrust['pathway'])\n",
        "if missing_pathways:\n",
        "    print(\"\\nWarning: Some Picrust pathways not found in parce:\")\n",
        "    print(list(missing_pathways)[:5])  # Show first 5 missing pathways if any\n",
        "#parced_picrust.to_csv(\"~/MIC/2_Micro/data_picrust/parced_picrust.csv\", index=False, sep=\"\\t\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "QPnDyZxmYel3",
        "outputId": "cd723e82-46d3-4243-8cc4-760cf6d419cc",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "parce.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EEKnKNYuvgR"
      },
      "source": [
        "This way of mapping from parce df was no effective, the pathways and EC in parce are no all the ones are on ECcontri. So it was decided to map it from ECcontri directly putting the pathways into the df from Picrust_Result, so just put the two columns description and pathways inside the ECcontri by the column Site, instead that from the column function aka EC numbers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCs2cimyuvgS"
      },
      "source": [
        "## 8.3. Map Econtri to pathways"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEkDJg5zuvgS"
      },
      "source": [
        "We can no directly map the description and the pathway from Picrust_Result into ECcontri because each site can have several pathways, so we reshaping the Picrust_Result to long format and so that each row corresponds to a pathway for a given site. It is no possible to do this on a go using the whole 1491288 rows on ECcontri, so it would have to be done on agreggated data, as suggested by McKinney, 2010.\n",
        "Source: McKinney, W. (2010). Data Structures for Statistical Computing in Python. Retrieved from https://pandas.pydata.org/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T09:52:01.119153Z",
          "iopub.status.busy": "2025-02-19T09:52:01.118714Z",
          "iopub.status.idle": "2025-02-19T09:52:01.429902Z",
          "shell.execute_reply": "2025-02-19T09:52:01.428917Z",
          "shell.execute_reply.started": "2025-02-19T09:52:01.119118Z"
        },
        "id": "npxOZlx8uvgS",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Reshape Picrust_Result to long format: each row now corresponds to a pathway for a given site\n",
        "picrust_long = Picrust_Result.melt(id_vars=['pathway', 'description'],\n",
        "                                   var_name='sample',\n",
        "                                   value_name='abundance')\n",
        "\n",
        "# Filter out rows where the abundance is 0 (assuming that's what you mean by \"pathway present\")\n",
        "picrust_long = picrust_long[picrust_long['abundance'] > 0]\n",
        "\n",
        "# Aggregate pathway info per site\n",
        "mapping = picrust_long.groupby('sample').agg({\n",
        "    'pathway': lambda x: list(x),\n",
        "    'description': lambda x: list(x)\n",
        "}).reset_index()\n",
        "\n",
        "# Merge the aggregated mapping with ECcontri\n",
        "ECcontri_agg_site = pd.merge(ECcontri, mapping, on='sample', how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "id": "KTFNSJ6bYel3",
        "outputId": "2f5baecd-23ed-43b0-e236-2a12d50104d6",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "ECcontri_agg_site.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T09:52:04.326451Z",
          "iopub.status.busy": "2025-02-19T09:52:04.326101Z",
          "iopub.status.idle": "2025-02-19T09:52:06.354359Z",
          "shell.execute_reply": "2025-02-19T09:52:06.353212Z",
          "shell.execute_reply.started": "2025-02-19T09:52:04.326425Z"
        },
        "id": "5EXHu-QeuvgS",
        "trusted": true
      },
      "outputs": [],
      "source": [
        " # Add genus information from otu_mapping\n",
        "ECcontri_agg_site['taxon'] = ECcontri_agg_site['taxon'].astype(str)\n",
        "otu_mapping['taxon'] = otu_mapping['taxon'].astype(str)\n",
        "\n",
        "ECcontri_otu= pd.merge(ECcontri_agg_site, otu_mapping, on='taxon', how='left', validate='m:1')\n",
        "\n",
        "unmapped = ECcontri_otu['Genus'].isna().sum()\n",
        "if unmapped > 0:\n",
        "    print(f\"Warning: {unmapped} rows could not be mapped to genera\")\n",
        "# Rename columns: here \"description\" becomes \"pathway\" and \"pathway\" becomes \"npath\"\n",
        "ECcontri_otu  = ECcontri_otu.rename(columns={\"sample\":\"Sites\", \"function\": \"EC\", \"taxon\": \"OTU\", \"description\":\"pathway\", \"pathway\":\"npath\",\n",
        "                                     \"taxon_abun\": \"abund_raw\", \"taxon_function_abun\": \"abund_contri\", \"taxon_rel_abun\": \"rel_abund_raw\",\n",
        "                                       \"taxon_rel_function_abun\": \"rel_abund_contri\", \"norm_taxon_function_contrib\" :\"norm_abund_contri\", \"genome_function_count\":\"genome_EC_count\"})\n",
        "# Organize columns in logical groups\n",
        "cols_order = ['Sites', 'Genus', 'OTU', 'EC', # Identification columns\n",
        "              'npath', 'pathway', # Pathway information\n",
        "              'abund_raw', 'rel_abund_raw', # Raw abundance metrics\n",
        "              'genome_EC_count', 'abund_contri', 'rel_abund_contri', 'norm_abund_contri'] # Contribution metrics\n",
        "# Reorder columns, takes like 4 minutes on this slow laptop\n",
        "ECcontri_otu = ECcontri_otu[cols_order]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPaVL4T0uvgS"
      },
      "source": [
        "ECcontri_otu is a comprehensive dataframe that combines site locations, taxonomic information (genera and OTUs), enzyme classifications (ECs), and pathways (code for pathway (npath) and description (pathway)). The associated abundance metrics belong to the original ECcontri. The abundance metrics include:\n",
        "abund_raw: The original count of each organism (OTU) at each site\n",
        "rel_abund_raw: The relative abundance of each organism at each site, expressed as a proportion of total counts\n",
        "genome_function_count represents the predicted number of copies of a particular EC number (enzyme) in an organism's genome. This prediction comes from PICRUSt's hidden-state prediction process, which infers gene family abundances for each organism based on its phylogenetic placement relative to reference genomes\n",
        "abund_contri: The contribution of each organism to a specific enzyme function, calculated by multiplying the raw abundance by the number of copies of that enzyme in the organism's genome\n",
        "rel_abund_contri: The relative contribution of each organism to the enzyme function, accounting for both abundance and genome copy number\n",
        "norm_abund_contri: The normalized contribution metric that allows comparison across different sites and functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-02-19T09:52:13.927696Z",
          "iopub.status.busy": "2025-02-19T09:52:13.927272Z",
          "iopub.status.idle": "2025-02-19T09:52:14.513867Z",
          "shell.execute_reply": "2025-02-19T09:52:14.512365Z",
          "shell.execute_reply.started": "2025-02-19T09:52:13.927663Z"
        },
        "id": "xeKQezwyuvgS",
        "outputId": "f9de7e6f-55ba-4e6a-cb2e-4eca32399bf0",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "'''# Analyze genome_function_count\n",
        "print(\"Genome function count statistics:\")\n",
        "print(\"\\nOverall statistics:\")\n",
        "print(ECcontri_otu['genome_EC_count'].describe())\n",
        "\n",
        "# Look at distribution by EC number\n",
        "print(\"\\nExample EC numbers and their genome counts:\")\n",
        "ec_counts = ECcontri_otu.groupby('EC')['genome_EC_count'].agg(['unique', 'mean', 'max']).head()\n",
        "print(ec_counts)\n",
        "\n",
        "# Check if genome_function_count is consistent for each OTU-EC pair\n",
        "print(\"\\nCheck if genome_EC_count is consistent for OTU-EC combinations:\")\n",
        "consistency_check = ECcontri_otu.groupby(['OTU', 'EC'])['genome_EC_count'].nunique()\n",
        "inconsistent = consistency_check[consistency_check > 1]\n",
        "if len(inconsistent) > 0:\n",
        "    print(f\"Found {len(inconsistent)} OTU-EC pairs with inconsistent genome counts\")\n",
        "else:\n",
        "    print(\"Genome counts are consistent for all OTU-EC pairs\")\n",
        "\n",
        "# Explain the metrics in the dataframe\n",
        "print(\"\\nDataframe Components:\")\n",
        "print(\"1. Abundance Metrics:\")\n",
        "print(\"   - abund_raw: Raw abundance of each organism in each site\")\n",
        "print(\"   - abund_contri: Organism's abundance contribution to function/pathway\")\n",
        "print(\"   - rel_abund_raw: Original relative abundance\")\n",
        "print(\"   - rel_abund_contri: Relative abundance contribution to pathway\")\n",
        "print(\"   - norm_abund_contri: Normalized abundance contribution\")\n",
        "print(\"\\n2. Genome Function Count:\")\n",
        "print(\"   Number of copies of each EC (enzyme) in organism's genome\")'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Genome function count statistics:\n",
        "\n",
        "Overall statistics:\n",
        "count    1.491288e+06\n",
        "mean     1.390277e+00\n",
        "std      1.071974e+00\n",
        "min      1.000000e+00\n",
        "25%      1.000000e+00\n",
        "50%      1.000000e+00\n",
        "75%      1.000000e+00\n",
        "max      1.000000e+01\n",
        "Name: genome_EC_count, dtype: float64\n",
        "\n",
        "Example EC numbers and their genome counts:\n",
        "                                    unique      mean  max\n",
        "EC                                                       \n",
        "EC:1.1.1.1              [3, 2, 1, 5, 4, 8]  2.310375    8\n",
        "EC:1.1.1.100  [8, 5, 2, 3, 4, 9, 10, 6, 1]  4.237317   10\n",
        "EC:1.1.1.102                           [1]  1.000000    1\n",
        "EC:1.1.1.103                           [1]  1.000000    1\n",
        "EC:1.1.1.105                           [1]  1.000000    1\n",
        "\n",
        "Check if genome_EC_count is consistent for OTU-EC combinations:\n",
        "Genome counts are consistent for all OTU-EC pairs\n",
        "\n",
        "Dataframe Components:\n",
        "1. Abundance Metrics:\n",
        "   - abund_raw: Raw abundance of each organism in each site\n",
        "   - abund_contri: Organism's abundance contribution to function/pathway\n",
        "   - rel_abund_raw: Original relative abundance\n",
        "   - rel_abund_contri: Relative abundance contribution to pathway\n",
        "   - norm_abund_contri: Normalized abundance contribution\n",
        "\n",
        "2. Genome Function Count:\n",
        "   Number of copies of each EC (enzyme) in organism's genome"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MA-sQJpQuvgS"
      },
      "source": [
        "Analysis of genome_function_count(genome_EC_count) shows that most organisms typically have just one copy of any given enzyme (EC number) in their genome, with 75% of all cases showing a single copy. However, there is notable variation, with some organisms having up to 10 copies of certain enzymes. The average across all cases is 1.4 copies per enzyme per organism.\n",
        "Some enzymes show more variation than others. For example:\n",
        "\n",
        "EC:1.1.1.1 varies from 1 to 8 copies across different organisms\n",
        "EC:1.1.1.100 shows the widest range, from 1 to 10 copies\n",
        "Many enzymes (like EC:1.1.1.102, 103, 105) consistently appear as single copies\n",
        "\n",
        "Importantly, the copy number is consistent for each organism-enzyme combination across all sites, indicating this is a stable genomic characteristic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTSu-jj9uvgS"
      },
      "source": [
        "_____________________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voefPo-VuvgS"
      },
      "source": [
        "Pathway Mapping Analysis\n",
        "\n",
        "There were identified a discrepancy between EC predictions and pathway abundances. Found 61 pathways with EC number evidence that were not included in final predictions. Total number of reference pathways: 574 (from MetaCyc), total pathways in final predictions: 366, example missing pathway: PWY-6486 supported by EC:4.2.1.41\n",
        "\n",
        "Implications\n",
        "This finding suggests that the pathway prediction pipeline might be filtering out potentially relevant pathways despite having supporting EC evidence. This could impact the biological interpretation of the functional profiles and warrants further investigation.\n",
        "So in this study we mapped the pathways dataframe directly to the parce file and in doing so we have also the reaction information, avoiding the discrepancy with the Picrust_Result missing pathways."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MP4zCAyYuvgS"
      },
      "source": [
        "_____________________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEJ63Uk0uvgS"
      },
      "source": [
        "Now ECcontri_otu has several rows and columns providing information of the EC contribution to the metrics to each enzime aka EC number to the sites, genera combination, however the pathways are from origin link to most of the sites. This is perhaps because the methos infwee dunxriona bAWS ON XOMON sets of reference genomes.  Then, same environment in this case heating and cooling water systems poses similar organisms with similar pathways, the difference being on the abundance. So in order for this data to be usable, it is necesary to parse the EC into human readable information from a external enzyme databases to retrieve functional information about an EC number. Common resources include:\n",
        "\n",
        "UniProt: query UniProt’s REST API to get enzyme details by searching with the EC number.\n",
        "ExPASy Enzyme Database: Provides enzyme information based on EC numbers.\n",
        "BRENDA: A comprehensive enzyme database that can be queried either via its web interface or programmatically (e.g., using the bioservices Python package). Following script creates an EnzymeRetriever class that handles API requests to UniProt, processes unique EC numbers to avoid duplicate requests\n",
        "Adds protein names, functions, and UniProt IDs to ECcontri_otu df and includes rate limiting to avoid API restrictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XksxGYwIuvgS"
      },
      "source": [
        "## 8.4 Retrieving EC from Uniprot Locally"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vTTKHr0buvgS",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import time\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "class BatchEnzymeRetriever:\n",
        "    def __init__(self, batch_size=50, save_every=10):\n",
        "        self.uniprot_api = \"https://rest.uniprot.org/uniprotkb/search\"\n",
        "        self.batch_size = batch_size\n",
        "        self.save_every = save_every\n",
        "        self.results_file = 'uniprot_results.csv'\n",
        "        self.progress_file = 'retrieval_progress.json'\n",
        "        self.current_position = 0\n",
        "        self.load_progress()\n",
        "\n",
        "    def load_progress(self):\n",
        "        \"\"\"Load progress from previous run\"\"\"\n",
        "        if Path(self.progress_file).exists():\n",
        "            with open(self.progress_file, 'r') as f:\n",
        "                self.current_position = json.load(f)['position']\n",
        "            print(f\"Resuming from position {self.current_position}\")\n",
        "        else:\n",
        "            print(\"Starting new retrieval process\")\n",
        "\n",
        "    def save_progress(self):\n",
        "        \"\"\"Save current progress\"\"\"\n",
        "        with open(self.progress_file, 'w') as f:\n",
        "            json.dump({'position': self.current_position}, f)\n",
        "\n",
        "    def get_uniprot_info(self, ec: str, organism: str) -> dict:\n",
        "        \"\"\"Get UniProt information for a specific EC-organism pair\"\"\"\n",
        "        query = f\"{ec} {organism}\"\n",
        "\n",
        "        params = {\n",
        "            'query': query,\n",
        "            'format': 'tsv',\n",
        "            'fields': 'id,ec,organism_name,protein_name',\n",
        "            'size': 10\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = requests.get(self.uniprot_api, params=params)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            # Parse results\n",
        "            lines = response.text.strip().split('\\n')\n",
        "            if len(lines) < 2:\n",
        "                return None\n",
        "\n",
        "            # Process results to find best match\n",
        "            best_match = None\n",
        "            best_score = -float('inf')\n",
        "\n",
        "            for line in lines[1:]:  # Skip header\n",
        "                parts = line.split('\\t')\n",
        "                if len(parts) < 4:\n",
        "                    continue\n",
        "\n",
        "                entry_id, ec_numbers, org_name, protein_name = parts\n",
        "\n",
        "                # Calculate score based on organism name simplicity\n",
        "                score = 0\n",
        "                if organism.lower() in org_name.lower():\n",
        "                    score += 100\n",
        "                    if org_name.lower().endswith(' sp') or org_name.lower().endswith(' sp.'):\n",
        "                        score += 200\n",
        "                    if any(char.isdigit() for char in org_name):\n",
        "                        score -= 100\n",
        "\n",
        "                    # Check EC number match\n",
        "                    if ec.replace('EC:', '') in ec_numbers.split('; '):\n",
        "                        score += 150\n",
        "\n",
        "                        if score > best_score:\n",
        "                            best_score = score\n",
        "                            best_match = {\n",
        "                                'uniprot_id': entry_id,\n",
        "                                'ec_number': ec,\n",
        "                                'protein_name': protein_name,\n",
        "                                'organism': org_name,\n",
        "                                'score': score\n",
        "                            }\n",
        "\n",
        "            return best_match\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error retrieving data for {ec} - {organism}: {e}\")\n",
        "            time.sleep(5)\n",
        "            return None\n",
        "\n",
        "    def process_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Process DataFrame in batches, saving progress\"\"\"\n",
        "\n",
        "        # Load existing results if any\n",
        "        results_df = pd.DataFrame()\n",
        "        if Path(self.results_file).exists():\n",
        "            results_df = pd.read_csv(self.results_file)\n",
        "            print(f\"Loaded {len(results_df)} existing results\")\n",
        "\n",
        "        # Get unique EC-organism pairs starting from current position\n",
        "        pairs = df[['EC', 'Genus']].drop_duplicates()\n",
        "        pairs = pairs.iloc[self.current_position:]\n",
        "\n",
        "        print(f\"Processing {len(pairs)} unique EC-organism pairs\")\n",
        "\n",
        "        for batch_start in range(0, len(pairs), self.batch_size):\n",
        "            batch = pairs.iloc[batch_start:batch_start + self.batch_size]\n",
        "            batch_results = []\n",
        "\n",
        "            print(f\"\\nProcessing batch {batch_start//self.batch_size + 1}\")\n",
        "\n",
        "            for _, row in batch.iterrows():\n",
        "                print(f\"Querying {row['EC']} - {row['Genus']}\")\n",
        "                result = self.get_uniprot_info(row['EC'], row['Genus'])\n",
        "\n",
        "                if result:\n",
        "                    batch_results.append(result)\n",
        "                time.sleep(1)  # Rate limiting\n",
        "\n",
        "            # Add batch results to DataFrame\n",
        "            if batch_results:\n",
        "                batch_df = pd.DataFrame(batch_results)\n",
        "                results_df = pd.concat([results_df, batch_df], ignore_index=True)\n",
        "\n",
        "                # Save periodically\n",
        "                if (batch_start//self.batch_size) % self.save_every == 0:\n",
        "                    results_df.to_csv(self.results_file, index=False)\n",
        "                    self.current_position += len(batch)\n",
        "                    self.save_progress()\n",
        "                    print(f\"Saved progress at position {self.current_position}\")\n",
        "\n",
        "        # Final save\n",
        "        if not results_df.empty:\n",
        "            results_df.to_csv(self.results_file, index=False)\n",
        "        return results_df\n",
        "\n",
        "def process_enzymes(input_data, batch_size: int = 50):\n",
        "    \"\"\"Main function to process enzyme data\"\"\"\n",
        "    # Handle both DataFrame and file path inputs\n",
        "    if isinstance(input_data, str):\n",
        "        df = pd.read_csv(input_data)\n",
        "        print(f\"Loaded {len(df)} rows from {input_data}\")\n",
        "    elif isinstance(input_data, pd.DataFrame):\n",
        "        df = input_data\n",
        "        print(f\"Processing DataFrame with {len(df)} rows\")\n",
        "    else:\n",
        "        raise ValueError(\"Input must be either a file path or a pandas DataFrame\")\n",
        "\n",
        "    # Initialize retriever\n",
        "    retriever = BatchEnzymeRetriever(batch_size=batch_size)\n",
        "\n",
        "    # Process data\n",
        "    results = retriever.process_dataframe(df)\n",
        "    print(\"\\nProcessing complete!\")\n",
        "    print(f\"Results saved to {retriever.results_file}\")\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "i127PUfOuvgT",
        "outputId": "d9cc2e19-8e22-4351-eb4f-0e11a0966e4d",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "'''\n",
        "#  DataFrame loaded:\n",
        "retriever = BatchEnzymeRetriever(batch_size=50)\n",
        "Picrust_Econtri = retriever.process_dataframe(ECcontri_otu)'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQt8CV11uvgT"
      },
      "source": [
        "## 8.5. Colab\n",
        "Made in colab for resource posibility."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-sX1OxiuvgT"
      },
      "source": [
        "original colab code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGPK3BTGuvgT",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class ColabEnzymeRetriever:\n",
        "    def __init__(self, batch_size=100, save_every=5):\n",
        "        self.uniprot_api = \"https://rest.uniprot.org/uniprotkb/search\"\n",
        "        self.batch_size = batch_size\n",
        "        self.save_every = save_every\n",
        "        self.results_file = 'uniprot_results.csv'\n",
        "        self.existing_results = None\n",
        "\n",
        "    def load_existing_results(self, file_path):\n",
        "        \"\"\"Load and validate existing results\"\"\"\n",
        "        self.existing_results = pd.read_csv(file_path)\n",
        "        print(f\"Loaded {len(self.existing_results)} existing results\")\n",
        "        return set(zip(self.existing_results['ec_number'],\n",
        "                      [org.split()[0] for org in self.existing_results['organism']]))\n",
        "\n",
        "    def get_uniprot_info(self, ec: str, organism: str) -> dict:\n",
        "        \"\"\"Get UniProt information for a specific EC-organism pair\"\"\"\n",
        "        query = f\"{ec} {organism}\"\n",
        "\n",
        "        params = {\n",
        "            'query': query,\n",
        "            'format': 'tsv',\n",
        "            'fields': 'id,ec,organism_name,protein_name',\n",
        "            'size': 10\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = requests.get(self.uniprot_api, params=params)\n",
        "            response.raise_for_status()\n",
        "            time.sleep(0.5)  # Reduced sleep time for Colab\n",
        "\n",
        "            lines = response.text.strip().split('\\n')\n",
        "            if len(lines) < 2:\n",
        "                return None\n",
        "\n",
        "            best_match = None\n",
        "            best_score = -float('inf')\n",
        "\n",
        "            for line in lines[1:]:\n",
        "                parts = line.split('\\t')\n",
        "                if len(parts) < 4:\n",
        "                    continue\n",
        "\n",
        "                entry_id, ec_numbers, org_name, protein_name = parts\n",
        "\n",
        "                score = 0\n",
        "                if organism.lower() in org_name.lower():\n",
        "                    score += 100\n",
        "                    if org_name.lower().endswith(' sp') or org_name.lower().endswith(' sp.'):\n",
        "                        score += 200\n",
        "                    if any(char.isdigit() for char in org_name):\n",
        "                        score -= 100\n",
        "\n",
        "                    if ec.replace('EC:', '') in ec_numbers.split('; '):\n",
        "                        score += 150\n",
        "\n",
        "                        if score > best_score:\n",
        "                            best_score = score\n",
        "                            best_match = {\n",
        "                                'uniprot_id': entry_id,\n",
        "                                'ec_number': ec,\n",
        "                                'protein_name': protein_name,\n",
        "                                'organism': org_name,\n",
        "                                'score': score\n",
        "                            }\n",
        "\n",
        "            return best_match\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error retrieving data for {ec} - {organism}: {e}\")\n",
        "            time.sleep(2)\n",
        "            return None\n",
        "\n",
        "    def process_remaining_pairs(self, unique_pairs: pd.DataFrame, processed_pairs: set) -> pd.DataFrame:\n",
        "        \"\"\"Process only the pairs that haven't been processed yet\"\"\"\n",
        "        results = []\n",
        "        pairs_to_process = []\n",
        "\n",
        "        # Get unprocessed pairs\n",
        "        for _, row in unique_pairs.iterrows():\n",
        "            if (row['EC'], row['Genus']) not in processed_pairs:\n",
        "                pairs_to_process.append((row['EC'], row['Genus']))\n",
        "\n",
        "        pairs_df = pd.DataFrame(pairs_to_process, columns=['EC', 'Genus'])\n",
        "        total_pairs = len(pairs_df)\n",
        "\n",
        "        print(f\"\\nTotal pairs to process: {total_pairs}\")\n",
        "\n",
        "        if total_pairs == 0:\n",
        "            print(\"No new pairs to process!\")\n",
        "            return self.existing_results\n",
        "\n",
        "        for batch_start in range(0, total_pairs, self.batch_size):\n",
        "            batch = pairs_df.iloc[batch_start:batch_start + self.batch_size]\n",
        "            batch_results = []\n",
        "\n",
        "            print(f\"\\nProcessing batch {batch_start//self.batch_size + 1} of {total_pairs//self.batch_size + 1}\")\n",
        "            print(f\"Progress: {batch_start}/{total_pairs} pairs ({(batch_start/total_pairs)*100:.1f}%)\")\n",
        "\n",
        "            current_ec = None\n",
        "            for _, row in batch.iterrows():\n",
        "                if current_ec != row['EC']:\n",
        "                    current_ec = row['EC']\n",
        "                    print(f\"\\nProcessing EC number: {current_ec}\")\n",
        "\n",
        "                result = self.get_uniprot_info(row['EC'], row['Genus'])\n",
        "                if result:\n",
        "                    batch_results.append(result)\n",
        "\n",
        "            if batch_results:\n",
        "                results.extend(batch_results)\n",
        "\n",
        "                # Save progress by combining with existing results\n",
        "                if (batch_start//self.batch_size) % self.save_every == 0:\n",
        "                    combined_results = pd.concat([self.existing_results, pd.DataFrame(results)], ignore_index=True)\n",
        "                    combined_results.to_csv(self.results_file, index=False)\n",
        "                    print(f\"Saved {len(combined_results)} total results to file\")\n",
        "\n",
        "        # Combine final results\n",
        "        final_results = pd.concat([self.existing_results, pd.DataFrame(results)], ignore_index=True)\n",
        "        final_results.to_csv(self.results_file, index=False)\n",
        "        return final_results\n",
        "\n",
        "# Main processing function\n",
        "def continue_enzyme_retrieval(data_file: str, existing_results_file: str):\n",
        "    \"\"\"Main function to continue enzyme data retrieval\"\"\"\n",
        "    # Initialize retriever\n",
        "    retriever = ColabEnzymeRetriever(batch_size=100)\n",
        "\n",
        "    # Load existing results\n",
        "    processed_pairs = retriever.load_existing_results(existing_results_file)\n",
        "\n",
        "    # Load and process data\n",
        "    df = pd.read_csv(data_file)\n",
        "    unique_pairs = df[['EC', 'Genus']].drop_duplicates()\n",
        "    print(f\"Total unique pairs in data: {len(unique_pairs)}\")\n",
        "    print(f\"Already processed pairs: {len(processed_pairs)}\")\n",
        "\n",
        "    # Process remaining pairs\n",
        "    results_df = retriever.process_remaining_pairs(unique_pairs, processed_pairs)\n",
        "\n",
        "    # Save and download final results\n",
        "    results_df.to_csv('uniprot_results_final.csv', index=False)\n",
        "    files.download('uniprot_results_final.csv')\n",
        "\n",
        "    return results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1sJyminuvgT",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Usage (after uploading files to Colab), ECcontri_otu was made in colab because it was too big to upload after transformed\n",
        "# results = continue_enzyme_retrieval('ECcontri_otu', 'uniprot_results.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UM7HpQiHuvgT"
      },
      "source": [
        "have to be modify after first run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTO90O2duvgT",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import time\n",
        "from pathlib import Path\n",
        "import logging\n",
        "from typing import Set, Tuple, Optional\n",
        "import json\n",
        "\n",
        "class ColabEnzymeRetriever:\n",
        "    def __init__(self, batch_size=100, save_every=5):\n",
        "        self.uniprot_api = \"https://rest.uniprot.org/uniprotkb/search\"\n",
        "        self.batch_size = batch_size\n",
        "        self.save_every = save_every\n",
        "        self.results_file = Path('uniprot_results.tsv')\n",
        "        self.state_file = Path('retrieval_state.json')\n",
        "        self.processed_pairs: Set[Tuple[str, str]] = set()\n",
        "        self.existing_results = None\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "        self.logger.setLevel(logging.INFO)\n",
        "\n",
        "        if not self.logger.handlers:\n",
        "            handler = logging.StreamHandler()\n",
        "            formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
        "            handler.setFormatter(formatter)\n",
        "            self.logger.addHandler(handler)\n",
        "\n",
        "    def load_existing_results(self, file_path: Path) -> pd.DataFrame:\n",
        "        \"\"\"Load and validate existing results\"\"\"\n",
        "        if file_path.exists():\n",
        "            try:\n",
        "                self.existing_results = pd.read_csv(file_path, sep='\\t')\n",
        "                self.logger.info(f\"Loaded {len(self.existing_results)} existing results\")\n",
        "\n",
        "                # Build set of processed pairs\n",
        "                self.processed_pairs = set()\n",
        "                for _, row in self.existing_results.iterrows():\n",
        "                    if pd.notna(row['ec_number']) and pd.notna(row['organism']):\n",
        "                        ec_num = str(row['ec_number']).strip()\n",
        "                        org = str(row['organism']).split()[0].strip()\n",
        "                        self.processed_pairs.add((ec_num, org))\n",
        "\n",
        "                return self.existing_results\n",
        "            except Exception as e:\n",
        "                self.logger.error(f\"Error loading results file: {e}\")\n",
        "                self.existing_results = pd.DataFrame(\n",
        "                    columns=['uniprot_id', 'ec_number', 'protein_name', 'organism', 'score']\n",
        "                )\n",
        "                return self.existing_results\n",
        "\n",
        "        self.existing_results = pd.DataFrame(\n",
        "            columns=['uniprot_id', 'ec_number', 'protein_name', 'organism', 'score']\n",
        "        )\n",
        "        return self.existing_results\n",
        "\n",
        "    def get_uniprot_info(self, ec: str, organism: str) -> Optional[dict]:\n",
        "        \"\"\"Get UniProt information for a specific EC-organism pair\"\"\"\n",
        "        if (ec, organism) in self.processed_pairs:\n",
        "            return None\n",
        "\n",
        "        query = f'({ec}) AND (organism_name:\"{organism}*\")'\n",
        "        params = {\n",
        "            'query': query,\n",
        "            'format': 'tsv',\n",
        "            'fields': 'id,ec,protein_name,organism_name',\n",
        "            'size': 10\n",
        "        }\n",
        "\n",
        "        max_retries = 3\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                response = requests.get(self.uniprot_api, params=params)\n",
        "                response.raise_for_status()\n",
        "                time.sleep(0.5)\n",
        "\n",
        "                lines = response.text.strip().split('\\n')\n",
        "                if len(lines) < 2:\n",
        "                    return None\n",
        "\n",
        "                best_match = None\n",
        "                best_score = -float('inf')\n",
        "\n",
        "                for line in lines[1:]:\n",
        "                    parts = line.split('\\t')\n",
        "                    if len(parts) < 4:\n",
        "                        continue\n",
        "\n",
        "                    uniprot_id, ec_numbers, protein_name, organism_name = parts\n",
        "\n",
        "                score = 0\n",
        "                if organism_name and isinstance(organism_name, str):\n",
        "                    name_parts = organism_name.split()\n",
        "                    genus = name_parts[0] if name_parts else \"\"\n",
        "\n",
        "                    # Exact genus match gets highest score\n",
        "                    if genus.lower() == organism.lower():\n",
        "                        score += 500\n",
        "                        # Prefer entries with just the genus name\n",
        "                        if len(name_parts) == 1:\n",
        "                            score += 300\n",
        "                        # Heavily penalize strain designations or subspecies\n",
        "                        elif len(name_parts) > 2 or any(char.isdigit() for char in organism_name):\n",
        "                            score -= 400\n",
        "\n",
        "                    if score > -float('inf'):\n",
        "                        if ec.replace('EC:', '') in ec_numbers.split('; '):\n",
        "                            score += 150\n",
        "\n",
        "                            if score > best_score:\n",
        "                                best_score = score\n",
        "                                best_match = {\n",
        "                                    'uniprot_id': uniprot_id,\n",
        "                                    'ec_number': ec,\n",
        "                                    'protein_name': protein_name,\n",
        "                                    'organism': organism_name,\n",
        "                                    'score': score\n",
        "                                }\n",
        "\n",
        "                return best_match if best_match else None\n",
        "\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                if attempt < max_retries - 1:\n",
        "                    time.sleep(2 ** attempt)\n",
        "                    continue\n",
        "                self.logger.error(f\"Error fetching data from UniProt: {e}\")\n",
        "                return None\n",
        "\n",
        "    def process_remaining_pairs(self, unique_pairs: pd.DataFrame, start_ec: str) -> pd.DataFrame:\n",
        "        \"\"\"Process remaining pairs with enforced starting point\"\"\"\n",
        "        # Ensure EC format consistency\n",
        "        if not start_ec.startswith('EC:'):\n",
        "            start_ec = f\"EC:{start_ec.replace('EC:', '')}\"\n",
        "\n",
        "        # Sort and filter pairs\n",
        "        unique_pairs = unique_pairs.sort_values(['EC', 'Genus']).reset_index(drop=True)\n",
        "        unique_pairs = unique_pairs[unique_pairs['EC'] >= start_ec].reset_index(drop=True)\n",
        "\n",
        "        if len(unique_pairs) == 0:\n",
        "            self.logger.warning(f\"No EC numbers found after {start_ec}\")\n",
        "            return self.existing_results\n",
        "\n",
        "        self.logger.info(f\"Starting processing from {unique_pairs.iloc[0]['EC']}\")\n",
        "        total_pairs = len(unique_pairs)\n",
        "\n",
        "        results = []\n",
        "        for idx in range(0, total_pairs, self.batch_size):\n",
        "            batch = unique_pairs.iloc[idx:idx + self.batch_size]\n",
        "            batch_results = []\n",
        "\n",
        "            self.logger.info(f\"\\nProcessing batch {idx//self.batch_size + 1} of {total_pairs//self.batch_size + 1}\")\n",
        "            self.logger.info(f\"Progress: {idx}/{total_pairs} pairs ({(idx/total_pairs)*100:.1f}%)\")\n",
        "\n",
        "            current_ec = None\n",
        "            for _, row in batch.iterrows():\n",
        "                if current_ec != row['EC']:\n",
        "                    current_ec = row['EC']\n",
        "                    self.logger.info(f\"\\nProcessing EC number: {current_ec}\")\n",
        "\n",
        "                if (row['EC'], row['Genus']) not in self.processed_pairs:\n",
        "                    result = self.get_uniprot_info(row['EC'], row['Genus'])\n",
        "                    if result:\n",
        "                        batch_results.append(result)\n",
        "                        self.processed_pairs.add((row['EC'], row['Genus']))\n",
        "\n",
        "            if batch_results:\n",
        "                results.extend(batch_results)\n",
        "\n",
        "                # Save progress periodically\n",
        "                if (idx//self.batch_size) % self.save_every == 0:\n",
        "                    combined_results = pd.concat(\n",
        "                        [self.existing_results, pd.DataFrame(results)],\n",
        "                        ignore_index=True\n",
        "                    )\n",
        "                    combined_results.to_csv(self.results_file, sep='\\t', index=False)\n",
        "                    self.logger.info(f\"Saved {len(combined_results)} total results to file\")\n",
        "\n",
        "        # Final save\n",
        "        final_results = pd.concat(\n",
        "            [self.existing_results, pd.DataFrame(results)],\n",
        "            ignore_index=True\n",
        "        )\n",
        "        final_results.to_csv(self.results_file, sep='\\t', index=False)\n",
        "\n",
        "        return final_results\n",
        "\n",
        "def continue_enzyme_retrieval(unique_pairs: pd.DataFrame, existing_results_file: Path, start_ec: str):\n",
        "    \"\"\"Main function to continue enzyme data retrieval\"\"\"\n",
        "    retriever = ColabEnzymeRetriever(batch_size=100)\n",
        "\n",
        "    # Load existing results\n",
        "    retriever.load_existing_results(existing_results_file)\n",
        "\n",
        "    # Ensure input data is properly formatted\n",
        "    if isinstance(unique_pairs, str):\n",
        "        unique_pairs = pd.read_csv(unique_pairs, sep='\\t')\n",
        "    elif isinstance(unique_pairs, pd.DataFrame):\n",
        "        unique_pairs = unique_pairs.copy()\n",
        "    else:\n",
        "        raise ValueError(\"Input must be either a file path or a pandas DataFrame\")\n",
        "\n",
        "    # Validate and prepare input data\n",
        "    required_columns = ['EC', 'Genus']\n",
        "    if not all(col in unique_pairs.columns for col in required_columns):\n",
        "        raise ValueError(f\"Input data must contain columns: {required_columns}\")\n",
        "\n",
        "    unique_pairs['EC'] = unique_pairs['EC'].astype(str).apply(lambda x: f\"EC:{x.replace('EC:', '')}\")\n",
        "    unique_pairs['Genus'] = unique_pairs['Genus'].astype(str).str.strip()\n",
        "    unique_pairs = unique_pairs[['EC', 'Genus']].drop_duplicates()\n",
        "\n",
        "    # Process remaining pairs\n",
        "    results_df = retriever.process_remaining_pairs(unique_pairs, start_ec)\n",
        "\n",
        "    # Save final results\n",
        "    final_path = Path('uniprot_results_final.tsv')\n",
        "    results_df.to_csv(final_path, sep='\\t', index=False)\n",
        "\n",
        "    return results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "mowtkQwJuvgT",
        "outputId": "46c3345f-7a16-42e4-b8c5-00bed7d0e300",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "'''uniprot_results_path = Path(base_dir '/uniprot_results.tsv')\n",
        "# Usage (after uploading files to Colab), ECcontri_otu was made in colab because it was too big to upload after transformed\n",
        "results = continue_enzyme_retrieval(ECcontri_otu, uniprot_results_path, start_ec=\"x.3.1.12\" )'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nARoxMBKuvgT"
      },
      "source": [
        "## 8.6. Cleaning and Preparing Retrieved Data to integrate to ECContri"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9r_F4kMtuvgU",
        "outputId": "87828c15-41d8-4974-c1ad-c0d640c368d9",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "df_1_path = Path(base_dir / \"uniprot_1_4_sorted.tsv\") # First file retrieved on first run 4 am\n",
        "df_2_path = Path(base_dir / \"uniprot_2_1.38_sorted.tsv\") # Same file retrieven when corrupted around 1:38 following day\n",
        "df_3_path = Path(base_dir / \"uniprot_3_sorted.tsv\") # Rerun done trying to get following EC numbers\n",
        "df_4_path = Path(base_dir / \"uniprot_4_missing_sorted.tsv\") # Final run in missing data\n",
        "\n",
        "df_1 = pd.read_csv(df_1_path, sep='\\t')\n",
        "df_2 = pd.read_csv(df_2_path, sep='\\t')\n",
        "df_3 = pd.read_csv(df_3_path, sep='\\t')\n",
        "df_4 = pd.read_csv(df_4_path, sep='\\t')\n",
        "print(len(df_1), len(df_2), len(df_3), len(df_4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9a2RbXR1uvgU",
        "outputId": "8eff9e0e-f042-4033-d159-7659d2f56a8a",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# extract EC and Genus from the Retrieved files, so I need to join them first\n",
        "retrieved = pd.concat([df_1, df_2, df_3, df_4], axis = 0)\n",
        "unique_pairs = ECcontri_otu[['EC', 'Genus']].drop_duplicates()\n",
        "len(unique_pairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dACO91ciYel5",
        "outputId": "2f73a703-b5a5-4562-fe7f-e15d85332f05",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "retrieved.head(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwbF_HFmuvgU"
      },
      "source": [
        "Extracting the Genus from the retrieved_pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jfKMErDWuvgU",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Function to extract the Genus from the organism str\n",
        "def extract_genus(organism_str):\n",
        "    # Assumes Genus is the first word that starts with an uppercase letter.\n",
        "    match = re.search(r'([A-Z][a-z]+)', organism_str)\n",
        "    return match.group(1) if match else None\n",
        "# Creating a Genus column in the retrieved dataframe.\n",
        "retrieved['Genus'] = retrieved['organism'].astype(str).apply(extract_genus)\n",
        "\n",
        "# if there are duplicates, we want the best entry based on score:\n",
        "retrieved_unique = retrieved.sort_values('score', ascending=False)\\\n",
        "                            .drop_duplicates(subset=['ec_number', 'Genus'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoQasxuauvgU",
        "outputId": "5e23acfb-ef95-4763-d08e-9d5327eb9745",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Merging using a left join on the two keys (EC_number and Genus). Plus an indicator of missing data.\n",
        "ECcontri_Uniprot  = pd.merge(\n",
        "    ECcontri_otu,\n",
        "    retrieved_unique[['ec_number', 'Genus', 'protein_name', 'score', 'uniprot_id']],\n",
        "    left_on=['EC', 'Genus'],\n",
        "    right_on=['ec_number', 'Genus'],\n",
        "    how='left',\n",
        "    suffixes=('', '_retr')\n",
        ")\n",
        "print(ECcontri_Uniprot.shape) # Very slow 1 minute, can kill the kernel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elCz1V27O-E3",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "ECcontri_Uniprot = ECcontri_Uniprot.drop(columns = [\"OTU\",\t\"EC\",\t\"npath\", \"ec_number\",\t\"score\",\t\"uniprot_id\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qA_nz2qFuvgU"
      },
      "source": [
        "ECcontri_uniprot_info is the final df mixed and is keep for reference only purposes. With the missing unique df I will retrive again the rest of the missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6iKlSuHuvgU",
        "outputId": "69606100-d709-4e0d-da15-23138f8a0c01",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "'''#Rows with no match from retrieved_unique will have '_merge' value of 'left_only'\n",
        "merged_unique = pd.merge(\n",
        "    unique_pairs,\n",
        "    retrieved_unique,\n",
        "    left_on=['EC', 'Genus'],\n",
        "    right_on=['ec_number', 'Genus'],\n",
        "    how='left',\n",
        "    indicator=True\n",
        ")\n",
        "\n",
        "# Filter unique pairs missing from retrieved data\n",
        "ECcontri_missing = merged_unique[merged_unique['_merge'] == 'left_only']\n",
        "print(\"Missing unique pairs count:\", ECcontri_missing.shape[0])\n",
        "ECcontri_missing = ECcontri_missing[[\"EC\", \"Genus\"]]\n",
        "file_path = os.path.join(output_dir, \"ECcontri_missing.tsv\")\n",
        "ECcontri_missing.to_csv(file_path, sep='\\t', index=False)'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cleaning anc collecting garbage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "del picrust_long\n",
        "del react\n",
        "del mean_abundances_react\n",
        "del retrieved\n",
        "del retrieved_unique\n",
        "del unique_pairs\n",
        "del df_1\n",
        "del df_2\n",
        "del df_3\n",
        "del df_4\n",
        "del EC\n",
        "del ECcontri\n",
        "del ECcontri_agg_site\n",
        "del ECcontri_otu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "del consistency_check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d2KfaHxuvgU"
      },
      "source": [
        "### Data Retrieval Completion Note\n",
        "After multiple retrieval attempts, 12,656 pairs remain unmapped out of approximately 1,500,000 total rows (0.84%). Given this small percentage and the diminishing returns from further retrieval attempts, we concluded that this level of completeness is acceptable for analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QY8Rxk24uvgU"
      },
      "source": [
        "## 8.7. Making an Integrated Picrust Result df: genera_matrix\n",
        "the data is found now in a long format, for the next plotting we will be grouping it and making it onto a new df. We can group by Site and Genus (or even by protein_name) to look at the overall enzymatic contributions and how they correlate with the risk categories. The pathways information is actually very dense, it has some entries upto 300 pathway for site. So in this way we do a pathway chart but wont be bringing it to the next visualisations. The aim being to identify which genera or enzymes are most abundant in high-risk sites, assess if the presence of certain metal-specific enzymes (like Fe-dependent dehydrogenases) is linked to corrosion risk, ultimately potentially filter out common background organisms that are less relevant and perhaps just endemic part of the water systems in general and no specific to corrosion. So a table will be created with following information\n",
        "|Sites|Genus|protein_name|norm_abund_contri|*Category|\n",
        "|--|--|--|--|--|\n",
        "\n",
        "*where category will be utilised for colouring purposes.   \n",
        "\n",
        "__Pivoting on Two Variables:__\n",
        "Using a pivot table with both Genus and protein_name as column levels is attempted in the next snipped, in order to capture contributions at that level. This will:\n",
        "- Generate a quantitative view of normalized abundance contributions per site\n",
        "- Maintain the hierarchical relationship between genera and their enzymes\n",
        "- Create a separate metabolic information matrix for pathway interpretation\n",
        "This structure allows to analyze both taxonomic patterns and specific enzyme contributions while maintaining the ability to link back to corrosion risk categories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T09:55:00.338798Z",
          "iopub.status.busy": "2025-02-19T09:55:00.338377Z",
          "iopub.status.idle": "2025-02-19T09:55:00.346377Z",
          "shell.execute_reply": "2025-02-19T09:55:00.344664Z",
          "shell.execute_reply.started": "2025-02-19T09:55:00.338766Z"
        },
        "id": "s_4Fb88NuvgV",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Define category dict outside so that all charts can use same dict\n",
        "category_dict = Integrated_T.T.iloc[0, 0:-1].astype(int).to_dict()\n",
        "\n",
        "# Define colors and categories\n",
        "category_colors = {1: '#008800',  # Dark green\n",
        "                   2: '#FF8C00',  # Dark orange\n",
        "                   3: '#FF0000'}   # Red\n",
        "\n",
        "categories_labels = {1: 'Normal Operation',\n",
        "              2: 'Early Warning',\n",
        "              3: 'System Failure'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpwCWh1JYel5"
      },
      "source": [
        "### Cleaning Proteins Names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjkVRoV9Yel5",
        "outputId": "46ab7df6-a9e8-4920-cd40-66004df14317",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing cleaning function:\n",
            "\n",
            "Original:  1,4-alpha-glucan branching enzyme GlgB -glucan branching enzyme\n",
            "Cleaned:   1,4-alpha-glucan branching enzyme GlgB -glucan\n",
            "\n",
            "Original:  Gluconate 5-dehydrogenase enzyme (EC 1.1.1.69) dehydrogenase\n",
            "Cleaned:   Gluconate 5-dehydrogenase enzyme dehydrogenase\n",
            "\n",
            "Original:  synthase protein synthase (EC 2.2.1.6) protein\n",
            "Cleaned:   synthase protein\n"
          ]
        }
      ],
      "source": [
        "def clean_protein_name(name):\n",
        "    \"\"\"\n",
        "    Enhanced protein name cleaning:\n",
        "    1. Remove EC numbers unless it's the only information\n",
        "    2. Remove redundant information in parentheses\n",
        "    3. Remove duplicated terms\n",
        "    4. Handle special cases\n",
        "    \"\"\"\n",
        "    if pd.isna(name):\n",
        "        return \"Uncharacterized protein\"\n",
        "\n",
        "    # If the name is just an EC number in any format, return it\n",
        "    if re.match(r'^[\\s\\(\\)]*EC\\s*[\\d\\.]+[\\s\\(\\)]*$', name):\n",
        "        return name.strip()\n",
        "\n",
        "    # Remove EC numbers and content in parentheses\n",
        "    name = re.sub(r'\\(EC\\s*[\\d\\.]+\\)', '', name)\n",
        "    name = re.sub(r'\\([^)]*\\)', '', name)\n",
        "\n",
        "    # Split into words and remove duplicates while preserving order\n",
        "    words = name.split()\n",
        "    seen = set()\n",
        "    unique_words = []\n",
        "    for word in words:\n",
        "        # Convert to lowercase for comparison but keep original case in result\n",
        "        lower_word = word.lower()\n",
        "        if lower_word not in seen:\n",
        "            seen.add(lower_word)\n",
        "            unique_words.append(word)\n",
        "\n",
        "    # Rejoin words\n",
        "    name = ' '.join(unique_words)\n",
        "\n",
        "    # Remove specific redundant patterns\n",
        "    redundant_patterns = [\n",
        "        (r'enzyme\\s+enzyme', 'enzyme'),\n",
        "        (r'synthase\\s+synthase', 'synthase'),\n",
        "        (r'transferase\\s+transferase', 'transferase'),\n",
        "        (r'-glucan\\s+glucan', 'glucan'),\n",
        "        (r'protein\\s+protein', 'protein')\n",
        "    ]\n",
        "\n",
        "    for pattern, replacement in redundant_patterns:\n",
        "        name = re.sub(pattern, replacement, name, flags=re.IGNORECASE)\n",
        "\n",
        "    return name.strip()\n",
        "\n",
        "def check_cleaning(df, n_samples=10):\n",
        "    \"\"\"\n",
        "    Check the cleaning results with before/after comparison\n",
        "    \"\"\"\n",
        "    sample_names = df['protein_name'].dropna().sample(n=n_samples)\n",
        "    cleaned_names = sample_names.apply(clean_protein_name)\n",
        "\n",
        "    print(\"Sample of name cleaning results:\")\n",
        "    for orig, cleaned in zip(sample_names, cleaned_names):\n",
        "        print(f\"\\nOriginal:  {orig}\")\n",
        "        print(f\"Cleaned:   {cleaned}\")\n",
        "\n",
        "# Test\n",
        "test_names = [\n",
        "    \"1,4-alpha-glucan branching enzyme GlgB -glucan branching enzyme\",\n",
        "    \"Gluconate 5-dehydrogenase enzyme (EC 1.1.1.69) dehydrogenase\",\n",
        "    \"synthase protein synthase (EC 2.2.1.6) protein\",\n",
        "]\n",
        "\n",
        "print(\"Testing cleaning function:\")\n",
        "for name in test_names:\n",
        "    cleaned = clean_protein_name(name)\n",
        "    print(f\"\\nOriginal:  {name}\")\n",
        "    print(f\"Cleaned:   {cleaned}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0hwxPe7Yel5"
      },
      "source": [
        "### Creating Basics Matrixes for Normalise and Relative abundances\n",
        "\n",
        "> Add blockquote\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Jkv-id-auvgV",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def create_matrix(df):\n",
        "    \"\"\"\n",
        "    Creates the base matrix with cleaned protein names\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    check_cleaning(df)\n",
        "\n",
        "    df['protein_name'] = df['protein_name'].apply(clean_protein_name)\n",
        "\n",
        "    # Create pivot table\n",
        "    base_matrix = df.pivot_table(\n",
        "        values='norm_abund_contri',\n",
        "        index='Sites',\n",
        "        columns=['Genus', 'protein_name'],\n",
        "        aggfunc='first',\n",
        "        fill_value=0,\n",
        "        observed=True\n",
        "    )\n",
        "\n",
        "    return base_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhbnsAc4Yel5",
        "outputId": "ce7808ee-7063-4fc9-9da4-c23817815c15",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample of name cleaning results:\n",
            "\n",
            "Original:  Aspartate--tRNA ligase (EC 6.1.1.12)\n",
            "Cleaned:   Aspartate--tRNA ligase\n",
            "\n",
            "Original:  Shikimate dehydrogenase (NADP(+)) (SDH) (EC 1.1.1.25)\n",
            "Cleaned:   Shikimate dehydrogenase )\n",
            "\n",
            "Original:  5-aminolevulinate synthase (EC 2.3.1.37) (5-aminolevulinic acid synthase) (Delta-ALA synthase) (Delta-aminolevulinate synthase)\n",
            "Cleaned:   5-aminolevulinate synthase\n",
            "\n",
            "Original:  aldehyde dehydrogenase (NAD(+)) (EC 1.2.1.3)\n",
            "Cleaned:   aldehyde dehydrogenase )\n",
            "\n",
            "Original:  RNA helicase (EC 3.6.4.13)\n",
            "Cleaned:   RNA helicase\n",
            "\n",
            "Original:  Glutamate synthase (NADPH/NADH), large chain (EC 1.4.1.14)\n",
            "Cleaned:   Glutamate synthase , large chain\n",
            "\n",
            "Original:  uroporphyrinogen-III C-methyltransferase (EC 2.1.1.107)\n",
            "Cleaned:   uroporphyrinogen-III C-methyltransferase\n",
            "\n",
            "Original:  Isoaspartyl peptidase (EC 3.4.19.5)\n",
            "Cleaned:   Isoaspartyl peptidase\n",
            "\n",
            "Original:  NAD(+) diphosphatase (EC 3.6.1.22)\n",
            "Cleaned:   NAD diphosphatase\n",
            "\n",
            "Original:  Acetyl-coenzyme A synthetase (AcCoA synthetase) (Acs) (EC 6.2.1.1) (Acetate--CoA ligase) (Acyl-activating enzyme)\n",
            "Cleaned:   Acetyl-coenzyme A synthetase\n"
          ]
        }
      ],
      "source": [
        "# Create and save the matrix, this is the killer kernel operation number one takes over 1-4 min if works\n",
        "base_matrix = create_matrix(ECcontri_Uniprot)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZKLqOyV2XC2"
      },
      "source": [
        "# Creating a Matrix for relative Abundance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-02-19T09:53:38.875032Z",
          "iopub.status.busy": "2025-02-19T09:53:38.874631Z",
          "iopub.status.idle": "2025-02-19T09:53:38.913843Z",
          "shell.execute_reply": "2025-02-19T09:53:38.912614Z",
          "shell.execute_reply.started": "2025-02-19T09:53:38.875001Z"
        },
        "id": "QVoGKd6EYel5",
        "outputId": "eca7a28a-b754-411d-ae75-37b7fd1642f0",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample of name cleaning results:\n",
            "\n",
            "Original:  tRNA pseudouridine synthase B (EC 5.4.99.25) (tRNA pseudouridine(55) synthase) (Psi55 synthase) (tRNA pseudouridylate synthase) (tRNA-uridine isomerase)\n",
            "Cleaned:   tRNA pseudouridine synthase B synthase)\n",
            "\n",
            "Original:  Adenosylmethionine-8-amino-7-oxononanoate aminotransferase (EC 2.6.1.62) (7,8-diamino-pelargonic acid aminotransferase) (DAPA AT) (DAPA aminotransferase) (7,8-diaminononanoate synthase) (DANS) (Diaminopelargonic acid synthase)\n",
            "Cleaned:   Adenosylmethionine-8-amino-7-oxononanoate aminotransferase\n",
            "\n",
            "Original:  ATP-dependent DNA helicase RecG (EC 3.6.4.12)\n",
            "Cleaned:   ATP-dependent DNA helicase RecG\n",
            "\n",
            "Original:  Myo-inositol-1-phosphate synthase (EC 5.5.1.4)\n",
            "Cleaned:   Myo-inositol-1-phosphate synthase\n",
            "\n",
            "Original:  Imidazoleglycerol-phosphate dehydratase (EC 4.2.1.19)\n",
            "Cleaned:   Imidazoleglycerol-phosphate dehydratase\n",
            "\n",
            "Original:  Porphobilinogen deaminase (PBG) (EC 2.5.1.61) (Hydroxymethylbilane synthase) (HMBS) (Pre-uroporphyrinogen synthase)\n",
            "Cleaned:   Porphobilinogen deaminase\n",
            "\n",
            "Original:  Uroporphyrinogen-III synthase (EC 4.2.1.75)\n",
            "Cleaned:   Uroporphyrinogen-III synthase\n",
            "\n",
            "Original:  Probable cytosol aminopeptidase (EC 3.4.11.1) (Leucine aminopeptidase) (LAP) (EC 3.4.11.10) (Leucyl aminopeptidase)\n",
            "Cleaned:   Probable cytosol aminopeptidase\n",
            "\n",
            "Original:  Nitronate monooxygenase (EC 1.13.12.16)\n",
            "Cleaned:   Nitronate monooxygenase\n",
            "\n",
            "Original:  Phosphatidate cytidylyltransferase (EC 2.7.7.41)\n",
            "Cleaned:   Phosphatidate cytidylyltransferase\n",
            "Matrix shape: (70, 37089)\n",
            "Index type: <class 'pandas.core.indexes.base.Index'>\n",
            "Index name: Sites\n"
          ]
        }
      ],
      "source": [
        "def create_matrix_rel(df):\n",
        "    \"\"\"\n",
        "    Creates the base matrix with relative abundance with cleaned protein names\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "\n",
        "    check_cleaning(df)\n",
        "\n",
        "    df['protein_name'] = df['protein_name'].apply(clean_protein_name)\n",
        "\n",
        "    # Create pivot table\n",
        "    base_matrix_relative = df.pivot_table(\n",
        "        values='rel_abund_raw',\n",
        "        index='Sites',\n",
        "        columns=['Genus', 'protein_name'],\n",
        "        aggfunc='first',\n",
        "        fill_value=0,\n",
        "        observed=True\n",
        "    )\n",
        "\n",
        "    # Ensure clean single-level index\n",
        "    if isinstance(base_matrix_relative.index, pd.MultiIndex):\n",
        "        base_matrix_relative = base_matrix_relative.reset_index()\n",
        "        base_matrix_relative = base_matrix_relative.set_index('Sites')\n",
        "\n",
        "    print(f\"Matrix shape: {base_matrix_relative.shape}\")\n",
        "    print(f\"Index type: {type(base_matrix_relative.index)}\")\n",
        "    print(f\"Index name: {base_matrix_relative.index.name}\")\n",
        "\n",
        "    return base_matrix_relative\n",
        "\n",
        "base_matrix_relative = create_matrix_rel(ECcontri_Uniprot)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fk6wmJJ9Yel5"
      },
      "source": [
        "### Saving by Parts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "EudW9WXbKN7j",
        "outputId": "20485e5f-8e9e-425e-d2d8-995a705df4f6",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "'''# Save components separately\n",
        "def save_matrix_components(base_matrix, base_path='matrix'):\n",
        "    \"\"\"\n",
        "    Save matrix components separately:\n",
        "    - Index (Sites)\n",
        "    - Column levels (Genus and protein_name)\n",
        "    - Values\n",
        "    \"\"\"\n",
        "    # Save index (Sites)\n",
        "    pd.Series(base_matrix.index).to_csv(f\"{base_path}_sites.csv\")\n",
        "\n",
        "    # Save column levels separately\n",
        "    for i, name in enumerate(base_matrix.columns.names):\n",
        "        level_values = base_matrix.columns.get_level_values(i)\n",
        "        pd.Series(level_values).to_csv(f\"{base_path}_columns_{name}.csv\")\n",
        "\n",
        "    # Save the actual values as numpy array\n",
        "    np.save(f\"{base_path}_values.npy\", base_matrix.values)\n",
        "\n",
        "    print(\"Components saved:\")\n",
        "    print(f\"- Sites: {base_path}_sites.csv\")\n",
        "    print(f\"- Genus: {base_path}_columns_Genus.csv\")\n",
        "    print(f\"- Proteins: {base_path}_columns_protein_name.csv\")\n",
        "    print(f\"- Values: {base_path}_values.npy\")\n",
        "\n",
        "# Save the components\n",
        "save_matrix_components(base_matrix)'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieVnJDEkYel6"
      },
      "source": [
        "### Introducing Risk Category for Plotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-02-19T09:53:51.013159Z",
          "iopub.status.busy": "2025-02-19T09:53:51.012759Z",
          "iopub.status.idle": "2025-02-19T09:53:51.072579Z",
          "shell.execute_reply": "2025-02-19T09:53:51.070761Z",
          "shell.execute_reply.started": "2025-02-19T09:53:51.013128Z"
        },
        "id": "1nCWQxjkYel6",
        "outputId": "08ecb758-5594-40d6-c766-f5408ac267b9",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cell executed at: 2025-02-23 13:26:28.939137\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "print(f\"Cell executed at: {datetime.now()}\")\n",
        "\n",
        "# First ensure sites are properly ordered numerically. Get current index and sort it\n",
        "current_index = base_matrix.index\n",
        "sorted_index = sorted(current_index, key=lambda x: int(x.split('_')[1]))\n",
        "base_matrix = base_matrix.reindex(sorted_index)\n",
        "\n",
        "# Now add the category mapping\n",
        "if category_dict is not None:\n",
        "    category_mapping = pd.Series(base_matrix.index.map(category_dict),\n",
        "                               index=base_matrix.index,\n",
        "                               name='Category')\n",
        "    base_matrix.index = pd.MultiIndex.from_arrays([base_matrix.index, category_mapping],\n",
        "                                                 names=['Sites', 'Category'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T09:52:47.158796Z",
          "iopub.status.busy": "2025-02-19T09:52:47.158361Z",
          "iopub.status.idle": "2025-02-19T09:52:47.167198Z",
          "shell.execute_reply": "2025-02-19T09:52:47.165854Z",
          "shell.execute_reply.started": "2025-02-19T09:52:47.158763Z"
        },
        "id": "UopuTKppYel6",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# First ensure sites are properly ordered numerically. Get current index and sort it\n",
        "current_index = base_matrix_relative.index\n",
        "sorted_index = sorted(current_index, key=lambda x: int(x.split('_')[1]))\n",
        "base_matrix_relative = base_matrix_relative.reindex(sorted_index)\n",
        "\n",
        "# Now add the category mapping\n",
        "if category_dict is not None:\n",
        "    category_mapping = pd.Series(base_matrix_relative.index.map(category_dict),\n",
        "                               index=base_matrix_relative.index,\n",
        "                               name='Category')\n",
        "    base_matrix_relative.index = pd.MultiIndex.from_arrays([base_matrix_relative.index, category_mapping],\n",
        "                                                 names=['Sites', 'Category'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "6LOdsv2xYel6",
        "outputId": "0ae9bf84-604e-4124-91d2-3a2c9804b240",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "'''# Saving Colab or VSCode\n",
        "file_path = os.path.join(output_dir, \"category_mapping\")\n",
        "\n",
        "category_mapping.to_csv(file_path, sep='\\t', index=False)'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_o2VvH3Yel6"
      },
      "source": [
        "## 8.8. Making Metabolic Sites information df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T10:08:51.95245Z",
          "iopub.status.busy": "2025-02-19T10:08:51.95208Z",
          "iopub.status.idle": "2025-02-19T10:08:51.959001Z",
          "shell.execute_reply": "2025-02-19T10:08:51.957396Z",
          "shell.execute_reply.started": "2025-02-19T10:08:51.952425Z"
        },
        "id": "EQAqPzKuIyhU",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def metabolic_sites_info(df):\n",
        "    \"\"\"\n",
        "    Create metabolic information DataFrame with site-specific aggregation.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : pandas.Data Input ECcontri_Uniprot DataFrame with 'Sites', 'Genus', 'pathway', 'protein_name' columns\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    pandas.DataFrame Aggregated metabolic information with sites preserved\n",
        "    \"\"\"\n",
        "    def safe_join(x):\n",
        "        return ', '.join(sorted(set(x.dropna().astype(str))))\n",
        "\n",
        "    # Group by both Sites and Genus to preserve site information\n",
        "    metabolic_info = df.groupby(['Sites', 'Genus'], observed=True).agg({\n",
        "        'pathway': safe_join,\n",
        "        'protein_name': safe_join,\n",
        "        'norm_abund_contri': 'sum'  # Add abundance information\n",
        "    }).rename(columns={\n",
        "        'pathway': 'Pathways',\n",
        "        'protein_name': 'Protein_Names',\n",
        "        'norm_abund_contri': 'norm_abund_contri'\n",
        "    })\n",
        "\n",
        "    return metabolic_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T10:08:55.017135Z",
          "iopub.status.busy": "2025-02-19T10:08:55.016713Z",
          "iopub.status.idle": "2025-02-19T10:11:13.307708Z",
          "shell.execute_reply": "2025-02-19T10:11:13.306289Z",
          "shell.execute_reply.started": "2025-02-19T10:08:55.017101Z"
        },
        "id": "mGkhsxDpYel6",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "metabolic_sites_info = metabolic_sites_info(ECcontri_Uniprot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "execution": {
          "iopub.execute_input": "2025-02-19T10:11:13.310193Z",
          "iopub.status.busy": "2025-02-19T10:11:13.309714Z",
          "iopub.status.idle": "2025-02-19T10:11:13.324335Z",
          "shell.execute_reply": "2025-02-19T10:11:13.322739Z",
          "shell.execute_reply.started": "2025-02-19T10:11:13.310147Z"
        },
        "id": "-oiPF4s6Yel6",
        "outputId": "c9caabea-227b-4e11-9c79-1f0ce8283062",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "metabolic_sites_info.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "Js54enJiYel6",
        "outputId": "0fe67a8e-5823-4529-e79a-927559b97d63",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "base_matrix.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buF9K_ksYel6",
        "outputId": "64496130-3952-418f-a856-717d505ce76f",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "print(metabolic_sites_info.shape, base_matrix.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cleaning and collecting garbage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1461"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "del ECcontri_Uniprot\n",
        "\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "jPA8kjB2IENR",
        "outputId": "d643a33c-d917-40b1-e787-099fe1f06359",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "'''# Saving on Colab or VSCode\n",
        "metabolic_sites_info= metabolic_sites_info(ECcontri_Uniprot)\n",
        "# Saving just in case\n",
        "file_path = os.path.join(output_dir, \"metabolic_sites_info.tsv\")\n",
        "metabolic_sites_info.to_csv(file_path, sep='\\t', index=False)'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "3TX0_kRdYel6",
        "outputId": "3b632a08-35ee-4ad1-d0de-9c5b80b9a5fb",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "'''import zipfile\n",
        "\n",
        "output_dir = Path(\"/kaggle/working/\")\n",
        "# Define the files to save\n",
        "files_to_save = [\n",
        "    output_dir / \"base_matrix.tsv\",\n",
        "    output_dir / \"category_mapping\",\n",
        "    output_dir / \"metabolic_sites_info.tsv\"\n",
        "]\n",
        "\n",
        "# Save only these files into a ZIP\n",
        "zip_path = output_dir / \"selected_results.zip\"\n",
        "with zipfile.ZipFile(zip_path, \"w\") as zipf:\n",
        "    for file in files_to_save:\n",
        "        zipf.write(file, arcname=file.name)  # Save with only filename'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUmCgiWWAx9A"
      },
      "source": [
        "## 8.9. Reading the Files\n",
        "The local machine struggled to work with the files, and killed the kernel, so Colab was continued to be used, however even with the high memory availbable it was struggling with memory fragmentation, it is believed the complexity of the dataa make the problem, chucking didnt improve problem, so the notebook was continued to be worked in Kaggle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "c9OF_EJwYel6",
        "outputId": "3f6e4db9-86f7-4f77-f9d2-a1500280e471",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "'''# Reading for Kaggle\n",
        "import shutil\n",
        "\n",
        "output_dir = Path(\"/kaggle/working/\")\n",
        "# Path to your uploaded dataset\n",
        "dataset_path = Path(\"/kaggle/input/results-basic\")\n",
        "\n",
        "try:\n",
        "    shutil.copytree(dataset_path, output_dir / dataset_path.name, dirs_exist_ok=True) # Copies the entire directory structure\n",
        "    print(f\"Directory '{dataset_path.name}' copied successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while copying the directory: {e}\")'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pw5qVLUcOuTl",
        "outputId": "736b59d4-c38c-4e70-8628-5358286e13c5",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import psutil\n",
        "import os\n",
        "\n",
        "def detailed_memory_check():\n",
        "    # Get memory info\n",
        "    mem = psutil.virtual_memory()\n",
        "\n",
        "    # Get process memory info\n",
        "    process = psutil.Process(os.getpid())\n",
        "    process_mem = process.memory_info()\n",
        "\n",
        "    print(\"System Memory Details:\")\n",
        "    print(f\"Total: {mem.total/1024**3:.2f} GB\")\n",
        "    print(f\"Available: {mem.available/1024**3:.2f} GB\")\n",
        "    print(f\"Used: {mem.used/1024**3:.2f} GB\")\n",
        "    print(f\"Free: {mem.free/1024**3:.2f} GB\")\n",
        "    print(f\"Percent used: {mem.percent}%\")\n",
        "\n",
        "    print(\"\\nProcess Memory Details:\")\n",
        "    print(f\"RSS (Physical): {process_mem.rss/1024**3:.2f} GB\")\n",
        "    print(f\"VMS (Virtual): {process_mem.vms/1024**3:.2f} GB\")\n",
        "\n",
        "detailed_memory_check()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "IZ1D49QFYel6",
        "outputId": "2c2b5404-dee5-4418-e2ec-92691f1fe1c3",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "'''# Reading\n",
        "metabolic_info_path  = Path(output_dir / \"metabolic_sites_info.tsv\")\n",
        "metabolic_sites_info = pd.read_csv(metabolic_info_path, sep='\\t',  low_memory=False)'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsyxkAFuuvgV"
      },
      "source": [
        "# 9. Analysign the Dominant Protein Enzymes, Pathways and Genes with the Principal Component Loadings\n",
        "Following script analyse the dominant Protein Enzymes, Pathways and Genes contributing to the first two PCs comming from section 7.1. The risk label is use here to color code the hue.\n",
        "## 9.1. Principal Components of Genera vs Risk Category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 631
        },
        "execution": {
          "iopub.execute_input": "2025-02-19T10:11:29.641918Z",
          "iopub.status.busy": "2025-02-19T10:11:29.641549Z",
          "iopub.status.idle": "2025-02-19T10:11:30.984997Z",
          "shell.execute_reply": "2025-02-19T10:11:30.983614Z",
          "shell.execute_reply.started": "2025-02-19T10:11:29.641883Z"
        },
        "id": "IorbqV1wZ4ng",
        "outputId": "7a228e92-911c-4957-aaa2-093712eb5710",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def prepare_genera_pca(base_matrix, category_mapping=None):\n",
        "    \"\"\"\n",
        "    Prepare genera data for PCA with handling of multi-index categories\n",
        "\n",
        "    Parameters: base_matrix : pandas.DataFrame, Matrix with multi-index (Sites, Category)\n",
        "                category_mapping : pandas.Series,  Category mapping\n",
        "\n",
        "    Returns:    X_pca : numpy.ndarray, PCA transformed data\n",
        "                explained_variance_ratio : numpy.ndarray  Explained variance ratios\n",
        "                loadings : pandas.DataFrame, PCA loadings\n",
        "                categories : pandas.Series,  Categories for each site\n",
        "    \"\"\"\n",
        "    # Extract categories if they're in the multi-index\n",
        "    if isinstance(base_matrix.index, pd.MultiIndex):\n",
        "        categories = base_matrix.index.get_level_values('Category')\n",
        "        # No need to drop category as it's in the index\n",
        "        X = base_matrix\n",
        "    else:\n",
        "        # Use provided category mapping or None\n",
        "        categories = category_mapping\n",
        "        X = base_matrix\n",
        "\n",
        "    # No need for iloc[1:] as we don't have enzyme names as first row anymore\n",
        "    X_for_scaling = X.astype(float)\n",
        "\n",
        "    # Standardize\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X_for_scaling)\n",
        "\n",
        "    # PCA\n",
        "    pca = PCA(n_components=2)\n",
        "    X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "    # Create loadings DataFrame with proper multi-index columns\n",
        "    loadings = pd.DataFrame(\n",
        "        pca.components_.T,\n",
        "        index=X.columns,  # preserving multi-index columns (Genus, protein_name)\n",
        "        columns=['PC1', 'PC2']\n",
        "    )\n",
        "\n",
        "    return X_pca, pca.explained_variance_ratio_, loadings, categories\n",
        "\n",
        "def plot_pca_results(X_pca, explained_variance, Category, title, category_colors, categories_labels,\n",
        "                     pc1_idx=0, pc2_idx=1):  # Add parameters for component indices\n",
        "    \"\"\"\n",
        "    Plot PCA with risk categories\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    X_pca : numpy array   PCA transformed data\n",
        "    explained_variance : numpy array        Explained variance ratios\n",
        "    Category : array-like  Category labels for each sample\n",
        "    title : str   Plot title\n",
        "    category_colors : dict  Mapping of categories to colors\n",
        "    categories_labels : dict  Mapping of categories to display labels\n",
        "    pc1_idx : int        Index of the first PC to plot (default 0 for PC1)\n",
        "    pc2_idx : int        Index of the second PC to plot (default 1 for PC2)\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 8))\n",
        "\n",
        "    # Plot using specified components\n",
        "    for category in sorted(set(Category)):\n",
        "        mask = Category == category\n",
        "        plt.scatter(\n",
        "            X_pca[mask, pc1_idx],  # Specified PC for x-axis\n",
        "            X_pca[mask, pc2_idx],  # Specified PC for y-axis\n",
        "            c=category_colors[category],\n",
        "            label=categories_labels[category],\n",
        "            alpha=0.7,\n",
        "            s=100\n",
        "        )\n",
        "\n",
        "    plt.xlabel(f'PC{pc1_idx+1} ({explained_variance[pc1_idx]:.1%} variance explained)')\n",
        "    plt.ylabel(f'PC{pc2_idx+1} ({explained_variance[pc2_idx]:.1%} variance explained)')\n",
        "    plt.title(title)\n",
        "    plt.legend(title='Risk Category')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# For genera PCA\n",
        "X_pca_genera, var_ratio_genera, loadings_genera, categories = prepare_genera_pca(base_matrix, category_mapping)\n",
        "# PC1 vs PC2 (default)\n",
        "plot_pca_results(X_pca_genera, var_ratio_genera, categories.values, \"PC1 vs PC2\",\n",
        "                 category_colors, categories_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "execution": {
          "iopub.execute_input": "2025-02-19T10:11:49.450275Z",
          "iopub.status.busy": "2025-02-19T10:11:49.449889Z",
          "iopub.status.idle": "2025-02-19T10:11:51.503161Z",
          "shell.execute_reply": "2025-02-19T10:11:51.501945Z",
          "shell.execute_reply.started": "2025-02-19T10:11:49.450245Z"
        },
        "id": "FUe9thnSe7cf",
        "outputId": "8524990c-0523-4118-d2aa-9a245c1e15fc",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def prepare_flexible_pca(data_matrix, categories=None, n_components=None):\n",
        "    \"\"\"\n",
        "    Prepare PCA with flexible number of components\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    data_matrix : pandas DataFrame   Input data with samples as rows and features as columns\n",
        "    categories : array-like,   Category labels for each sample\n",
        "    n_components : int,   Number of components to calculate (None for all possible)\n",
        "    n_plot : int   Number of components to return for plotting\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    X_pca : numpy array   PCA transformed data (first n_plot components)\n",
        "    explained_variance : numpy array  Explained variance ratios for all components\n",
        "    loadings : pandas DataFrame  PCA loadings with feature names as index\n",
        "    \"\"\"\n",
        "    # Standardize\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(data_matrix)\n",
        "\n",
        "    # PCA\n",
        "    pca = PCA(n_components=n_components)\n",
        "    X_pca_full = pca.fit_transform(X_scaled)\n",
        "\n",
        "    # Get loadings for all components\n",
        "    loadings = pd.DataFrame(\n",
        "        pca.components_.T,\n",
        "        index=data_matrix.columns,\n",
        "        columns=[f'PC{i+1}' for i in range(pca.n_components_)]\n",
        "    )\n",
        "\n",
        "    # Return only requested components for plotting\n",
        "    X_pca = X_pca_full\n",
        "\n",
        "    # Return all calculated components\n",
        "    return X_pca_full, pca.explained_variance_ratio_, loadings\n",
        "\n",
        "# Calculate PCA with all components\n",
        "X_pca_all, var_ratio_all, loadings_all = prepare_flexible_pca(base_matrix_relative)\n",
        "\n",
        "# Plot different component combinations\n",
        "# PC1 vs PC2 (default)\n",
        "plot_pca_results(X_pca_all, var_ratio_all, categories.values, \"PC1 vs PC2\",\n",
        "                 category_colors, categories_labels)\n",
        "\n",
        "# PC2 vs PC3\n",
        "plot_pca_results(X_pca_all, var_ratio_all, categories.values, \"PC2 vs PC3\",\n",
        "                 category_colors, categories_labels,\n",
        "                 pc1_idx=1, pc2_idx=2)\n",
        "\n",
        "# PC3 vs PC4\n",
        "plot_pca_results(X_pca_all, var_ratio_all, categories.values, \"PC3 vs PC4\",\n",
        "                 category_colors, categories_labels,\n",
        "                 pc1_idx=2, pc2_idx=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7vGk-dvJxXL"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeKymKOe5A3k"
      },
      "source": [
        "## 9.2. Principal Component of Protein by Risk Category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 630
        },
        "execution": {
          "iopub.execute_input": "2025-02-19T10:14:45.493097Z",
          "iopub.status.busy": "2025-02-19T10:14:45.492634Z",
          "iopub.status.idle": "2025-02-19T10:18:02.267216Z",
          "shell.execute_reply": "2025-02-19T10:18:02.265636Z",
          "shell.execute_reply.started": "2025-02-19T10:14:45.493066Z"
        },
        "id": "ol9qvarP5jRz",
        "outputId": "0ca5673f-b11a-417b-d4aa-68ab24387773",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def prepare_protein_pca(metabolic_info, category_dict):\n",
        "    \"\"\"\n",
        "    Convert protein strings to numeric features for PCA, aggregating by site first\n",
        "    \"\"\"\n",
        "    # First aggregate by Sites\n",
        "    site_protein_data = metabolic_info.groupby('Sites', observed=True).agg({\n",
        "        'Protein_Names': lambda x: ', '.join(x.dropna())\n",
        "    })\n",
        "\n",
        "    # Create set of unique proteins\n",
        "    all_proteins = set()\n",
        "    for proteins_str in site_protein_data['Protein_Names'].dropna():\n",
        "        proteins = [p.strip() for p in proteins_str.split(',')]\n",
        "        all_proteins.update(proteins)\n",
        "\n",
        "    # Create binary matrix at site level\n",
        "    protein_data = {}\n",
        "    for protein in all_proteins:\n",
        "        if protein:\n",
        "            protein_escaped = re.escape(protein)\n",
        "            protein_data[protein] = site_protein_data['Protein_Names'].str.contains(\n",
        "                protein_escaped,\n",
        "                regex=True,\n",
        "                na=False\n",
        "            ).astype(int)\n",
        "\n",
        "    protein_matrix = pd.DataFrame(protein_data, index=site_protein_data.index)\n",
        "\n",
        "    # Run PCA\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(protein_matrix)\n",
        "    pca = PCA(n_components=2)\n",
        "    X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "    loadings = pd.DataFrame(\n",
        "        pca.components_.T,\n",
        "        index=protein_matrix.columns,\n",
        "        columns=['PC1', 'PC2']\n",
        "    )\n",
        "\n",
        "    return X_pca, pca.explained_variance_ratio_, loadings, protein_matrix\n",
        "    # For protein PCA # 4 min\n",
        "X_pca_protein, var_ratio_protein, loadings_protein, protein_matrix = prepare_protein_pca(metabolic_sites_info, category_dict)\n",
        "plot_pca_results(X_pca_protein, var_ratio_protein, categories.values, \"Protein PCA by Risk Category\", category_colors, categories_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p72ly8fYYel7"
      },
      "source": [
        "## 9.3. Top Protein Loadings by Category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-02-19T10:20:33.294993Z",
          "iopub.status.busy": "2025-02-19T10:20:33.294612Z",
          "iopub.status.idle": "2025-02-19T10:20:33.319388Z",
          "shell.execute_reply": "2025-02-19T10:20:33.318181Z",
          "shell.execute_reply.started": "2025-02-19T10:20:33.294963Z"
        },
        "id": "0L5F06YaEBiW",
        "outputId": "e4bb9ec7-408a-4380-bcff-c1ca26f3d9c3",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def analyze_protein_loadings(loadings, top_n=20):\n",
        "    \"\"\"\n",
        "    Analyze protein loadings to find most influential proteins\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    loadings : pandas DataFrame\n",
        "        PCA loadings with proteins as index\n",
        "    top_n : int\n",
        "        Number of top proteins to return\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dict with top proteins for each PC and their contributions\n",
        "    \"\"\"\n",
        "    # Calculate magnitude of contribution for each protein\n",
        "    loadings['magnitude'] = np.sqrt(loadings['PC1']**2 + loadings['PC2']**2)\n",
        "\n",
        "    # Get top contributors overall\n",
        "    top_overall = loadings.nlargest(top_n, 'magnitude')\n",
        "\n",
        "    # Get top contributors for each PC\n",
        "    top_pc1_pos = loadings.nlargest(top_n, 'PC1')\n",
        "    top_pc1_neg = loadings.nsmallest(top_n, 'PC1')\n",
        "    top_pc2_pos = loadings.nlargest(top_n, 'PC2')\n",
        "    top_pc2_neg = loadings.nsmallest(top_n, 'PC2')\n",
        "\n",
        "    return {\n",
        "        'top_overall': top_overall,\n",
        "        'top_pc1_positive': top_pc1_pos,\n",
        "        'top_pc1_negative': top_pc1_neg,\n",
        "        'top_pc2_positive': top_pc2_pos,\n",
        "        'top_pc2_negative': top_pc2_neg\n",
        "    }\n",
        "\n",
        "# Use after running PCA:\n",
        "loading_analysis = analyze_protein_loadings(loadings_protein)\n",
        "\n",
        "# Print top contributors\n",
        "print(\"Top 20 proteins contributing to separation:\")\n",
        "print(loading_analysis['top_overall'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnA0pU4EsLXX"
      },
      "source": [
        "A lecture of the top 20 proteins contributing to separation on the exactly the same magnitude make it suspicius to the fact that maybe we not really taking the 20 top but just the 20 first, and indistiguisible will be just be all contributing on the same fashion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEoWk_0x73_U"
      },
      "source": [
        "### Retrieving Statistically Significant Groups\n",
        "\n",
        "From notebook 3_Feature_selection the file finalist.xlsx contain the groups worked and that were statistically significant in relation to the risk label. This groups posses interest since the relationship to the label could show better understanding in contrast with the different groups of known bacteria, core taxa, checked bacteria and the mixed groups.\n",
        "The idea is to understand if the core taxa which make up a large influence on the comunities on the water and cooling systems are also influencing corrosion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSgS-kec8Gg6"
      },
      "outputs": [],
      "source": [
        "source_groups = {\n",
        "    \"known_bacteria\": known_bacteria_list,\n",
        "    \"pure_checked\": pure_checked_list,\n",
        "    \"pure_core\": pure_core_list,\n",
        "    \"checked_core\": checked_core_list\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owvu2k118Hyk"
      },
      "outputs": [],
      "source": [
        "Influencers_uniques_path = base_dir / \"finalist_dfs.xlsx\"\n",
        "# Integrated taxa from origin genus as headers with levels 6 for the genera, 7 for the GID, muss be cleaned\n",
        "Influencers_uniques = pd.read_excel(Influencers_uniques_path, sheet_name='Influencers_uniques', header=[0,1,2,3,4,5,6,7], engine ='openpyxl')\n",
        "# Drop first row (index 0) and first column in one chain\n",
        "Influencers_uniques = Influencers_uniques.drop(index=0)\n",
        "Influencers_uniques = Influencers_uniques.drop(Influencers_uniques.columns[0], axis=1)\n",
        "Influencers_uniques = Influencers_uniques.astype({'Sites': str})\n",
        "# Remove 'Unnamed' level names\n",
        "Influencers_uniques.columns = Influencers_uniques.columns.map(lambda x: tuple('' if \"Unnamed\" in str(level) else level for level in x))\n",
        "Influencers_uniques_list= Influencers_uniques.columns.get_level_values(6)\n",
        "Influencers_uniques_list= Influencers_uniques_list[Influencers_uniques_list !='']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3dgDHQr8Xi2"
      },
      "source": [
        "\n",
        "### Updating the groups to visualise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPOjPHN88U8O"
      },
      "outputs": [],
      "source": [
        "source_groups = {\n",
        "    \"known_bacteria\": known_bacteria_list,\n",
        "    \"pure_checked\": pure_checked_list,\n",
        "    \"pure_core\": pure_core_list,\n",
        "    \"checked_core\": checked_core_list,\n",
        "    \"Influencers_uniques\": Influencers_uniques_list,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nXUOBVBJWGk"
      },
      "source": [
        "## 9.4. Analysing Top Proteins by Category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "k0oorAB98DCg",
        "outputId": "736ebf16-a3ae-46a2-bfdc-0725da39d33f",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def analyze_top_proteins_by_category(base_matrix, category_dict, source_groups, n_top=20):\n",
        "    \"\"\"\n",
        "    Analyze top proteins for each risk category and source group\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    base_matrix : DataFrame with MultiIndex columns (Genus, protein_name)\n",
        "    category_dict : Dict mapping sites to categories (1,2,3)\n",
        "    source_groups : Dict mapping group names to list of genera\n",
        "    n_top : Number of top proteins to show\n",
        "    \"\"\"\n",
        "    # Get sites for each category\n",
        "    sites_by_category = {\n",
        "        cat: [site for site, c in category_dict.items() if c == cat]\n",
        "        for cat in [1, 2, 3]  # Explicitly use categories 1,2,3\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # For each bacteria group (known, pure_checked, etc.)\n",
        "    for group_name, genera in source_groups.items():\n",
        "        print(f\"\\nAnalyzing {group_name}...\")\n",
        "\n",
        "        # Filter for genera in this group\n",
        "        group_cols = [col for col in base_matrix.columns if col[0] in genera]\n",
        "        if not group_cols:\n",
        "            print(f\"No data found for {group_name}\")\n",
        "            continue\n",
        "\n",
        "        group_data = base_matrix[group_cols]\n",
        "\n",
        "        # Analyze each risk category\n",
        "        group_results = {}\n",
        "        for cat, sites in sites_by_category.items():\n",
        "            # Get data for sites in this category\n",
        "            cat_data = group_data.loc[sites]\n",
        "\n",
        "            # Calculate mean abundance for each protein-genus combination\n",
        "            mean_abundances = cat_data.mean()\n",
        "            top_proteins = mean_abundances.nlargest(n_top)\n",
        "\n",
        "            group_results[cat] = top_proteins\n",
        "\n",
        "        results[group_name] = group_results\n",
        "\n",
        "        # Plot results for this group\n",
        "        plt.figure(figsize=(20, 10))\n",
        "        plt.suptitle(f\"Top {n_top} Proteins - {group_name}\", y=1.02, fontsize=14)\n",
        "\n",
        "        for i, cat in enumerate([1, 2, 3], 1):\n",
        "            plt.subplot(1, 3, i)\n",
        "\n",
        "            if cat in group_results:\n",
        "                top = group_results[cat]\n",
        "\n",
        "                # Create labels combining genus and protein\n",
        "                labels = [f\"{genus}\\n{protein[:30]}...\"\n",
        "                         for genus, protein in top.index]\n",
        "\n",
        "                # Plot\n",
        "                sns.barplot(x=top.values,\n",
        "                          y=labels,\n",
        "                          color=category_colors[cat],\n",
        "                          alpha=0.7)\n",
        "\n",
        "                plt.title(f\"{categories_labels[cat]}\")\n",
        "                plt.xlabel(\"Mean Abundance\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    return results\n",
        "\n",
        "# Call the function\n",
        "results = analyze_top_proteins_by_category(base_matrix, category_dict, source_groups)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQ-5nx_6DMvN"
      },
      "source": [
        "## Top Genera & Proteins in One Category\n",
        "Subset by category, compute total abundance for each genus, pick the top n. For each of those genera, pick the top 𝑛 n proteins. Stack into a long DataFrame for boxplotting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0t7eYAELEi2x"
      },
      "outputs": [],
      "source": [
        "def pick_top_proteins_for_category(base_matrix, cat, n_top, n_genera, category_level=1):\n",
        "    \"\"\"\n",
        "    Return a DataFrame in long form of the top proteins for the specified category.\n",
        "\n",
        "    Steps:\n",
        "    1) Filter rows by category\n",
        "    2) Identify top n_genera by total abundance\n",
        "    3) For each top genus, pick the n_top most abundant proteins\n",
        "    4) Return a long DataFrame of those columns only\n",
        "    \"\"\"\n",
        "    # 1) Subset rows for the chosen category\n",
        "    cat_data = base_matrix.xs(cat, level=category_level, axis=0, drop_level=False)\n",
        "\n",
        "    # 2) Calculate mean abundance for each (Genus, Protein)\n",
        "    col_means = cat_data.mean(axis=0)  # Series indexed by (Genus, Protein)\n",
        "\n",
        "    # 3) Identify top n_genera by total abundance\n",
        "    genus_sums = col_means.groupby(level=0).sum()  # sum across proteins within each genus\n",
        "    top_genera = genus_sums.nlargest(n_genera).index\n",
        "\n",
        "    # 4) For each genus in top_genera, pick top n_top proteins\n",
        "    all_top_cols = []\n",
        "    for genus in top_genera:\n",
        "        # Get columns belonging to this genus\n",
        "        genus_cols = [col for col in col_means.index if col[0] == genus]\n",
        "        # Among those, pick the n_top highest\n",
        "        top_genus_proteins = col_means[genus_cols].nlargest(n_top).index\n",
        "        all_top_cols.extend(top_genus_proteins)\n",
        "\n",
        "    # Subset cat_data to these selected columns\n",
        "    top_data = cat_data[all_top_cols]\n",
        "\n",
        "    # Convert to long form using stack\n",
        "    df_long = (\n",
        "        top_data\n",
        "        .stack(level=list(range(top_data.columns.nlevels)))  # stack both (Genus, Protein)\n",
        "        .reset_index()\n",
        "    )\n",
        "\n",
        "    # After stacking, columns typically become [<row_idx1>, <row_idx2>, \"Genus\", \"Protein\", 0]\n",
        "    if len(df_long.columns) == 5:\n",
        "        df_long.columns = [\"Site\", \"Category\", \"Genus\", \"Protein\", \"Abundance\"]\n",
        "    else:\n",
        "        df_long.columns = [\"Site\", \"Genus\", \"Protein\", \"Abundance\"]\n",
        "\n",
        "    # Add a combined label\n",
        "    df_long[\"Feature\"] = df_long[\"Genus\"] + \" | \" + df_long[\"Protein\"].str[:30] + \"...\"\n",
        "    # Keep track of which category these data came from\n",
        "    df_long[\"Cat\"] = cat\n",
        "\n",
        "    return df_long"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIAe_wGbICUC"
      },
      "source": [
        "## Plotting Top Protein-Genera for Category\n",
        "The plotting loops over the categories, uses pick_top_proteins_for_category to build a long DataFrame for each category an d creates one subplot per category with a boxplot of the chosen features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5VbhkEIADFTd",
        "outputId": "eb1c92dd-066c-4876-cc19-1c42aa5d25a3"
      },
      "outputs": [],
      "source": [
        "def plot_top_proteins_across_categories(base_matrix, categories=[1, 2, 3],\n",
        "                                        n_top=5, n_genera=10, category_level=1):\n",
        "    \"\"\"\n",
        "    Create side-by-side boxplots of the top proteins from multiple categories.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    base_matrix : pd.DataFrame\n",
        "        Rows: (Site, Category), Columns: (Genus, Protein)\n",
        "    categories : list\n",
        "        Which category values to plot, e.g. [1,2,3].\n",
        "    n_top : int\n",
        "        Number of top proteins per genus.\n",
        "    n_genera : int\n",
        "        Number of top genera to consider per category.\n",
        "    category_level : int\n",
        "        The level in the row MultiIndex that holds the category.\n",
        "    \"\"\"\n",
        "    # Prepare subplots\n",
        "    fig, axes = plt.subplots(1, len(categories), figsize=(10*len(categories), 15), sharey=True)\n",
        "    if len(categories) == 1:\n",
        "        axes = [axes]  # ensure it's iterable\n",
        "\n",
        "    # For each category, pick top proteins, then plot in its own subplot\n",
        "    for i, cat in enumerate(categories):\n",
        "        df_long = pick_top_proteins_for_category(base_matrix, cat, n_top, n_genera, category_level)\n",
        "\n",
        "        # Boxplot in the ith subplot\n",
        "        sns.boxplot(ax=axes[i], x='Abundance', y='Feature', data=df_long)\n",
        "        axes[i].set_title(f\"Category {cat}: Top {n_top} Proteins\\nfrom Top {n_genera} Genera\")\n",
        "        axes[i].set_xlabel(\"Abundance\")\n",
        "        axes[i].set_ylabel(\"Genus | Protein\")\n",
        "        #axes[i].tick_params(axis='y', rotation=90)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_top_proteins_across_categories(base_matrix, categories=[1, 2, 3],  n_top=5, n_genera=10, category_level=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "FNRMNDJ9L9yz",
        "outputId": "0cdb236e-6070-43fb-eceb-75645d71db73"
      },
      "outputs": [],
      "source": [
        "plot_top_proteins_across_categories(base_matrix_relative, categories=[1, 2, 3],  n_top=5, n_genera=5, category_level=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oz1pJ1mhFjbm"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Retrieving Meaningful Names "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I need to search in the database but at the same time to capture some naming that allow me to classify protein as well as pathways with my references I have on drive or with the table of metabolism on my main doc,\n",
        "using the notebook lm to search into the docs my own literature review? so to find how to profit from this naming to make a bloody table or dictionary with no one but maybe several columns?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m296sDKJQW27"
      },
      "source": [
        "Charts can be overwhelming and difficult to read, in order to be able to compare better, an statistical test will be made in order to elucidate which of this genus-protein combinations have no positive or negative significance with the risk label and therefore wont serve to differentiate for the final model, it is not that they are no important, just that they can no be reliable at the time to decide what microorganism is influencing corrosion.\n",
        "\n",
        "First the previous code to plot  would be modified to get all the proteins- genus pars and their respective classification into categories so that a statistical analyis could be done contrasting the presence and abundance of each of the combinations, so that will be doable to search for the protein function pathway that are relevant to corosion studies.\n",
        "1. Clasify proteins on their presence in each cat, identify the common in all the categories so to discard them and identify the proteins enriched on cat 2 and 3 in order to keep them for further analyisis.\n",
        "2. Significance test will confirm the differencial abundance by comparing cat 2 and cat 3 usign Kruskal-Wallis. Then we keep only proteins with a significant difference in abundance between 2 and 3 and the inverse.\n",
        "3. That way we can search programaticaly for the meaning of the protein function on the databases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dq8C9vSywZjZ"
      },
      "source": [
        "### Filtering pairs Bacteria-Protein by significance to the risk category by claude\n",
        "It was thought to filter the data by Bacteria, however if this point of view is stablish, some of the pionier species will be neglected and maybe some bystanders will be left. So we are going to filter at the protein level instead of the genus level, which means that for a data to continue it has to have some sort of involvement with corrosion phenomena."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQbYGLCDGd64"
      },
      "outputs": [],
      "source": [
        "'''import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import kruskal\n",
        "from typing import List, Tuple, Dict\n",
        "\n",
        "def classify_proteins_by_category(base_matrix, category_level: int = 1) -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Classify proteins based on their presence and abundance patterns across categories.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    base_matrix : pd.DataFrame\n",
        "        MultiIndex DataFrame with (Site, Category) as rows and (Genus, Protein) as columns\n",
        "    category_level : int\n",
        "        Level in the row MultiIndex that contains categories\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    Dict[str, pd.DataFrame]: Dictionary containing classified proteins with their statistics\n",
        "    \"\"\"\n",
        "    # Get unique categories\n",
        "    categories = sorted(base_matrix.index.get_level_values(category_level).unique())\n",
        "\n",
        "    # Create dictionary to store presence/absence matrices for each category\n",
        "    cat_data = {}\n",
        "    for cat in categories:\n",
        "        cat_data[cat] = base_matrix.xs(cat, level=category_level, axis=0)\n",
        "\n",
        "    # Calculate mean abundance for each protein in each category\n",
        "    cat_means = {cat: data.mean() for cat, data in cat_data.items()}\n",
        "\n",
        "    # Calculate presence (where abundance > 0) for each protein in each category\n",
        "    cat_presence = {cat: (data > 0).any() for cat, data in cat_data.items()}\n",
        "\n",
        "    # Create classification DataFrame\n",
        "    classification_data = []\n",
        "    for col in base_matrix.columns:\n",
        "        genus, protein = col\n",
        "        presence_pattern = tuple(cat_presence[cat][col] for cat in categories)\n",
        "        means = [cat_means[cat][col] for cat in categories]\n",
        "\n",
        "        # Determine abundance pattern\n",
        "        abundance_pattern = \"increasing\" if all(means[i] <= means[i+1] for i in range(len(means)-1)) else \\\n",
        "                          \"decreasing\" if all(means[i] >= means[i+1] for i in range(len(means)-1)) else \\\n",
        "                          \"mixed\"\n",
        "\n",
        "        classification_data.append({\n",
        "            'Genus': genus,\n",
        "            'Protein': protein,\n",
        "            'Presence_Pattern': presence_pattern,\n",
        "            'Abundance_Pattern': abundance_pattern,\n",
        "            **{f'Mean_Cat_{cat}': means[i] for i, cat in enumerate(categories)}\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(classification_data)\n",
        "\n",
        "def perform_statistical_tests(base_matrix, classifications: pd.DataFrame,\n",
        "                            category_level: int = 1, alpha: float = 0.05) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Perform Kruskal-Wallis tests and post-hoc analysis on classified proteins.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    base_matrix : pd.DataFrame\n",
        "        Original data matrix\n",
        "    classifications : pd.DataFrame\n",
        "        Output from classify_proteins_by_category\n",
        "    category_level : int\n",
        "        Level in the row MultiIndex that contains categories\n",
        "    alpha : float\n",
        "        Significance level for statistical tests\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    pd.DataFrame: Statistical test results\n",
        "    \"\"\"\n",
        "    categories = sorted(base_matrix.index.get_level_values(category_level).unique())\n",
        "\n",
        "    # Prepare results storage\n",
        "    stat_results = []\n",
        "\n",
        "    for _, row in classifications.iterrows():\n",
        "        genus, protein = row['Genus'], row['Protein']\n",
        "        col = (genus, protein)\n",
        "\n",
        "        # Get data for each category\n",
        "        cat_data = [base_matrix.xs(cat, level=category_level)[col] for cat in categories]\n",
        "\n",
        "        # Perform Kruskal-Wallis test\n",
        "        h_stat, p_val = kruskal(*cat_data)\n",
        "\n",
        "        # Calculate effect sizes (difference between medians)\n",
        "        medians = [data.median() for data in cat_data]\n",
        "        effect_sizes = [medians[i+1] - medians[i] for i in range(len(medians)-1)]\n",
        "\n",
        "        # Check if pattern meets criteria (cat3 > cat1 and consistent increases)\n",
        "        valid_pattern = (medians[-1] > medians[0]) and all(eff >= 0 for eff in effect_sizes)\n",
        "\n",
        "        if p_val < alpha and valid_pattern:\n",
        "            stat_results.append({\n",
        "                'Genus': genus,\n",
        "                'Protein': protein,\n",
        "                'H_statistic': h_stat,\n",
        "                'p_value': p_val,\n",
        "                'Pattern_Valid': valid_pattern,\n",
        "                **{f'Median_Cat_{cat}': med for cat, med in zip(categories, medians)},\n",
        "                **{f'Effect_Size_{i+1}_to_{i+2}': eff for i, eff in enumerate(effect_sizes)}\n",
        "            })\n",
        "\n",
        "    return pd.DataFrame(stat_results).sort_values('p_value')\n",
        "\n",
        "def analyze_protein_patterns(base_matrix, category_level: int = 1, alpha: float = 0.05):\n",
        "    \"\"\"\n",
        "    Main function to analyze protein patterns across categories.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    base_matrix : pd.DataFrame\n",
        "        Input data matrix\n",
        "    category_level : int\n",
        "        Level in the row MultiIndex that contains categories\n",
        "    alpha : float\n",
        "        Significance level for statistical tests\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    Tuple[pd.DataFrame, pd.DataFrame]: Classifications and statistical results\n",
        "    \"\"\"\n",
        "    # Step 1: Classify proteins\n",
        "    classifications = classify_proteins_by_category(base_matrix, category_level)\n",
        "\n",
        "    # Step 2: Perform statistical tests\n",
        "    significant_results = perform_statistical_tests(base_matrix, classifications,\n",
        "                                                 category_level, alpha)\n",
        "\n",
        "    return classifications, significant_results\n",
        "# Run the analysis\n",
        "classifications, significant_results = analyze_protein_patterns(base_matrix, category_level=1, alpha=0.05)'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKPfQ-TpBedk",
        "outputId": "560f9b0b-d6e3-40bd-e59b-79d70abb4baf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Genus</th>\n",
              "      <th>Protein</th>\n",
              "      <th>H_statistic</th>\n",
              "      <th>p_value</th>\n",
              "      <th>Pattern_Valid</th>\n",
              "      <th>Median_Cat_1</th>\n",
              "      <th>Median_Cat_2</th>\n",
              "      <th>Median_Cat_3</th>\n",
              "      <th>Effect_Size_1_to_2</th>\n",
              "      <th>Effect_Size_2_to_3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>894</th>\n",
              "      <td>Halomonas</td>\n",
              "      <td>Allantoin racemase</td>\n",
              "      <td>11.489951</td>\n",
              "      <td>0.003199</td>\n",
              "      <td>True</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.040519</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.040519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>973</th>\n",
              "      <td>Halomonas</td>\n",
              "      <td>D-malate dehydrogenase</td>\n",
              "      <td>11.003577</td>\n",
              "      <td>0.004079</td>\n",
              "      <td>True</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001870</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001870</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1306</th>\n",
              "      <td>Halomonas</td>\n",
              "      <td>Tartrate dehydrogenase/decarboxylase</td>\n",
              "      <td>11.003577</td>\n",
              "      <td>0.004079</td>\n",
              "      <td>True</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001870</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001870</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1229</th>\n",
              "      <td>Halomonas</td>\n",
              "      <td>Propionyl-CoA synthetase</td>\n",
              "      <td>10.950022</td>\n",
              "      <td>0.004190</td>\n",
              "      <td>True</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.002950</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.002950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1344</th>\n",
              "      <td>Halomonas</td>\n",
              "      <td>UDP-glucuronate 4-epimerase</td>\n",
              "      <td>10.774652</td>\n",
              "      <td>0.004574</td>\n",
              "      <td>True</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000792</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000792</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          Genus                               Protein  H_statistic   p_value  \\\n",
              "894   Halomonas                    Allantoin racemase    11.489951  0.003199   \n",
              "973   Halomonas                D-malate dehydrogenase    11.003577  0.004079   \n",
              "1306  Halomonas  Tartrate dehydrogenase/decarboxylase    11.003577  0.004079   \n",
              "1229  Halomonas              Propionyl-CoA synthetase    10.950022  0.004190   \n",
              "1344  Halomonas           UDP-glucuronate 4-epimerase    10.774652  0.004574   \n",
              "\n",
              "      Pattern_Valid  Median_Cat_1  Median_Cat_2  Median_Cat_3  \\\n",
              "894            True           0.0           0.0      0.040519   \n",
              "973            True           0.0           0.0      0.001870   \n",
              "1306           True           0.0           0.0      0.001870   \n",
              "1229           True           0.0           0.0      0.002950   \n",
              "1344           True           0.0           0.0      0.000792   \n",
              "\n",
              "      Effect_Size_1_to_2  Effect_Size_2_to_3  \n",
              "894                  0.0            0.040519  \n",
              "973                  0.0            0.001870  \n",
              "1306                 0.0            0.001870  \n",
              "1229                 0.0            0.002950  \n",
              "1344                 0.0            0.000792  "
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "significant_results.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Acetobacterium',\n",
              " 'Achromobacter',\n",
              " 'Acidisoma',\n",
              " 'Acidovorax',\n",
              " 'Aestuariimicrobium',\n",
              " 'Afipia',\n",
              " 'Anoxybacillus',\n",
              " 'Azospira',\n",
              " 'Bacillus',\n",
              " 'Beta_proteobacterium',\n",
              " 'Blastomonas',\n",
              " 'Brachybacterium',\n",
              " 'Bradyrhizobium',\n",
              " 'Brevibacterium',\n",
              " 'Brevundimonas',\n",
              " 'Bulleidia',\n",
              " 'Candidatus_desulforudis',\n",
              " 'Caulobacter',\n",
              " 'Chryseobacterium',\n",
              " 'Clostridium_sensu_stricto_12',\n",
              " 'Cutibacterium',\n",
              " 'Dechloromonas',\n",
              " 'Desulfobacterium',\n",
              " 'Desulfobulbus',\n",
              " 'Desulfomicrobium',\n",
              " 'Desulfosporosinus',\n",
              " 'Desulfotomaculum',\n",
              " 'Desulfovibrio',\n",
              " 'Enhydrobacter',\n",
              " 'Enterococcus',\n",
              " 'Erysipelothrix',\n",
              " 'Flavisolibacter',\n",
              " 'Gallionella',\n",
              " 'Gelria',\n",
              " 'Geothrix',\n",
              " 'Herbaspirillum',\n",
              " 'Hydrogenophaga',\n",
              " 'Legionella',\n",
              " 'Methylocystis',\n",
              " 'Micrococcus',\n",
              " 'Mycobacterium',\n",
              " 'Mycoplana',\n",
              " 'Neisseria',\n",
              " 'Nitrospira',\n",
              " 'Oerskovia',\n",
              " 'Opitutus',\n",
              " 'Oxalobacteraceae_unclassified',\n",
              " 'Oxobacter',\n",
              " 'Paracoccus',\n",
              " 'Phenylobacterium',\n",
              " 'Phreatobacter',\n",
              " 'Porphyrobacter',\n",
              " 'Prevotella',\n",
              " 'Propionibacterium',\n",
              " 'Propionivibrio',\n",
              " 'Psb-m-3',\n",
              " 'Pseudarthrobacter',\n",
              " 'Pseudoalteromonas',\n",
              " 'Pseudorhodoferax',\n",
              " 'Pseudoxanthomonas',\n",
              " 'Ralstonia',\n",
              " 'Ruminiclostridium_1',\n",
              " 'Sediminibacterium',\n",
              " 'Shewanella',\n",
              " 'Silanimonas',\n",
              " 'Simplicispira',\n",
              " 'Smithella',\n",
              " 'Sphingobium',\n",
              " 'Sphingomonas',\n",
              " 'Sphingopyxis',\n",
              " 'Staphylococcus',\n",
              " 'Streptococcus',\n",
              " 'Syntrophus',\n",
              " 'Tepidimonas',\n",
              " 'Tessaracoccus',\n",
              " 'Thermincola',\n",
              " 'Thiobacillus',\n",
              " 'Treponema',\n",
              " 'Variovorax',\n",
              " 'Wchb1-05'}"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_genera =classifications['Genus'].unique()\n",
        "significant_genera = significant_results['Genus'].unique()\n",
        "genera_removed = set(all_genera) - set(significant_genera)\n",
        "genera_removed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7AigABQiIH9"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2P2IgJdFjM2"
      },
      "outputs": [],
      "source": [
        "# View classifications\n",
        "print(\"Protein Classifications:\")\n",
        "print(classifications.head())\n",
        "\n",
        "# View significant results\n",
        "print(\"\\nSignificant Results:\")\n",
        "print(significant_results.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_830Bc_bGY3w"
      },
      "source": [
        "## Adapting for Source Groups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JaOwWSY1DFP9"
      },
      "outputs": [],
      "source": [
        "# Suppose known_bacteria_list is your list of genera\n",
        "group_cols_known = [col for col in base_matrix.columns if col[0] in known_bacteria_list]\n",
        "base_matrix_known = base_matrix.loc[:, group_cols_known]\n",
        "\n",
        "plot_top_proteins_across_categories(\n",
        "    base_matrix_known,\n",
        "    categories=[1,2,3],\n",
        "    n_top=5,\n",
        "    n_genera=10,\n",
        "    category_level=1\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tLvr9eYYel7"
      },
      "source": [
        "# 10. Pathways Analysis\n",
        "\n",
        "## 10.1. Pathways distribution by Risk Category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T10:23:06.776683Z",
          "iopub.status.busy": "2025-02-19T10:23:06.776256Z",
          "iopub.status.idle": "2025-02-19T10:23:13.31924Z",
          "shell.execute_reply": "2025-02-19T10:23:13.31809Z",
          "shell.execute_reply.started": "2025-02-19T10:23:06.776651Z"
        },
        "id": "dakf67H0Yel7",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def prepare_pathway_pca(metabolic_info, use_col='Pathways'):\n",
        "    \"\"\"\n",
        "    Convert pathway strings to numeric features for PCA\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    metabolic_info : pandas.DataFrame,   DataFrame with 'Pathways' column containing comma-separated pathway strings\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    X_pca : numpy.ndarray,    PCA transformed data\n",
        "    explained_variance_ratio : numpy.ndarray,     Explained variance ratios\n",
        "    loadings : pandas.DataFrame,   PCA loadings with pathway names as index\n",
        "    pathway_matrix : pandas.Dataframe,  Binary matrix of pathway presence/absence (useful for further analysis)\n",
        "    \"\"\"\n",
        "    # Handle NaN values first\n",
        "    valid_data = metabolic_info[metabolic_info[use_col].notna()]\n",
        "\n",
        "    # Create set of unique items with explicit string handling\n",
        "    all_items = set()\n",
        "    for item_str in valid_data[use_col]:\n",
        "        if isinstance(item_str, str):  # Ensure it's a string\n",
        "            items = [i.strip() for i in item_str.strip('[]').split(',') if i.strip()]\n",
        "            all_items.update(items)\n",
        "\n",
        "    # Create binary matrix with explicit index preservation\n",
        "    data_dict = {}\n",
        "    original_index = metabolic_info.index\n",
        "\n",
        "    for item in all_items:\n",
        "        if item:  # Skip empty strings\n",
        "            item_escaped = re.escape(item)\n",
        "            data_dict[item] = metabolic_info[use_col].str.contains(\n",
        "                item_escaped,\n",
        "                regex=True,\n",
        "                na=False\n",
        "            ).astype(int)\n",
        "\n",
        "    data_matrix = pd.DataFrame(data_dict, index=original_index)\n",
        "\n",
        "    # Print debug info\n",
        "    print(f\"Created matrix with {data_matrix.shape[1]} features\")\n",
        "    print(f\"Non-zero entries: {data_matrix.astype(bool).sum().sum()}\")\n",
        "\n",
        "    # Run PCA with explicit scaling\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(data_matrix)\n",
        "    pca = PCA(n_components=2)\n",
        "    X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "    loadings = pd.DataFrame(\n",
        "        pca.components_.T,\n",
        "        index=data_matrix.columns,\n",
        "        columns=['PC1', 'PC2']\n",
        "    )\n",
        "\n",
        "    return X_pca, pca.explained_variance_ratio_, loadings, data_matrix\n",
        "\n",
        "def plot_metabolic_pca_results(X_pca, explained_variance, metabolic_sites_info, category_dict, title, category_colors, categories_labels):\n",
        "    \"\"\"\n",
        "    Plot PCA results for pathways with risk categories\n",
        "\n",
        "    Parameters:     X_pca : numpy array  PCA transformed coordinates\n",
        "                    explained_variance : numpy array   Explained variance ratios\n",
        "                    metabolic_info : pandas DataFrame   The metabolic info DataFrame with Sites index\n",
        "                    category_dict : dict     Mapping of sites to categories\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 8))\n",
        "\n",
        "    # Get categories for each site in metabolic_info\n",
        "    if isinstance(metabolic_sites_info.index, pd.MultiIndex):\n",
        "        sites = metabolic_sites_info.index.get_level_values('Sites')\n",
        "    else:\n",
        "        sites = metabolic_sites_info.index\n",
        "\n",
        "    plot_categories = pd.Series(sites).map(category_dict)\n",
        "\n",
        "    # Plot each category\n",
        "    for category in sorted(set(plot_categories)):\n",
        "        mask = plot_categories == category\n",
        "        plt.scatter( X_pca[mask, 0], X_pca[mask, 1], c=category_colors[category],\n",
        "            label=categories_labels[category], alpha=0.7, s=100)\n",
        "\n",
        "    plt.xlabel(f'PC1 ({explained_variance[0]:.1%} variance explained)')\n",
        "    plt.ylabel(f'PC2 ({explained_variance[1]:.1%} variance explained)')\n",
        "    plt.title(title)\n",
        "    plt.legend(title='Risk Category')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "# For pathway PCA\n",
        "X_pca_path, var_ratio_path, loadings_path, pathway_matrix = prepare_pathway_pca(metabolic_sites_info, use_col='Pathways')\n",
        "\n",
        "plot_metabolic_pca_results( X_pca_path, var_ratio_path,  metabolic_sites_info, category_dict, \"Pathways PCA by Risk Category\", category_colors, categories_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqmkIJeaYel7"
      },
      "source": [
        "## 10.2. Top Pathways Loadings by Category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T10:27:14.136175Z",
          "iopub.status.busy": "2025-02-19T10:27:14.13575Z",
          "iopub.status.idle": "2025-02-19T10:27:15.194506Z",
          "shell.execute_reply": "2025-02-19T10:27:15.193297Z",
          "shell.execute_reply.started": "2025-02-19T10:27:14.136142Z"
        },
        "id": "gY-sQA13Yel7",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def plot_pca_loadings_heatmap(loadings, top_n=20):\n",
        "    \"\"\"Plot a heatmap of pathway loadings for PC1 and PC2.\n",
        "       Parameters:     loadings: DataFrame with PCA loadings\n",
        "       top_n: Number of top pathways to display     \"\"\"\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    # Select top pathways based on absolute contribution to PC1 and PC2\n",
        "    top_pathways = (loadings[['PC1', 'PC2']].abs().sum(axis=1).nlargest(top_n).index)\n",
        "    # Filter the loadings dataframe\n",
        "    heatmap_data = loadings.loc[top_pathways, ['PC1', 'PC2']]\n",
        "    sns.heatmap(heatmap_data, annot=True, cmap='coolwarm', center=0)\n",
        "    plt.title('Top Pathway Contributions to PC1 and PC2')\n",
        "    plt.xlabel('Principal Components')\n",
        "    plt.ylabel('Pathways')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_pca_loadings_heatmap(loadings_genera)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXRurjWFuvgW"
      },
      "source": [
        "## 10.3. Pathways patterns by source groups"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GH3EfeFrYel8"
      },
      "source": [
        "|Sites|---|site_1|site_1|site_1|site_2|site_2|site_2|site_2|\n",
        "|---|---|---|---|---|---|---|---|---|\n",
        "|Genus|---|genus_1|genus_2|genus3|genus_2|genus_70|genus_154|genus_520|\n",
        "|Pathways|---|---|---|---|---|---|---|---|\n",
        "|pathway_1|---|---|---|---|---|---|---|---|\n",
        "|pathway_2|---|---|---|---|---|---|---|---|\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T13:18:06.432257Z",
          "iopub.status.busy": "2025-02-19T13:18:06.431835Z",
          "iopub.status.idle": "2025-02-19T13:18:06.44645Z",
          "shell.execute_reply": "2025-02-19T13:18:06.445098Z",
          "shell.execute_reply.started": "2025-02-19T13:18:06.432202Z"
        },
        "id": "G0i9EqyPYel8",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def analyze_bacterial_groups(base_matrix, metabolic_sites_info, source_groups):\n",
        "    \"\"\"\n",
        "    Analyze relationships between bacterial groups and functional patterns.\n",
        "\n",
        "    Parameters:\n",
        "    - base_matrix: DataFrame with sites and functional data.\n",
        "      (Columns are multi-indexed (Site, Genus) or similar structure.)\n",
        "    - metabolic_sites_info: DataFrame with site-genus level information.\n",
        "    - source_groups: dict with group names as keys and list of genera as values.\n",
        "\n",
        "    Returns:\n",
        "    - results: dict with analysis results for each group.\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "\n",
        "    for source_name, genus_list in source_groups.items():\n",
        "\n",
        "        # Filter columns where the first level (e.g., site or genus) is in the group list.\n",
        "        group_cols = [col for col in base_matrix.columns if col[0] in genus_list]\n",
        "        group_data = base_matrix.loc[:, group_cols]\n",
        "\n",
        "        # Standardize the data\n",
        "        scaler = MinMaxScaler() # Changing from standard scaler to robustscaler\n",
        "        scaled_data = scaler.fit_transform(group_data)\n",
        "\n",
        "        # PCA analysis\n",
        "        pca = PCA(n_components=5)\n",
        "        pca_result = pca.fit_transform(scaled_data)\n",
        "\n",
        "        print(\"\\nPCA Variance Explained:\")\n",
        "        for i, var in enumerate(pca.explained_variance_ratio_):\n",
        "            print(f\"PC{i+1}: {var:.2%}\")\n",
        "        print(f\"Total variance explained: {sum(pca.explained_variance_ratio_):.2%}\")\n",
        "\n",
        "        # UMAP analysis\n",
        "        reducer = umap.UMAP(random_state=42)\n",
        "        umap_result = reducer.fit_transform(scaled_data)\n",
        "\n",
        "        # Save results for current group\n",
        "        results[source_name] = {\n",
        "            'pca': pca_result,\n",
        "            'umap': umap_result,\n",
        "            'pca_explained': pca.explained_variance_ratio_,\n",
        "            'data': group_data\n",
        "        }\n",
        "\n",
        "        # Plottinextract categories from base_matrix index if available.\n",
        "        try:\n",
        "            categories = base_matrix.index.get_level_values('Category')\n",
        "        except (KeyError, AttributeError):\n",
        "            # If no 'Category' level, assign a default category (e.g., all 1)\n",
        "            categories = pd.Series(np.ones(group_data.shape[0]), index=group_data.index)\n",
        "\n",
        "        # PCA plot\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "        for cat in sorted(set(categories)):\n",
        "            mask = categories == cat\n",
        "            ax1.scatter(pca_result[mask, 0],\n",
        "                        pca_result[mask, 1],\n",
        "                        c=category_colors.get(cat, '#000000'),\n",
        "                        label=categories_labels.get(cat, f'Cat {cat}'),\n",
        "                        alpha=0.7)\n",
        "        ax1.set_title(f'PCA - {source_name}')\n",
        "        ax1.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
        "        ax1.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
        "        ax1.legend()\n",
        "\n",
        "        # UMAP plot\n",
        "        for cat in sorted(set(categories)):\n",
        "            mask = categories == cat\n",
        "            ax2.scatter(umap_result[mask, 0],\n",
        "                        umap_result[mask, 1],\n",
        "                        c=category_colors.get(cat, '#000000'),\n",
        "                        label=categories_labels.get(cat, f'Cat {cat}'),\n",
        "                        alpha=0.7)\n",
        "        ax2.set_title(f'UMAP - {source_name}')\n",
        "        ax2.set_xlabel('UMAP 1')\n",
        "        ax2.set_ylabel('UMAP 2')\n",
        "        ax2.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # PCA Explained Variance plot\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(range(1, len(pca.explained_variance_ratio_) + 1),\n",
        "                 np.cumsum(pca.explained_variance_ratio_), 'bo-')\n",
        "        plt.xlabel('Number of Components')\n",
        "        plt.ylabel('Cumulative Explained Variance Ratio')\n",
        "        plt.title(f'PCA Explained Variance - {source_name}')\n",
        "        plt.grid(True)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BjwgGTo22fJB"
      },
      "outputs": [],
      "source": [
        "print(f\"{known_bacteria}: group_data.shape = {group_data.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNhqT9G92y19"
      },
      "outputs": [],
      "source": [
        "known_bacteria_list = ['Clostridium', 'Corynebacterium', 'Novosphingobium', 'Streptococcus', 'Thiobacillus', 'Acetobacterium', 'Bacillus', 'Desulfotomaculum', 'Desulfovibrio', 'Micrococcus', 'Propionibacterium',\n",
        " 'Pseudomonas', 'Staphylococcus', 'Desulfobacterium', 'Desulfobulbus', 'Gallionella', 'Shewanella']\n",
        "\n",
        "group_cols_known = [col for col in base_matrix.columns if col[0] in known_bacteria_list]\n",
        "group_data_known = base_matrix.loc[:, group_cols_known]\n",
        "\n",
        "group_data_known.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJteNaoZ46f7"
      },
      "outputs": [],
      "source": [
        "print(group_data_known.head(), group_data_known.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWvbkrMc6_Yu"
      },
      "outputs": [],
      "source": [
        "for gname, glist in source_groups.items():\n",
        "    group_cols = [col for col in base_matrix.columns if col[0] in glist]\n",
        "    tmp_data = base_matrix.loc[:, group_cols]\n",
        "    print(gname, tmp_data.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ad6mjcN7MhN"
      },
      "outputs": [],
      "source": [
        "corr_matrix = group_data_known.corr()\n",
        "corr_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z55hs_rh7ULF"
      },
      "outputs": [],
      "source": [
        "corr_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eH_ifJ0wnnLo"
      },
      "source": [
        "## Bacterial Groups Analysis Component"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IsAIvUBYntEu"
      },
      "outputs": [],
      "source": [
        "def analyze_combined_groups(base_matrix, source_groups, group_names=['checked_core', 'Influencers_uniques']):\n",
        "    \"\"\"\n",
        "    Analyze combined bacterial groups while preserving their individual contributions.\n",
        "\n",
        "    Parameters:\n",
        "    - base_matrix: DataFrame with sites and functional data\n",
        "    - source_groups: dict with group names as keys and list of genera as values\n",
        "    - group_names: list of group names to combine\n",
        "\n",
        "    Returns:\n",
        "    - Combined analysis results including PCA, UMAP and variance explained\n",
        "    \"\"\"\n",
        "    # Filter for selected groups\n",
        "    selected_genera = []\n",
        "    for group in group_names:\n",
        "        selected_genera.extend(source_groups[group])\n",
        "\n",
        "    # Remove duplicates while preserving order\n",
        "    selected_genera = list(dict.fromkeys(selected_genera))\n",
        "\n",
        "    # Filter columns for selected genera\n",
        "    group_cols = [col for col in base_matrix.columns if col[0] in selected_genera]\n",
        "    combined_data = base_matrix.loc[:, group_cols]\n",
        "\n",
        "    # Remove zero columns\n",
        "    combined_data = combined_data.loc[:, (combined_data != 0).any(axis=0)]\n",
        "\n",
        "    # Standardize\n",
        "    scaler = StandardScaler()\n",
        "    scaled_data = scaler.fit_transform(combined_data)\n",
        "\n",
        "    # PCA\n",
        "    pca = PCA(n_components=3)\n",
        "    pca_result = pca.fit_transform(scaled_data)\n",
        "\n",
        "    # UMAP\n",
        "    reducer = umap.UMAP(random_state=42)\n",
        "    umap_result = reducer.fit_transform(scaled_data)\n",
        "\n",
        "    results = {\n",
        "        'pca': pca_result,\n",
        "        'umap': umap_result,\n",
        "        'pca_explained': pca.explained_variance_ratio_,\n",
        "        'data': combined_data,\n",
        "        'genera': selected_genera\n",
        "    }\n",
        "\n",
        "    # Plottinextract categories from base_matrix index if available.\n",
        "    try:\n",
        "        categories = base_matrix.index.get_level_values('Category')\n",
        "    except (KeyError, AttributeError):\n",
        "        # If no 'Category' level, assign a default category (e.g., all 1)\n",
        "        categories = pd.Series(np.ones(combined_data.shape[0]), index=combined_data.index)\n",
        "\n",
        "    # PCA plot\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "    for cat in sorted(set(categories)):\n",
        "        mask = categories == cat\n",
        "        ax1.scatter(pca_result[mask, 0],\n",
        "                    pca_result[mask, 1],\n",
        "                    c=category_colors.get(cat, '#000000'),\n",
        "                    label=categories_labels.get(cat, f'Cat {cat}'),\n",
        "                    alpha=0.7)\n",
        "    ax1.set_title(f'PCA - {combined_data}')\n",
        "    ax1.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
        "    ax1.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
        "    ax1.legend()\n",
        "\n",
        "    # UMAP plot\n",
        "    for cat in sorted(set(categories)):\n",
        "        mask = categories == cat\n",
        "        ax2.scatter(umap_result[mask, 0],\n",
        "                    umap_result[mask, 1],\n",
        "                    c=category_colors.get(cat, '#000000'),\n",
        "                    label=categories_labels.get(cat, f'Cat {cat}'),\n",
        "                    alpha=0.7)\n",
        "    ax2.set_title(f'UMAP - {group_names}')\n",
        "    ax2.set_xlabel('UMAP 1')\n",
        "    ax2.set_ylabel('UMAP 2')\n",
        "    ax2.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # PCA Explained Variance plot\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(range(1, len(pca.explained_variance_ratio_) + 1),\n",
        "              np.cumsum(pca.explained_variance_ratio_), 'bo-')\n",
        "    plt.xlabel('Number of Components')\n",
        "    plt.ylabel('Cumulative Explained Variance Ratio')\n",
        "    plt.title(f'PCA Explained Variance - {group_names}')\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Voj9pVqlAR3E"
      },
      "outputs": [],
      "source": [
        "results = analyze_combined_groups(base_matrix, source_groups, group_names=['checked_core', 'Influencers_uniques'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuOHFlIQJ1YD"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJhbme4LoEgY"
      },
      "outputs": [],
      "source": [
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ytCoE1icYel8",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "from statsmodels.stats.multitest import multipletests\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def analyze_category_enrichment(base_matrix, category_dict, source_groups):\n",
        "    \"\"\"\n",
        "    Analyze pathway enrichment within each risk category.\n",
        "    Uses relative abundance and statistical testing to identify\n",
        "    significantly enriched proteins in each category.\n",
        "    \"\"\"\n",
        "    # Get the sites and categories from the MultiIndex\n",
        "    sites_categories = pd.Series(\n",
        "        base_matrix.index.get_level_values('Category'),\n",
        "        index=base_matrix.index.get_level_values('Sites')\n",
        "    )\n",
        "\n",
        "    def get_enrichment_for_group(group_data, category):\n",
        "        # Get data for this category\n",
        "        cat_mask = group_data.index.get_level_values(\"Category\")  == category\n",
        "        cat_data = group_data[cat_mask]\n",
        "        other_data= group_data[~cat_mask]\n",
        "\n",
        "        # Calculate mean abundances\n",
        "        cat_means = cat_data.mean()\n",
        "        other_means = other_data.mean()\n",
        "\n",
        "        # Calculate fold change\n",
        "        fold_change = np.log2(cat_means / other_means)\n",
        "\n",
        "        # Perform statistical test (Mann-Whitney U)\n",
        "        pvalues = []\n",
        "        for col in group_data.columns:\n",
        "            stat, pval = stats.mannwhitneyu(\n",
        "                cat_data[col],\n",
        "                other_data[col],\n",
        "                alternative='greater'\n",
        "            )\n",
        "            pvalues.append(pval)\n",
        "\n",
        "        # Create results DataFrame\n",
        "        results = pd.DataFrame({\n",
        "            'fold_change': fold_change,\n",
        "            'pvalue': pvalues,\n",
        "            'mean_abundance': cat_means\n",
        "        })\n",
        "\n",
        "        # Add multiple testing correction\n",
        "        results['padj'] = multipletests(results['pvalue'], method='fdr_bh')[1]\n",
        "\n",
        "        return results\n",
        "\n",
        "    enrichment_results = {}\n",
        "\n",
        "    # Analyze each source group\n",
        "    for group_name, genera in source_groups.items():\n",
        "        print(f\"\\nAnalyzing {group_name}...\")\n",
        "\n",
        "        # Filter for genera in this group\n",
        "        group_cols = [col for col in base_matrix.columns if col[0] in genera]\n",
        "        if not group_cols:\n",
        "            continue\n",
        "\n",
        "        group_data = base_matrix[group_cols]\n",
        "\n",
        "        # Get enrichment for each category\n",
        "        group_results = {}\n",
        "        for cat in [1, 2, 3]:\n",
        "            results = get_enrichment_for_group(group_data, cat)\n",
        "            group_results[cat] = results\n",
        "\n",
        "        enrichment_results[group_name] = group_results\n",
        "\n",
        "        # Plot volcano plots for each category\n",
        "        plt.figure(figsize=(15, 5))\n",
        "        plt.suptitle(f\"Protein Enrichment Analysis - {group_name}\", y=1.05)\n",
        "\n",
        "        for i, cat in enumerate([1, 2, 3], 1):\n",
        "            results = group_results[cat]\n",
        "\n",
        "            plt.subplot(1, 3, i)\n",
        "\n",
        "            # Create volcano plot\n",
        "            plt.scatter(\n",
        "                results['fold_change'],\n",
        "                -np.log10(results['padj']),\n",
        "                alpha=0.6,\n",
        "                c=category_colors[cat],\n",
        "                s= results['mean_abundance']*1000\n",
        "            )\n",
        "\n",
        "            # Add significance lines\n",
        "            plt.axhline(-np.log10(0.05), color='red', linestyle='--', alpha=0.3)\n",
        "            plt.axvline(0, color='black', linestyle='--', alpha=0.3)\n",
        "\n",
        "            plt.title(f\"{categories_labels[cat]}\")\n",
        "            plt.xlabel(\"Log2 Fold Change\")\n",
        "            plt.ylabel(\"-log10(adjusted p-value)\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Print top enriched proteins\n",
        "        for cat in [1, 2, 3]:\n",
        "            results = group_results[cat]\n",
        "            significant = results[results['padj'] < 0.05].sort_values('fold_change', ascending=False)\n",
        "\n",
        "            print(f\"\\nTop enriched proteins in {categories_labels[cat]} for {group_name}:\")\n",
        "            if len(significant) > 0:\n",
        "                print(significant.head(10))\n",
        "            else:\n",
        "                print(\"No significantly enriched proteins found\")\n",
        "\n",
        "    return enrichment_results\n",
        "\n",
        "# Run the analysis\n",
        "enrichment_results = analyze_category_enrichment(base_matrix, category_dict, source_groups)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpDMQg0fZVGn"
      },
      "outputs": [],
      "source": [
        "def create_comparison_table(enrichment_results):\n",
        "    \"\"\"\n",
        "    Create a structured comparison table from enrichment results\n",
        "    \"\"\"\n",
        "    # Create empty list to store rows\n",
        "    comparison_rows = []\n",
        "\n",
        "    for group_name, group_results in enrichment_results.items():\n",
        "        for category in [1, 2, 3]:\n",
        "            if category in group_results:\n",
        "                results = group_results[category]\n",
        "                significant = results[results['padj'] < 0.05]\n",
        "\n",
        "                if len(significant) > 0:\n",
        "                    for idx, row in significant.head(10).iterrows():\n",
        "                        comparison_rows.append({\n",
        "                            'Group': group_name,\n",
        "                            'Category': categories_labels[category],\n",
        "                            'Genus': idx[0],\n",
        "                            'Protein': idx[1],\n",
        "                            'Fold_Change': row['fold_change'],\n",
        "                            'Padj': row['padj'],\n",
        "                            'Mean_Abundance': row['mean_abundance']\n",
        "                        })\n",
        "\n",
        "    # Create DataFrame\n",
        "    comparison_df = pd.DataFrame(comparison_rows)\n",
        "\n",
        "    # Save to CSV with proper formatting\n",
        "    comparison_df.to_csv('enrichment_comparison.csv', index=False)\n",
        "\n",
        "    return comparison_df\n",
        "\n",
        "# Create comparison table\n",
        "comparison_table = create_comparison_table(enrichment_results)\n",
        "\n",
        "# Display formatted table\n",
        "print(\"\\nComparison Table Preview:\")\n",
        "print(comparison_table.to_string())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zoPuDB4Yel8"
      },
      "source": [
        "__________________________________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxMZgylFpBX4"
      },
      "source": [
        "https://www.youtube.com/watch?v=jQVNsyAnDMo\n",
        "\n",
        "https://microreact.org/\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWCQOr4KuvgW",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def create_integrated_visualization(df, results, metadata=None):\n",
        "    \"\"\"\n",
        "    Create an integrated visualization combining PCA, clustering, and metadata\n",
        "\n",
        "    Parameters:\n",
        "    df: Original pathway data\n",
        "    results: Results from explore_pathway_patterns\n",
        "    metadata: DataFrame with risk labels, materials, etc.\n",
        "    \"\"\"\n",
        "    fig = plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # 1. PCA with clustering\n",
        "    pca_data = results['pca']['components']\n",
        "    clusters = results['clustering'][5]['kmeans']  # Using k=5 clusters\n",
        "\n",
        "    plt.subplot(2, 2, 1)\n",
        "    scatter = plt.scatter(pca_data[:, 0], pca_data[:, 1],\n",
        "                         c=clusters, cmap='Set2', alpha=0.6)\n",
        "    plt.title('PCA Components with Clusters')\n",
        "    plt.xlabel('PC1')\n",
        "    plt.ylabel('PC2')\n",
        "    plt.colorbar(scatter, label='Cluster')\n",
        "\n",
        "    # 2. Top pathway contributions\n",
        "    plt.subplot(2, 2, 2)\n",
        "    top_loadings = abs(results['pca']['loadings']['PC1']).nlargest(10)\n",
        "    sns.barplot(x=top_loadings.values, y=top_loadings.index)\n",
        "    plt.title('Top 10 Pathways Contributing to PC1')\n",
        "    plt.xlabel('Absolute Loading')\n",
        "\n",
        "    # 3. Correlation structure summary\n",
        "    plt.subplot(2, 2, 3)\n",
        "    corr_summary = results['correlation'].abs().mean()\n",
        "    sns.histplot(corr_summary, bins=50)\n",
        "    plt.title('Distribution of Mean Correlation Strengths')\n",
        "    plt.xlabel('Mean |Correlation|')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZvr6vjWuvgW",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "create_integrated_visualization(base_matrix, results_patterns, metadata=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bk9_ZGhfuvgW"
      },
      "source": [
        "## 9.3 Analysing Pathways Organic Fate\n",
        "\n",
        "Now the task is to identify the most abundant pathways in the samples, focusing specifically on organic matter-related metabolism. Ultimately creating visualizations to understand pathway distributions and analyze correlations between pathways."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11HYL0iNuvgW",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def analyze_metabolic_pathways(df):\n",
        "    \"\"\"\n",
        "    Analyze metabolic pathways from PICRUSt output\n",
        "\n",
        "    Parameters:\n",
        "    df: pandas DataFrame with pathways as index and samples as columns\n",
        "    \"\"\"\n",
        "    # Calculate mean abundance across samples for each pathway\n",
        "    mean_abundance = df.mean(axis=1).sort_values(ascending=False)\n",
        "\n",
        "    # Get top 20 most abundant pathways\n",
        "    top_pathways = mean_abundance.head(20)\n",
        "\n",
        "    # Create heatmap of top pathways across samples\n",
        "    plt.figure(figsize=(15, 8))\n",
        "    sns.heatmap(df.loc[top_pathways.index],\n",
        "                cmap='YlOrRd',\n",
        "                center=0,\n",
        "                robust=True,\n",
        "                xticklabels=True,\n",
        "                yticklabels=True)\n",
        "    plt.title('Top 20 Most Abundant Pathways Across Samples')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Filter for organic matter metabolism related pathways\n",
        "    organic_terms = ['carbon', 'carbohydrate', 'lipid', 'fatty acid',\n",
        "                    'organic acid', 'amino acid', 'degradation']\n",
        "\n",
        "    organic_pathways = df.index[df.index.str.lower().str.contains('|'.join(organic_terms))]\n",
        "    organic_data = df.loc[organic_pathways]\n",
        "\n",
        "    # Calculate summary statistics for organic matter pathways\n",
        "    pathway_stats = pd.DataFrame({\n",
        "        'mean_abundance': organic_data.mean(axis=1),\n",
        "        'std_abundance': organic_data.std(axis=1),\n",
        "        'cv': organic_data.std(axis=1) / organic_data.mean(axis=1) * 100\n",
        "    }).sort_values('mean_abundance', ascending=False)\n",
        "\n",
        "    return pathway_stats, organic_data\n",
        "\n",
        "def plot_pathway_distribution(pathway_stats):\n",
        "    \"\"\"Plot distribution of pathway abundances\"\"\"\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.boxplot(data=pathway_stats.reset_index(),\n",
        "                x='mean_abundance',\n",
        "                y='index',\n",
        "                order=pathway_stats.index[:15])\n",
        "    plt.title('Top 15 Organic Matter Related Pathways')\n",
        "    plt.xlabel('Mean Abundance')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Calling the function\n",
        "stats, organic_data = analyze_metabolic_pathways(Picrust_Result)\n",
        "plot_pathway_distribution(stats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZkQ1j5X5fHL",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def analyze_pathway_patterns(df):\n",
        "    \"\"\"\n",
        "    Analyze pathway patterns using sites vs pathways abundances\n",
        "    \"\"\"\n",
        "    # Create the correct matrix: sites vs pathways with abundances\n",
        "    pathway_matrix = df.pivot_table(\n",
        "        values='norm_abund_contri',\n",
        "        index='Sites',          # Sites as rows\n",
        "        columns='Pathways',     # Pathways as columns\n",
        "        aggfunc='sum',          # Sum abundances\n",
        "        fill_value=0\n",
        "    )\n",
        "\n",
        "    print(\"Matrix shape:\", pathway_matrix.shape)\n",
        "\n",
        "    # Standardize data\n",
        "    scaler = StandardScaler()\n",
        "    scaled_data = scaler.fit_transform(pathway_matrix)\n",
        "\n",
        "    # PCA\n",
        "    pca = PCA(n_components=5)\n",
        "    X_pca = pca.fit_transform(scaled_data)\n",
        "\n",
        "    # UMAP\n",
        "    reducer = umap.UMAP(random_state=42)\n",
        "    umap_result = reducer.fit_transform(scaled_data)\n",
        "\n",
        "    # Get categories for sites\n",
        "    categories = pd.Series(pathway_matrix.index).map(lambda x: category_dict[x])\n",
        "\n",
        "    # Plot both PCA and UMAP\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
        "\n",
        "    # PCA explained variance\n",
        "    ax1.plot(range(1, 6), pca.explained_variance_ratio_, 'bo-')\n",
        "    print(\"\\nPCA Variance Explained:\")\n",
        "    print(f\"Total (5 components): {sum(pca.explained_variance_ratio_):.2%}\")\n",
        "    ax1.set_title('PCA Explained Variance')\n",
        "    ax1.set_xlabel('Component')\n",
        "    ax1.set_ylabel('Explained Variance Ratio')\n",
        "\n",
        "    # PCA scatter\n",
        "    for cat in category_colors.keys():\n",
        "        mask = categories == cat\n",
        "        ax2.scatter(X_pca[mask, 0],\n",
        "                   X_pca[mask, 1],\n",
        "                   c=category_colors[cat],\n",
        "                   label=categories_labels[cat],\n",
        "                   alpha=0.7)\n",
        "\n",
        "    ax2.set_title('PCA First Two Components')\n",
        "    ax2.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
        "    ax2.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
        "    ax2.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # UMAP plot\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    for cat in category_colors.keys():\n",
        "        mask = categories == cat\n",
        "        plt.scatter(umap_result[mask, 0],\n",
        "                   umap_result[mask, 1],\n",
        "                   c=category_colors[cat],\n",
        "                   label=categories_labels[cat],\n",
        "                   alpha=0.7)\n",
        "\n",
        "    plt.title('UMAP Projection of Pathways')\n",
        "    plt.xlabel('UMAP1')\n",
        "    plt.ylabel('UMAP2')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Run the analysis\n",
        "results = analyze_pathway_patterns(metabolic_sites_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9GJgu3yuvgW",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# To analyze specific pathways of interest:\n",
        "def analyze_specific_pathways(df, pathway_list):\n",
        "    \"\"\"\n",
        "    Analyze specific pathways of interest\n",
        "\n",
        "    Parameters:\n",
        "    df: DataFrame with pathway data\n",
        "    pathway_list: list of pathway names to analyze\n",
        "    \"\"\"\n",
        "    specific_data = df.loc[df.index.str.contains('|'.join(pathway_list), case=False)]\n",
        "\n",
        "    # Create correlation matrix for these pathways\n",
        "    corr = specific_data.T.corr()\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(corr, annot=True, cmap='coolwarm', center=0)\n",
        "    plt.title('Correlation between Selected Pathways')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return specific_data.describe()\n",
        "\n",
        "# Calling the funtion\n",
        "Description = analyze_specific_pathways(Picrust_Result, Picrust_Result.index.tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HEIwON2uvgW"
      },
      "source": [
        "## 9.4. Pathways Relevant to Corrosion\n",
        "This code witll categorise pathways into key groups: sulfur metabolism (critical for sulfate-reducing bacteria), Metal-related pathways (iron, manganese, etc.); organic acid production (which can influence local pH); biofilm formation (important for corrosion processes) and electron transfer mechanisms. Then it would analyse correlations between these different categories to understand potential synergistic effects, identifying the most abundant pathways in each category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8FN-ChvuvgW",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def analyze_corrosion_pathways(df):\n",
        "    \"\"\"\n",
        "    Analyze pathways relevant to microbially influenced corrosion (MIC)\n",
        "\n",
        "    Parameters:\n",
        "    df: pandas DataFrame with pathways as index and samples as columns\n",
        "    \"\"\"\n",
        "    # Define relevant pathway terms for different corrosion mechanisms\n",
        "    pathway_categories = {\n",
        "        'sulfur': ['sulfur', 'sulfate', 'sulfide', 'thiosulfate', 'sulfite', 'sulfonate'],\n",
        "        'metal': ['iron', 'metal', 'Fe', 'manganese', 'chromium', 'nickel'],\n",
        "        'organic_acid': ['organic acid', 'acetate', 'formate', 'lactate', 'pyruvate'],\n",
        "        'biofilm': ['biofilm', 'exopolysaccharide', 'EPS', 'adhesion'],\n",
        "        'electron_transfer': ['cytochrome', 'electron transport', 'oxidoreductase']\n",
        "    }\n",
        "\n",
        "    # Function to filter pathways by category\n",
        "    def get_category_pathways(terms):\n",
        "        return df.index[df.index.str.lower().str.contains('|'.join(terms), regex=True)]\n",
        "\n",
        "    # Analyze each category\n",
        "    category_data = {}\n",
        "    category_stats = {}\n",
        "\n",
        "    for category, terms in pathway_categories.items():\n",
        "        pathways = get_category_pathways(terms)\n",
        "        if len(pathways) > 0:\n",
        "            category_data[category] = df.loc[pathways]\n",
        "            category_stats[category] = pd.DataFrame({\n",
        "                'mean_abundance': category_data[category].mean(axis=1),\n",
        "                'std_abundance': category_data[category].std(axis=1),\n",
        "                'cv': category_data[category].std(axis=1) / category_data[category].mean(axis=1) * 100\n",
        "            }).sort_values('mean_abundance', ascending=False)\n",
        "\n",
        "    return category_data, category_stats\n",
        "\n",
        "def plot_corrosion_pathways(category_data, category_stats):\n",
        "    \"\"\"\n",
        "    Create visualizations for corrosion-related pathways\n",
        "    \"\"\"\n",
        "    # Plot top pathways for each category\n",
        "    for category, data in category_stats.items():\n",
        "        if len(data) > 0:\n",
        "            plt.figure(figsize=(12, min(6, max(3, len(data)*0.3))))\n",
        "            sns.barplot(data=data.head(10).reset_index(),\n",
        "                       x='mean_abundance',\n",
        "                       y='index',\n",
        "                       palette='YlOrRd')\n",
        "            plt.title(f'Top {min(10, len(data))} {category.replace(\"_\", \" \").title()} Related Pathways')\n",
        "            plt.xlabel('Mean Abundance')\n",
        "            plt.ylabel('Pathway')\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "    # Create correlation heatmap between categories\n",
        "    category_means = pd.DataFrame({\n",
        "        cat: data.mean(axis=1) for cat, data in category_data.items()\n",
        "    })\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(category_means.corr(),\n",
        "                annot=True,\n",
        "                cmap='coolwarm',\n",
        "                center=0,\n",
        "                vmin=-1,\n",
        "                vmax=1)\n",
        "    plt.title('Correlation between Pathway Categories')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def analyze_pathway_interactions(df, category_data):\n",
        "    \"\"\"\n",
        "    Analyze interactions between different pathway categories\n",
        "    \"\"\"\n",
        "    # Calculate mean abundance for each category\n",
        "    category_abundances = pd.DataFrame({\n",
        "        category: data.mean(axis=0)\n",
        "        for category, data in category_data.items()\n",
        "    })\n",
        "\n",
        "    # Calculate correlations between categories\n",
        "    correlations = category_abundances.corr()\n",
        "\n",
        "    # Identify potential synergistic relationships\n",
        "    high_correlations = correlations.unstack()\n",
        "    high_correlations = high_correlations[high_correlations != 1.0]\n",
        "    high_correlations = high_correlations[abs(high_correlations) > 0.5]\n",
        "\n",
        "    return category_abundances, correlations, high_correlations.sort_values(ascending=False)\n",
        "\n",
        "# Analysing Corrosion Pathways\n",
        "category_data, category_stats = analyze_corrosion_pathways(Picrust_Result)\n",
        "plot_corrosion_pathways(category_data, category_stats)\n",
        "abundances, correlations, high_corr = analyze_pathway_interactions(Picrust_Result, category_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bi4wuyN7uvgW"
      },
      "source": [
        "## 9.5. Heating and Cooling Systems Pathway Analysis\n",
        "Creating independent analyses:\n",
        "\n",
        "Failure analysis (based on human assessment/estimation)\n",
        "Microbiological analysis (16S rRNA)\n",
        "Physicochemical parameters\n",
        "\n",
        "\n",
        "Using physicochemical parameters as labels/indicators of corrosion state - this is quite clever because it gives you an objective measure without directly mixing in the biological data\n",
        "Then planning to correlate the microbial communities with these states through machine learning\n",
        "\n",
        "And now you want to use PICRUSt's functional predictions to validate your assumptions about organic matter metabolism. This is very valuable because:\n",
        "\n",
        "It can help confirm if the bacteria you've identified through correlations actually have the metabolic capacity to influence corrosion\n",
        "It might reveal unexpected metabolic pathways that could explain the correlations you're seeing\n",
        "The following script will Validate your organic matter assumptions by:\n",
        "\n",
        "Breaking down different types of organic matter processing\n",
        "Looking at both degradation and synthesis pathways\n",
        "Identifying transport mechanisms\n",
        "\n",
        "Connect with your physicochemical parameters by analyzing pathways that could influence:\n",
        "\n",
        "pH modulation\n",
        "Temperature response\n",
        "Metal interactions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWBttrFbuvgW",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def analyze_system_pathways(df):\n",
        "    \"\"\"\n",
        "    Analyze pathways relevant to heating/cooling system corrosion\n",
        "\n",
        "    Parameters:\n",
        "    df: pandas DataFrame with pathways as index and samples as columns\n",
        "    \"\"\"\n",
        "    # Define pathway categories relevant to system conditions\n",
        "    pathway_categories = {\n",
        "        # Water chemistry influence\n",
        "        'ph_modulation': ['acid', 'alkaline', 'proton pump', 'pH homeostasis'],\n",
        "\n",
        "        # Temperature adaptation\n",
        "        'temp_response': ['heat shock', 'cold shock', 'temperature response'],\n",
        "\n",
        "        # Organic matter processing\n",
        "        'carbon_metabolism': [\n",
        "            'carbon fixation', 'carbon utilization',\n",
        "            'organic acid', 'fatty acid',\n",
        "            'carbohydrate metabolism'\n",
        "        ],\n",
        "\n",
        "        # Corrosion-related\n",
        "        'metal_interaction': [\n",
        "            'iron', 'metal', 'oxidation-reduction',\n",
        "            'electron transport', 'metal binding'\n",
        "        ],\n",
        "\n",
        "        # Biofilm formation\n",
        "        'surface_attachment': [\n",
        "            'biofilm', 'adhesion', 'exopolysaccharide',\n",
        "            'extracellular matrix'\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    # Filter and analyze pathways\n",
        "    def get_category_pathways(terms):\n",
        "        return df.index[df.index.str.lower().str.contains('|'.join(terms), regex=True)]\n",
        "\n",
        "    category_data = {}\n",
        "    category_stats = {}\n",
        "\n",
        "    for category, terms in pathway_categories.items():\n",
        "        pathways = get_category_pathways(terms)\n",
        "        if len(pathways) > 0:\n",
        "            category_data[category] = df.loc[pathways]\n",
        "\n",
        "            # Calculate basic statistics\n",
        "            category_stats[category] = pd.DataFrame({\n",
        "                'mean_abundance': category_data[category].mean(axis=1),\n",
        "                'std_abundance': category_data[category].std(axis=1),\n",
        "                'cv': category_data[category].std(axis=1) / category_data[category].mean(axis=1) * 100,\n",
        "                'presence': (category_data[category] > 0).mean(axis=1) * 100  # % of samples with pathway\n",
        "            }).sort_values('mean_abundance', ascending=False)\n",
        "\n",
        "    return category_data, category_stats\n",
        "\n",
        "def analyze_organic_matter_pathways(df):\n",
        "    \"\"\"\n",
        "    Detailed analysis of organic matter related pathways\n",
        "    \"\"\"\n",
        "    # Specific organic matter categories\n",
        "    organic_categories = {\n",
        "        'degradation': ['degradation', 'breakdown', 'catabolism'],\n",
        "        'synthesis': ['biosynthesis', 'anabolism', 'synthesis'],\n",
        "        'transport': ['transport', 'uptake', 'export'],\n",
        "        'modification': ['modification', 'conversion', 'transformation']\n",
        "    }\n",
        "\n",
        "    organic_data = {}\n",
        "\n",
        "    for category, terms in organic_categories.items():\n",
        "        pathways = df.index[df.index.str.lower().str.contains(\n",
        "            '|'.join(terms), regex=True\n",
        "        ) & df.index.str.lower().str.contains(\n",
        "            'organic|carbon|fatty acid|lipid|protein|amino acid'\n",
        "        )]\n",
        "        if len(pathways) > 0:\n",
        "            organic_data[category] = df.loc[pathways]\n",
        "\n",
        "    return organic_data\n",
        "\n",
        "def plot_pathway_distributions(category_stats, category_data):\n",
        "    \"\"\"\n",
        "    Create visualizations for pathway distributions\n",
        "    \"\"\"\n",
        "    for category, stats in category_stats.items():\n",
        "        if len(stats) > 0:\n",
        "            # Create subplot with dual axis\n",
        "            fig, ax1 = plt.subplots(figsize=(12, min(8, max(4, len(stats)*0.3))))\n",
        "            ax2 = ax1.twinx()\n",
        "\n",
        "            # Plot mean abundance\n",
        "            sns.barplot(data=stats.head(10).reset_index(),\n",
        "                       x='mean_abundance',\n",
        "                       y='index',\n",
        "                       color='skyblue',\n",
        "                       ax=ax1)\n",
        "\n",
        "            # Plot presence percentage\n",
        "            stats.head(10)['presence'].plot(\n",
        "                marker='o',\n",
        "                color='red',\n",
        "                ax=ax2\n",
        "            )\n",
        "\n",
        "            ax1.set_title(f'{category.replace(\"_\", \" \").title()} Pathways')\n",
        "            ax1.set_xlabel('Mean Abundance')\n",
        "            ax2.set_xlabel('Presence (%)')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "# Calling the analysis\n",
        "category_data, category_stats = analyze_system_pathways(Picrust_Result)\n",
        "organic_data = analyze_organic_matter_pathways(Picrust_Result)\n",
        "plot_pathway_distributions(category_stats, category_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFrF4oTRuvgX"
      },
      "source": [
        "I have a big gap on the cation anion account and then used mackensy, 2012 method from the usgs to check ec measured Vs calculated and cation Vs ions. It is a big gap still, but I have a lot of OM so I could no assume as normally that OM is CH4 so I attribute it to small organic acids and put acetate and oxalate as OM representatives, I have a small study of small acids form on failure analysis and also report of a mass that has a magnetic consistency, so I infere that those muss be some organic metalic compound but only accounted for AC- and Ox-2, I thought better to chose this other compounds Fe rich but I don't know how to do it actually. So in my bacteria I actually found lots of them with Ac- metabolism whiles I was looking at the families I realise no only oxobacter accendants, but others similar, also got important biofilm formers, there is also halogen related and should be, big deal of difference make the material and location cause water treatment, unfortunately the annotations are no to be taken as parameters but can serve as annotations you understand the difference?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXMFmbSruvgX"
      },
      "source": [
        "validate assumptions about:\n",
        "\n",
        "Organic acid presence (by showing metabolic capability)\n",
        "Metal-organic complex formation (through siderophore and metal-binding pathways)\n",
        "Biofilm formation potential (which can influence local chemistry)\n",
        "\n",
        "Validate acetate/oxalate assumptions by showing if these metabolic pathways are actually present\n",
        "Look for other potential organic acid pathways might want to consider\n",
        "Identify metal-organic interaction pathways that could explain magnetic mass observation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3sujtH1uvgX",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def analyze_organic_metal_pathways(df):\n",
        "    \"\"\"\n",
        "    Analyze pathways related to organic acid metabolism and metal interactions\n",
        "\n",
        "    Parameters:\n",
        "    df: pandas DataFrame with pathways as index and samples as columns\n",
        "    \"\"\"\n",
        "    # Define specific pathway categories\n",
        "    pathway_categories = {\n",
        "        'organic_acid_metabolism': [\n",
        "            'acetate', 'acetic acid', 'acetyl',\n",
        "            'oxalate', 'oxalic acid',\n",
        "            'organic acid', 'fatty acid',\n",
        "            'carboxylic acid'\n",
        "        ],\n",
        "\n",
        "        'metal_organic_interaction': [\n",
        "            'siderophore', 'metal binding',\n",
        "            'iron complex', 'metal transport',\n",
        "            'metallophore', 'metal organic'\n",
        "        ],\n",
        "\n",
        "        'biofilm_formation': [\n",
        "            'biofilm', 'exopolysaccharide',\n",
        "            'extracellular matrix', 'adhesion'\n",
        "        ],\n",
        "\n",
        "        'halogen_related': [\n",
        "            'halogen', 'chloride', 'bromide',\n",
        "            'halide', 'dehalogenation'\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    # Analyze each category\n",
        "    def get_category_pathways(terms):\n",
        "        return df.index[df.index.str.lower().str.contains('|'.join(terms), regex=True)]\n",
        "\n",
        "    pathway_data = {}\n",
        "    pathway_stats = {}\n",
        "\n",
        "    for category, terms in pathway_categories.items():\n",
        "        pathways = get_category_pathways(terms)\n",
        "        if len(pathways) > 0:\n",
        "            pathway_data[category] = df.loc[pathways]\n",
        "\n",
        "            # Calculate comprehensive statistics\n",
        "            pathway_stats[category] = pd.DataFrame({\n",
        "                'mean_abundance': pathway_data[category].mean(axis=1),\n",
        "                'std_abundance': pathway_data[category].std(axis=1),\n",
        "                'cv': pathway_data[category].std(axis=1) / pathway_data[category].mean(axis=1) * 100,\n",
        "                'presence': (pathway_data[category] > 0).mean(axis=1) * 100,  # % of samples with pathway\n",
        "                'relative_abundance': pathway_data[category].mean(axis=1) / df.mean(axis=1).mean() * 100\n",
        "            }).sort_values('mean_abundance', ascending=False)\n",
        "\n",
        "    return pathway_data, pathway_stats\n",
        "\n",
        "def analyze_pathway_relationships(pathway_data):\n",
        "    \"\"\"\n",
        "    Analyze relationships between different pathway categories\n",
        "    \"\"\"\n",
        "    # Calculate mean abundance for each category across samples\n",
        "    category_means = pd.DataFrame({\n",
        "        category: data.mean(axis=0)\n",
        "        for category, data in pathway_data.items()\n",
        "    })\n",
        "\n",
        "    # Calculate correlations\n",
        "    correlations = category_means.corr()\n",
        "\n",
        "    # Identify potential functional relationships\n",
        "    high_correlations = correlations.unstack()\n",
        "    high_correlations = high_correlations[high_correlations != 1.0]\n",
        "    high_correlations = high_correlations[abs(high_correlations) > 0.5]\n",
        "\n",
        "    return category_means, correlations, high_correlations.sort_values(ascending=False)\n",
        "\n",
        "def plot_pathway_analysis(pathway_stats, pathway_data):\n",
        "    \"\"\"\n",
        "    Create visualizations for pathway analysis\n",
        "    \"\"\"\n",
        "    for category, stats in pathway_stats.items():\n",
        "        if len(stats) > 0:\n",
        "            # Create subplot with dual axis\n",
        "            fig, ax1 = plt.subplots(figsize=(12, min(8, max(4, len(stats)*0.3))))\n",
        "            ax2 = ax1.twinx()\n",
        "\n",
        "            # Plot abundance and relative abundance\n",
        "            sns.barplot(data=stats.head(10).reset_index(),\n",
        "                       x='relative_abundance',\n",
        "                       y='index',\n",
        "                       color='skyblue',\n",
        "                       ax=ax1)\n",
        "\n",
        "            stats.head(10)['presence'].plot(\n",
        "                marker='o',\n",
        "                color='red',\n",
        "                ax=ax2\n",
        "            )\n",
        "\n",
        "            ax1.set_title(f'{category.replace(\"_\", \" \").title()} Pathways')\n",
        "            ax1.set_xlabel('Relative Abundance (%)')\n",
        "            ax2.set_xlabel('Presence (%)')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "# calling the function\n",
        "pathway_data, pathway_stats = analyze_organic_metal_pathways(Picrust_Result)category_means, correlations, high_corr = analyze_pathway_relationships(pathway_data)\n",
        "plot_pathway_analysis(pathway_stats, pathway_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvV8Sor3uvgX"
      },
      "source": [
        "## 9.6. Corrosion Relevant Pathways\n",
        "\n",
        "Focus on corrosion-relevant pathways by categorizing them into:\n",
        "\n",
        "Organic acid metabolism (relevant to your acetate/oxalate observations)\n",
        "Sulfur metabolism\n",
        "Metal interactions\n",
        "Biofilm formation\n",
        "\n",
        "\n",
        "Handle the high-dimensional data by:\n",
        "\n",
        "Using dimensionality reduction (PCA)\n",
        "Calculating summary statistics\n",
        "Visualizing key patterns\n",
        "\n",
        "\n",
        "Address your specific interests:\n",
        "\n",
        "Organic matter metabolism pathways\n",
        "Metal-organic interactions\n",
        "Correlations with physicochemical parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EThn4vkFuvgX",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def analyze_corrosion_pathways(df):\n",
        "    \"\"\"\n",
        "    Analyze pathways relevant to microbially influenced corrosion\n",
        "    \"\"\"\n",
        "    # Define pathway categories relevant to corrosion\n",
        "    pathway_categories = {\n",
        "        'organic_acid': [\n",
        "            'CENTFERM-PWY',  # Central fermentation pathways\n",
        "            'FERMENTATION-PWY',  # Mixed acid fermentation\n",
        "            'GLYCOLYSIS',  # Glucose fermentation\n",
        "            'PWY-5100',  # Pyruvate fermentation\n",
        "            'GALACTUROCAT-PWY'  # Galacturonate degradation\n",
        "        ],\n",
        "        'sulfur': [\n",
        "            'PWY-6932',  # Sulfate reduction\n",
        "            'SO4ASSIM-PWY',  # Sulfate assimilation\n",
        "            'SULFATE-CYS-PWY'  # Sulfate to cysteine\n",
        "        ],\n",
        "        'metal_interaction': [\n",
        "            'PWY-7219',  # Iron oxidation\n",
        "            'PWY-7221',  # Iron reduction\n",
        "            'HEME-BIOSYNTHESIS-II',  # Iron-containing compounds\n",
        "            'P125-PWY'  # Metal resistance\n",
        "        ],\n",
        "        'biofilm': [\n",
        "            'COLANSYN-PWY',  # Colanic acid (biofilm)\n",
        "            'EXOPOLYSACC-PWY',  # Exopolysaccharide\n",
        "            'GLUCOSE1PMETAB-PWY'  # UDP-glucose synthesis\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    # Extract relevant pathways and their abundances\n",
        "    relevant_pathways = {}\n",
        "    for category, pathways in pathway_categories.items():\n",
        "        category_data = df[df.index.isin(pathways)]\n",
        "        if not category_data.empty:\n",
        "            relevant_pathways[category] = category_data\n",
        "\n",
        "    # Calculate summary statistics\n",
        "    summary_stats = {}\n",
        "    for category, data in relevant_pathways.items():\n",
        "        summary_stats[category] = {\n",
        "            'mean_abundance': data.mean().mean(),\n",
        "            'std_abundance': data.mean().std(),\n",
        "            'present_in_samples': (data > 0).mean().mean() * 100,\n",
        "            'pathways_found': len(data)\n",
        "        }\n",
        "\n",
        "    # Dimension reduction for visualization\n",
        "    if df.shape[0] > 0:\n",
        "        # Standardize the data\n",
        "        scaler = StandardScaler()\n",
        "        scaled_data = scaler.fit_transform(df.T)\n",
        "\n",
        "        # PCA\n",
        "        pca = PCA(n_components=2)\n",
        "        pca_result = pca.fit_transform(scaled_data)\n",
        "\n",
        "        return relevant_pathways, summary_stats, pca_result, pca.explained_variance_ratio_\n",
        "\n",
        "    return relevant_pathways, summary_stats, None, None\n",
        "\n",
        "def plot_pathway_analysis(relevant_pathways, summary_stats, pca_result=None, explained_variance=None):\n",
        "    \"\"\"\n",
        "    Create visualizations for pathway analysis\n",
        "    \"\"\"\n",
        "    # Plot mean abundances by category\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    categories = list(summary_stats.keys())\n",
        "    means = [stats['mean_abundance'] for stats in summary_stats.values()]\n",
        "    presence = [stats['present_in_samples'] for stats in summary_stats.values()]\n",
        "\n",
        "    ax1 = plt.gca()\n",
        "    ax2 = ax1.twinx()\n",
        "\n",
        "    bars = ax1.bar(categories, means, alpha=0.6, color='skyblue')\n",
        "    ax1.set_ylabel('Mean Abundance')\n",
        "\n",
        "    line = ax2.plot(categories, presence, 'ro-', label='Presence %')\n",
        "    ax2.set_ylabel('Presence in Samples (%)')\n",
        "\n",
        "    plt.title('Pathway Categories Overview')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # If PCA results available, plot them\n",
        "    if pca_result is not None:\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        plt.scatter(pca_result[:, 0], pca_result[:, 1], alpha=0.6)\n",
        "        plt.xlabel(f'PC1 ({explained_variance[0]*100:.1f}%)')\n",
        "        plt.ylabel(f'PC2 ({explained_variance[1]*100:.1f}%)')\n",
        "        plt.title('PCA of Pathway Abundances')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# Calling the functions\n",
        "relevant_pathways, summary_stats, pca_result, explained_variance = analyze_corrosion_pathways(Picrust_Result)\n",
        "plot_pathway_analysis(relevant_pathways, summary_stats, pca_result, explained_variance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZgXC50euvgX"
      },
      "source": [
        "## 9.7. Functional Pathway Clustering Analysis\n",
        "Hierarchical Clustering:\n",
        "\n",
        "Groups pathways based on their abundance patterns\n",
        "Creates a dendrogram to visualize relationships\n",
        "Automatically determines optimal number of clusters\n",
        "\n",
        "\n",
        "Correlation-based Analysis:\n",
        "\n",
        "Identifies pathways that behave similarly across samples\n",
        "Creates correlation heatmap to visualize relationships\n",
        "Helps identify functional modules\n",
        "\n",
        "\n",
        "Feature Creation:\n",
        "\n",
        "Generates new features based on cluster statistics:\n",
        "\n",
        "Mean abundance per cluster\n",
        "Total abundance per cluster\n",
        "Pathway diversity within clusters\n",
        "\n",
        "Reduce dimensionality while maintaining biological meaning\n",
        "Identify functional modules that might be working together\n",
        "Create more robust features for your ML analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6NsHQCquvgX",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def cluster_pathways(df, n_clusters=None, corr_threshold=0.7):\n",
        "    \"\"\"\n",
        "    Cluster pathways based on their functional similarity\n",
        "\n",
        "    Parameters:\n",
        "    df: DataFrame with pathways as rows and samples as columns\n",
        "    n_clusters: Number of clusters (if None, determined automatically)\n",
        "    corr_threshold: Correlation threshold for considering pathways related\n",
        "    \"\"\"\n",
        "    # Standardize the data\n",
        "    scaler = StandardScaler()\n",
        "    scaled_data = scaler.fit_transform(df.T).T\n",
        "\n",
        "    # Calculate correlation matrix\n",
        "    corr_matrix = np.corrcoef(scaled_data)\n",
        "\n",
        "    # Create linkage matrix for hierarchical clustering\n",
        "    linkage_matrix = hierarchy.linkage(pdist(scaled_data), method='ward')\n",
        "\n",
        "    if n_clusters is None:\n",
        "        # Automatically determine number of clusters using elbow method\n",
        "        last = linkage_matrix[-10:, 2]\n",
        "        acceleration = np.diff(last, 2)\n",
        "        n_clusters = len(last) - np.argmax(acceleration) + 1\n",
        "\n",
        "    # Perform clustering\n",
        "    clustering = AgglomerativeClustering(n_clusters=n_clusters)\n",
        "    cluster_labels = clustering.fit_predict(scaled_data)\n",
        "\n",
        "    # Create cluster summary\n",
        "    cluster_summary = pd.DataFrame({\n",
        "        'pathway': df.index,\n",
        "        'cluster': cluster_labels\n",
        "    })\n",
        "\n",
        "    return cluster_labels, linkage_matrix, corr_matrix, cluster_summary\n",
        "\n",
        "def analyze_pathway_clusters(df, cluster_labels):\n",
        "    \"\"\"\n",
        "    Analyze the characteristics of each pathway cluster\n",
        "    \"\"\"\n",
        "    cluster_stats = {}\n",
        "\n",
        "    for cluster in np.unique(cluster_labels):\n",
        "        # Get pathways in this cluster\n",
        "        cluster_paths = df.index[cluster_labels == cluster]\n",
        "        cluster_data = df.loc[cluster_paths]\n",
        "\n",
        "        # Calculate statistics\n",
        "        cluster_stats[cluster] = {\n",
        "            'size': len(cluster_paths),\n",
        "            'mean_abundance': cluster_data.mean().mean(),\n",
        "            'std_abundance': cluster_data.mean().std(),\n",
        "            'pathways': list(cluster_paths),\n",
        "            'correlation': np.corrcoef(cluster_data),\n",
        "            'total_abundance': cluster_data.sum().mean()\n",
        "        }\n",
        "\n",
        "    return cluster_stats\n",
        "\n",
        "def plot_pathway_clusters(df, linkage_matrix, corr_matrix, cluster_labels, cluster_stats):\n",
        "    \"\"\"\n",
        "    Create visualizations for pathway clusters\n",
        "    \"\"\"\n",
        "    # Plot dendrogram\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    plt.title('Pathway Clustering Dendrogram')\n",
        "    hierarchy.dendrogram(linkage_matrix, labels=df.index, leaf_rotation=90)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Plot correlation heatmap\n",
        "    plt.figure(figsize=(12, 12))\n",
        "    sns.heatmap(pd.DataFrame(corr_matrix, index=df.index, columns=df.index),\n",
        "                cmap='coolwarm', center=0, vmin=-1, vmax=1)\n",
        "    plt.title('Pathway Correlation Heatmap')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Plot cluster sizes and abundances\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    clusters = list(cluster_stats.keys())\n",
        "    sizes = [stats['size'] for stats in cluster_stats.values()]\n",
        "    abundances = [stats['mean_abundance'] for stats in cluster_stats.values()]\n",
        "\n",
        "    ax1 = plt.gca()\n",
        "    ax2 = ax1.twinx()\n",
        "\n",
        "    ax1.bar(clusters, sizes, alpha=0.6, color='skyblue')\n",
        "    ax1.set_ylabel('Number of Pathways')\n",
        "\n",
        "    ax2.plot(clusters, abundances, 'ro-')\n",
        "    ax2.set_ylabel('Mean Abundance')\n",
        "\n",
        "    plt.title('Cluster Sizes and Abundances')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def create_cluster_features(df, cluster_labels):\n",
        "    \"\"\"\n",
        "    Create new features based on pathway clusters\n",
        "    \"\"\"\n",
        "    n_clusters = len(np.unique(cluster_labels))\n",
        "    cluster_features = pd.DataFrame(index=df.columns)\n",
        "\n",
        "    for cluster in range(n_clusters):\n",
        "        # Get pathways in this cluster\n",
        "        cluster_paths = df.index[cluster_labels == cluster]\n",
        "\n",
        "        # Calculate mean abundance for cluster\n",
        "        cluster_features[f'cluster_{cluster}'] = df.loc[cluster_paths].mean()\n",
        "\n",
        "        # Calculate total abundance for cluster\n",
        "        cluster_features[f'cluster_{cluster}_total'] = df.loc[cluster_paths].sum()\n",
        "\n",
        "        # Calculate diversity within cluster\n",
        "        cluster_features[f'cluster_{cluster}_diversity'] = (df.loc[cluster_paths] > 0).sum()\n",
        "\n",
        "    return cluster_features\n",
        "\n",
        "# Calling the fUNCTION\n",
        "cluster_labels, linkage_matrix, corr_matrix, cluster_summary = cluster_pathways(Picrust_Result)\n",
        "cluster_stats = analyze_pathway_clusters(Picrust_Result, cluster_labels)\n",
        "plot_pathway_clusters(Picrust_Result, linkage_matrix, corr_matrix, cluster_labels, cluster_stats)\n",
        "cluster_features = create_cluster_features(Picrust_Result, cluster_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Os65js9EuvgX"
      },
      "source": [
        "I dont know hte gfollowinag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vgw9Pwp8uvgX",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "def analyze_organic_metal_pathways(df):\n",
        "    \"\"\"\n",
        "    Analyze pathways related to organic acid metabolism and metal interactions\n",
        "\n",
        "    Parameters:\n",
        "    df: pandas DataFrame with pathways as index and samples as columns\n",
        "    \"\"\"\n",
        "    # Define specific pathway categories\n",
        "    pathway_categories = {\n",
        "        'organic_acid_metabolism': [\n",
        "            'acetate', 'acetic acid', 'acetyl',\n",
        "            'oxalate', 'oxalic acid',\n",
        "            'organic acid', 'fatty acid',\n",
        "            'carboxylic acid'\n",
        "        ],\n",
        "\n",
        "        'metal_organic_interaction': [\n",
        "            'siderophore', 'metal binding',\n",
        "            'iron complex', 'metal transport',\n",
        "            'metallophore', 'metal organic'\n",
        "        ],\n",
        "\n",
        "        'biofilm_formation': [\n",
        "            'biofilm', 'exopolysaccharide',\n",
        "            'extracellular matrix', 'adhesion'\n",
        "        ],\n",
        "\n",
        "        'halogen_related': [\n",
        "            'halogen', 'chloride', 'bromide',\n",
        "            'halide', 'dehalogenation'\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    # Analyze each category\n",
        "    def get_category_pathways(terms):\n",
        "        return df.index[df.index.str.lower().str.contains('|'.join(terms), regex=True)]\n",
        "\n",
        "    pathway_data = {}\n",
        "    pathway_stats = {}\n",
        "\n",
        "    for category, terms in pathway_categories.items():\n",
        "        pathways = get_category_pathways(terms)\n",
        "        if len(pathways) > 0:\n",
        "            pathway_data[category] = df.loc[pathways]\n",
        "\n",
        "            # Calculate comprehensive statistics\n",
        "            pathway_stats[category] = pd.DataFrame({\n",
        "                'mean_abundance': pathway_data[category].mean(axis=1),\n",
        "                'std_abundance': pathway_data[category].std(axis=1),\n",
        "                'cv': pathway_data[category].std(axis=1) / pathway_data[category].mean(axis=1) * 100,\n",
        "                'presence': (pathway_data[category] > 0).mean(axis=1) * 100,  # % of samples with pathway\n",
        "                'relative_abundance': pathway_data[category].mean(axis=1) / df.mean(axis=1).mean() * 100\n",
        "            }).sort_values('mean_abundance', ascending=False)\n",
        "\n",
        "    return pathway_data, pathway_stats\n",
        "\n",
        "def analyze_pathway_relationships(pathway_data):\n",
        "    \"\"\"\n",
        "    Analyze relationships between different pathway categories\n",
        "    \"\"\"\n",
        "    # Calculate mean abundance for each category across samples\n",
        "    category_means = pd.DataFrame({\n",
        "        category: data.mean(axis=0)\n",
        "        for category, data in pathway_data.items()\n",
        "    })\n",
        "\n",
        "    # Calculate correlations\n",
        "    correlations = category_means.corr()\n",
        "\n",
        "    # Identify potential functional relationships\n",
        "    high_correlations = correlations.unstack()\n",
        "    high_correlations = high_correlations[high_correlations != 1.0]\n",
        "    high_correlations = high_correlations[abs(high_correlations) > 0.5]\n",
        "\n",
        "    return category_means, correlations, high_correlations.sort_values(ascending=False)\n",
        "\n",
        "def plot_pathway_analysis(pathway_stats, pathway_data):\n",
        "    \"\"\"\n",
        "    Create visualizations for pathway analysis\n",
        "    \"\"\"\n",
        "    for category, stats in pathway_stats.items():\n",
        "        if len(stats) > 0:\n",
        "            # Create subplot with dual axis\n",
        "            fig, ax1 = plt.subplots(figsize=(12, min(8, max(4, len(stats)*0.3))))\n",
        "            ax2 = ax1.twinx()\n",
        "\n",
        "            # Plot abundance and relative abundance\n",
        "            sns.barplot(data=stats.head(10).reset_index(),\n",
        "                       x='relative_abundance',\n",
        "                       y='index',\n",
        "                       color='skyblue',\n",
        "                       ax=ax1)\n",
        "\n",
        "            stats.head(10)['presence'].plot(\n",
        "                marker='o',\n",
        "                color='red',\n",
        "                ax=ax2\n",
        "            )\n",
        "\n",
        "            ax1.set_title(f'{category.replace(\"_\", \" \").title()} Pathways')\n",
        "            ax1.set_xlabel('Relative Abundance (%)')\n",
        "            ax2.set_xlabel('Presence (%)')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Le_mzeY_uvgX",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Calling the function\n",
        "pathway_data, pathway_stats = analyze_organic_metal_pathways(Picrust_Result)\n",
        "category_means, correlations, high_corr = analyze_pathway_relationships(pathway_data)\n",
        "plot_pathway_analysis(pathway_stats, pathway_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wf76SqphuvgX"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EQUT3fwuvgX"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djNOlMWkuvgX",
        "trusted": true
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "databundleVersionId": 11132286,
          "datasetId": 6686232,
          "sourceId": 10776455,
          "sourceType": "datasetVersion"
        },
        {
          "databundleVersionId": 11151079,
          "datasetId": 6677417,
          "sourceId": 10793499,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30886,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
