{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDED5j4zCqR_"
      },
      "source": [
        "# Sequence Analysis and Functional Prediction Pipeline\n",
        "\n",
        "## 1. Introduction\n",
        "This notebook analyzes the functional and sequence relationships between newly identified bacteria and known corrosion-influencing microorganisms. The analysis builds upon previous findings where:\n",
        "- Statistical significance was established between the selected bacteria and corrosion risk (Notebook 3)\n",
        "- Literature validation confirmed corrosion influence for many bacteria (Notebook 4)\n",
        "- Evolutionary relationships were mapped through phylogenetic analysis (Notebook 5)\n",
        "\n",
        "The study focuses on bacteria from operational heating and cooling water systems, primarily in Germany. Using 16S rRNA data (bootstrap-validated from Notebook 5), this analysis employs PICRUSt2 to predict metabolic functions and compare functional profiles between different bacterial groups.\n",
        "\n",
        "### Analysis Approaches\n",
        "We implement two classification strategies:\n",
        "\n",
        "1. Simple Classification:\n",
        "   - Known corrosion-causing bacteria (usual_taxa)\n",
        "   - Other bacteria (combining checked_taxa and core_taxa)\n",
        "\n",
        "2. Detailed Classification:\n",
        "   - Known corrosion-causing bacteria (usual_taxa)\n",
        "   - Pure checked bacteria (exclusive to checked_taxa)\n",
        "   - Pure core bacteria (exclusive to core_taxa)\n",
        "   - Checked-core bacteria (overlap between checked and core taxa)\n",
        "\n",
        "This detailed approach allows for more nuanced analysis of functional profiles and better understanding of potential corrosion mechanisms across different bacterial groups.\n",
        "\n",
        "### Analysis Goals:\n",
        "- Predict metabolic functions from 16S sequences\n",
        "- Focus on corrosion-relevant pathways (sulfur/iron metabolism)\n",
        "- Compare functional profiles between known corrosion-causing bacteria and newly identified candidates\n",
        "- Validate whether statistical correlations reflect genuine metabolic capabilities associated with corrosion processes\n",
        "\n",
        "### Directory Structure:\n",
        " Following is the structure of the notebook data named data_picrus  \n",
        "data_tree  \n",
        " ├── sequences/  \n",
        " │   ├── known.fasta : sequences of known corrosion-causing bacteria  \n",
        " │   ├── candidate.fasta : sequences of potential new corrosion-causing bacteria  \n",
        " |   └── other files  \n",
        " data_picrus  \n",
        " └── picrust_results/  \n",
        "      ├── known_bacteria/  \n",
        "      |               ├── EC_predictions/       : enzyme predictions  \n",
        "      |               ├── pathway_predictions/  : metabolic pathway abundance  \n",
        "      |               ├── KO_predictions/       : KEGG ortholog predictions  \n",
        "      |               └── other_picrust_files/  \n",
        "      ├── candidate_bacteria/  \n",
        "      |               ├── EC_predictions/       : enzyme predictions  \n",
        "      |               ├── pathway_predictions/  : metabolic pathway abundance  \n",
        "      |               ├── KO_predictions/       : KEGG ortholog predictions  \n",
        "      |               └── other_picrust_files/  : final comparison summary\n",
        "      ├── core_bacteria/\n",
        "      |               ├── EC_predictions/       : enzyme predictions  \n",
        "      |               ├── pathway_predictions/  : metabolic pathway abundance  \n",
        "      |               ├── KO_predictions/       : KEGG ortholog predictions  \n",
        "      |               └── other_picrust_files/  \n",
        "      │      \n",
        "      └── functional_comparison.xlsx  \n",
        "\n",
        "Picrust2 works using its reference database that was installed with the package   \n",
        "~/miniconda3/envs/picrust2/lib/python3.9/site-packages/picrust2/default_files/prokaryotic/pro_ref\n",
        "\n",
        "About picrust2  \n",
        "https://evomics.org/wp-content/uploads/2015/01/presentation_evomics-05-picrust_01-18-15.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeIn7RIBCqSD"
      },
      "source": [
        "# 2. Loading and Preparing the Data\n",
        "\n",
        "## 2.1 Colab Initialisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-02-19T09:40:11.357108Z",
          "iopub.status.busy": "2025-02-19T09:40:11.356719Z",
          "iopub.status.idle": "2025-02-19T09:40:11.366864Z",
          "shell.execute_reply": "2025-02-19T09:40:11.365324Z",
          "shell.execute_reply.started": "2025-02-19T09:40:11.357071Z"
        },
        "id": "5U6gm_R_JQjt",
        "outputId": "acab03ee-1f10-48ff-b34b-b07938bc7b8f",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Colab specific\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#change the path\n",
        "os.chdir('/content/drive/MyDrive/MIC/data_picrust')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWqOI6IDD1qW"
      },
      "source": [
        "__Importing PICRUST IN COLAB__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T09:40:11.368681Z",
          "iopub.status.busy": "2025-02-19T09:40:11.368247Z",
          "iopub.status.idle": "2025-02-19T09:40:11.395272Z",
          "shell.execute_reply": "2025-02-19T09:40:11.394026Z",
          "shell.execute_reply.started": "2025-02-19T09:40:11.368644Z"
        },
        "id": "uKimriI3hmTq",
        "trusted": true,
        "vscode": {
          "languageId": "javascript"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "e88c6718-ffc7-40f3-9ff8-97260d48f043"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"# Install miniconda and initialize:\\n!wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\\n!bash Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local/miniconda3\\n!conda config --add channels defaults\\n!conda config --add channels bioconda\\n!conda config --add channels conda-forge\\n# Imports for colab\\nimport condacolab\\nimport sys\\nsys.path.append('/usr/local/miniconda3/lib/python3.7/site-packages/')\\n\\n# Install PICRUSt2 and its dependencies\\n%conda install -c bioconda -c conda-forge picrust2=2.4.1 -y\\n# Verify installations%\\n%conda list | grep picrust2\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "'''# Install miniconda and initialize:\n",
        "!wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
        "!bash Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local/miniconda3\n",
        "!conda config --add channels defaults\n",
        "!conda config --add channels bioconda\n",
        "!conda config --add channels conda-forge\n",
        "# Imports for colab\n",
        "import condacolab\n",
        "import sys\n",
        "sys.path.append('/usr/local/miniconda3/lib/python3.7/site-packages/')\n",
        "\n",
        "# Install PICRUSt2 and its dependencies\n",
        "%conda install -c bioconda -c conda-forge picrust2=2.4.1 -y\n",
        "# Verify installations%\n",
        "%conda list | grep picrust2'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzaNXN9suvgG"
      },
      "source": [
        "### Using Pro colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T09:40:11.397167Z",
          "iopub.status.busy": "2025-02-19T09:40:11.396676Z",
          "iopub.status.idle": "2025-02-19T09:40:11.419557Z",
          "shell.execute_reply": "2025-02-19T09:40:11.418358Z",
          "shell.execute_reply.started": "2025-02-19T09:40:11.397109Z"
        },
        "id": "qF94FGxn2iUL",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5d39cc37-4900-4b59-a35b-6e57dbb88913"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"import sys\\nprint([module for module in sys.modules if 'tensorflow' in module])\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "'''import sys\n",
        "print([module for module in sys.modules if 'tensorflow' in module])'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-02-19T09:43:44.41279Z",
          "iopub.status.busy": "2025-02-19T09:43:44.412414Z",
          "iopub.status.idle": "2025-02-19T09:44:02.324275Z",
          "shell.execute_reply": "2025-02-19T09:44:02.322737Z",
          "shell.execute_reply.started": "2025-02-19T09:43:44.41276Z"
        },
        "id": "3gWJfdx3Ni1f",
        "outputId": "34d0b27d-39ce-401c-bf30-7485d57b3a31",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gputil\n",
            "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gputil: filename=GPUtil-1.4.0-py3-none-any.whl size=7392 sha256=5dd649b5b7e96beb9e1c2afbec3d6a897b08c9f59ca68488f5fb5fb316a2ba1b\n",
            "  Stored in directory: /root/.cache/pip/wheels/2b/4d/8f/55fb4f7b9b591891e8d3f72977c4ec6c7763b39c19f0861595\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (5.9.5)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.11/dist-packages (4.11.0)\n",
            "Collecting memory_profiler\n",
            "  Downloading memory_profiler-0.61.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from memory_profiler) (5.9.5)\n",
            "Downloading memory_profiler-0.61.0-py3-none-any.whl (31 kB)\n",
            "Installing collected packages: memory_profiler\n",
            "Successfully installed memory_profiler-0.61.0\n",
            "Your runtime has 54.8 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ],
      "source": [
        "# Set up memory footprint support libraries\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "!pip install memory_profiler\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTKk2e3eYelt"
      },
      "source": [
        "### Kaggle / Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-02-19T09:44:02.326561Z",
          "iopub.status.busy": "2025-02-19T09:44:02.326066Z",
          "iopub.status.idle": "2025-02-19T09:44:11.408729Z",
          "shell.execute_reply": "2025-02-19T09:44:11.407173Z",
          "shell.execute_reply.started": "2025-02-19T09:44:02.326507Z"
        },
        "id": "KC8v0oJRuvgH",
        "outputId": "70c040a0-b096-454a-ee82-56bba4bdaf90",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting biopython\n",
            "  Downloading biopython-1.85-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from biopython) (1.26.4)\n",
            "Downloading biopython-1.85-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: biopython\n",
            "Successfully installed biopython-1.85\n",
            "Collecting biom-format\n",
            "  Downloading biom-format-2.1.16.tar.gz (11.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from biom-format) (8.1.8)\n",
            "Requirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.11/dist-packages (from biom-format) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from biom-format) (1.13.1)\n",
            "Requirement already satisfied: pandas>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from biom-format) (2.2.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from biom-format) (3.12.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.20.0->biom-format) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.20.0->biom-format) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.20.0->biom-format) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=0.20.0->biom-format) (1.17.0)\n",
            "Building wheels for collected packages: biom-format\n",
            "  Building wheel for biom-format (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for biom-format: filename=biom_format-2.1.16-cp311-cp311-linux_x86_64.whl size=12182969 sha256=f521ad0f17b0df45a2e4c6736ce2d20d91857a9dfa1f81c3ce422ec48f1c96b1\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/6b/58/a879e8fbae2479a3d1a68719f3a062fe62701d6494f1b74f5e\n",
            "Successfully built biom-format\n",
            "Installing collected packages: biom-format\n",
            "Successfully installed biom-format-2.1.16\n",
            "Collecting umap-learn\n",
            "  Downloading umap_learn-0.5.7-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (1.6.1)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (0.61.0)\n",
            "Collecting pynndescent>=0.5 (from umap-learn)\n",
            "  Downloading pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from umap-learn) (4.67.1)\n",
            "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.2->umap-learn) (0.44.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.11/dist-packages (from pynndescent>=0.5->umap-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22->umap-learn) (3.5.0)\n",
            "Downloading umap_learn-0.5.7-py3-none-any.whl (88 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.8/88.8 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m194.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pynndescent, umap-learn\n",
            "Successfully installed pynndescent-0.5.13 umap-learn-0.5.7\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (5.3.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install biopython\n",
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "!pip install biom-format\n",
        "%pip install umap-learn\n",
        "!pip install lxml pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiHOVZiIuvgH"
      },
      "source": [
        "# 2.2. Importing Libraries,  Making Directories and Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T09:41:13.071035Z",
          "iopub.status.busy": "2025-02-19T09:41:13.070714Z",
          "iopub.status.idle": "2025-02-19T09:41:14.601408Z",
          "shell.execute_reply": "2025-02-19T09:41:14.6003Z",
          "shell.execute_reply.started": "2025-02-19T09:41:13.071005Z"
        },
        "id": "l92DnCZ3CqSD",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Standard library imports\n",
        "import os\n",
        "import sys\n",
        "import ast\n",
        "import subprocess\n",
        "import logging\n",
        "from datetime import datetime\n",
        "import shutil\n",
        "from io import StringIO\n",
        "from pathlib import Path\n",
        "# Data processing imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import openpyxl\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import gzip\n",
        "#%matplotlib inline\n",
        "#plt.style.use('seaborn')\n",
        "import seaborn as sns\n",
        "from natsort import natsorted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T10:36:09.575981Z",
          "iopub.status.busy": "2025-02-19T10:36:09.575602Z",
          "iopub.status.idle": "2025-02-19T10:36:09.582325Z",
          "shell.execute_reply": "2025-02-19T10:36:09.58091Z",
          "shell.execute_reply.started": "2025-02-19T10:36:09.575949Z"
        },
        "id": "cLkUWNNbwYbV",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import umap\n",
        "# datascience libraries\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.cluster import hierarchy\n",
        "from sklearn.decomposition import PCA, NMF\n",
        "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
        "from scipy.stats import spearmanr\n",
        "from scipy.spatial.distance import pdist\n",
        "import networkx as nx\n",
        "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
        "\n",
        "# from Bio\n",
        "from Bio import SeqIO\n",
        "from Bio.Seq import Seq\n",
        "from Bio.SeqRecord import SeqRecord\n",
        "\n",
        "# BIOM handling\n",
        "from biom import Table\n",
        "from biom.util import biom_open\n",
        "from biom import load_table\n",
        "\n",
        "#imports retrieval\n",
        "import requests\n",
        "import time\n",
        "import json\n",
        "from lxml import etree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "execution": {
          "iopub.execute_input": "2025-02-19T09:42:02.982923Z",
          "iopub.status.busy": "2025-02-19T09:42:02.982171Z",
          "iopub.status.idle": "2025-02-19T09:42:02.989557Z",
          "shell.execute_reply": "2025-02-19T09:42:02.988248Z",
          "shell.execute_reply.started": "2025-02-19T09:42:02.98286Z"
        },
        "id": "Jhg73Rb9CqSF",
        "outputId": "69d5df80-248a-4a41-d01b-d08d090cf672",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# For Kaggle work\\nbase_dir = Path(\"/kaggle/input/data-picrust\")\\nabundance_excel= base_dir / \"merged_to_sequence.xlsx\"\\nfasta_file_final = base_dir / \"final_sequences_gg.fasta\"\\noutput_dir = Path(\"/kaggle/working/\") '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# Set up logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "\n",
        "# Directory Structure Definitions\n",
        "SIMPLE_BASE = {\n",
        "    'known': 'simple_known_mic',\n",
        "    'other': 'simple_candidate_mic'\n",
        "}\n",
        "\n",
        "DETAILED_BASE = {\n",
        "    'known': 'detailed_known_mic',\n",
        "    'pure_checked': 'detailed_pure_checked_mic',\n",
        "    'pure_core': 'detailed_pure_core_mic',\n",
        "    'checked_core': 'detailed_checked_core_mic'\n",
        "}\n",
        "\n",
        "SUBDIRS = [\n",
        "    'EC_predictions',\n",
        "    'pathway_predictions',\n",
        "    'KO_predictions',\n",
        "    'other_picrust_files'\n",
        "]\n",
        "\n",
        "# Base Paths\n",
        "if \"google.colab\" in sys.modules:\n",
        "    base_dir = Path(\"/content/drive/MyDrive/MIC/data_picrust\")\n",
        "else:\n",
        "    base_dir = Path(\"/home/beatriz/MIC/2_Micro/data_picrust\")\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "base_dir = Path(\"/home/beatriz/MIC/2_Micro/data_picrust\")\n",
        "base_dir.mkdir(parents=True, exist_ok=True)\n",
        "abundance_excel= Path(\"/home/beatriz/MIC/2_Micro/data_Ref/merged_to_sequence.xlsx\")\n",
        "fasta_file_final = Path(\"/home/beatriz/MIC/2_Micro/data_qiime/results_match_gg/final_sequences_gg.fasta\")\n",
        "aligned_fasta = Path(\"/home/beatriz/MIC/2_Micro/data_qiime/results_match_gg/aligned-dna-sequences_gg.fasta\")\n",
        "large_dir = Path(\"/home/beatriz/MIC/MIC_large\")\n",
        "output_dir = base_dir\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "# for colab\n",
        "# Create output directory if it doesn't exist\n",
        "base_dir = Path(\"/content/drive/MyDrive/MIC/data_picrust/\")\n",
        "base_dir.mkdir(parents=True, exist_ok=True)\n",
        "abundance_excel= Path(\"/content/drive/MyDrive/MIC/data_picrust/merged_to_sequence.xlsx\")\n",
        "fasta_file_final = Path(\"/content/drive/MyDrive/MIC/data_picrust/final_sequences_gg.fasta\")\n",
        "aligned_fasta = Path(\"/content/drive/MyDrive/MIC/data_picrust/aligned-dna-sequences_gg.fasta\")\n",
        "results_file = base_dir / \"/content/drive/MyDrive/MIC/data_picrust/functional_comparison.xlsx\"\n",
        "output_dir = base_dir  # Separate output directory\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "large_dir = Path(\"/content/drive/MyDrive/MIC/MIC_large\")\n",
        "large_dir.mkdir(parents=True, exist_ok=True)\n",
        "db_dir = Path(\"/content/drive/MyDrive/MIC/Databases\")\n",
        "db_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "'''\n",
        "# For Kaggle work\n",
        "base_dir = Path(\"/kaggle/input/data-picrust\")\n",
        "abundance_excel= base_dir / \"merged_to_sequence.xlsx\"\n",
        "fasta_file_final = base_dir / \"final_sequences_gg.fasta\"\n",
        "output_dir = Path(\"/kaggle/working/\") '''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COW1kGFZCqSG"
      },
      "source": [
        "The fasta file come from the Alternative Sequences finding from the Greenes Genes Database, from the taxonomy in this study made in section 7 in the 5_Sequences_qiime notebook: final_sequences_gg.fasta. Abundance dataframe come from the data from notebook 4 merged_to_sequence.xlsx sheet=core_check_usual_taxa which is a unified df between 3 different groups explained previously: cora_taxa (>20% 60 abundance features), usual_taxa (17 high literature ranking bacteria influencing corrosion) and checked_taxa (30 statistically significant to the corrosion risk label) in total 85 features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T09:50:11.074331Z",
          "iopub.status.busy": "2025-02-19T09:50:11.073894Z",
          "iopub.status.idle": "2025-02-19T09:50:11.267133Z",
          "shell.execute_reply": "2025-02-19T09:50:11.265805Z",
          "shell.execute_reply.started": "2025-02-19T09:50:11.074292Z"
        },
        "id": "qzMCSdlPuvgI",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Integrated taxa from origin genus as headers with levels 6 for the genera, 7 for the GID, muss be cleaned\n",
        "Integrated_T = pd.read_excel(abundance_excel, sheet_name='core_check_usual_taxa', header=[0,1,2,3,4,5,6,7], engine ='openpyxl')\n",
        "# Drop first row (index 0) and first column in one chain\n",
        "Integrated_T = Integrated_T.drop(index=0).drop(Integrated_T.columns[0], axis=1)\n",
        "Integrated_T= Integrated_T.astype({'Sites': str})\n",
        "Integrated_T['Sites'] = Integrated_T['Sites'].fillna('Source')\n",
        "# Remove 'Unnamed' level names\n",
        "Integrated_T.columns = Integrated_T.columns.map(lambda x: tuple('' if 'Unnamed' in str(level) else level for level in x))\n",
        "# Changing dtypes to category whiles respecting structure\n",
        "Integrated_T[\"Category\"] = Integrated_T[\"Category\"].astype(\"Int64\")\n",
        "Integrated_T= Integrated_T.set_index(\"Sites\")\n",
        "pre_Integrated = Integrated_T.T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T09:50:15.084302Z",
          "iopub.status.busy": "2025-02-19T09:50:15.083951Z",
          "iopub.status.idle": "2025-02-19T09:50:15.122637Z",
          "shell.execute_reply": "2025-02-19T09:50:15.121449Z",
          "shell.execute_reply.started": "2025-02-19T09:50:15.084276Z"
        },
        "id": "aZSzaNSQuvgI",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        },
        "outputId": "9ab8762a-ad5d-450f-ce71-2cca53046b39"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        Category Rhodocyclales_Rhodocyclaceae_Azospira  \\\n",
              "                                              Bacteria   \n",
              "                                        Proteobacteria   \n",
              "                                    Betaproteobacteria   \n",
              "                                         Rhodocyclales   \n",
              "                                        Rhodocyclaceae   \n",
              "                                              Azospira   \n",
              "                                                   110   \n",
              "Sites                                                    \n",
              "site_67        3                              0.004886   \n",
              "site_68        3                                     0   \n",
              "site_69        1                                  1.47   \n",
              "site_70        1                                  1.72   \n",
              "Source      <NA>                              chk-core   \n",
              "\n",
              "        Actinomycetales_Dermabacteraceae_Brachybacterium  \\\n",
              "                                                Bacteria   \n",
              "                                          Actinobacteria   \n",
              "                                          Actinobacteria   \n",
              "                                         Actinomycetales   \n",
              "                                        Dermabacteraceae   \n",
              "                                         Brachybacterium   \n",
              "                                                     140   \n",
              "Sites                                                      \n",
              "site_67                                                0   \n",
              "site_68                                         0.021172   \n",
              "site_69                                                0   \n",
              "site_70                                                0   \n",
              "Source                                               chk   \n",
              "\n",
              "        Actinomycetales_Brevibacteriaceae_Brevibacterium  \\\n",
              "                                                Bacteria   \n",
              "                                          Actinobacteria   \n",
              "                                          Actinobacteria   \n",
              "                                         Actinomycetales   \n",
              "                                       Brevibacteriaceae   \n",
              "                                          Brevibacterium   \n",
              "                                                     145   \n",
              "Sites                                                      \n",
              "site_67                                                0   \n",
              "site_68                                                0   \n",
              "site_69                                                0   \n",
              "site_70                                                0   \n",
              "Source                                               chk   \n",
              "\n",
              "        Erysipelotrichales_Erysipelotrichaceae_Bulleidia  \\\n",
              "                                                Bacteria   \n",
              "                                              Firmicutes   \n",
              "                                         Erysipelotrichi   \n",
              "                                      Erysipelotrichales   \n",
              "                                     Erysipelotrichaceae   \n",
              "                                               Bulleidia   \n",
              "                                                     154   \n",
              "Sites                                                      \n",
              "site_67                                                0   \n",
              "site_68                                                0   \n",
              "site_69                                                0   \n",
              "site_70                                                0   \n",
              "Source                                               chk   \n",
              "\n",
              "        Clostridiales_Clostridiaceae_Clostridium  \\\n",
              "                                        Bacteria   \n",
              "                                      Firmicutes   \n",
              "                                      Clostridia   \n",
              "                                   Clostridiales   \n",
              "                                  Clostridiaceae   \n",
              "                                     Clostridium   \n",
              "                                             214   \n",
              "Sites                                              \n",
              "site_67                                        0   \n",
              "site_68                                        0   \n",
              "site_69                                        0   \n",
              "site_70                                        0   \n",
              "Source                               chk-core-us   \n",
              "\n",
              "        Actinomycetales_Corynebacteriaceae_Corynebacterium  \\\n",
              "                                                  Bacteria   \n",
              "                                            Actinobacteria   \n",
              "                                            Actinobacteria   \n",
              "                                           Actinomycetales   \n",
              "                                        Corynebacteriaceae   \n",
              "                                           Corynebacterium   \n",
              "                                                       229   \n",
              "Sites                                                        \n",
              "site_67                                                  0   \n",
              "site_68                                                  0   \n",
              "site_69                                                  0   \n",
              "site_70                                                  0   \n",
              "Source                                         chk-core-us   \n",
              "\n",
              "        Lactobacillales_Enterococcaceae_Enterococcus  \\\n",
              "                                            Bacteria   \n",
              "                                          Firmicutes   \n",
              "                                             Bacilli   \n",
              "                                     Lactobacillales   \n",
              "                                     Enterococcaceae   \n",
              "                                        Enterococcus   \n",
              "                                                 300   \n",
              "Sites                                                  \n",
              "site_67                                            0   \n",
              "site_68                                            0   \n",
              "site_69                                            0   \n",
              "site_70                                            0   \n",
              "Source                                           chk   \n",
              "\n",
              "        Thermoanaerobacterales_Thermoanaerobacteraceae_Gelria  \\\n",
              "                                                     Bacteria   \n",
              "                                                   Firmicutes   \n",
              "                                                   Clostridia   \n",
              "                                       Thermoanaerobacterales   \n",
              "                                      Thermoanaerobacteraceae   \n",
              "                                                       Gelria   \n",
              "                                                          334   \n",
              "Sites                                                           \n",
              "site_67                                                  0      \n",
              "site_68                                                  0      \n",
              "site_69                                                  0      \n",
              "site_70                                                2.3      \n",
              "Source                                                 chk      \n",
              "\n",
              "        Oceanospirillales_Halomonadaceae_Halomonas  ...  \\\n",
              "                                          Bacteria  ...   \n",
              "                                    Proteobacteria  ...   \n",
              "                               Gammaproteobacteria  ...   \n",
              "                                 Oceanospirillales  ...   \n",
              "                                    Halomonadaceae  ...   \n",
              "                                         Halomonas  ...   \n",
              "                                               354  ...   \n",
              "Sites                                               ...   \n",
              "site_67                                   0.942935  ...   \n",
              "site_68                                  28.889007  ...   \n",
              "site_69                                          0  ...   \n",
              "site_70                                          0  ...   \n",
              "Source                                    chk-core  ...   \n",
              "\n",
              "        Actinomycetales_Propionibacteriaceae_Tessaracoccus  \\\n",
              "                                                  Bacteria   \n",
              "                                            Actinobacteria   \n",
              "                                            Actinobacteria   \n",
              "                                           Actinomycetales   \n",
              "                                      Propionibacteriaceae   \n",
              "                                             Tessaracoccus   \n",
              "                                                       715   \n",
              "Sites                                                        \n",
              "site_67                                                  0   \n",
              "site_68                                                  0   \n",
              "site_69                                                  0   \n",
              "site_70                                                  0   \n",
              "Source                                                core   \n",
              "\n",
              "        Clostridiales_Peptococcaceae_Thermincola  \\\n",
              "                                        Bacteria   \n",
              "                                      Firmicutes   \n",
              "                                      Clostridia   \n",
              "                                   Clostridiales   \n",
              "                                  Peptococcaceae   \n",
              "                                     Thermincola   \n",
              "                                             719   \n",
              "Sites                                              \n",
              "site_67                                        0   \n",
              "site_68                                        0   \n",
              "site_69                                     1.63   \n",
              "site_70                                        0   \n",
              "Source                                      core   \n",
              "\n",
              "        Spirochaetales_Spirochaetaceae_Treponema  \\\n",
              "                                        Bacteria   \n",
              "                                    Spirochaetes   \n",
              "                                    Spirochaetes   \n",
              "                                  Spirochaetales   \n",
              "                                 Spirochaetaceae   \n",
              "                                       Treponema   \n",
              "                                             731   \n",
              "Sites                                              \n",
              "site_67                                        0   \n",
              "site_68                                        0   \n",
              "site_69                                        0   \n",
              "site_70                                        0   \n",
              "Source                                      core   \n",
              "\n",
              "        Burkholderiales_Oxalobacteraceae_Oxalobacteraceae_unclassified  \\\n",
              "                                                              Bacteria   \n",
              "                                                        Proteobacteria   \n",
              "                                                    Betaproteobacteria   \n",
              "                                                       Burkholderiales   \n",
              "                                                      Oxalobacteraceae   \n",
              "                                         Oxalobacteraceae_unclassified   \n",
              "                                                                   853   \n",
              "Sites                                                                    \n",
              "site_67                                                  0               \n",
              "site_68                                                  0               \n",
              "site_69                                                  0               \n",
              "site_70                                                  0               \n",
              "Source                                                core               \n",
              "\n",
              "        Burkholderiales_Comamonadaceae_Variovorax  \\\n",
              "                                         Bacteria   \n",
              "                                   Proteobacteria   \n",
              "                               Betaproteobacteria   \n",
              "                                  Burkholderiales   \n",
              "                                   Comamonadaceae   \n",
              "                                       Variovorax   \n",
              "                                              863   \n",
              "Sites                                               \n",
              "site_67                                  0.151456   \n",
              "site_68                                         0   \n",
              "site_69                                         0   \n",
              "site_70                                         0   \n",
              "Source                                       core   \n",
              "\n",
              "        Anaerolineales_Anaerolinaceae_Wchb1-05  \\\n",
              "                                      Bacteria   \n",
              "                                   Chloroflexi   \n",
              "                                  Anaerolineae   \n",
              "                                Anaerolineales   \n",
              "                                Anaerolinaceae   \n",
              "                                      Wchb1-05   \n",
              "                                           867   \n",
              "Sites                                            \n",
              "site_67                                      0   \n",
              "site_68                                      0   \n",
              "site_69                                      0   \n",
              "site_70                                      0   \n",
              "Source                                    core   \n",
              "\n",
              "        Desulfobacterales_Desulfobacteraceae_Desulfobacterium  \\\n",
              "                                                     Bacteria   \n",
              "                                               Proteobacteria   \n",
              "                                          Deltaproteobacteria   \n",
              "                                            Desulfobacterales   \n",
              "                                           Desulfobacteraceae   \n",
              "                                             Desulfobacterium   \n",
              "                                                          264   \n",
              "Sites                                                           \n",
              "site_67                                                  0      \n",
              "site_68                                                  0      \n",
              "site_69                                                  0      \n",
              "site_70                                                  0      \n",
              "Source                                                  us      \n",
              "\n",
              "        Desulfobacterales_Desulfobulbaceae_Desulfobulbus  \\\n",
              "                                                Bacteria   \n",
              "                                          Proteobacteria   \n",
              "                                     Deltaproteobacteria   \n",
              "                                       Desulfobacterales   \n",
              "                                        Desulfobulbaceae   \n",
              "                                           Desulfobulbus   \n",
              "                                                     265   \n",
              "Sites                                                      \n",
              "site_67                                                0   \n",
              "site_68                                                0   \n",
              "site_69                                                0   \n",
              "site_70                                                0   \n",
              "Source                                                us   \n",
              "\n",
              "        Gallionellales_Gallionellaceae_Gallionella  \\\n",
              "                                          Bacteria   \n",
              "                                    Proteobacteria   \n",
              "                                Betaproteobacteria   \n",
              "                                    Gallionellales   \n",
              "                                   Gallionellaceae   \n",
              "                                       Gallionella   \n",
              "                                               332   \n",
              "Sites                                                \n",
              "site_67                                          0   \n",
              "site_68                                          0   \n",
              "site_69                                          0   \n",
              "site_70                                          0   \n",
              "Source                                          us   \n",
              "\n",
              "        Alteromonadales_Shewanellaceae_Shewanella  \n",
              "                                         Bacteria  \n",
              "                                   Proteobacteria  \n",
              "                              Gammaproteobacteria  \n",
              "                                  Alteromonadales  \n",
              "                                   Shewanellaceae  \n",
              "                                       Shewanella  \n",
              "                                              656  \n",
              "Sites                                              \n",
              "site_67                                  0.004886  \n",
              "site_68                                  0.005293  \n",
              "site_69                                         0  \n",
              "site_70                                         0  \n",
              "Source                                         us  \n",
              "\n",
              "[5 rows x 86 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b3dbee9f-6948-403a-a933-dac970bdfa43\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr:last-of-type th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>Category</th>\n",
              "      <th>Rhodocyclales_Rhodocyclaceae_Azospira</th>\n",
              "      <th>Actinomycetales_Dermabacteraceae_Brachybacterium</th>\n",
              "      <th>Actinomycetales_Brevibacteriaceae_Brevibacterium</th>\n",
              "      <th>Erysipelotrichales_Erysipelotrichaceae_Bulleidia</th>\n",
              "      <th>Clostridiales_Clostridiaceae_Clostridium</th>\n",
              "      <th>Actinomycetales_Corynebacteriaceae_Corynebacterium</th>\n",
              "      <th>Lactobacillales_Enterococcaceae_Enterococcus</th>\n",
              "      <th>Thermoanaerobacterales_Thermoanaerobacteraceae_Gelria</th>\n",
              "      <th>Oceanospirillales_Halomonadaceae_Halomonas</th>\n",
              "      <th>...</th>\n",
              "      <th>Actinomycetales_Propionibacteriaceae_Tessaracoccus</th>\n",
              "      <th>Clostridiales_Peptococcaceae_Thermincola</th>\n",
              "      <th>Spirochaetales_Spirochaetaceae_Treponema</th>\n",
              "      <th>Burkholderiales_Oxalobacteraceae_Oxalobacteraceae_unclassified</th>\n",
              "      <th>Burkholderiales_Comamonadaceae_Variovorax</th>\n",
              "      <th>Anaerolineales_Anaerolinaceae_Wchb1-05</th>\n",
              "      <th>Desulfobacterales_Desulfobacteraceae_Desulfobacterium</th>\n",
              "      <th>Desulfobacterales_Desulfobulbaceae_Desulfobulbus</th>\n",
              "      <th>Gallionellales_Gallionellaceae_Gallionella</th>\n",
              "      <th>Alteromonadales_Shewanellaceae_Shewanella</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>Bacteria</th>\n",
              "      <th>Bacteria</th>\n",
              "      <th>Bacteria</th>\n",
              "      <th>Bacteria</th>\n",
              "      <th>Bacteria</th>\n",
              "      <th>Bacteria</th>\n",
              "      <th>Bacteria</th>\n",
              "      <th>Bacteria</th>\n",
              "      <th>Bacteria</th>\n",
              "      <th>...</th>\n",
              "      <th>Bacteria</th>\n",
              "      <th>Bacteria</th>\n",
              "      <th>Bacteria</th>\n",
              "      <th>Bacteria</th>\n",
              "      <th>Bacteria</th>\n",
              "      <th>Bacteria</th>\n",
              "      <th>Bacteria</th>\n",
              "      <th>Bacteria</th>\n",
              "      <th>Bacteria</th>\n",
              "      <th>Bacteria</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>Proteobacteria</th>\n",
              "      <th>Actinobacteria</th>\n",
              "      <th>Actinobacteria</th>\n",
              "      <th>Firmicutes</th>\n",
              "      <th>Firmicutes</th>\n",
              "      <th>Actinobacteria</th>\n",
              "      <th>Firmicutes</th>\n",
              "      <th>Firmicutes</th>\n",
              "      <th>Proteobacteria</th>\n",
              "      <th>...</th>\n",
              "      <th>Actinobacteria</th>\n",
              "      <th>Firmicutes</th>\n",
              "      <th>Spirochaetes</th>\n",
              "      <th>Proteobacteria</th>\n",
              "      <th>Proteobacteria</th>\n",
              "      <th>Chloroflexi</th>\n",
              "      <th>Proteobacteria</th>\n",
              "      <th>Proteobacteria</th>\n",
              "      <th>Proteobacteria</th>\n",
              "      <th>Proteobacteria</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>Betaproteobacteria</th>\n",
              "      <th>Actinobacteria</th>\n",
              "      <th>Actinobacteria</th>\n",
              "      <th>Erysipelotrichi</th>\n",
              "      <th>Clostridia</th>\n",
              "      <th>Actinobacteria</th>\n",
              "      <th>Bacilli</th>\n",
              "      <th>Clostridia</th>\n",
              "      <th>Gammaproteobacteria</th>\n",
              "      <th>...</th>\n",
              "      <th>Actinobacteria</th>\n",
              "      <th>Clostridia</th>\n",
              "      <th>Spirochaetes</th>\n",
              "      <th>Betaproteobacteria</th>\n",
              "      <th>Betaproteobacteria</th>\n",
              "      <th>Anaerolineae</th>\n",
              "      <th>Deltaproteobacteria</th>\n",
              "      <th>Deltaproteobacteria</th>\n",
              "      <th>Betaproteobacteria</th>\n",
              "      <th>Gammaproteobacteria</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>Rhodocyclales</th>\n",
              "      <th>Actinomycetales</th>\n",
              "      <th>Actinomycetales</th>\n",
              "      <th>Erysipelotrichales</th>\n",
              "      <th>Clostridiales</th>\n",
              "      <th>Actinomycetales</th>\n",
              "      <th>Lactobacillales</th>\n",
              "      <th>Thermoanaerobacterales</th>\n",
              "      <th>Oceanospirillales</th>\n",
              "      <th>...</th>\n",
              "      <th>Actinomycetales</th>\n",
              "      <th>Clostridiales</th>\n",
              "      <th>Spirochaetales</th>\n",
              "      <th>Burkholderiales</th>\n",
              "      <th>Burkholderiales</th>\n",
              "      <th>Anaerolineales</th>\n",
              "      <th>Desulfobacterales</th>\n",
              "      <th>Desulfobacterales</th>\n",
              "      <th>Gallionellales</th>\n",
              "      <th>Alteromonadales</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>Rhodocyclaceae</th>\n",
              "      <th>Dermabacteraceae</th>\n",
              "      <th>Brevibacteriaceae</th>\n",
              "      <th>Erysipelotrichaceae</th>\n",
              "      <th>Clostridiaceae</th>\n",
              "      <th>Corynebacteriaceae</th>\n",
              "      <th>Enterococcaceae</th>\n",
              "      <th>Thermoanaerobacteraceae</th>\n",
              "      <th>Halomonadaceae</th>\n",
              "      <th>...</th>\n",
              "      <th>Propionibacteriaceae</th>\n",
              "      <th>Peptococcaceae</th>\n",
              "      <th>Spirochaetaceae</th>\n",
              "      <th>Oxalobacteraceae</th>\n",
              "      <th>Comamonadaceae</th>\n",
              "      <th>Anaerolinaceae</th>\n",
              "      <th>Desulfobacteraceae</th>\n",
              "      <th>Desulfobulbaceae</th>\n",
              "      <th>Gallionellaceae</th>\n",
              "      <th>Shewanellaceae</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>Azospira</th>\n",
              "      <th>Brachybacterium</th>\n",
              "      <th>Brevibacterium</th>\n",
              "      <th>Bulleidia</th>\n",
              "      <th>Clostridium</th>\n",
              "      <th>Corynebacterium</th>\n",
              "      <th>Enterococcus</th>\n",
              "      <th>Gelria</th>\n",
              "      <th>Halomonas</th>\n",
              "      <th>...</th>\n",
              "      <th>Tessaracoccus</th>\n",
              "      <th>Thermincola</th>\n",
              "      <th>Treponema</th>\n",
              "      <th>Oxalobacteraceae_unclassified</th>\n",
              "      <th>Variovorax</th>\n",
              "      <th>Wchb1-05</th>\n",
              "      <th>Desulfobacterium</th>\n",
              "      <th>Desulfobulbus</th>\n",
              "      <th>Gallionella</th>\n",
              "      <th>Shewanella</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>110</th>\n",
              "      <th>140</th>\n",
              "      <th>145</th>\n",
              "      <th>154</th>\n",
              "      <th>214</th>\n",
              "      <th>229</th>\n",
              "      <th>300</th>\n",
              "      <th>334</th>\n",
              "      <th>354</th>\n",
              "      <th>...</th>\n",
              "      <th>715</th>\n",
              "      <th>719</th>\n",
              "      <th>731</th>\n",
              "      <th>853</th>\n",
              "      <th>863</th>\n",
              "      <th>867</th>\n",
              "      <th>264</th>\n",
              "      <th>265</th>\n",
              "      <th>332</th>\n",
              "      <th>656</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Sites</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>site_67</th>\n",
              "      <td>3</td>\n",
              "      <td>0.004886</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.942935</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.151456</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.004886</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>site_68</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0.021172</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>28.889007</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.005293</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>site_69</th>\n",
              "      <td>1</td>\n",
              "      <td>1.47</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.63</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>site_70</th>\n",
              "      <td>1</td>\n",
              "      <td>1.72</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2.3</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Source</th>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>chk-core</td>\n",
              "      <td>chk</td>\n",
              "      <td>chk</td>\n",
              "      <td>chk</td>\n",
              "      <td>chk-core-us</td>\n",
              "      <td>chk-core-us</td>\n",
              "      <td>chk</td>\n",
              "      <td>chk</td>\n",
              "      <td>chk-core</td>\n",
              "      <td>...</td>\n",
              "      <td>core</td>\n",
              "      <td>core</td>\n",
              "      <td>core</td>\n",
              "      <td>core</td>\n",
              "      <td>core</td>\n",
              "      <td>core</td>\n",
              "      <td>us</td>\n",
              "      <td>us</td>\n",
              "      <td>us</td>\n",
              "      <td>us</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 86 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b3dbee9f-6948-403a-a933-dac970bdfa43')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b3dbee9f-6948-403a-a933-dac970bdfa43 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b3dbee9f-6948-403a-a933-dac970bdfa43');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-cdc22863-153d-4094-82c0-8424e7a42a03\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-cdc22863-153d-4094-82c0-8424e7a42a03')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-cdc22863-153d-4094-82c0-8424e7a42a03 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "Integrated_T.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdOLY5SPuvgI"
      },
      "source": [
        "## 2.3. Making Sequences for Picrust fasta file\n",
        "\n",
        "Picrust Functional Analyiss requires a biom table with otus as index, samples as headers and abundance as values. The present biom has genus names but is needs instead Otus instead. The other input file for picrust is the representative sequences table that consist of the sequences per genera followed by the frequency of that genera on the whole sample, this is done directly by the software. The fasta file requires the otus instead of the genera names and the sequences non aligned coming from notebook 5. The following scrips will formate the data to picrust."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-02-19T09:42:03.38396Z",
          "iopub.status.busy": "2025-02-19T09:42:03.383656Z",
          "iopub.status.idle": "2025-02-19T09:42:03.422952Z",
          "shell.execute_reply": "2025-02-19T09:42:03.421779Z",
          "shell.execute_reply.started": "2025-02-19T09:42:03.383933Z"
        },
        "id": "cizMvGCGuvgI",
        "outputId": "875463fa-f6bf-4148-f2c0-a748564823a7",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "85"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# Read and modify sequences\n",
        "new_records = []\n",
        "for record in SeqIO.parse(fasta_file_final, \"fasta\"):\n",
        "    match = re.search(r\"\\s(\\d+)\\s\", record.description)  # Look for digits surrounded by spaces\n",
        "    if match:\n",
        "        otu_id = match.group(1)\n",
        "    else:\n",
        "        print(f\"Warning: Could not extract OTU ID from description: {record.description}\")\n",
        "        continue  # Skip this record if OTU ID not found\n",
        "\n",
        "    # Create new record with only OTU as ID\n",
        "    new_record = SeqRecord(\n",
        "        record.seq,\n",
        "        id=otu_id,\n",
        "        description=\"\"  # Empty description to keep only ID\n",
        "    )\n",
        "    new_records.append(new_record)\n",
        "\n",
        "# Write modified FASTA\n",
        "output_fasta_path = Path(output_dir / \"sequences_for_picrust.fasta\")\n",
        "\n",
        "SeqIO.write(new_records, output_fasta_path, \"fasta\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RN-pFp1cuvgI"
      },
      "source": [
        "## 2.4. Making of Dataframes for 2 Different Pipelines\n",
        "The following script is the path to the biom file but also to the Integrate dataframe which create dataframes that discriminate its origin in order to pass then through picrust different pipelines, to know: Simple_Base that compares the known bacteria namely usual_taxa against the other features to understand their relationships on the function of their metabolism, an additional group is put forward as simply_candidate_mic which corresponds to the bacteria no previously linked to corrosion but showing an statistical significance with the risk label, those come from the checked_taxa and in this study are: genera(GID): Bulleida (154); Mycoplana (471), Oxobacter (512) and Oerskovia (). Also as showing an favor behaviour against corrosion are presented: Phenylobacterium (549), Gelria(334), Porphyrobacteria (564) and Tepidimonas (712)\n",
        "SIMPLE_BASE = {'known': 'simple_known_mic', 'other': 'simple_candidate_mic'}\n",
        "The second pipeline comprises a more detailed separation of the bacteria and that is: The Known bacteria as previously, pure_checked corresponding to the statistical significant genera, pure_core correspondent to the core taxa on the systems and the combination of the core and checked taxa.\n",
        "DETAILED_BASE = {'known': 'detailed_known_mic','pure_checked': 'detailed_pure_checked_mic',\n",
        "    'pure_core': 'detailed_pure_core_mic', 'checked_core': 'detailed_checked_core_mic'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akl8MkZjuvgI"
      },
      "source": [
        "__Making the Integrated dataframe__\n",
        "The original dataframe has a column for source, indicating from which df  came from (core, usual, checked), this script proceses that datadrame into individual dfs and the combined preserving the source for further analysis. The Integrated dataframe continues to be process on the next step to become the biom abundance df."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-02-19T09:42:03.431944Z",
          "iopub.status.busy": "2025-02-19T09:42:03.43168Z",
          "iopub.status.idle": "2025-02-19T09:42:03.497242Z",
          "shell.execute_reply": "2025-02-19T09:42:03.496104Z",
          "shell.execute_reply.started": "2025-02-19T09:42:03.431915Z"
        },
        "id": "MWukq8fUCqSH",
        "outputId": "b2d1d628-18c1-4f59-f736-3f7603f8d8a4",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Detailed Classification Results:\n",
            "Known corrosion bacteria: 17\n",
            "Pure checked bacteria: 19\n",
            "Pure core bacteria: 46\n",
            "Checked-core bacteria: 3\n",
            "\n",
            "Total classified taxa: 85\n",
            "Total in dataset: 85\n"
          ]
        }
      ],
      "source": [
        "def process_integrated_data(df):\n",
        "    \"\"\"\n",
        "    Process the integrated DataFrame to create a new DataFrame with clear column names\n",
        "    and preserve all values including source information.\n",
        "\n",
        "    Parameters:\n",
        "    df (pandas.DataFrame): Input DataFrame with MultiIndex index and site columns\n",
        "\n",
        "    Returns:\n",
        "    pandas.DataFrame: Processed DataFrame with clear structure\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract genera and GIDs from the index MultiIndex\n",
        "    genera = df.index.get_level_values(6)[1:]  # Skip first row\n",
        "    gids = pd.to_numeric(df.index.get_level_values(7)[1:], errors='coerce')\n",
        "\n",
        "    # Create a new DataFrame with the extracted information\n",
        "    result_df = pd.DataFrame({\n",
        "        'Genus': genera,\n",
        "        'GID': gids\n",
        "    })\n",
        "\n",
        "    # Add the site values from the original DataFrame\n",
        "    for col in df.columns:\n",
        "        result_df[col] = df.iloc[1:][col].values\n",
        "\n",
        "    # Clean up the DataFrame\n",
        "    result_df['GID'] = pd.to_numeric(result_df['GID'], errors='coerce')\n",
        "    result_df = result_df.dropna(subset=['GID'])\n",
        "    result_df['GID'] = result_df['GID'].astype(int)\n",
        "\n",
        "    return result_df\n",
        "\n",
        "def get_taxa_groups(df):\n",
        "    \"\"\"\n",
        "    Separate the processed DataFrame into different taxa groups based on Source column\n",
        "\n",
        "    Parameters:\n",
        "    df (pandas.DataFrame): Processed DataFrame from process_integrated_data()\n",
        "\n",
        "    Returns:\n",
        "    dict: Dictionary containing DataFrames for different taxa groups\n",
        "    \"\"\"\n",
        "    # Split the data into groups based on 'Source' column patterns\n",
        "\n",
        "    # Known corrosion bacteria (any pattern with 'us')\n",
        "    known_bacteria = df[df['Source'].str.contains('us', case=False, na=False)]\n",
        "\n",
        "    # Pure checked bacteria (only 'chk' without 'core' or 'us')\n",
        "    pure_checked = df[\n",
        "        df['Source'].str.contains('chk', case=False, na=False) &\n",
        "        ~df['Source'].str.contains('core|us', case=False, na=False)\n",
        "    ]\n",
        "\n",
        "    # Pure core bacteria (only 'core' without 'chk' or 'us')\n",
        "    pure_core = df[\n",
        "        df['Source'].str.contains('core', case=False, na=False) &\n",
        "        ~df['Source'].str.contains('chk|us', case=False, na=False)\n",
        "    ]\n",
        "\n",
        "    # Checked-core bacteria (contains both 'core' and 'chk' but no 'us')\n",
        "    checked_core = df[\n",
        "        df['Source'].str.contains('chk.*core|core.*chk', case=False, na=False) &\n",
        "        ~df['Source'].str.contains('us', case=False, na=False)\n",
        "    ]\n",
        "\n",
        "    # Create groups dictionary\n",
        "    taxa_groups = {\n",
        "        'known_bacteria': known_bacteria,\n",
        "        'pure_checked': pure_checked,\n",
        "        'pure_core': pure_core,\n",
        "        'checked_core': checked_core\n",
        "    }\n",
        "\n",
        "    # Print summary statistics\n",
        "    print(\"\\nDetailed Classification Results:\")\n",
        "    print(f\"Known corrosion bacteria: {len(known_bacteria)}\")\n",
        "    print(f\"Pure checked bacteria: {len(pure_checked)}\")\n",
        "    print(f\"Pure core bacteria: {len(pure_core)}\")\n",
        "    print(f\"Checked-core bacteria: {len(checked_core)}\")\n",
        "\n",
        "    # Verify total matches expected\n",
        "    total_classified = len(known_bacteria) + len(pure_checked) + len(pure_core) + len(checked_core)\n",
        "    print(f\"\\nTotal classified taxa: {total_classified}\")\n",
        "    print(f\"Total in dataset: {len(df)}\")\n",
        "\n",
        "    return taxa_groups\n",
        "\n",
        "# Usage example:\n",
        "Integrated = process_integrated_data(pre_Integrated)\n",
        "\n",
        "# Get the groups\n",
        "taxa_groups = get_taxa_groups(Integrated)\n",
        "\n",
        "# Access individual groups -\n",
        "known_bacteria = taxa_groups['known_bacteria']\n",
        "pure_core = taxa_groups['pure_core']\n",
        "pure_checked = taxa_groups['pure_checked']\n",
        "checked_core = taxa_groups['checked_core']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkfsNjfki4Ap",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "fa5a4864-175f-45d8-8d11-fd4c7a2602bf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        Genus  GID     site_1   site_2    site_3    site_4    site_5 site_6  \\\n",
              "0    Azospira  110  26.928048  1.85923  3.093543  2.573991  2.709369      0   \n",
              "8   Halomonas  354          0        0  0.024552         0         0      0   \n",
              "21    Psb-m-3  581          0        0         0         0         0      0   \n",
              "\n",
              "      site_7 site_8  ...   site_62   site_63 site_64    site_65 site_66  \\\n",
              "0   2.146235   0.54  ...  0.571304  0.624133    0.26   4.518236     0.4   \n",
              "8   0.002425      0  ...         0         0       0          0       0   \n",
              "21         0      0  ...         0         0       0  14.480131       0   \n",
              "\n",
              "     site_67    site_68 site_69 site_70    Source  \n",
              "0   0.004886          0    1.47    1.72  chk-core  \n",
              "8   0.942935  28.889007       0       0  chk-core  \n",
              "21         0          0       0       0  chk-core  \n",
              "\n",
              "[3 rows x 73 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-28c95d6c-8b67-4fd8-bda3-c94e6e4dd197\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Genus</th>\n",
              "      <th>GID</th>\n",
              "      <th>site_1</th>\n",
              "      <th>site_2</th>\n",
              "      <th>site_3</th>\n",
              "      <th>site_4</th>\n",
              "      <th>site_5</th>\n",
              "      <th>site_6</th>\n",
              "      <th>site_7</th>\n",
              "      <th>site_8</th>\n",
              "      <th>...</th>\n",
              "      <th>site_62</th>\n",
              "      <th>site_63</th>\n",
              "      <th>site_64</th>\n",
              "      <th>site_65</th>\n",
              "      <th>site_66</th>\n",
              "      <th>site_67</th>\n",
              "      <th>site_68</th>\n",
              "      <th>site_69</th>\n",
              "      <th>site_70</th>\n",
              "      <th>Source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Azospira</td>\n",
              "      <td>110</td>\n",
              "      <td>26.928048</td>\n",
              "      <td>1.85923</td>\n",
              "      <td>3.093543</td>\n",
              "      <td>2.573991</td>\n",
              "      <td>2.709369</td>\n",
              "      <td>0</td>\n",
              "      <td>2.146235</td>\n",
              "      <td>0.54</td>\n",
              "      <td>...</td>\n",
              "      <td>0.571304</td>\n",
              "      <td>0.624133</td>\n",
              "      <td>0.26</td>\n",
              "      <td>4.518236</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.004886</td>\n",
              "      <td>0</td>\n",
              "      <td>1.47</td>\n",
              "      <td>1.72</td>\n",
              "      <td>chk-core</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Halomonas</td>\n",
              "      <td>354</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.024552</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.002425</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.942935</td>\n",
              "      <td>28.889007</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>chk-core</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>Psb-m-3</td>\n",
              "      <td>581</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>14.480131</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>chk-core</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows × 73 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-28c95d6c-8b67-4fd8-bda3-c94e6e4dd197')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-28c95d6c-8b67-4fd8-bda3-c94e6e4dd197 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-28c95d6c-8b67-4fd8-bda3-c94e6e4dd197');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a9625e4c-3e3e-4e1d-b476-b6919bc32d8a\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a9625e4c-3e3e-4e1d-b476-b6919bc32d8a')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a9625e4c-3e3e-4e1d-b476-b6919bc32d8a button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_fbb771e8-6ec8-444f-8400-0eb4de30a391\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('checked_core')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_fbb771e8-6ec8-444f-8400-0eb4de30a391 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('checked_core');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "checked_core"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "checked_core"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g56zbUtiuvgJ"
      },
      "source": [
        "## 2.5. Making the Abundanc Biom dataframe for Picrust\n",
        "\n",
        "The final biom should have as index the Otus numbers no the genera names and a clean formate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T09:42:03.498862Z",
          "iopub.status.busy": "2025-02-19T09:42:03.498435Z",
          "iopub.status.idle": "2025-02-19T09:42:03.533685Z",
          "shell.execute_reply": "2025-02-19T09:42:03.532594Z",
          "shell.execute_reply.started": "2025-02-19T09:42:03.498824Z"
        },
        "id": "Qn6xPmvfuvgJ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# droping source and genus and putting GID as index\n",
        "pre_biom= Integrated.drop(columns=[\"Source\", \"GID\"])\n",
        "pre_biom= pre_biom.set_index(\"Genus\").astype(str)\n",
        "# Ensure all data values are float\n",
        "pre_biom = pre_biom.astype(float)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mI5nSqwXuvgJ"
      },
      "source": [
        "__changing genera to otus__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-02-19T09:42:03.535198Z",
          "iopub.status.busy": "2025-02-19T09:42:03.534792Z",
          "iopub.status.idle": "2025-02-19T09:42:03.555967Z",
          "shell.execute_reply": "2025-02-19T09:42:03.554671Z",
          "shell.execute_reply.started": "2025-02-19T09:42:03.535169Z"
        },
        "id": "amKUleGLuvgJ",
        "outputId": "26331595-f18d-429b-c3cd-d2346069ddd8",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample genus to OTU mappings:\n",
            "Corynebacterium -> 1042616\n",
            "Caulobacter -> 866365\n",
            "Legionella -> 838066\n",
            "Sediminibacterium -> 781203\n",
            "Smithella -> 713656\n"
          ]
        }
      ],
      "source": [
        "# Create genus to OTU mapping from FASTA headers\n",
        "genus_to_otu = {}\n",
        "for record in SeqIO.parse(fasta_file_final, \"fasta\"):\n",
        "    parts = record.description.split()\n",
        "    if len(parts) >= 3:\n",
        "        genus = parts[0]\n",
        "        otu = parts[1]  # We'll use the first OTU number\n",
        "        genus_to_otu[genus] = otu\n",
        "\n",
        "# Print a few mappings to verify\n",
        "print(\"Sample genus to OTU mappings:\")\n",
        "for i, (genus, otu) in enumerate(list(genus_to_otu.items())[:5]):\n",
        "    print(f\"{genus} -> {otu}\")\n",
        "\n",
        "# Replace genus with OTU in the index\n",
        "pre_biom.index = pre_biom.index.map(lambda x: genus_to_otu.get(x, x))\n",
        "\n",
        "# Remove the 'Genus' name from the index\n",
        "pre_biom.index.name = \"OTU\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFYJS2KuuvgJ"
      },
      "source": [
        "__Calculation counts for picrust2__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 475
        },
        "execution": {
          "iopub.execute_input": "2025-02-19T09:42:03.557141Z",
          "iopub.status.busy": "2025-02-19T09:42:03.55673Z",
          "iopub.status.idle": "2025-02-19T09:42:03.597345Z",
          "shell.execute_reply": "2025-02-19T09:42:03.596224Z",
          "shell.execute_reply.started": "2025-02-19T09:42:03.557111Z"
        },
        "id": "bwLLzgWLuvgJ",
        "outputId": "d8582236-eadf-47bf-e183-344419c5c891",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         site_1  site_2  site_3  site_4  site_5  site_6  site_7  site_8  \\\n",
              "OTU                                                                       \n",
              "690515   269280   18592   30935   25740   27094       0   21462    5400   \n",
              "519902        0       0       0       0       0       0       0       0   \n",
              "1134896       0       0       0       0       0       0       0       0   \n",
              "336037        0       0       0       0       0       0       0       0   \n",
              "1124194       0       0       0       0       0       0       0       0   \n",
              "...         ...     ...     ...     ...     ...     ...     ...     ...   \n",
              "4463039       0       0       0       0       0       0       0       0   \n",
              "556957        0       0       0       0       0       0       0       0   \n",
              "162379     1723     190       0      66     184       0     461       0   \n",
              "638460        0       0       0       0       0       0       0   15000   \n",
              "4378477    1077    1328    1350   13289    1657       0    1164       0   \n",
              "\n",
              "         site_9  site_10  ...  site_61  site_62  site_63  site_64  site_65  \\\n",
              "OTU                       ...                                                \n",
              "690515    39032        0  ...     3533     5713     6241     2600    45182   \n",
              "519902        0        0  ...        0        0        0        0      544   \n",
              "1134896       0        0  ...        0        0        0        0     2177   \n",
              "336037        0        0  ...        0        0        0        0        0   \n",
              "1124194       0        0  ...        0        0        0        0     5444   \n",
              "...         ...      ...  ...      ...      ...      ...      ...      ...   \n",
              "4463039       0        0  ...        0        0        0        0     7621   \n",
              "556957        0        0  ...        0        0        0        0        0   \n",
              "162379       89        0  ...        0     1439     1156        0    25585   \n",
              "638460       30        0  ...        0      131      231        0        0   \n",
              "4378477    1995        0  ...        0       44        0        0        0   \n",
              "\n",
              "         site_66  site_67  site_68  site_69  site_70  \n",
              "OTU                                                   \n",
              "690515      4000       49        0    14700    17200  \n",
              "519902         0        0      212        0        0  \n",
              "1134896        0        0        0        0        0  \n",
              "336037         0        0        0        0        0  \n",
              "1124194        0        0        0        0        0  \n",
              "...          ...      ...      ...      ...      ...  \n",
              "4463039        0        0        0        0        0  \n",
              "556957         0        0        0        0        0  \n",
              "162379         0        0        0        0        0  \n",
              "638460         0        0        0        0        0  \n",
              "4378477        0       49       53        0        0  \n",
              "\n",
              "[85 rows x 70 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6e18054f-dad5-498b-b0e9-1b2e8a109de2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>site_1</th>\n",
              "      <th>site_2</th>\n",
              "      <th>site_3</th>\n",
              "      <th>site_4</th>\n",
              "      <th>site_5</th>\n",
              "      <th>site_6</th>\n",
              "      <th>site_7</th>\n",
              "      <th>site_8</th>\n",
              "      <th>site_9</th>\n",
              "      <th>site_10</th>\n",
              "      <th>...</th>\n",
              "      <th>site_61</th>\n",
              "      <th>site_62</th>\n",
              "      <th>site_63</th>\n",
              "      <th>site_64</th>\n",
              "      <th>site_65</th>\n",
              "      <th>site_66</th>\n",
              "      <th>site_67</th>\n",
              "      <th>site_68</th>\n",
              "      <th>site_69</th>\n",
              "      <th>site_70</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>OTU</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>690515</th>\n",
              "      <td>269280</td>\n",
              "      <td>18592</td>\n",
              "      <td>30935</td>\n",
              "      <td>25740</td>\n",
              "      <td>27094</td>\n",
              "      <td>0</td>\n",
              "      <td>21462</td>\n",
              "      <td>5400</td>\n",
              "      <td>39032</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>3533</td>\n",
              "      <td>5713</td>\n",
              "      <td>6241</td>\n",
              "      <td>2600</td>\n",
              "      <td>45182</td>\n",
              "      <td>4000</td>\n",
              "      <td>49</td>\n",
              "      <td>0</td>\n",
              "      <td>14700</td>\n",
              "      <td>17200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>519902</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>544</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>212</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1134896</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2177</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>336037</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1124194</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5444</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4463039</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7621</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>556957</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>162379</th>\n",
              "      <td>1723</td>\n",
              "      <td>190</td>\n",
              "      <td>0</td>\n",
              "      <td>66</td>\n",
              "      <td>184</td>\n",
              "      <td>0</td>\n",
              "      <td>461</td>\n",
              "      <td>0</td>\n",
              "      <td>89</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1439</td>\n",
              "      <td>1156</td>\n",
              "      <td>0</td>\n",
              "      <td>25585</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>638460</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>15000</td>\n",
              "      <td>30</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>131</td>\n",
              "      <td>231</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4378477</th>\n",
              "      <td>1077</td>\n",
              "      <td>1328</td>\n",
              "      <td>1350</td>\n",
              "      <td>13289</td>\n",
              "      <td>1657</td>\n",
              "      <td>0</td>\n",
              "      <td>1164</td>\n",
              "      <td>0</td>\n",
              "      <td>1995</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>44</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>49</td>\n",
              "      <td>53</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>85 rows × 70 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6e18054f-dad5-498b-b0e9-1b2e8a109de2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6e18054f-dad5-498b-b0e9-1b2e8a109de2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6e18054f-dad5-498b-b0e9-1b2e8a109de2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-5ba082b6-6f81-44e2-aa51-aead66ba814a\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5ba082b6-6f81-44e2-aa51-aead66ba814a')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-5ba082b6-6f81-44e2-aa51-aead66ba814a button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_f2013322-2e03-47c9-868f-8818c7bddcd9\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('count_pre_biom')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_f2013322-2e03-47c9-868f-8818c7bddcd9 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('count_pre_biom');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "count_pre_biom"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "scaling_factor = 10000\n",
        "# Multiply by scaling factor and round to nearest integer\n",
        "count_pre_biom = np.round(pre_biom * scaling_factor).astype(int)\n",
        "count_pre_biom"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gu2uuKLHuvgJ"
      },
      "source": [
        "__Creating the biom table formate__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T09:42:03.59893Z",
          "iopub.status.busy": "2025-02-19T09:42:03.598532Z",
          "iopub.status.idle": "2025-02-19T09:42:03.622266Z",
          "shell.execute_reply": "2025-02-19T09:42:03.621214Z",
          "shell.execute_reply.started": "2025-02-19T09:42:03.598882Z"
        },
        "id": "PsCMsci7w8R7",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "b86ac445-2a53-4dfa-847b-09233102a1f8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# Create BIOM table with type specification\\nbiom_table = Table(data=count_pre_biom.values,\\n                  observation_ids=count_pre_biom.index.astype(str),\\n                  sample_ids=count_pre_biom.columns.astype(str),\\n                  type=\"OTU table\",\\n                  create_date=datetime.now().isoformat(),\\n                  generated_by=\"BIOM-Format\",\\n                  matrix_type=\"sparse\",\\n                  matrix_element_type=\"float\")\\n\\n# Save with explicit format\\noutput_path = output_dir / \"count_abundance_85.biom\"\\n\\nwith biom_open(output_path, \\'w\\') as f:\\n    biom_table.to_hdf5(f, generated_by=\"BIOM-Format\")'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "'''# Create BIOM table with type specification\n",
        "biom_table = Table(data=count_pre_biom.values,\n",
        "                  observation_ids=count_pre_biom.index.astype(str),\n",
        "                  sample_ids=count_pre_biom.columns.astype(str),\n",
        "                  type=\"OTU table\",\n",
        "                  create_date=datetime.now().isoformat(),\n",
        "                  generated_by=\"BIOM-Format\",\n",
        "                  matrix_type=\"sparse\",\n",
        "                  matrix_element_type=\"float\")\n",
        "\n",
        "# Save with explicit format\n",
        "output_path = output_dir / \"count_abundance_85.biom\"\n",
        "\n",
        "with biom_open(output_path, 'w') as f:\n",
        "    biom_table.to_hdf5(f, generated_by=\"BIOM-Format\")'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T09:42:03.623693Z",
          "iopub.status.busy": "2025-02-19T09:42:03.623367Z",
          "iopub.status.idle": "2025-02-19T09:42:06.698815Z",
          "shell.execute_reply": "2025-02-19T09:42:06.697254Z",
          "shell.execute_reply.started": "2025-02-19T09:42:03.623666Z"
        },
        "id": "gV5uEFS_uvgJ",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "e858c46d-b493-437b-bd3a-edde8ed7a4ee"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# Validate the table structure\\nprint(\"\\nValidating table...\")\\n!biom validate-table -i {output_path}\\n#/home/beatriz/MIC/2_Micro/data_picrust/count_abundance_85.biom\\n\\n# Show table info\\n!biom summarize-table -i {output_path}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "'''# Validate the table structure\n",
        "print(\"\\nValidating table...\")\n",
        "!biom validate-table -i {output_path}\n",
        "#/home/beatriz/MIC/2_Micro/data_picrust/count_abundance_85.biom\n",
        "\n",
        "# Show table info\n",
        "!biom summarize-table -i {output_path}'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzS6DEGb-PJG"
      },
      "source": [
        "Validating table...\n",
        "\n",
        "The input file is a valid BIOM-formatted file.\n",
        "Num samples: 70\n",
        "Num observations: 85\n",
        "Total count: 56747993\n",
        "Table density (fraction of non-zero values): 0.405\n",
        "\n",
        "Counts/sample summary:\n",
        " Min: 181800.000\n",
        " Max: 990578.000\n",
        " Median: 851078.500\n",
        " Mean: 810685.614\n",
        " Std. dev.: 157876.192\n",
        " Sample Metadata Categories: None provided\n",
        " Observation Metadata Categories: None provided\n",
        "\n",
        "Counts/sample detail:\n",
        "site_69: 181800.000\n",
        "site_67: 217903.000\n",
        "site_70: 270600.000\n",
        "site_26: 582999.000\n",
        "site_21: 589725.000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmMvQVofuvgK"
      },
      "source": [
        "# 3. Making the representative sequences\n",
        "\n",
        "__Convert Abundance Biom table and the Sequences into a QIIME2 artifact__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T09:42:06.700877Z",
          "iopub.status.busy": "2025-02-19T09:42:06.700455Z",
          "iopub.status.idle": "2025-02-19T09:42:06.72345Z",
          "shell.execute_reply": "2025-02-19T09:42:06.722298Z",
          "shell.execute_reply.started": "2025-02-19T09:42:06.700816Z"
        },
        "id": "yXcOOPN7uvgK",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c18e3db-bc31-4d9f-df6d-b6ec9cdea341"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Representative Sequences head:\n",
            ">1042616 ATGAACGCTGGCGGCGTGCTTAACACATGCAAGTCGAACGGAAAGGCCCTTGCTTGCAAGGGTGCTCGAGTGGCGAACGGGTGAGTAACACGTAAGTGATCTGCCCTGCACTGGGGGATAAGCTTGGGAAACTGGGTCTAATACCCTATAGGACTGCATCGTGGTTGGTGTGGTGGAAAGGTTTTTCTGGTGTGGGATGAGCTTGCGGCCTATCAGCTTGTTGGTGGGGTAATGGCCTACCAAGGCGGCGACGGGTAGCCGGCCTGAGAGGGTGTGCGGCCACATTGGGACTGAGATACAGCCCACACTCCTACGGGAGGCAGCAGTGGGGAATTTTGCACAATGGGCGGAAGCCTGATGCAGCGACGTCGTGTGGNGGATGAAGGCCTTCAGGTTGTAAACTCCTTTCGACAGGGACGAAGTTTTTTTGACGGTACCTGGATAAGAAGCACCGGCTAACTACGTGCCAGCACCCGCGGTAATACGTAGGGTGCGAGCGTTGTCCAGATTTACTGGGCATAAAGGGCTCGTAGGTTGTGTGTCGCGTCGTCTGTGTAATCCAGGGGCTTAACTTTTGGTTGGCAGGCGATACGGGCATTGCTTGAGTGCTGTAGGGGAGACTGGAATTCCTGGTGTAGCGGTGAAATGCGCAGATATCAGGAGGAACACCGATGGCGAAGGCAGGTCTCTGGGCAGTTACTGACGCTGAGGAGCGAGAGCGTGGGTAGCGAACAGGATTAGATACCCTGGTAGTCTATGCTGTAAACGGTGGGCGCTAGGTGTGAGTCCCTTCCACGGGGTTTGTGCCGTAGCTAACGCTTTAAGCGCCCCGCCTGGGGAGTACGGCCGCAAGGCTAAAACTCAAAGGAATTGACGGGGGCCCGCACAAGCGGCGGAGCATGTGGATTAATTCGATGCAACGCGAAGAACCTTACCTGGGCTTGACATATGTGGGATTGCGGCAGAGATGTTGTTTCCCTTCGTGGCTCACATACAGGTGGTGCATGGTTGTCGTCAGCTCGTGTCGTGAGATGTTGGGTTAAGTCCCGCAACGAGCGCAACCCTTGTCTTATGTTGCCAGCATGTTGTGGTGGGGACTCGTGAGAGACTGCCGGGGTTAACTCGGAGGAAGGTGGGGATGACGTCAAATCATCATGCCCCTTATGTCCAGNGCTTCACACATGCTACAATGGCTGGTACAGTGCGTGTGCGACACTGTGAGGTGGAGCTAATCGCTAAAGCCAGTCTCAGTTCGGATTGGGGTCTGNCACTCGACCCTATGAAGTCGGAGTCGCTAGTAATCGCAGATCAGCAGTGCTGCGGTGAATACGTTCCCGGGCCT 31.25\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('/content/drive/MyDrive/MIC/data_picrust/representative_sequences')"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "def create_rep_seqs_with_freq(sequence_file, pre_biom_df, output_fasta):\n",
        "    \"\"\"\n",
        "    Create representative sequences with frequencies written to output\n",
        "\n",
        "    Args:\n",
        "        sequence_file: Path to FASTA file with OTU sequences\n",
        "        pre_biom_df: DataFrame with abundance data\n",
        "        output_fasta: Path to save sequences with frequencies\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Calculate total frequency for each OTU\n",
        "        total_frequencies = round(pre_biom_df.sum(axis=1), 2)\n",
        "\n",
        "        with open(output_fasta, 'w') as out:\n",
        "            for record in SeqIO.parse(sequence_file, \"fasta\"):\n",
        "                otu_id = record.id\n",
        "\n",
        "                if otu_id in total_frequencies.index:\n",
        "                    freq = total_frequencies[otu_id]\n",
        "                    sequence = str(record.seq)\n",
        "\n",
        "                    # Write sequence with frequency to FASTA\n",
        "                    out.write(f\">{otu_id} {sequence} {freq}\\n\")\n",
        "\n",
        "        # First lines of the file\n",
        "        print(\"Representative Sequences head:\")\n",
        "        with open(output_fasta, 'r') as f:\n",
        "            for i, line in enumerate(f):\n",
        "                if i < 1:  # Show first 3 sequences (header + sequence lines)\n",
        "                    print(line.strip())\n",
        "        return output_fasta\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred: {e}\")\n",
        "    return None\n",
        "\n",
        "\n",
        "# Representative sequences\n",
        "sequences_for_picrust = output_dir / \"sequences_for_picrust.fasta\"\n",
        "\n",
        "output_fasta = output_dir/ \"representative_sequences\"\n",
        "\n",
        "repres_sequ = create_rep_seqs_with_freq(sequences_for_picrust, pre_biom, output_fasta)\n",
        "repres_sequ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ku8lUve9uvgK"
      },
      "source": [
        "__Disclamer:__ These notebook was mean to do the analysis of the functional mechanisms of bacteria using picrust2, however the capacity of the laptop was no sufficient to run it, nor colab on public library, nor a virtual machine, that is the reason why the analysis was undertaken in the galaxy website, where the data resides.\n",
        "https://usegalaxy.eu/  \n",
        "username= magicalex238"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zv-yUkfmCqSN"
      },
      "source": [
        "## 3.1. Classifying Bacteria by their Source DataFrame\n",
        "Two distinct classification approaches are implemented to categorize bacteria. The simple approach (get_bacteria_sources_simple) divides bacteria into known corrosion-causers (usual_taxa) and candidates (all others). The detailed approach (get_bacteria_sources_detailed) provides finer categorization by separating bacteria into known corrosion-causers, pure checked taxa, pure core taxa, and those present in both checked and core datasets. Please notice that this function uses df Integrated for source clasification and no abundance.biom which will be used for the picrust2 pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T09:42:06.731117Z",
          "iopub.status.busy": "2025-02-19T09:42:06.730712Z",
          "iopub.status.idle": "2025-02-19T09:42:06.745406Z",
          "shell.execute_reply": "2025-02-19T09:42:06.7441Z",
          "shell.execute_reply.started": "2025-02-19T09:42:06.73108Z"
        },
        "id": "5bDVrPwWCqSR",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def get_bacteria_sources_simple(Integrated_df):\n",
        "    \"\"\"\n",
        "    Simple classification:\n",
        "    1. Known (anything with 'us')\n",
        "    2. All others (combined chk, core, chk-core)\n",
        "    \"\"\"\n",
        "    # Get genera and gids from column levels 6 and 7\n",
        "    genera = Integrated_df[\"Genus\"]\n",
        "    gids = Integrated_df[\"GID\"]\n",
        "\n",
        "    # Look for Source in the data, not index\n",
        "    sources = Integrated_df['Source'] if 'Source' in Integrated_df.columns else None\n",
        "\n",
        "    known_bacteria = {}     # usual_taxa\n",
        "    other_bacteria = {}     # everything else\n",
        "\n",
        "    sources_found = set()\n",
        "    source ={}\n",
        "    patterns = ['us', 'core-us', 'chk-us', 'chk-core-us']\n",
        "\n",
        "    for i, (genus, gid) in enumerate (zip(genera, gids)):\n",
        "        if source is not None:  # Check if source exists for this genus\n",
        "            source = str(sources.iloc[i]).strip().lower()\n",
        "            sources_found.add(source)\n",
        "\n",
        "            if source in patterns:\n",
        "                known_bacteria[genus] = int(gid) if str(gid).isdigit() else gid\n",
        "            else:\n",
        "                other_bacteria[genus] = int(gid) if str(gid).isdigit() else gid\n",
        "\n",
        "    print(\"\\nSimple Classification Results:\")\n",
        "    print(f\"Known corrosion bacteria: {len(known_bacteria)}\")\n",
        "    print(f\"Other bacteria: {len(other_bacteria)}\")\n",
        "    print(\"\\nSources found:\", sources_found)\n",
        "\n",
        "    return {\n",
        "        'known_bacteria': known_bacteria,\n",
        "        'other_bacteria': other_bacteria\n",
        "    }\n",
        "\n",
        "def get_bacteria_sources_detailed(Integrated_df):\n",
        "    \"\"\"\n",
        "    Detailed classification with all possible combinations:\n",
        "    1. Known (usual_taxa)\n",
        "    2. Pure checked (only 'chk')\n",
        "    3. Pure core (only 'core')\n",
        "    4. Checked-core (overlap 'chk-core')\n",
        "    \"\"\"\n",
        "\n",
        "    genera = Integrated_df[\"Genus\"]\n",
        "    gids = Integrated_df[\"GID\"]\n",
        "\n",
        "    sources = Integrated_df['Source'] if 'Source' in Integrated_df.columns else None\n",
        "\n",
        "    known_bacteria = {}      # usual_taxa\n",
        "    pure_checked = {}        # only 'chk' checked_taxa\n",
        "    pure_core = {}          # only 'core' core_taxa\n",
        "    checked_core = {}       # 'chk-core' checked and core taxa\n",
        "    source ={}\n",
        "    sources_found = set()\n",
        "    patterns = ['us', 'core-us', 'chk-us', 'chk-core-us']\n",
        "\n",
        "    for i, (genus, gid) in enumerate (zip(genera, gids)):\n",
        "        if source is not None:  # Check if source exists for this genus\n",
        "            source = str(sources.iloc[i]).strip().lower()\n",
        "            sources_found.add(source)\n",
        "\n",
        "            if source in patterns:\n",
        "                known_bacteria[genus] = int(gid) if str(gid).isdigit() else gid\n",
        "                continue\n",
        "\n",
        "            # Then handle other combinations\n",
        "            if source == 'chk':\n",
        "                pure_checked[genus] = gid\n",
        "            elif source == 'core':\n",
        "                pure_core[genus] = gid\n",
        "            elif 'chk-core' in source:\n",
        "                checked_core[genus] = gid\n",
        "\n",
        "    print(\"\\nDetailed Classification Results:\")\n",
        "    print(f\"Known corrosion bacteria: {len(known_bacteria)}\")\n",
        "    print(f\"Pure checked bacteria: {len(pure_checked)}\")\n",
        "    print(f\"Pure core bacteria: {len(pure_core)}\")\n",
        "    print(f\"Checked-core bacteria: {len(checked_core)}\")\n",
        "    print(\"\\nSources found:\", sources_found)\n",
        "\n",
        "    return {\n",
        "        'known_bacteria': known_bacteria,\n",
        "        'pure_checked': pure_checked,\n",
        "        'pure_core': pure_core,\n",
        "        'checked_core': checked_core\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T09:42:06.748865Z",
          "iopub.status.busy": "2025-02-19T09:42:06.748512Z",
          "iopub.status.idle": "2025-02-19T09:42:06.766435Z",
          "shell.execute_reply": "2025-02-19T09:42:06.765138Z",
          "shell.execute_reply.started": "2025-02-19T09:42:06.748825Z"
        },
        "id": "LSxCpNd8Yelz",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6f5891c-b6dd-42fe-cfc1-badf0eb5ba9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Simple Classification Results:\n",
            "Known corrosion bacteria: 17\n",
            "Other bacteria: 68\n",
            "\n",
            "Sources found: {'core-us', 'chk', 'core', 'us', 'chk-core', 'chk-us', 'chk-core-us'}\n",
            "\n",
            "Detailed Classification Results:\n",
            "Known corrosion bacteria: 17\n",
            "Pure checked bacteria: 19\n",
            "Pure core bacteria: 46\n",
            "Checked-core bacteria: 3\n",
            "\n",
            "Sources found: {'core-us', 'chk', 'core', 'us', 'chk-core', 'chk-us', 'chk-core-us'}\n"
          ]
        }
      ],
      "source": [
        "sources_simple = get_bacteria_sources_simple(Integrated)\n",
        "\n",
        "sources_detail = get_bacteria_sources_detailed(Integrated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T09:42:06.767806Z",
          "iopub.status.busy": "2025-02-19T09:42:06.767438Z",
          "iopub.status.idle": "2025-02-19T09:42:06.785756Z",
          "shell.execute_reply": "2025-02-19T09:42:06.784698Z",
          "shell.execute_reply.started": "2025-02-19T09:42:06.767762Z"
        },
        "id": "QtD3K9OIYelz",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Extracting the genus lists for each group:\n",
        "known_bacteria_list = list(sources_detail['known_bacteria'].keys())\n",
        "pure_checked_list = list(sources_detail['pure_checked'].keys())\n",
        "pure_core_list = list(sources_detail['pure_core'].keys())\n",
        "checked_core_list = list(sources_detail['checked_core'].keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCA_PPxRYel0"
      },
      "source": [
        "The lists will be utilised later in order to groupby this list int he analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23P2k1QwCqSR"
      },
      "source": [
        "## 3.2. Prepare picrust data and Creating Directories for PICRUSt2 Input\n",
        "The check_missing_genera function processes the integrated data and handles data quality control. Known problematic genera (e.g., 'Clostridium_sensu_stricto_12', 'Oxalobacteraceae_unclassified') are flagged for exclusion to prevent analysis errors. The function also creates an organized directory structure as outlined in the introduction, with separate paths for different bacterial classifications (known_mic, candidate_mic, etc.) and their respective analysis outputs (EC_predictions, pathway_predictions, KO_predictions). Following function prepares the data for picrust analysis but both dataframes the abundance.biom and Integrated have some bacteria that were no sequenciated mostly cause are no known specimens. So it is necesary to do same procedure to both dfs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T09:42:06.787249Z",
          "iopub.status.busy": "2025-02-19T09:42:06.78684Z",
          "iopub.status.idle": "2025-02-19T09:42:06.800994Z",
          "shell.execute_reply": "2025-02-19T09:42:06.799794Z",
          "shell.execute_reply.started": "2025-02-19T09:42:06.787219Z"
        },
        "id": "bNfnbXfKCqSS",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c11a4aa4-487f-420f-8179-6f6e834cc8cb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'  except Exception as e:\\n    logging.error(f\"Error creating directory structure: {str(e)}\")\\n    return False'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "def prepare_picrust_data(Integrated_df, aligned_file, function_type='simple'):\n",
        "    \"\"\"\n",
        "    Prepare data for PICRUSt analysis with choice of  function_type method\n",
        "\n",
        "    Args:\n",
        "        Integrated_df: Input DataFrame\n",
        "        aligned_file: Path to aligned sequences\n",
        "        function_type: 'simple' or 'detailed'\n",
        "    \"\"\"\n",
        "    # Get bacteria source_groups based on chosen  function_type\n",
        "    if  function_type == 'simple':\n",
        "        source_groups = get_bacteria_sources_simple(Integrated_df)\n",
        "    else:\n",
        "        source_groups= get_bacteria_sources_detailed(Integrated_df)\n",
        "\n",
        "    # Create appropriate directory structure\n",
        "    create_directory_structure(function_type)\n",
        "\n",
        "    return source_groups\n",
        "\n",
        "def create_directory_structure(function_type='simple'):\n",
        "    \"\"\"Create directory structure for PICRUSt analysis\"\"\"\n",
        "    base_dir = Path(\"/home/beatriz/MIC/2_Micro/data_picrust\")\n",
        "    base_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    if function_type == 'simple':\n",
        "        directories = SIMPLE_BASE\n",
        "    else:\n",
        "        directories = DETAILED_BASE\n",
        "\n",
        "    # Create all required directories\n",
        "    for dir_name in directories.values():\n",
        "        for subdir in SUBDIRS:\n",
        "            (base_dir / dir_name / subdir).mkdir(parents=True, exist_ok=True)\n",
        "    logging.info(\"Directory structure created successfully\")\n",
        "\n",
        "    return True\n",
        "\n",
        "'''  except Exception as e:\n",
        "    logging.error(f\"Error creating directory structure: {str(e)}\")\n",
        "    return False'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T09:42:06.802491Z",
          "iopub.status.busy": "2025-02-19T09:42:06.802091Z",
          "iopub.status.idle": "2025-02-19T09:42:06.825255Z",
          "shell.execute_reply": "2025-02-19T09:42:06.823564Z",
          "shell.execute_reply.started": "2025-02-19T09:42:06.802433Z"
        },
        "id": "q4v7JVU8uvgK",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def verify_input_files():\n",
        "    \"\"\"Verify that input files exist and are readable\"\"\"\n",
        "    missing_files = []\n",
        "\n",
        "    if not fasta_file.exists():\n",
        "        missing_files.append(str(fasta_file))\n",
        "    if not biom_table.exists():\n",
        "        missing_files.append(str(biom_table))\n",
        "\n",
        "    if missing_files:\n",
        "        logging.error(f\"Missing input files: {', '.join(missing_files)}\")\n",
        "        return False\n",
        "\n",
        "    logging.info(\"All input files found\")\n",
        "    return True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUwADMKZCqSS"
      },
      "source": [
        "# 4. PICRUSt Pipeline Definition\n",
        "The pipeline processes the aligned sequence data from notebook 5 that has or not undergo cleaning of the sequences as previously done on section 2. Also processes the biom_table in order to account on this anylsis on abundance. It queries the PICRUSt database to predict potential metabolic pathways for each genus. This prediction is based on evolutionary relationships and known genomic capabilities of related organisms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T09:42:06.826915Z",
          "iopub.status.busy": "2025-02-19T09:42:06.826589Z",
          "iopub.status.idle": "2025-02-19T09:42:06.844098Z",
          "shell.execute_reply": "2025-02-19T09:42:06.842947Z",
          "shell.execute_reply.started": "2025-02-19T09:42:06.826883Z"
        },
        "id": "srMpS5DkCqSS",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def run_picrust2_pipeline(fasta_file, biom_file, output_dir):\n",
        "    \"\"\"\n",
        "    Run the main PICRUSt2 pipeline on input sequences and BIOM table.\n",
        "\n",
        "    Args:\n",
        "        fasta_file: Path to the aligned sequences FASTA file.\n",
        "        biom_file: Path to the BIOM table (without extra columns).\n",
        "        output_dir: Directory for PICRUSt2 output.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Run main PICRUSt2 pipeline\n",
        "        cmd = [\n",
        "            'picrust2_pipeline.py',\n",
        "            '-s', fasta_file,        # Input FASTA file with aligned sequences\n",
        "            '-i', biom_file,         # BIOM table with abundance data\n",
        "            '-o', output_dir,        # Output directory\n",
        "            '--processes', '4',      # Parallel processes\n",
        "            '--verbose',\n",
        "            '--min_align', '0.25'    # Note the split here\n",
        "        ]\n",
        "        subprocess.run(cmd, check=True)\n",
        "\n",
        "        # Add pathway descriptions if the pathway file exists\n",
        "        pathway_file = os.path.join(output_dir, 'pathways_out/path_abun_unstrat.tsv.gz')\n",
        "        if os.path.exists(pathway_file):\n",
        "            cmd_desc = [\n",
        "                'add_descriptions.py',\n",
        "                '-i', pathway_file,\n",
        "                '-m', 'PATHWAY',\n",
        "                '-o', os.path.join(output_dir, 'pathways_with_descriptions.tsv')\n",
        "            ]\n",
        "            subprocess.run(cmd_desc, check=True)\n",
        "\n",
        "        print(f\"PICRUSt2 pipeline completed successfully for {output_dir}\")\n",
        "        return True\n",
        "\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Error running PICRUSt2: {e}\")\n",
        "        return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rx_DyzHbCqSS"
      },
      "source": [
        "# 5. Analysis of Pathways\n",
        "The analysis focuses on metabolic pathways known to be involved in microbially influenced corrosion, including sulfur metabolism, organic acid production, iron metabolism, and biofilm formation. These pathways were selected based on documented mechanisms of known corrosion-inducing bacteria. Separate pipeline runs for simple and detailed classifications ensure proper pathway analysis for each bacterial group."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T09:42:06.845667Z",
          "iopub.status.busy": "2025-02-19T09:42:06.845294Z",
          "iopub.status.idle": "2025-02-19T09:42:06.867161Z",
          "shell.execute_reply": "2025-02-19T09:42:06.865768Z",
          "shell.execute_reply.started": "2025-02-19T09:42:06.845641Z"
        },
        "id": "8eP8MAidCqSS",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def analyze_functional_profiles(picrust_output_dir, bacteria_list):\n",
        "    \"\"\"\n",
        "    Analyze functional profiles with focus on corrosion-relevant pathways\n",
        "\n",
        "    Parameters:\n",
        "    picrust_output_dir: directory containing PICRUSt2 output\n",
        "    bacteria_list: list of bacteria names to analyze\n",
        "    \"\"\"\n",
        "    # Define corrosion-relevant pathways\n",
        "    relevant_pathways = [\n",
        "        'Sulfur metabolism',\n",
        "        'Iron metabolism',\n",
        "        'Energy metabolism',\n",
        "        'Biofilm formation',\n",
        "        'Metal transport',\n",
        "        'ochre formation',\n",
        "        'iron oxide deposits',\n",
        "        'iron precipitation',\n",
        "        'rust formation',\n",
        "        'organic acid production',\n",
        "        'acetate production',\n",
        "        'lactate metabolism',\n",
        "        'formate production',\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        # Read PICRUSt2 output\n",
        "        pathway_file = os.path.join(picrust_output_dir, 'pathways_with_descriptions.tsv')\n",
        "        pathways_df = pd.read_csv(pathway_file, sep='\\t')\n",
        "\n",
        "        # Filter for relevant pathways\n",
        "        filtered_pathways = pathways_df[\n",
        "            pathways_df['description'].str.contains('|'.join(relevant_pathways),\n",
        "                                                  case=False,\n",
        "                                                  na=False)]\n",
        "\n",
        "        # Calculate pathway abundances per bacteria\n",
        "        pathway_abundances = filtered_pathways.groupby('description').sum()\n",
        "\n",
        "        # Calculate pathway similarities between bacteria\n",
        "        pathway_similarities = {}\n",
        "        for bacteria in bacteria_list:\n",
        "            if bacteria in pathways_df.columns:\n",
        "                similarities = pathways_df[bacteria].corr(pathways_df[list(bacteria_list)])\n",
        "                pathway_similarities[bacteria] = similarities\n",
        "\n",
        "        # Predict functional potential\n",
        "        functional_predictions = {}\n",
        "        for pathway in relevant_pathways:\n",
        "            pathway_presence = filtered_pathways[\n",
        "                filtered_pathways['description'].str.contains(pathway, case=False)\n",
        "            ]\n",
        "            if not pathway_presence.empty:\n",
        "                functional_predictions[pathway] = {\n",
        "                    'presence': len(pathway_presence),\n",
        "                    'mean_abundance': pathway_presence.mean().mean(),\n",
        "                    'max_abundance': pathway_presence.max().max()\n",
        "                }\n",
        "\n",
        "        # Calculate correlation scores\n",
        "        correlation_scores = {}\n",
        "        for bacteria in bacteria_list:\n",
        "            if bacteria in pathways_df.columns:\n",
        "                correlations = pathways_df[bacteria].corr(\n",
        "                    pathways_df[filtered_pathways.index]\n",
        "                )\n",
        "                correlation_scores[bacteria] = {\n",
        "                    'mean_correlation': correlations.mean(),\n",
        "                    'max_correlation': correlations.max(),\n",
        "                    'key_pathways': correlations.nlargest(5).index.tolist()\n",
        "                }\n",
        "\n",
        "        comparison_results = {\n",
        "            'pathway_similarities': pathway_similarities,\n",
        "            'functional_predictions': functional_predictions,\n",
        "            'correlation_scores': correlation_scores,\n",
        "            'pathway_abundances': pathway_abundances.to_dict()\n",
        "        }\n",
        "\n",
        "        return filtered_pathways, comparison_results\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in pathway analysis: {str(e)}\")\n",
        "        return None, None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-EEla9jCqSS"
      },
      "source": [
        "## 5.2. Testing the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T09:42:06.868761Z",
          "iopub.status.busy": "2025-02-19T09:42:06.868347Z",
          "iopub.status.idle": "2025-02-19T09:42:06.891494Z",
          "shell.execute_reply": "2025-02-19T09:42:06.890301Z",
          "shell.execute_reply.started": "2025-02-19T09:42:06.868719Z"
        },
        "id": "mnwckS6sCqST",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "41bba70d-f656-4c07-cab6-16a0b09e1eef"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"# ---- RUNNING THE PIPELINE ----\\n\\n# Set paths\\nfasta_file = Path('/home/beatriz/MIC/2_Micro/data_tree/accession_sequences.fasta')\\nabundance_biom_file =  Path('/home/beatriz/MIC/2_Micro/data_picrust/abundance_accession.biom')\\noutput_dir = 'picrust_output'\\n\\n# List of bacteria to analyze\\nbacteria_of_interest = ['Azospira', 'Brachybacterium', 'Bulleidia']\\n\\n# Run PICRUSt2\\nif run_picrust2_pipeline(aligned_fasta_file,\\n                         abundance_biom_file,\\n                         output_dir\\n                        ):\\n    # Analyze functional profiles if the pipeline completes successfully\\n    filtered_pathways, abundances = analyze_functional_profiles(output_dir, bacteria_of_interest)\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "'''# ---- RUNNING THE PIPELINE ----\n",
        "\n",
        "# Set paths\n",
        "fasta_file = Path('/home/beatriz/MIC/2_Micro/data_tree/accession_sequences.fasta')\n",
        "abundance_biom_file =  Path('/home/beatriz/MIC/2_Micro/data_picrust/abundance_accession.biom')\n",
        "output_dir = 'picrust_output'\n",
        "\n",
        "# List of bacteria to analyze\n",
        "bacteria_of_interest = ['Azospira', 'Brachybacterium', 'Bulleidia']\n",
        "\n",
        "# Run PICRUSt2\n",
        "if run_picrust2_pipeline(aligned_fasta_file,\n",
        "                         abundance_biom_file,\n",
        "                         output_dir\n",
        "                        ):\n",
        "    # Analyze functional profiles if the pipeline completes successfully\n",
        "    filtered_pathways, abundances = analyze_functional_profiles(output_dir, bacteria_of_interest)'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03KIf3UaCqST"
      },
      "source": [
        "# 6. Functional Analysis\n",
        "## 6.1 Running picrust full pipeline 1\n",
        "The analysis workflow begins by categorizing bacteria into source groups using the classification functions. These categorized data are then processed through the PICRUSt pipeline to predict metabolic capabilities. The functional analysis examines pathway presence, abundance, and correlations between different bacterial groups to identify potential corrosion-related metabolic patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T09:42:06.893107Z",
          "iopub.status.busy": "2025-02-19T09:42:06.892812Z",
          "iopub.status.idle": "2025-02-19T09:42:06.910241Z",
          "shell.execute_reply": "2025-02-19T09:42:06.909114Z",
          "shell.execute_reply.started": "2025-02-19T09:42:06.893083Z"
        },
        "id": "uHpbek-BCqST",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def run_functional_analysis(df, Integrated_df, aligned_file, analysis_type='simple'):\n",
        "    \"\"\"\n",
        "    Run complete functional analysis pipeline for either simple or detailed classification\n",
        "\n",
        "    Parameters:\n",
        "    df: Input DataFrame\n",
        "    aligned_file: Path to aligned sequences file\n",
        "    analysis_type: 'simple' or 'detailed'\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"Starting {analysis_type} classification analysis\")\n",
        "        print(f\"{'='*50}\")\n",
        "\n",
        "        # Prepare data and get source groups\n",
        "        print(\"\\nStep 1: Preparing data...\")\n",
        "\n",
        "        source_groups = prepare_picrust_data(Integrated_df, aligned_file, function_type=analysis_type)\n",
        "\n",
        "        if not source_groups:\n",
        "            raise ValueError(\"Failed to prepare data: No source groups returned\")\n",
        "\n",
        "        # Base directory for PICRUSt output\n",
        "        base_dir = Path(\"~MIC/2_Micro/data_picrust\")\n",
        "\n",
        "        results = {}\n",
        "\n",
        "        if analysis_type == 'simple':\n",
        "            # Run analysis for simple classification\n",
        "            # Known bacteria\n",
        "            known_output_dir = base_dir /SIMPLE_BASE['known']\n",
        "            success_known = run_picrust2_pipeline(aligned_file, df, str(known_output_dir))\n",
        "            if success_known:\n",
        "                results_known = analyze_functional_profiles(str(known_output_dir),\n",
        "                                                        source_groups['known_bacteria'].keys())\n",
        "\n",
        "            # Other bacteria\n",
        "            other_output_dir = base_dir / SIMPLE_BASE['other']\n",
        "            success_other = run_picrust2_pipeline(aligned_file, str(other_output_dir))\n",
        "            if success_other:\n",
        "                results_other = analyze_functional_profiles(str(other_output_dir),\n",
        "                                                        source_groups['other_bacteria'].keys())\n",
        "\n",
        "        else:\n",
        "            # Run analysis for detailed classification\n",
        "            for group, dir_name in DETAILED_BASE.items():\n",
        "\n",
        "                # Known bacteria\n",
        "                known_output_dir = base_dir / DETAILED_BASE['known']\n",
        "                success_known = run_picrust2_pipeline(aligned_file, str(known_output_dir))\n",
        "                if success_known:\n",
        "                    results_known = analyze_functional_profiles(str(known_output_dir),\n",
        "                                                            source_groups['known_bacteria'].keys())\n",
        "\n",
        "                # Pure checked bacteria\n",
        "                checked_output_dir = base_dir /  DETAILED_BASE['pure_checked']\n",
        "                success_checked = run_picrust2_pipeline(aligned_file, str(checked_output_dir))\n",
        "                if success_checked:\n",
        "                    results_checked = analyze_functional_profiles(str(checked_output_dir),\n",
        "                                                            source_groups['pure_checked'].keys())\n",
        "\n",
        "                # Pure core bacteria\n",
        "                core_output_dir = base_dir /DETAILED_BASE['pure_core']\n",
        "                success_core = run_picrust2_pipeline(aligned_file, str(core_output_dir))\n",
        "                if success_core:\n",
        "                    results_core = analyze_functional_profiles(str(core_output_dir),\n",
        "                                                            source_groups['pure_core'].keys())\n",
        "\n",
        "                # Checked-core bacteria\n",
        "                checked_core_output_dir = base_dir /DETAILED_BASE['checked_core']\n",
        "                success_checked_core = run_picrust2_pipeline(aligned_file, str(checked_core_output_dir))\n",
        "                if success_checked_core:\n",
        "                    results_checked_core = analyze_functional_profiles(str(checked_core_output_dir),\n",
        "                                                                    source_groups['checked_core'].keys())\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Error running PICRUSt2: {e}\")\n",
        "\n",
        "        return \"Analysis completed successfully\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T09:42:06.911832Z",
          "iopub.status.busy": "2025-02-19T09:42:06.911563Z",
          "iopub.status.idle": "2025-02-19T09:42:06.934059Z",
          "shell.execute_reply": "2025-02-19T09:42:06.932768Z",
          "shell.execute_reply.started": "2025-02-19T09:42:06.91181Z"
        },
        "id": "3NyEekbBCqST",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "8d6be26c-26a3-418e-9c22-2e97b0ead870"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"# Run the analysis for both types\\n# Simple source classification\\nsimple_results = run_functional_analysis(biom_table, aligned_file, analysis_type='simple') # output_biom\\n\\n# Detailed source classification\\ndetailed_results = run_functional_analysis(biom_table, aligned_file, analysis_type='detailed')\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "'''# Run the analysis for both types\n",
        "# Simple source classification\n",
        "simple_results = run_functional_analysis(biom_table, aligned_file, analysis_type='simple') # output_biom\n",
        "\n",
        "# Detailed source classification\n",
        "detailed_results = run_functional_analysis(biom_table, aligned_file, analysis_type='detailed')'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4rP1kdUCqST"
      },
      "source": [
        "## 6.2 Running picrust full pipeline 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T09:42:06.935833Z",
          "iopub.status.busy": "2025-02-19T09:42:06.935418Z",
          "iopub.status.idle": "2025-02-19T09:42:06.951967Z",
          "shell.execute_reply": "2025-02-19T09:42:06.950733Z",
          "shell.execute_reply.started": "2025-02-19T09:42:06.935796Z"
        },
        "id": "5A2a1CNLCqSW",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def run_picrust2_pipeline(fasta_file, output_dir, min_align =0.5):\n",
        "    \"\"\"\n",
        "    Run PICRUSt2 pipeline with improved error handling and path management\n",
        "\n",
        "    Args:\n",
        "        fasta_file: Path to aligned sequences fasta file (str or Path)\n",
        "        output_dir: Directory for PICRUSt2 output (str or Path)\n",
        "    \"\"\"\n",
        "    import subprocess\n",
        "    import os\n",
        "    from pathlib import Path\n",
        "\n",
        "    # Convert paths to strings\n",
        "    fasta_file = str(fasta_file)\n",
        "    output_dir = str(output_dir)\n",
        "\n",
        "    try:\n",
        "        # Verify picrust2 is available\n",
        "        picrust_check = subprocess.run(['which', 'picrust2_pipeline.py'],\n",
        "                                     capture_output=True,\n",
        "                                     text=True)\n",
        "        if picrust_check.returncode != 0:\n",
        "            raise RuntimeError(\"picrust2_pipeline.py not found. Please ensure PICRUSt2 is properly installed.\")\n",
        "\n",
        "        # Create output directory\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # Construct command as a single string\n",
        "        cmd = f\"picrust2_pipeline.py -s {fasta_file} -i {fasta_file} -o {output_dir} --processes 1 --verbose\"\n",
        "\n",
        "        # Run pipeline\n",
        "        print(f\"Running command: {cmd}\")\n",
        "        process = subprocess.run(cmd,\n",
        "                               shell=True,  # Use shell to handle command string\n",
        "                               check=True,\n",
        "                               capture_output=True,\n",
        "                               text=True)\n",
        "\n",
        "        print(\"PICRUSt2 Output:\")\n",
        "        print(process.stdout)\n",
        "\n",
        "        if process.stderr:\n",
        "            print(\"Warnings/Errors:\")\n",
        "            print(process.stderr)\n",
        "\n",
        "        # Add descriptions if pathway file exists\n",
        "        pathway_file = os.path.join(output_dir, 'pathways_out/path_abun_unstrat.tsv.gz')\n",
        "        if os.path.exists(pathway_file):\n",
        "            desc_cmd = f\"add_descriptions.py -i {pathway_file} -m PATHWAY -o {os.path.join(output_dir, 'pathways_with_descriptions.tsv')}\"\n",
        "            subprocess.run(desc_cmd, shell=True, check=True)\n",
        "\n",
        "        print(f\"PICRUSt2 pipeline completed successfully for {output_dir}\")\n",
        "        return True\n",
        "\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Error running PICRUSt2 command: {e}\")\n",
        "        print(f\"Command output: {e.output}\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"Error in pipeline: {str(e)}\")\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T09:42:06.953585Z",
          "iopub.status.busy": "2025-02-19T09:42:06.953173Z",
          "iopub.status.idle": "2025-02-19T09:42:06.976246Z",
          "shell.execute_reply": "2025-02-19T09:42:06.975194Z",
          "shell.execute_reply.started": "2025-02-19T09:42:06.953546Z"
        },
        "id": "Hbhd5NNkCqSX",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "bccee364-fa5a-4334-fed3-6d11871fd2f5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# For original sequences\\naligned_file = aligned_fasta\\noutput_dir = Path(\"~MIC/2_Micro/data_picrust/original_results\")\\nsuccess = run_picrust2_pipeline(aligned_file, output_dir)\\n\\n# For improved sequences\\noptimized_file = Path(\"~/MIC/2_Micro/data_picrust/picrust_optimized_sequences.fasta\")\\noptimized_output = Path(\"~/MIC/2_Micro/data_picrust/optimized_results\")\\nsuccess_opt = run_picrust2_pipeline(optimized_file, optimized_output)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "'''# For original sequences\n",
        "aligned_file = aligned_fasta\n",
        "output_dir = Path(\"~MIC/2_Micro/data_picrust/original_results\")\n",
        "success = run_picrust2_pipeline(aligned_file, output_dir)\n",
        "\n",
        "# For improved sequences\n",
        "optimized_file = Path(\"~/MIC/2_Micro/data_picrust/picrust_optimized_sequences.fasta\")\n",
        "optimized_output = Path(\"~/MIC/2_Micro/data_picrust/optimized_results\")\n",
        "success_opt = run_picrust2_pipeline(optimized_file, optimized_output)'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EDOctt8uvgP"
      },
      "source": [
        "# 7. Findings and Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXu7ACOAuvgP"
      },
      "source": [
        "The PICRUSt2 pipeline generated a series of interconnected files revealing the functional potential of the microbial community. These files collectively map metabolic pathways, enzymatic functions, and taxonomic relationships, providing a multi-layered view of microbial functional capabilities across samples. Detailed view of the files found in the folder ~data_picrust are located in the manuscript."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Se5YK4UfuvgP"
      },
      "source": [
        "Picrust_Result_SEPP and Picrust_Result_EPA contain the descriptions, pathways and abundance of the full pipeline of picrust."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "execution": {
          "iopub.execute_input": "2025-02-19T09:42:06.977754Z",
          "iopub.status.busy": "2025-02-19T09:42:06.977416Z",
          "iopub.status.idle": "2025-02-19T09:42:07.065439Z",
          "shell.execute_reply": "2025-02-19T09:42:07.064304Z",
          "shell.execute_reply.started": "2025-02-19T09:42:06.977725Z"
        },
        "id": "BZ8njtbRuvgP",
        "outputId": "8420ea2d-bd12-4658-b96f-4591b4402386",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'MetaCyc_SEPP_path = Path(base_dir / \"Galaxy35_Add_descriptions_SEPP.tabular\")\\nPicrust_Result_SEPP= pd.read_csv(MetaCyc_SEPP_path, sep = \"\\t\")\\nPicrust_Result_SEPP.set_index(\"description\", inplace=True)\\nPicrust_Result_SEPP = Picrust_Result_SEPP.drop(\"pathway\", axis=1)\\nPicrust_Result_SEPP.index.name = \"pathway\"'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 91
        }
      ],
      "source": [
        "MetaCyc_EPA_path = base_dir / \"Galaxy19_PICRUSt2_Add_descriptions_on_data_8.tabular\"\n",
        "\n",
        "Picrust_Result= pd.read_csv(MetaCyc_EPA_path, sep = \"\\t\")\n",
        "Picrust_Result_EPA= pd.read_csv(MetaCyc_EPA_path, sep = \"\\t\")\n",
        "Picrust_Result_EPA.set_index(\"description\", inplace=True)\n",
        "Picrust_Result_EPA = Picrust_Result_EPA.drop(\"pathway\", axis=1)\n",
        "Picrust_Result_EPA.index.name = \"pathway\"\n",
        "'''MetaCyc_SEPP_path = Path(base_dir / \"Galaxy35_Add_descriptions_SEPP.tabular\")\n",
        "Picrust_Result_SEPP= pd.read_csv(MetaCyc_SEPP_path, sep = \"\\t\")\n",
        "Picrust_Result_SEPP.set_index(\"description\", inplace=True)\n",
        "Picrust_Result_SEPP = Picrust_Result_SEPP.drop(\"pathway\", axis=1)\n",
        "Picrust_Result_SEPP.index.name = \"pathway\"'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H39IXM7-uvgP"
      },
      "source": [
        "## 7.1. Placement Algorithm EPA vs SEPP\n",
        "nsti_SEPP and nsti_EPA Corresponds to a sample-wide measure of how closely related the microbial taxa in that sample are to known reference genomes with two different placement algoritms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T09:42:07.067061Z",
          "iopub.status.busy": "2025-02-19T09:42:07.066653Z",
          "iopub.status.idle": "2025-02-19T09:42:07.094101Z",
          "shell.execute_reply": "2025-02-19T09:42:07.092827Z",
          "shell.execute_reply.started": "2025-02-19T09:42:07.067031Z"
        },
        "id": "TW5Je0oouvgP",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "eb608b93-8008-46a1-968c-2cbbde4faccd"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/MIC/data_picrust/Galaxy13_EC_weighted_nsti.tabular'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-f41c44f66563>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnsti_path_EPA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m  \u001b[0;34m/\u001b[0m \u001b[0;34m\"Galaxy13_EC_weighted_nsti.tabular\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnsti_EPA\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnsti_path_EPA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnsti_path_SEPP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m  \u001b[0;34m/\u001b[0m \u001b[0;34m\"Galaxy20_EC_weighted_nsti_SEPP.tabular\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnsti_SEPP\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnsti_path_SEPP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/MIC/data_picrust/Galaxy13_EC_weighted_nsti.tabular'"
          ]
        }
      ],
      "source": [
        "nsti_path_EPA = Path(base_dir  / \"Galaxy13_EC_weighted_nsti.tabular\")\n",
        "nsti_EPA= pd.read_csv(nsti_path_EPA, sep = \"\\t\")\n",
        "nsti_path_SEPP = Path(base_dir  / \"Galaxy20_EC_weighted_nsti_SEPP.tabular\")\n",
        "nsti_SEPP= pd.read_csv(nsti_path_SEPP, sep = \"\\t\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T09:42:07.095532Z",
          "iopub.status.busy": "2025-02-19T09:42:07.095203Z",
          "iopub.status.idle": "2025-02-19T09:42:07.861889Z",
          "shell.execute_reply": "2025-02-19T09:42:07.860659Z",
          "shell.execute_reply.started": "2025-02-19T09:42:07.095491Z"
        },
        "id": "ESTXja6nuvgP",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Create the scatter plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(nsti_EPA['sample'], nsti_EPA['weighted_NSTI'], alpha=0.5, label= \"EPA\", color=\"blue\")\n",
        "plt.scatter(nsti_SEPP['sample'], nsti_SEPP['weighted_NSTI'], alpha=0.5, label= \"SEPP\", color=\"gray\")\n",
        "\n",
        "# Add the threshold line\n",
        "plt.axhline(y=0.15, color='black', linestyle='--', label='Threshold (0.15)')\n",
        "\n",
        "# Customize the plot\n",
        "plt.title('NSTI Values by Site')\n",
        "plt.xlabel('Site')\n",
        "plt.ylabel('NSTI Value')\n",
        "plt.legend()\n",
        "\n",
        "# Rotate x-axis labels if there are many samples\n",
        "plt.xticks(rotation=90)\n",
        "\n",
        "# Adjust layout and display the plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_fb468duvgP"
      },
      "source": [
        "Interestingly, the results are no as expected, it was though that the algorithm for placing the sequences more convenient for the present samples was SEPP because it is design specially for 16sRNA samples and diverse microbios communities, however the samples show another story. I fail to realise that the present data has been validated with the greenes genes database with the purpose of finding more compatibility with the picrust2 database, and therefore the EPA algoritm is performing much better on the all of samples using EPA placement algoritm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUPl-r2huvgQ"
      },
      "source": [
        "## 7.2. Explore Pathway Patterns\n",
        "The pathway analysis strategy is to do a preliminar exploration before diving into specific hypotheses about organic matter metabolism and corrosion. It was chosen to start with unbiased exploratory data analysis of the PICRUSt pathways. The aim is to let the data reveal natural patterns without preconceptions. That helps to identify unexpected relationships between pathways, providing a baseline understanding of pathway distributions and relationships. This will guide subsequent targeted analyses of corrosion-relevant pathways.\n",
        "The following script takes multiple perspectives in order to visualise the data without bias and let it reveal itself. We do PCA for linear patterns, NMF for modular organization, UMAP for non-linear relationships and take different clustering approaches. The aim being to look for natural Patterns without predefined categories, so that strong strong correlations can be identified regardless of pathway type. It is visualised the distribution of pathway abundances, correlation structure, hierarchical relationships and non-linear patterns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfLGAWTXuvgQ"
      },
      "source": [
        "__Category Dict__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T09:42:07.863417Z",
          "iopub.status.busy": "2025-02-19T09:42:07.863112Z",
          "iopub.status.idle": "2025-02-19T09:42:07.870061Z",
          "shell.execute_reply": "2025-02-19T09:42:07.868875Z",
          "shell.execute_reply.started": "2025-02-19T09:42:07.863391Z"
        },
        "id": "bSQwysg7uvgQ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Define category dict outside so that all charts can use same dict\n",
        "\n",
        "category_dict = Integrated_T.T.iloc[0, 0:-1].to_dict()\n",
        "\n",
        "# Define colors and categories\n",
        "category_colors = {1: '#008800',  # Dark green\n",
        "                   2: '#FF8C00',  # Dark orange\n",
        "                   3: '#FF0000'}   # Red\n",
        "\n",
        "categories_labels = {1: 'Normal Operation',\n",
        "              2: 'Early Warning',\n",
        "              3: 'System Failure'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T09:42:07.871445Z",
          "iopub.status.busy": "2025-02-19T09:42:07.871137Z",
          "iopub.status.idle": "2025-02-19T09:42:07.89862Z",
          "shell.execute_reply": "2025-02-19T09:42:07.897406Z",
          "shell.execute_reply.started": "2025-02-19T09:42:07.871405Z"
        },
        "id": "TvoGFdS5uvgQ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def explore_pathway_patterns(df):\n",
        "    \"\"\"\n",
        "    Explore pathway patterns using multiple analytical approaches\n",
        "    \"\"\"\n",
        "    # Standardize data\n",
        "    scaler = StandardScaler()\n",
        "    scaled_data = scaler.fit_transform(df)\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    def plot_exploration_results(df, results, category_dict, category_colors, categories_labels):\n",
        "        \"\"\"\n",
        "        Create visualizations for the exploratory analysis with consistent category colors\n",
        "        \"\"\"\n",
        "        # 1. Distribution of pathway abundances with category colors - side by side\n",
        "        plt.figure(figsize=(12, 6))\n",
        "\n",
        "        # Create dictionary to store abundances by category\n",
        "        category_abundances = {cat_id: [] for cat_id in categories_labels.keys()}\n",
        "\n",
        "        # Group abundances by category\n",
        "        for site_col in df.columns:\n",
        "            if site_col.startswith('site_'):\n",
        "                site_num = int(site_col.split('_')[1])\n",
        "                category = category_dict.get(f'site_{site_num}', 0)\n",
        "                if category in category_abundances:\n",
        "                    category_abundances[category].extend(df[site_col].values)\n",
        "\n",
        "        # Plot distribution for each category side by side\n",
        "        for category_id in categories_labels.keys():\n",
        "            sns.histplot(data=category_abundances[category_id],\n",
        "                        bins=50,\n",
        "                        color=category_colors[category_id],\n",
        "                        label=categories_labels[category_id],\n",
        "                        alpha=0.6,\n",
        "                        multiple=\"layer\")\n",
        "\n",
        "        plt.title('Distribution of Pathway Abundances by Category')\n",
        "        plt.xlabel('Abundance')\n",
        "        plt.yscale('log')\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    # 2. PCA Dimensionality Reduction\n",
        "    pca = PCA(n_components=5)\n",
        "    X_pca = pca.fit_transform(scaled_data)\n",
        "    results['pca'] = {\n",
        "        'components': X_pca,\n",
        "        'explained_variance': pca.explained_variance_ratio_,\n",
        "        'loadings': pd.DataFrame(\n",
        "            pca.components_.T,\n",
        "            index=df.columns,\n",
        "            columns=[f'PC{i+1}' for i in range(5)])}\n",
        "\n",
        "    # 3 NMF for pathway modules\n",
        "    nmf = NMF(n_components=5, init='random', random_state=0, max_iter=400)\n",
        "    W = nmf.fit_transform(df.clip(lower=0))\n",
        "    H = nmf.components_\n",
        "    results['nmf'] = {\n",
        "        'W': pd.DataFrame(W, index=df.index, columns=[f'NMF{i+1}' for i in range(5)]), # Pathway contributions\n",
        "        'H': pd.DataFrame(H, columns=df.columns, index=[f'NMF{i+1}' for i in range(5)]), # Sample patterns\n",
        "        'reconstruction_err': nmf.reconstruction_err_ }\n",
        "\n",
        "    # 4 UMAP for non-linear patterns\n",
        "    umap_reducer = umap.UMAP(random_state=0)\n",
        "    umap_result = umap_reducer.fit_transform(scaled_data)\n",
        "    results['umap'] = pd.DataFrame(umap_result, index=df.index, columns=['UMAP1', 'UMAP2'])\n",
        "\n",
        "    # 5. Multiple Clustering Approaches / Hierarchical clustering\n",
        "    linkage_matrix = hierarchy.linkage(scaled_data, method='ward')\n",
        "\n",
        "    # Try different numbers of clusters\n",
        "    cluster_results = {}\n",
        "    for n_clusters in [5, 10, 15]:\n",
        "        # Hierarchical\n",
        "        hc = AgglomerativeClustering(n_clusters=n_clusters)\n",
        "        hc_labels = hc.fit_predict(scaled_data)\n",
        "\n",
        "        # K-means\n",
        "        km = KMeans(n_clusters=n_clusters, random_state=0)\n",
        "        km_labels = km.fit_predict(scaled_data)\n",
        "\n",
        "        cluster_results[n_clusters] = {'hierarchical': pd.Series(hc_labels, index=df.index, name='cluster'),\n",
        "            'kmeans': pd.Series(km_labels, index=df.index, name='cluster')}\n",
        "\n",
        "    results['clustering'] = cluster_results\n",
        "    results['linkage'] = linkage_matrix\n",
        "\n",
        "    # 4. Correlation Analysis/Spearman correlation for non-linear relationships\n",
        "    corr_matrix = spearmanr(df.T)[0]\n",
        "    results['correlation'] = pd.DataFrame(corr_matrix, index=df.index, columns=df.index)\n",
        "\n",
        "    return results, X_pca\n",
        "\n",
        "def plot_exploration_results(df, results, category_dict, category_colors, categories_labels):\n",
        "    \"\"\"\n",
        "    Create visualizations for the exploratory analysis with colored categories_labels\n",
        "    \"\"\"\n",
        "    # Modified PCA visualization with categories\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # Create subplots\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(range(1, 6), results['pca']['explained_variance'], 'bo-')\n",
        "    plt.title('PCA Explained Variance')\n",
        "    plt.xlabel('Component')\n",
        "    plt.ylabel('Explained Variance Ratio')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "\n",
        "    # Get PCA components\n",
        "    pca_data = results['pca']['components']\n",
        "\n",
        "    # Plot each category separately to create the legend\n",
        "    for category_id in categories_labels.keys():\n",
        "        # Get indices for current category\n",
        "        category_mask = [category_dict.get(f'site_{i+1}', 0) == category_id\n",
        "                        for i in range(len(pca_data))]\n",
        "\n",
        "        # Plot points for current category\n",
        "        plt.scatter(pca_data[category_mask, 0],\n",
        "                   pca_data[category_mask, 1],\n",
        "                   c=category_colors[category_id],\n",
        "                   label=categories_labels[category_id],\n",
        "                   alpha=0.6)\n",
        "\n",
        "    plt.title('PCA First Two Components')\n",
        "    plt.xlabel('PC1')\n",
        "    plt.ylabel('PC2')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    # 3. UMAP visualization with categories\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    umap_df = results['umap']\n",
        "\n",
        "    for category_id in categories_labels.keys():\n",
        "        category_mask = [category_dict.get(f'site_{i+1}', 0) == category_id\n",
        "                        for i in range(len(umap_df))]\n",
        "        category_data = umap_df[category_mask]\n",
        "\n",
        "        plt.scatter(category_data['UMAP1'],\n",
        "                   category_data['UMAP2'],\n",
        "                   c=category_colors[category_id],\n",
        "                   label=categories_labels[category_id],\n",
        "                   alpha=0.6)\n",
        "\n",
        "    plt.title('UMAP Projection of Pathways by Category')\n",
        "    plt.xlabel('UMAP1')\n",
        "    plt.ylabel('UMAP2')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # 4. Hierarchical clustering dendrogram - simplified version\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    dendrogram = hierarchy.dendrogram(\n",
        "        results['linkage'],\n",
        "        labels=df.index,  # Use index , columns instead of index\n",
        "        leaf_rotation=90,\n",
        "        leaf_font_size=8\n",
        "    )\n",
        "    plt.title('Pathway Clustering Dendrogram')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # 5. Correlation heatmap\n",
        "    plt.figure(figsize=(12, 12))\n",
        "    mask = np.triu(np.ones_like(results['correlation']))\n",
        "\n",
        "    # Create a custom colormap that uses your category colors\n",
        "    custom_cmap = sns.diverging_palette(220, 20, as_cmap=True)\n",
        "\n",
        "    sns.heatmap(results['correlation'],\n",
        "                mask=mask,\n",
        "                cmap=custom_cmap,\n",
        "                center=0,\n",
        "                vmin=-1,\n",
        "                vmax=1)\n",
        "    plt.title('Pathway Correlation Heatmap')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def identify_key_patterns(df, results):\n",
        "    \"\"\"\n",
        "    Identify and summarize key patterns in the data\n",
        "    \"\"\"\n",
        "    patterns = {}\n",
        "\n",
        "    # Find highly correlated pathway groups\n",
        "    corr = results['correlation']\n",
        "    high_corr = pd.DataFrame(\n",
        "        [(i, j, corr.loc[i,j])\n",
        "         for i in corr.index\n",
        "         for j in corr.index\n",
        "         if i < j and abs(corr.loc[i,j]) > 0.8],\n",
        "        columns=['pathway1', 'pathway2', 'correlation']\n",
        "    ).sort_values('correlation', ascending=False)\n",
        "\n",
        "    # Find pathways with strong PCA loadings\n",
        "    loadings = results['pca']['loadings']\n",
        "    strong_loadings = pd.DataFrame({\n",
        "        'PC1_contribution': abs(loadings['PC1']),\n",
        "        'PC2_contribution': abs(loadings['PC2'])\n",
        "    }).sort_values('PC1_contribution', ascending=False)\n",
        "\n",
        "    patterns['high_correlations'] = high_corr\n",
        "    patterns['strong_loadings'] = strong_loadings\n",
        "\n",
        "    return patterns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T09:42:07.900108Z",
          "iopub.status.busy": "2025-02-19T09:42:07.899753Z",
          "iopub.status.idle": "2025-02-19T09:42:26.644907Z",
          "shell.execute_reply": "2025-02-19T09:42:26.643394Z",
          "shell.execute_reply.started": "2025-02-19T09:42:07.900067Z"
        },
        "id": "TrS9Ii_GuvgQ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Calling the function for the pipeline using EPA algoritm\n",
        "results_SEPP, X_pca_SEPP = explore_pathway_patterns(Picrust_Result_SEPP)\n",
        "plot_exploration_results(Picrust_Result_SEPP, results_SEPP, category_dict, category_colors, categories_labels)\n",
        "patterns_SEPP = identify_key_patterns(Picrust_Result_SEPP, results_SEPP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T09:42:26.646674Z",
          "iopub.status.busy": "2025-02-19T09:42:26.646265Z"
        },
        "id": "EDmJPqM7uvgQ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Calling the function for the pipeline using EPA algoritm\n",
        "results_EPA, X_pca_EPA = explore_pathway_patterns(Picrust_Result_EPA)\n",
        "plot_exploration_results(Picrust_Result_EPA, results_EPA, category_dict, category_colors, categories_labels)\n",
        "patterns_EPA = identify_key_patterns(Picrust_Result_EPA, results_EPA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R59lKZ60uvgQ"
      },
      "source": [
        "__Discussing first results__\n",
        "\n",
        "The distribution of pathway abundances shows a typical microbial community pattern with few dominant pathways, suggesting key metabolic processes are essential across samples. PCA analysis reveals that only two components explain over 80% of the variance, indicating that metabolism in these systems might be driven by two major functional groups. The UMAP visualization confirms this binary pattern through two distinct clusters, demonstrating the robustness of this separation across different dimensional reduction techniques. The hierarchical clustering dendrogram further validates this division by showing two major branches, which notably align with previously observed physicochemical patterns in our Pourbaix plot analysis. The correlation heatmap exhibits strong relationships between specific pathway groups, suggesting coordinated metabolic activities that require detailed pathway mapping for full biological interpretation. EPA sequence placement shows much better differenciation on the pc plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T09:50:52.239156Z",
          "iopub.status.busy": "2025-02-19T09:50:52.238781Z",
          "iopub.status.idle": "2025-02-19T09:50:52.264494Z",
          "shell.execute_reply": "2025-02-19T09:50:52.263075Z",
          "shell.execute_reply.started": "2025-02-19T09:50:52.23913Z"
        },
        "id": "C9qoJdFfYel2",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "Picrust_Result.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RI1NL6_uuvgQ"
      },
      "source": [
        "## 7.3. Distribution of pathway abundances and Heatmap Hierarchies\n",
        "In the following script we map the column pathway on the dataframe Picrust_Result_raw to the actual names provided by the Galaxy website that corresponds to the MetaCyc pathways. We will end up with the original Picrust_Results df with disernible names.After the 20 most abundant pathways will be plotted and the heatmap with the hierarchichal pathways drawn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T09:50:57.989611Z",
          "iopub.status.busy": "2025-02-19T09:50:57.98922Z",
          "iopub.status.idle": "2025-02-19T09:50:57.996428Z",
          "shell.execute_reply": "2025-02-19T09:50:57.995153Z",
          "shell.execute_reply.started": "2025-02-19T09:50:57.98958Z"
        },
        "id": "Zczh30NruvgQ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Define category dict outside so that all charts can use same dict\n",
        "\n",
        "category_dict = Integrated_T.T.iloc[0, 0:-1].to_dict()\n",
        "\n",
        "# Define colors and categories\n",
        "category_colors = {1: '#008800',  # Dark green\n",
        "                   2: '#FF8C00',  # Dark orange\n",
        "                   3: '#FF0000'}   # Red\n",
        "\n",
        "categories_labels = {1: 'Normal Operation',\n",
        "              2: 'Early Warning',\n",
        "              3: 'System Failure'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T09:51:10.614855Z",
          "iopub.status.busy": "2025-02-19T09:51:10.614412Z",
          "iopub.status.idle": "2025-02-19T09:51:12.397866Z",
          "shell.execute_reply": "2025-02-19T09:51:12.39672Z",
          "shell.execute_reply.started": "2025-02-19T09:51:10.614824Z"
        },
        "id": "praqb_f6uvgQ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def analyze_pathway_patterns(df, mean_abundances, category_dict, top_n=20):\n",
        "    \"\"\"\n",
        "    Create two separate visualizations for pathway analysis:\n",
        "    1. Stacked bar chart of top pathways by system state\n",
        "    2. Correlation heatmap of top pathways\n",
        "\n",
        "    Parameters:\n",
        "    df: DataFrame with pathway data\n",
        "    mean_abundances: Series with pre-calculated mean abundances\n",
        "    category_dict: Dictionary mapping sites to risk categories\n",
        "    top_n: Number of top pathways to display\n",
        "    \"\"\"\n",
        "    # Get top pathways\n",
        "    top_pathways = mean_abundances.nlargest(top_n)\n",
        "\n",
        "    # 1. Stacked Bar Chart\n",
        "    plt.figure(figsize=(15, 8))\n",
        "\n",
        "    # Prepare data for stacking\n",
        "    pathway_data = []\n",
        "    for pathway in top_pathways.index:\n",
        "        cat_means = {}\n",
        "        for cat in [1, 2, 3]:\n",
        "            cat_sites = [site for site, c in category_dict.items() if c == cat]\n",
        "            if cat_sites:\n",
        "                cat_means[cat] = df.loc[pathway, cat_sites].mean()\n",
        "            else:\n",
        "                cat_means[cat] = 0\n",
        "        pathway_data.append((pathway, cat_means))\n",
        "\n",
        "    # Create stacked bars\n",
        "    bottoms = np.zeros(len(top_pathways))\n",
        "    for cat in [1, 2, 3]:\n",
        "        values = [d[1][cat] for d in pathway_data]\n",
        "        plt.bar(range(len(top_pathways)), values, bottom=bottoms,\n",
        "                label=categories_labels[cat], color=category_colors[cat], alpha=0.7)\n",
        "        bottoms += values\n",
        "\n",
        "    plt.title('Top 20 Most Abundant Pathways by System State', fontsize=14, pad=20)\n",
        "    plt.xlabel('Pathway', fontsize=12)\n",
        "    plt.ylabel('Mean Abundance', fontsize=12)\n",
        "    plt.xticks(range(len(top_pathways)), top_pathways.index,\n",
        "               rotation=45, ha='right', fontsize=10)\n",
        "    plt.legend(title='System State', title_fontsize=12, fontsize=10)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # 2. Correlation Heatmap (separate figure)\n",
        "    plt.figure(figsize=(15, 12))\n",
        "    top_data = df.loc[top_pathways.index]\n",
        "    corr = top_data.T.corr()\n",
        "\n",
        "    # Create mask for upper triangle\n",
        "    mask = np.triu(np.ones_like(corr), k=1)\n",
        "\n",
        "    # Create heatmap with improved readability\n",
        "    sns.heatmap(corr,\n",
        "                mask=mask,\n",
        "                cmap='coolwarm',\n",
        "                center=0,\n",
        "                annot=True,\n",
        "                fmt='.2f',\n",
        "                square=True,\n",
        "                cbar_kws={'label': 'Correlation Coefficient'},\n",
        "                annot_kws={'size': 8})\n",
        "\n",
        "    plt.title('Pathway Correlation Heatmap\\n(Top 20 Most Abundant)',\n",
        "              fontsize=14,\n",
        "              pad=20)\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return corr, top_data\n",
        "\n",
        "#Calculate mean abundances and run analysis\n",
        "mean_abundances_epa = Picrust_Result_EPA.mean(axis=1)\n",
        "corr_epa, top_data = analyze_pathway_patterns(Picrust_Result_EPA, mean_abundances_epa, category_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meleGILNuvgR"
      },
      "source": [
        "__Discussing the 20 biggest metabolisms and their Hierarchical Heatmap__\n",
        "The metabolic pathway analysis reveals aerobic respiration as the dominant metabolism, showing approximately 75% higher abundance than other pathways across all systems. Correlation analysis highlights strong relationships between aerobic respiration and key metabolic processes, including TCA cycles and amino acid biosynthesis pathways, particularly those involved in biofilm formation. While these patterns provide insights into the overall metabolic landscape, a more detailed analysis separating corroded and non-corroded systems, along with integration of physicochemical variables and risk labels, would be necessary for actionable conclusions about corrosion processes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LmHp51DYel2"
      },
      "source": [
        "## 7.4. Distribution of Reactions abundances and Heatmap Hierarchies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T09:58:35.11704Z",
          "iopub.status.busy": "2025-02-19T09:58:35.116608Z",
          "iopub.status.idle": "2025-02-19T09:58:35.177891Z",
          "shell.execute_reply": "2025-02-19T09:58:35.176717Z",
          "shell.execute_reply.started": "2025-02-19T09:58:35.117011Z"
        },
        "id": "PPFMKGrgYel2",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#parsing pathways (PWY) to the reactions (RXN), parce has a single column with 575 rows, that will mean that the patways can be more than once with different reactions\n",
        "parce_path = Path(base_dir / \"Galaxy17_parsed_mapfile.tabular\")\n",
        "parce= pd.read_csv(parce_path, sep = \"\\t\")\n",
        "# reaction is a regroup file comprises the list of reactions in the index and the sites with abundances, similar to the pathways with abundances master file\n",
        "# whiles pathways has 366 rows (pathway), react has 2956 rows(reactions)\n",
        "react_path =Path(base_dir / \"Galaxy18_regrouped_infile.tabular\")\n",
        "react= pd.read_csv(react_path, sep = \"\\t\")\n",
        "react = react.set_index(\"function\")\n",
        "react.index = react.index.astype(str)\n",
        "# Sort columns numerically\n",
        "def sort_sites_numerically(df):\n",
        "    # Extract site numbers\n",
        "    site_numbers = [int(col.replace('site_', '')) for col in df.columns if col.startswith('site_')]\n",
        "\n",
        "    # Create sorted column list\n",
        "    sorted_cols = ['site_' + str(num) for num in sorted(site_numbers)]\n",
        "\n",
        "    # Return reordered dataframe\n",
        "    return df[sorted_cols]\n",
        "react = sort_sites_numerically(react)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDPL2I7v65sa"
      },
      "outputs": [],
      "source": [
        "# 1. Parse 'parce' (Corrected - Handle multi-line entries, hyphens, and missing matches)\n",
        "parce_mapping = {}\n",
        "current_entry = \"\"\n",
        "for line in parce.iloc[:, 0]:  # Iterate through the first column (handling multi-line entries)\n",
        "    if line.strip():  # Check if the line is not empty or just whitespace\n",
        "        current_entry += line.strip() + \" \"\n",
        "    else:  # Empty line indicates the end of an entry\n",
        "        if current_entry:  # Check if we have accumulated an entry\n",
        "            reactions = current_entry.split(\"-\")  # Split by hyphens\n",
        "            for reaction in reactions:\n",
        "                match = parce.index[parce.iloc[:, 0] == current_entry.strip()].tolist()\n",
        "                if match:  # Check if a match was found\n",
        "                    parce_mapping[reaction.strip()] = match[0]  # Get the first match\n",
        "            current_entry = \"\"\n",
        "if current_entry: #for the last entry\n",
        "    reactions = current_entry.split(\"-\")  # Split by hyphens\n",
        "    for reaction in reactions:\n",
        "        match = parce.index[parce.iloc[:, 0] == current_entry.strip()].tolist()\n",
        "        if match:  # Check if a match was found\n",
        "            parce_mapping[reaction.strip()] = match[0]  # Get the first match\n",
        "    current_entry = \"\"\n",
        "\n",
        "# 2. Function to selectively replace names (Corrected and Robust)\n",
        "def replace_name(name):\n",
        "    if name in parce_mapping:\n",
        "        return parce_mapping[name]\n",
        "\n",
        "    if isinstance(name, str):\n",
        "        for key, value in parce_mapping.items():\n",
        "            if isinstance(key, str) and key in name:\n",
        "                return value\n",
        "    return name\n",
        "\n",
        "# 3. Apply the function to the index of 'react'\n",
        "react.index = react.index.map(replace_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T09:59:04.293272Z",
          "iopub.status.busy": "2025-02-19T09:59:04.292801Z",
          "iopub.status.idle": "2025-02-19T09:59:04.382736Z",
          "shell.execute_reply": "2025-02-19T09:59:04.380879Z",
          "shell.execute_reply.started": "2025-02-19T09:59:04.293235Z"
        },
        "id": "A7kaHJFkYel2",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def analyze_pathway_patterns(df, mean_abundances, category_dict, top_n=20):\n",
        "    \"\"\"\n",
        "    Analyzes pathway patterns for a DataFrame with 'function' as index and 'Sites' as columns.\n",
        "    This is the FINAL, CORRECTED implementation.\n",
        "    \"\"\"\n",
        "    top_functions = mean_abundances.nlargest(top_n)\n",
        "    df.index = df.index.astype(str)\n",
        "\n",
        "    # 1. Stacked Bar Chart\n",
        "    plt.figure(figsize=(15, 8))\n",
        "\n",
        "    function_data = []\n",
        "    for function in top_functions.index:\n",
        "        cat_means = {}\n",
        "        for cat in [1, 2, 3]:\n",
        "            cat_sites = [site for site, c in category_dict.items() if c == cat]\n",
        "            # Optimized site selection:\n",
        "            relevant_sites = list(df.columns.intersection(cat_sites)) # More efficient intersection\n",
        "            if relevant_sites:\n",
        "                cat_means[cat] = df.loc[function, relevant_sites].mean()\n",
        "            else:\n",
        "                cat_means[cat] = 0\n",
        "        function_data.append((function, cat_means))\n",
        "\n",
        "    bottoms = np.zeros(len(top_functions))\n",
        "    for cat in [1, 2, 3]:\n",
        "        values = [d[1][cat] for d in function_data]\n",
        "        plt.bar(range(len(top_functions)), values, bottom=bottoms,\n",
        "                label=categories_labels[cat], color=category_colors[cat], alpha=0.7)\n",
        "        bottoms += values\n",
        "\n",
        "    plt.title('Top 20 Most Abundant Functions by System State', fontsize=14, pad=20)\n",
        "    plt.xlabel('Function', fontsize=12)\n",
        "    plt.ylabel('Mean Abundance', fontsize=12)\n",
        "    plt.xticks(range(len(top_functions)), top_functions.index, rotation=45, ha='right', fontsize=10)\n",
        "    plt.legend(title='System State', title_fontsize=12, fontsize=10)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # 2. Correlation Heatmap\n",
        "    plt.figure(figsize=(15, 12))\n",
        "    top_data = df.loc[top_functions.index]\n",
        "    # Convert all columns of top_data to numeric, coercing errors to NaN\n",
        "    top_data = top_data.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "    # Drop rows with any NaN values to ensure only numeric data is used for correlation\n",
        "    top_data = top_data.dropna(axis=1, how='all')\n",
        "    # Check if there are any columns left after dropping NaNs\n",
        "    if top_data.empty:\n",
        "        print(\"Warning: DataFrame is empty after dropping NaN columns. Skipping correlation heatmap.\")\n",
        "        return None  # Or return an empty DataFrame or a placeholder\n",
        "\n",
        "    corr = top_data.T.corr()  # Transpose for function correlation\n",
        "\n",
        "    mask = np.triu(np.ones_like(corr), k=1)  # Mask for upper triangle\n",
        "    sns.heatmap(corr, mask=mask, cmap='coolwarm', center=0, annot=True, fmt='.2f',\n",
        "                square=True, cbar_kws={'label': 'Correlation Coefficient'}, annot_kws={'size': 8})\n",
        "\n",
        "    plt.title('Function Correlation Heatmap\\n(Top 20 Most Abundant)', fontsize=14, pad=20)\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return corr, top_data\n",
        "\n",
        "# Convert numeric columns to the correct data type\n",
        "for col in react.columns[1:]:  # Exclude 'function' column\n",
        "    try:\n",
        "        react[col] = pd.to_numeric(react[col], errors='coerce') # Skip errors but convert rest\n",
        "    except ValueError:\n",
        "        print(f\"Could not convert column '{col}' to numeric. Check its contents.\")\n",
        "        # Handle the error or investigate the column for non-numeric values\n",
        "\n",
        "# Calculate the mean after type conversion\n",
        "mean_abundances_react = react.mean(axis=1, numeric_only=True) # Specify only numeric in case strings remain\n",
        "corr_react, top_data = analyze_pathway_patterns(react, mean_abundances_react, category_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0Sbd6A7uvgR"
      },
      "source": [
        "# 8. Mapping the Pathways back to the Genera\n",
        "\n",
        "The result we obtained from the picrust pipeline contain the following dataframes, here described so it would be possible to parse. Following are the files description with the shape\n",
        "| Picrust_Result | Picrust_Result | Picrust_Result | parce | parce | parce | ECcontri | ECcontri | ECcontri | ECcontri | React | React |\n",
        "|---|---|---|---|---|---|---|---|---|---|---|---|\n",
        "| pathway | description | Sites/abund | pathway | RXN | EC number | EC number | varios abundances | Sites | OTU | Sites/abund | Reactions |\n",
        "|366,72|366,72|366,72|574,1|574,1|574,1|1491288, 9|1491288, 9|1491288, 9|1491288, 9| (2955, 71)|(2955, 71)|\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T09:51:24.070313Z",
          "iopub.status.busy": "2025-02-19T09:51:24.06992Z",
          "iopub.status.idle": "2025-02-19T09:51:26.166719Z",
          "shell.execute_reply": "2025-02-19T09:51:26.165435Z",
          "shell.execute_reply.started": "2025-02-19T09:51:24.070284Z"
        },
        "id": "RGMCpOrRuvgR",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#EC_path = Path(base_dir / \"Galaxy9-[EC_T].tabular\")\n",
        "#EC= pd.read_csv(EC_path, sep = \"\\t\")\n",
        "# ECcontri and KOcontri files contain sample, function (EC/KO number), taxon (genus/OTU ID), and abundance metrics.\n",
        "ECcontri_path =  Path(large_dir / \"Galaxy26_contrib.tabular\") # for VSCODE\n",
        "\n",
        "#ECcontri_path =  Path(base_dir / \"Galaxy26_contrib.tabular\") # for Kaggle\n",
        "ECcontri= pd.read_csv(ECcontri_path, sep = \"\\t\")\n",
        "#KOcontri_path = Path(large_dir / \"Galaxy30-[KO_pred_metagenome_contrib].tabular\")\n",
        "#KOcontri= pd.read_csv(KOcontri_path, sep = \"\\t\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HB57hdTiuvgR"
      },
      "source": [
        "## 8.1. Mapping Genera to Otu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T09:51:27.806009Z",
          "iopub.status.busy": "2025-02-19T09:51:27.80564Z",
          "iopub.status.idle": "2025-02-19T09:51:27.817542Z",
          "shell.execute_reply": "2025-02-19T09:51:27.81602Z",
          "shell.execute_reply.started": "2025-02-19T09:51:27.805982Z"
        },
        "id": "C_7z90_iuvgR",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Mapping the Genera to Otu for the Taxonomy assigment requeriment\n",
        "def create_otu_mapping(fasta_file_final):\n",
        "    \"\"\"Creates a DataFrame mapping OTUs to genera from a FASTA file\n",
        "    Args: fasta_file (str): Path to FASTA file\n",
        "    Returns: pd.DataFrame: DataFrame with columns ['Genus', 'OTU']\n",
        "    \"\"\"\n",
        "    mapping_data = []\n",
        "\n",
        "    for record in SeqIO.parse(fasta_file_final, \"fasta\"):\n",
        "        # Split description to get genus and OTU\n",
        "        parts = record.description.split()\n",
        "        genus = parts[0]\n",
        "        otu = parts[1]  # Take first OTU number\n",
        "\n",
        "        mapping_data.append({'Genus': genus,'OTU': otu})\n",
        "\n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame(mapping_data).sort_values('Genus')\n",
        "\n",
        "    return df\n",
        "\n",
        "otu_mapping = create_otu_mapping(fasta_file_final)\n",
        "# Change the name of the Otus since they using taxon\n",
        "otu_mapping = otu_mapping.rename(columns={\"OTU\" : \"taxon\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_HSFhUmuvgR"
      },
      "source": [
        "Now with the parce file where there is info for pathway, reactions and also it is the EC numbers, the EC numbers will be extracted because they are encoded inside the parce file. So that we can link the EC with the pathways in the ECcontri df. Precisely the stratified Pathway Abundance contributions (KO/EC + taxon + Taxon abundance +  others ) = KOcontri, ECcontri will be join by the taxon (which is same as the otus) with the file where is located the taxonomy Assigment = Otus + Genera Falta. Ultimately, the pathway descriptions file = Full pipipeline results(pathways + abundances) + description(human readable pathway) = Picrust_Result will also be join in order to make the visualisations, we doing that in steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SWsDEMsuvgR"
      },
      "source": [
        "### Map pathways to ECcontri\n",
        "\n",
        "Map pathways (Picrust_Result) to Parce → Ensures accurate EC-pathway links.   \n",
        "Map reactions (React) to Parce → Ensures correct RXN-pathway links.   \n",
        "Use these mappings to update ECcontri → Assign pathways to EC numbers.  \n",
        "Handle unmapped EC numbers → Keep them separate for review.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjaxRTlluvgR"
      },
      "source": [
        "## 8.2. Decomvoluting Parce :\n",
        "separating the contents of the parsing file parce df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T09:51:38.05516Z",
          "iopub.status.busy": "2025-02-19T09:51:38.054805Z",
          "iopub.status.idle": "2025-02-19T09:51:38.082039Z",
          "shell.execute_reply": "2025-02-19T09:51:38.080827Z",
          "shell.execute_reply.started": "2025-02-19T09:51:38.055132Z"
        },
        "id": "r7mUFRVcuvgR",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "19bd9d9b-3361-422f-c9ad-6546ddd1020a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'parce' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-89-c2ed37e7b8b9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Parse parce file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparce\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'parce' is not defined"
          ]
        }
      ],
      "source": [
        "# Initialize lists\n",
        "pathways = []\n",
        "all_ec_numbers = []\n",
        "reactions = []\n",
        "\n",
        "# Parse parce file\n",
        "for line in parce.iloc[:, 0]:\n",
        "    parts = line.split(' ', 1)\n",
        "    if len(parts) == 2:\n",
        "        pathway = parts[0]\n",
        "        reaction_list = parts[1].strip().split()\n",
        "\n",
        "        # Find EC numbers\n",
        "        ec_nums = []\n",
        "        rxns = []\n",
        "\n",
        "        for rxn in reaction_list:\n",
        "            if rxn.count('.') == 3:\n",
        "                ec_part = rxn.split('-RXN')[0]\n",
        "                if all(p.replace('-','').isdigit() for p in ec_part.split('.')):\n",
        "                    ec_nums.append(ec_part)\n",
        "                    continue\n",
        "            rxns.append(rxn)\n",
        "\n",
        "        # Pad ec_nums list to always have 7 elements\n",
        "        ec_nums.extend([None] * (7 - len(ec_nums)))\n",
        "\n",
        "        # Add to lists\n",
        "        pathways.append(pathway)\n",
        "        all_ec_numbers.append(ec_nums)\n",
        "        reactions.append(' '.join(rxns))\n",
        "\n",
        "# Create DataFrame\n",
        "parce_reference = pd.DataFrame({\n",
        "    'pathway': pathways,\n",
        "    'EC1': [ecs[0] for ecs in all_ec_numbers],\n",
        "    'EC2': [ecs[1] for ecs in all_ec_numbers],\n",
        "    'EC3': [ecs[2] for ecs in all_ec_numbers],\n",
        "    'EC4': [ecs[3] for ecs in all_ec_numbers],\n",
        "    'EC5': [ecs[4] for ecs in all_ec_numbers],\n",
        "    'EC6': [ecs[5] for ecs in all_ec_numbers],\n",
        "    'EC7': [ecs[6] for ecs in all_ec_numbers],\n",
        "    'other_reactions': reactions\n",
        "})\n",
        "\n",
        "# Display first few rows\n",
        "print(\"First 5 rows of parce data with all EC numbers:\")\n",
        "print(parce_reference.head().to_string())\n",
        "\n",
        "# Print example of a pathway with many EC numbers\n",
        "print(\"\\nExample of pathway with many EC numbers:\")\n",
        "print(parce_reference[parce_reference['EC7'].notna()].iloc[0].to_string())\n",
        "\n",
        "#parce_reference.to_csv(\"~/MIC/2_Micro/data_picrust/EC_path_parce.csv\", index=False, sep=\"\\t\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhNsmPeouvgR"
      },
      "source": [
        "enriched_picrust"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T09:51:53.678217Z",
          "iopub.status.busy": "2025-02-19T09:51:53.677836Z",
          "iopub.status.idle": "2025-02-19T09:51:53.693849Z",
          "shell.execute_reply": "2025-02-19T09:51:53.692763Z",
          "shell.execute_reply.started": "2025-02-19T09:51:53.678186Z"
        },
        "id": "3FsuN4hYuvgR",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "2bc387a3-a910-4714-d541-79132ee0231f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'parce_reference' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-92-7eff17e3f4e4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Filter parce_reference to only include needed pathways\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mparced_picrust\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparce_reference\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparce_reference\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pathway'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpicrust_pathways\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Print statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'parce_reference' is not defined"
          ]
        }
      ],
      "source": [
        "# Get list of pathways we need from Picrust_Result\n",
        "picrust_pathways = set(Picrust_Result['pathway'])\n",
        "\n",
        "# Filter parce_reference to only include needed pathways\n",
        "parced_picrust = parce_reference[parce_reference['pathway'].isin(picrust_pathways)]\n",
        "\n",
        "# Print statistics\n",
        "print(\"Matching Statistics:\")\n",
        "print(f\"Total pathways in Picrust_Result: {len(picrust_pathways)}\")\n",
        "print(f\"Matched pathways from parced_picrust: {len(parced_picrust)}\")\n",
        "\n",
        "# Show some examples of the matched data\n",
        "print(\"\\nFirst few matched pathways:\")\n",
        "print(parced_picrust.head().to_string())\n",
        "\n",
        "# Check if we missed any pathways\n",
        "missing_pathways = picrust_pathways - set(parced_picrust['pathway'])\n",
        "if missing_pathways:\n",
        "    print(\"\\nWarning: Some Picrust pathways not found in parce:\")\n",
        "    print(list(missing_pathways)[:5])  # Show first 5 missing pathways if any\n",
        "#parced_picrust.to_csv(\"~/MIC/2_Micro/data_picrust/parced_picrust.csv\", index=False, sep=\"\\t\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPnDyZxmYel3",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "parce.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EEKnKNYuvgR"
      },
      "source": [
        "This way of mapping from parce df was no effective, the pathways and EC in parce are no all the ones are on ECcontri. So it was decided to map it from ECcontri directly putting the pathways into the df from Picrust_Result, so just put the two columns description and pathways inside the ECcontri by the column Site, instead that from the column function aka EC numbers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCs2cimyuvgS"
      },
      "source": [
        "## 8.3. Map Econtri to pathways"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEkDJg5zuvgS"
      },
      "source": [
        "We can no directly map the description and the pathway from Picrust_Result into ECcontri because each site can have several pathways, so we reshaping the Picrust_Result to long format and so that each row corresponds to a pathway for a given site. It is no possible to do this on a go using the whole 1491288 rows on ECcontri, so it would have to be done on agreggated data, as suggested by McKinney, 2010.\n",
        "Source: McKinney, W. (2010). Data Structures for Statistical Computing in Python. Retrieved from https://pandas.pydata.org/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T09:52:01.119153Z",
          "iopub.status.busy": "2025-02-19T09:52:01.118714Z",
          "iopub.status.idle": "2025-02-19T09:52:01.429902Z",
          "shell.execute_reply": "2025-02-19T09:52:01.428917Z",
          "shell.execute_reply.started": "2025-02-19T09:52:01.119118Z"
        },
        "id": "npxOZlx8uvgS",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Reshape Picrust_Result to long format: each row now corresponds to a pathway for a given site\n",
        "picrust_long = Picrust_Result.melt(id_vars=['pathway', 'description'],\n",
        "                                   var_name='sample',\n",
        "                                   value_name='abundance')\n",
        "\n",
        "# Filter out rows where the abundance is 0 (assuming that's what you mean by \"pathway present\")\n",
        "picrust_long = picrust_long[picrust_long['abundance'] > 0]\n",
        "\n",
        "# Aggregate pathway info per site\n",
        "mapping = picrust_long.groupby('sample').agg({\n",
        "    'pathway': lambda x: list(x),\n",
        "    'description': lambda x: list(x)\n",
        "}).reset_index()\n",
        "\n",
        "# Merge the aggregated mapping with ECcontri\n",
        "ECcontri_agg_site = pd.merge(ECcontri, mapping, on='sample', how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KTFNSJ6bYel3",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "96371146-32d9-4d81-8f4e-6ca28b124c09"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   sample    function   taxon  taxon_abun  taxon_rel_abun  \\\n",
              "0  site_1  EC:1.1.1.1  104313     52779.0        7.212466   \n",
              "1  site_1  EC:1.1.1.1  120274     15295.0        2.090124   \n",
              "2  site_1  EC:1.1.1.1  141042     62473.0        8.537191   \n",
              "3  site_1  EC:1.1.1.1  156371      7109.0        0.971474   \n",
              "4  site_1  EC:1.1.1.1  162379       861.5        0.117727   \n",
              "\n",
              "   genome_function_count  taxon_function_abun  taxon_rel_function_abun  \\\n",
              "0                      3             158337.0                21.637399   \n",
              "1                      2              30590.0                 4.180249   \n",
              "2                      1              62473.0                 8.537191   \n",
              "3                      1               7109.0                 0.971474   \n",
              "4                      2               1723.0                 0.235455   \n",
              "\n",
              "   norm_taxon_function_contrib  \\\n",
              "0                     0.113280   \n",
              "1                     0.021885   \n",
              "2                     0.044695   \n",
              "3                     0.005086   \n",
              "4                     0.001233   \n",
              "\n",
              "                                             pathway  \\\n",
              "0  [1CMET2-PWY, 3-HYDROXYPHENYLACETATE-DEGRADATIO...   \n",
              "1  [1CMET2-PWY, 3-HYDROXYPHENYLACETATE-DEGRADATIO...   \n",
              "2  [1CMET2-PWY, 3-HYDROXYPHENYLACETATE-DEGRADATIO...   \n",
              "3  [1CMET2-PWY, 3-HYDROXYPHENYLACETATE-DEGRADATIO...   \n",
              "4  [1CMET2-PWY, 3-HYDROXYPHENYLACETATE-DEGRADATIO...   \n",
              "\n",
              "                                         description  \n",
              "0  [N10-formyl-tetrahydrofolate biosynthesis, 4-h...  \n",
              "1  [N10-formyl-tetrahydrofolate biosynthesis, 4-h...  \n",
              "2  [N10-formyl-tetrahydrofolate biosynthesis, 4-h...  \n",
              "3  [N10-formyl-tetrahydrofolate biosynthesis, 4-h...  \n",
              "4  [N10-formyl-tetrahydrofolate biosynthesis, 4-h...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9c6241f5-143f-464c-b0e9-67d132f98a0d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sample</th>\n",
              "      <th>function</th>\n",
              "      <th>taxon</th>\n",
              "      <th>taxon_abun</th>\n",
              "      <th>taxon_rel_abun</th>\n",
              "      <th>genome_function_count</th>\n",
              "      <th>taxon_function_abun</th>\n",
              "      <th>taxon_rel_function_abun</th>\n",
              "      <th>norm_taxon_function_contrib</th>\n",
              "      <th>pathway</th>\n",
              "      <th>description</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>site_1</td>\n",
              "      <td>EC:1.1.1.1</td>\n",
              "      <td>104313</td>\n",
              "      <td>52779.0</td>\n",
              "      <td>7.212466</td>\n",
              "      <td>3</td>\n",
              "      <td>158337.0</td>\n",
              "      <td>21.637399</td>\n",
              "      <td>0.113280</td>\n",
              "      <td>[1CMET2-PWY, 3-HYDROXYPHENYLACETATE-DEGRADATIO...</td>\n",
              "      <td>[N10-formyl-tetrahydrofolate biosynthesis, 4-h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>site_1</td>\n",
              "      <td>EC:1.1.1.1</td>\n",
              "      <td>120274</td>\n",
              "      <td>15295.0</td>\n",
              "      <td>2.090124</td>\n",
              "      <td>2</td>\n",
              "      <td>30590.0</td>\n",
              "      <td>4.180249</td>\n",
              "      <td>0.021885</td>\n",
              "      <td>[1CMET2-PWY, 3-HYDROXYPHENYLACETATE-DEGRADATIO...</td>\n",
              "      <td>[N10-formyl-tetrahydrofolate biosynthesis, 4-h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>site_1</td>\n",
              "      <td>EC:1.1.1.1</td>\n",
              "      <td>141042</td>\n",
              "      <td>62473.0</td>\n",
              "      <td>8.537191</td>\n",
              "      <td>1</td>\n",
              "      <td>62473.0</td>\n",
              "      <td>8.537191</td>\n",
              "      <td>0.044695</td>\n",
              "      <td>[1CMET2-PWY, 3-HYDROXYPHENYLACETATE-DEGRADATIO...</td>\n",
              "      <td>[N10-formyl-tetrahydrofolate biosynthesis, 4-h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>site_1</td>\n",
              "      <td>EC:1.1.1.1</td>\n",
              "      <td>156371</td>\n",
              "      <td>7109.0</td>\n",
              "      <td>0.971474</td>\n",
              "      <td>1</td>\n",
              "      <td>7109.0</td>\n",
              "      <td>0.971474</td>\n",
              "      <td>0.005086</td>\n",
              "      <td>[1CMET2-PWY, 3-HYDROXYPHENYLACETATE-DEGRADATIO...</td>\n",
              "      <td>[N10-formyl-tetrahydrofolate biosynthesis, 4-h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>site_1</td>\n",
              "      <td>EC:1.1.1.1</td>\n",
              "      <td>162379</td>\n",
              "      <td>861.5</td>\n",
              "      <td>0.117727</td>\n",
              "      <td>2</td>\n",
              "      <td>1723.0</td>\n",
              "      <td>0.235455</td>\n",
              "      <td>0.001233</td>\n",
              "      <td>[1CMET2-PWY, 3-HYDROXYPHENYLACETATE-DEGRADATIO...</td>\n",
              "      <td>[N10-formyl-tetrahydrofolate biosynthesis, 4-h...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9c6241f5-143f-464c-b0e9-67d132f98a0d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9c6241f5-143f-464c-b0e9-67d132f98a0d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9c6241f5-143f-464c-b0e9-67d132f98a0d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-7934cb2f-47ac-48f9-8ab9-0102675aff89\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7934cb2f-47ac-48f9-8ab9-0102675aff89')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-7934cb2f-47ac-48f9-8ab9-0102675aff89 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "ECcontri_agg_site"
            }
          },
          "metadata": {},
          "execution_count": 94
        }
      ],
      "source": [
        "ECcontri_agg_site.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T09:52:04.326451Z",
          "iopub.status.busy": "2025-02-19T09:52:04.326101Z",
          "iopub.status.idle": "2025-02-19T09:52:06.354359Z",
          "shell.execute_reply": "2025-02-19T09:52:06.353212Z",
          "shell.execute_reply.started": "2025-02-19T09:52:04.326425Z"
        },
        "id": "5EXHu-QeuvgS",
        "trusted": true
      },
      "outputs": [],
      "source": [
        " # Add genus information from otu_mapping\n",
        "ECcontri_agg_site['taxon'] = ECcontri_agg_site['taxon'].astype(str)\n",
        "otu_mapping['taxon'] = otu_mapping['taxon'].astype(str)\n",
        "\n",
        "ECcontri_otu= pd.merge(ECcontri_agg_site, otu_mapping, on='taxon', how='left', validate='m:1')\n",
        "\n",
        "unmapped = ECcontri_otu['Genus'].isna().sum()\n",
        "if unmapped > 0:\n",
        "    print(f\"Warning: {unmapped} rows could not be mapped to genera\")\n",
        "# Rename columns: here \"description\" becomes \"pathway\" and \"pathway\" becomes \"npath\"\n",
        "ECcontri_otu  = ECcontri_otu.rename(columns={\"sample\":\"Sites\", \"function\": \"EC\", \"taxon\": \"OTU\", \"description\":\"pathway\", \"pathway\":\"npath\",\n",
        "                                     \"taxon_abun\": \"abund_raw\", \"taxon_function_abun\": \"abund_contri\", \"taxon_rel_abun\": \"rel_abund_raw\",\n",
        "                                       \"taxon_rel_function_abun\": \"rel_abund_contri\", \"norm_taxon_function_contrib\" :\"norm_abund_contri\", \"genome_function_count\":\"genome_EC_count\"})\n",
        "# Organize columns in logical groups\n",
        "cols_order = ['Sites', 'Genus', 'OTU', 'EC', # Identification columns\n",
        "              'npath', 'pathway', # Pathway information\n",
        "              'abund_raw', 'rel_abund_raw', # Raw abundance metrics\n",
        "              'genome_EC_count', 'abund_contri', 'rel_abund_contri', 'norm_abund_contri'] # Contribution metrics\n",
        "# Reorder columns, takes like 4 minutes on this slow laptop\n",
        "ECcontri_otu = ECcontri_otu[cols_order]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(ECcontri_otu.head( ))"
      ],
      "metadata": {
        "id": "3vbLXDuinL1h",
        "outputId": "388dcf51-1a40-4c22-dd0a-8fcbeae8606f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Sites          Genus     OTU          EC  \\\n",
            "0  site_1    Pseudomonas  104313  EC:1.1.1.1   \n",
            "1  site_1  Simplicispira  120274  EC:1.1.1.1   \n",
            "2  site_1     Acidovorax  141042  EC:1.1.1.1   \n",
            "3  site_1  Dechloromonas  156371  EC:1.1.1.1   \n",
            "4  site_1  Desulfobulbus  162379  EC:1.1.1.1   \n",
            "\n",
            "                                               npath  \\\n",
            "0  [1CMET2-PWY, 3-HYDROXYPHENYLACETATE-DEGRADATIO...   \n",
            "1  [1CMET2-PWY, 3-HYDROXYPHENYLACETATE-DEGRADATIO...   \n",
            "2  [1CMET2-PWY, 3-HYDROXYPHENYLACETATE-DEGRADATIO...   \n",
            "3  [1CMET2-PWY, 3-HYDROXYPHENYLACETATE-DEGRADATIO...   \n",
            "4  [1CMET2-PWY, 3-HYDROXYPHENYLACETATE-DEGRADATIO...   \n",
            "\n",
            "                                             pathway  abund_raw  \\\n",
            "0  [N10-formyl-tetrahydrofolate biosynthesis, 4-h...    52779.0   \n",
            "1  [N10-formyl-tetrahydrofolate biosynthesis, 4-h...    15295.0   \n",
            "2  [N10-formyl-tetrahydrofolate biosynthesis, 4-h...    62473.0   \n",
            "3  [N10-formyl-tetrahydrofolate biosynthesis, 4-h...     7109.0   \n",
            "4  [N10-formyl-tetrahydrofolate biosynthesis, 4-h...      861.5   \n",
            "\n",
            "   rel_abund_raw  genome_EC_count  abund_contri  rel_abund_contri  \\\n",
            "0       7.212466                3      158337.0         21.637399   \n",
            "1       2.090124                2       30590.0          4.180249   \n",
            "2       8.537191                1       62473.0          8.537191   \n",
            "3       0.971474                1        7109.0          0.971474   \n",
            "4       0.117727                2        1723.0          0.235455   \n",
            "\n",
            "   norm_abund_contri  \n",
            "0           0.113280  \n",
            "1           0.021885  \n",
            "2           0.044695  \n",
            "3           0.005086  \n",
            "4           0.001233  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPaVL4T0uvgS"
      },
      "source": [
        "ECcontri_otu is a comprehensive dataframe that combines site locations, taxonomic information (genera and OTUs), enzyme classifications (ECs), and pathways (code for pathway (npath) and description (pathway)). The associated abundance metrics belong to the original ECcontri. The abundance metrics include:\n",
        "abund_raw: The original count of each organism (OTU) at each site\n",
        "rel_abund_raw: The relative abundance of each organism at each site, expressed as a proportion of total counts\n",
        "genome_function_count represents the predicted number of copies of a particular EC number (enzyme) in an organism's genome. This prediction comes from PICRUSt's hidden-state prediction process, which infers gene family abundances for each organism based on its phylogenetic placement relative to reference genomes\n",
        "abund_contri: The contribution of each organism to a specific enzyme function, calculated by multiplying the raw abundance by the number of copies of that enzyme in the organism's genome\n",
        "rel_abund_contri: The relative contribution of each organism to the enzyme function, accounting for both abundance and genome copy number\n",
        "norm_abund_contri: The normalized contribution metric that allows comparison across different sites and functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T09:52:13.927696Z",
          "iopub.status.busy": "2025-02-19T09:52:13.927272Z",
          "iopub.status.idle": "2025-02-19T09:52:14.513867Z",
          "shell.execute_reply": "2025-02-19T09:52:14.512365Z",
          "shell.execute_reply.started": "2025-02-19T09:52:13.927663Z"
        },
        "id": "xeKQezwyuvgS",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "'''# Analyze genome_function_count\n",
        "print(\"Genome function count statistics:\")\n",
        "print(\"\\nOverall statistics:\")\n",
        "print(ECcontri_otu['genome_EC_count'].describe())\n",
        "\n",
        "# Look at distribution by EC number\n",
        "print(\"\\nExample EC numbers and their genome counts:\")\n",
        "ec_counts = ECcontri_otu.groupby('EC')['genome_EC_count'].agg(['unique', 'mean', 'max']).head()\n",
        "print(ec_counts)\n",
        "\n",
        "# Check if genome_function_count is consistent for each OTU-EC pair\n",
        "print(\"\\nCheck if genome_EC_count is consistent for OTU-EC combinations:\")\n",
        "consistency_check = ECcontri_otu.groupby(['OTU', 'EC'])['genome_EC_count'].nunique()\n",
        "inconsistent = consistency_check[consistency_check > 1]\n",
        "if len(inconsistent) > 0:\n",
        "    print(f\"Found {len(inconsistent)} OTU-EC pairs with inconsistent genome counts\")\n",
        "else:\n",
        "    print(\"Genome counts are consistent for all OTU-EC pairs\")\n",
        "\n",
        "# Explain the metrics in the dataframe\n",
        "print(\"\\nDataframe Components:\")\n",
        "print(\"1. Abundance Metrics:\")\n",
        "print(\"   - abund_raw: Raw abundance of each organism in each site\")\n",
        "print(\"   - abund_contri: Organism's abundance contribution to function/pathway\")\n",
        "print(\"   - rel_abund_raw: Original relative abundance\")\n",
        "print(\"   - rel_abund_contri: Relative abundance contribution to pathway\")\n",
        "print(\"   - norm_abund_contri: Normalized abundance contribution\")\n",
        "print(\"\\n2. Genome Function Count:\")\n",
        "print(\"   Number of copies of each EC (enzyme) in organism's genome\")'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpjxR3vk-PJK"
      },
      "source": [
        "Genome function count statistics:\n",
        "\n",
        "Overall statistics:\n",
        "count    1.491288e+06\n",
        "mean     1.390277e+00\n",
        "std      1.071974e+00\n",
        "min      1.000000e+00\n",
        "25%      1.000000e+00\n",
        "50%      1.000000e+00\n",
        "75%      1.000000e+00\n",
        "max      1.000000e+01\n",
        "Name: genome_EC_count, dtype: float64\n",
        "\n",
        "Example EC numbers and their genome counts:\n",
        "                                    unique      mean  max\n",
        "EC                                                       \n",
        "EC:1.1.1.1              [3, 2, 1, 5, 4, 8]  2.310375    8\n",
        "EC:1.1.1.100  [8, 5, 2, 3, 4, 9, 10, 6, 1]  4.237317   10\n",
        "EC:1.1.1.102                           [1]  1.000000    1\n",
        "EC:1.1.1.103                           [1]  1.000000    1\n",
        "EC:1.1.1.105                           [1]  1.000000    1\n",
        "\n",
        "Check if genome_EC_count is consistent for OTU-EC combinations:\n",
        "Genome counts are consistent for all OTU-EC pairs\n",
        "\n",
        "Dataframe Components:\n",
        "1. Abundance Metrics:\n",
        "   - abund_raw: Raw abundance of each organism in each site\n",
        "   - abund_contri: Organism's abundance contribution to function/pathway\n",
        "   - rel_abund_raw: Original relative abundance\n",
        "   - rel_abund_contri: Relative abundance contribution to pathway\n",
        "   - norm_abund_contri: Normalized abundance contribution\n",
        "\n",
        "2. Genome Function Count:\n",
        "   Number of copies of each EC (enzyme) in organism's genome"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MA-sQJpQuvgS"
      },
      "source": [
        "Analysis of genome_function_count(genome_EC_count) shows that most organisms typically have just one copy of any given enzyme (EC number) in their genome, with 75% of all cases showing a single copy. However, there is notable variation, with some organisms having up to 10 copies of certain enzymes. The average across all cases is 1.4 copies per enzyme per organism.\n",
        "Some enzymes show more variation than others. For example:\n",
        "\n",
        "EC:1.1.1.1 varies from 1 to 8 copies across different organisms\n",
        "EC:1.1.1.100 shows the widest range, from 1 to 10 copies\n",
        "Many enzymes (like EC:1.1.1.102, 103, 105) consistently appear as single copies\n",
        "\n",
        "Importantly, the copy number is consistent for each organism-enzyme combination across all sites, indicating this is a stable genomic characteristic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTSu-jj9uvgS"
      },
      "source": [
        "_____________________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voefPo-VuvgS"
      },
      "source": [
        "Pathway Mapping Analysis\n",
        "\n",
        "There were identified a discrepancy between EC predictions and pathway abundances. Found 61 pathways with EC number evidence that were not included in final predictions. Total number of reference pathways: 574 (from MetaCyc), total pathways in final predictions: 366, example missing pathway: PWY-6486 supported by EC:4.2.1.41\n",
        "\n",
        "Implications\n",
        "This finding suggests that the pathway prediction pipeline might be filtering out potentially relevant pathways despite having supporting EC evidence. This could impact the biological interpretation of the functional profiles and warrants further investigation.\n",
        "So in this study we mapped the pathways dataframe directly to the parce file and in doing so we have also the reaction information, avoiding the discrepancy with the Picrust_Result missing pathways."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MP4zCAyYuvgS"
      },
      "source": [
        "_____________________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEJ63Uk0uvgS"
      },
      "source": [
        "Now ECcontri_otu has several rows and columns providing information of the EC contribution to the metrics to each enzime aka EC number to the sites, genera combination, however the pathways are from origin link to most of the sites. This is perhaps because the methos infwee dunxriona bAWS ON XOMON sets of reference genomes.  Then, same environment in this case heating and cooling water systems poses similar organisms with similar pathways, the difference being on the abundance. So in order for this data to be usable, it is necesary to parse the EC into human readable information from a external enzyme databases to retrieve functional information about an EC number. Common resources include:\n",
        "\n",
        "UniProt: query UniProt’s REST API to get enzyme details by searching with the EC number.\n",
        "ExPASy Enzyme Database: Provides enzyme information based on EC numbers.\n",
        "BRENDA: A comprehensive enzyme database that can be queried either via its web interface or programmatically (e.g., using the bioservices Python package). Following script creates an EnzymeRetriever class that handles API requests to UniProt, processes unique EC numbers to avoid duplicate requests\n",
        "Adds protein names, functions, and UniProt IDs to ECcontri_otu df and includes rate limiting to avoid API restrictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XksxGYwIuvgS"
      },
      "source": [
        "## 8.4 Retrieving EC from Uniprot Locally"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vTTKHr0buvgS",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import time\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "class BatchEnzymeRetriever:\n",
        "    def __init__(self, batch_size=50, save_every=10):\n",
        "        self.uniprot_api = \"https://rest.uniprot.org/uniprotkb/search\"\n",
        "        self.batch_size = batch_size\n",
        "        self.save_every = save_every\n",
        "        self.results_file = 'uniprot_results.csv'\n",
        "        self.progress_file = 'retrieval_progress.json'\n",
        "        self.current_position = 0\n",
        "        self.load_progress()\n",
        "\n",
        "    def load_progress(self):\n",
        "        \"\"\"Load progress from previous run\"\"\"\n",
        "        if Path(self.progress_file).exists():\n",
        "            with open(self.progress_file, 'r') as f:\n",
        "                self.current_position = json.load(f)['position']\n",
        "            print(f\"Resuming from position {self.current_position}\")\n",
        "        else:\n",
        "            print(\"Starting new retrieval process\")\n",
        "\n",
        "    def save_progress(self):\n",
        "        \"\"\"Save current progress\"\"\"\n",
        "        with open(self.progress_file, 'w') as f:\n",
        "            json.dump({'position': self.current_position}, f)\n",
        "\n",
        "    def get_uniprot_info(self, ec: str, organism: str) -> dict:\n",
        "        \"\"\"Get UniProt information for a specific EC-organism pair\"\"\"\n",
        "        query = f\"{ec} {organism}\"\n",
        "\n",
        "        params = {\n",
        "            'query': query,\n",
        "            'format': 'tsv',\n",
        "            'fields': 'id,ec,organism_name,protein_name',\n",
        "            'size': 10\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = requests.get(self.uniprot_api, params=params)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            # Parse results\n",
        "            lines = response.text.strip().split('\\n')\n",
        "            if len(lines) < 2:\n",
        "                return None\n",
        "\n",
        "            # Process results to find best match\n",
        "            best_match = None\n",
        "            best_score = -float('inf')\n",
        "\n",
        "            for line in lines[1:]:  # Skip header\n",
        "                parts = line.split('\\t')\n",
        "                if len(parts) < 4:\n",
        "                    continue\n",
        "\n",
        "                entry_id, ec_numbers, org_name, protein_name = parts\n",
        "\n",
        "                # Calculate score based on organism name simplicity\n",
        "                score = 0\n",
        "                if organism.lower() in org_name.lower():\n",
        "                    score += 100\n",
        "                    if org_name.lower().endswith(' sp') or org_name.lower().endswith(' sp.'):\n",
        "                        score += 200\n",
        "                    if any(char.isdigit() for char in org_name):\n",
        "                        score -= 100\n",
        "\n",
        "                    # Check EC number match\n",
        "                    if ec.replace('EC:', '') in ec_numbers.split('; '):\n",
        "                        score += 150\n",
        "\n",
        "                        if score > best_score:\n",
        "                            best_score = score\n",
        "                            best_match = {\n",
        "                                'uniprot_id': entry_id,\n",
        "                                'ec_number': ec,\n",
        "                                'protein_name': protein_name,\n",
        "                                'organism': org_name,\n",
        "                                'score': score\n",
        "                            }\n",
        "\n",
        "            return best_match\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error retrieving data for {ec} - {organism}: {e}\")\n",
        "            time.sleep(5)\n",
        "            return None\n",
        "\n",
        "    def process_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Process DataFrame in batches, saving progress\"\"\"\n",
        "\n",
        "        # Load existing results if any\n",
        "        results_df = pd.DataFrame()\n",
        "        if Path(self.results_file).exists():\n",
        "            results_df = pd.read_csv(self.results_file)\n",
        "            print(f\"Loaded {len(results_df)} existing results\")\n",
        "\n",
        "        # Get unique EC-organism pairs starting from current position\n",
        "        pairs = df[['EC', 'Genus']].drop_duplicates()\n",
        "        pairs = pairs.iloc[self.current_position:]\n",
        "\n",
        "        print(f\"Processing {len(pairs)} unique EC-organism pairs\")\n",
        "\n",
        "        for batch_start in range(0, len(pairs), self.batch_size):\n",
        "            batch = pairs.iloc[batch_start:batch_start + self.batch_size]\n",
        "            batch_results = []\n",
        "\n",
        "            print(f\"\\nProcessing batch {batch_start//self.batch_size + 1}\")\n",
        "\n",
        "            for _, row in batch.iterrows():\n",
        "                print(f\"Querying {row['EC']} - {row['Genus']}\")\n",
        "                result = self.get_uniprot_info(row['EC'], row['Genus'])\n",
        "\n",
        "                if result:\n",
        "                    batch_results.append(result)\n",
        "                time.sleep(1)  # Rate limiting\n",
        "\n",
        "            # Add batch results to DataFrame\n",
        "            if batch_results:\n",
        "                batch_df = pd.DataFrame(batch_results)\n",
        "                results_df = pd.concat([results_df, batch_df], ignore_index=True)\n",
        "\n",
        "                # Save periodically\n",
        "                if (batch_start//self.batch_size) % self.save_every == 0:\n",
        "                    results_df.to_csv(self.results_file, index=False)\n",
        "                    self.current_position += len(batch)\n",
        "                    self.save_progress()\n",
        "                    print(f\"Saved progress at position {self.current_position}\")\n",
        "\n",
        "        # Final save\n",
        "        if not results_df.empty:\n",
        "            results_df.to_csv(self.results_file, index=False)\n",
        "        return results_df\n",
        "\n",
        "def process_enzymes(input_data, batch_size: int = 50):\n",
        "    \"\"\"Main function to process enzyme data\"\"\"\n",
        "    # Handle both DataFrame and file path inputs\n",
        "    if isinstance(input_data, str):\n",
        "        df = pd.read_csv(input_data)\n",
        "        print(f\"Loaded {len(df)} rows from {input_data}\")\n",
        "    elif isinstance(input_data, pd.DataFrame):\n",
        "        df = input_data\n",
        "        print(f\"Processing DataFrame with {len(df)} rows\")\n",
        "    else:\n",
        "        raise ValueError(\"Input must be either a file path or a pandas DataFrame\")\n",
        "\n",
        "    # Initialize retriever\n",
        "    retriever = BatchEnzymeRetriever(batch_size=batch_size)\n",
        "\n",
        "    # Process data\n",
        "    results = retriever.process_dataframe(df)\n",
        "    print(\"\\nProcessing complete!\")\n",
        "    print(f\"Results saved to {retriever.results_file}\")\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i127PUfOuvgT",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "'''\n",
        "#  DataFrame loaded:\n",
        "retriever = BatchEnzymeRetriever(batch_size=50)\n",
        "Picrust_Econtri = retriever.process_dataframe(ECcontri_otu)'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQt8CV11uvgT"
      },
      "source": [
        "## 8.5. Colab\n",
        "Made in colab for resource posibility."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-sX1OxiuvgT"
      },
      "source": [
        "original colab code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGPK3BTGuvgT",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class ColabEnzymeRetriever:\n",
        "    def __init__(self, batch_size=100, save_every=5):\n",
        "        self.uniprot_api = \"https://rest.uniprot.org/uniprotkb/search\"\n",
        "        self.batch_size = batch_size\n",
        "        self.save_every = save_every\n",
        "        self.results_file = 'uniprot_results.csv'\n",
        "        self.existing_results = None\n",
        "\n",
        "    def load_existing_results(self, file_path):\n",
        "        \"\"\"Load and validate existing results\"\"\"\n",
        "        self.existing_results = pd.read_csv(file_path)\n",
        "        print(f\"Loaded {len(self.existing_results)} existing results\")\n",
        "        return set(zip(self.existing_results['ec_number'],\n",
        "                      [org.split()[0] for org in self.existing_results['organism']]))\n",
        "\n",
        "    def get_uniprot_info(self, ec: str, organism: str) -> dict:\n",
        "        \"\"\"Get UniProt information for a specific EC-organism pair\"\"\"\n",
        "        query = f\"{ec} {organism}\"\n",
        "\n",
        "        params = {\n",
        "            'query': query,\n",
        "            'format': 'tsv',\n",
        "            'fields': 'id,ec,organism_name,protein_name',\n",
        "            'size': 10\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = requests.get(self.uniprot_api, params=params)\n",
        "            response.raise_for_status()\n",
        "            time.sleep(0.5)  # Reduced sleep time for Colab\n",
        "\n",
        "            lines = response.text.strip().split('\\n')\n",
        "            if len(lines) < 2:\n",
        "                return None\n",
        "\n",
        "            best_match = None\n",
        "            best_score = -float('inf')\n",
        "\n",
        "            for line in lines[1:]:\n",
        "                parts = line.split('\\t')\n",
        "                if len(parts) < 4:\n",
        "                    continue\n",
        "\n",
        "                entry_id, ec_numbers, org_name, protein_name = parts\n",
        "\n",
        "                score = 0\n",
        "                if organism.lower() in org_name.lower():\n",
        "                    score += 100\n",
        "                    if org_name.lower().endswith(' sp') or org_name.lower().endswith(' sp.'):\n",
        "                        score += 200\n",
        "                    if any(char.isdigit() for char in org_name):\n",
        "                        score -= 100\n",
        "\n",
        "                    if ec.replace('EC:', '') in ec_numbers.split('; '):\n",
        "                        score += 150\n",
        "\n",
        "                        if score > best_score:\n",
        "                            best_score = score\n",
        "                            best_match = {\n",
        "                                'uniprot_id': entry_id,\n",
        "                                'ec_number': ec,\n",
        "                                'protein_name': protein_name,\n",
        "                                'organism': org_name,\n",
        "                                'score': score\n",
        "                            }\n",
        "\n",
        "            return best_match\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error retrieving data for {ec} - {organism}: {e}\")\n",
        "            time.sleep(2)\n",
        "            return None\n",
        "\n",
        "    def process_remaining_pairs(self, unique_pairs: pd.DataFrame, processed_pairs: set) -> pd.DataFrame:\n",
        "        \"\"\"Process only the pairs that haven't been processed yet\"\"\"\n",
        "        results = []\n",
        "        pairs_to_process = []\n",
        "\n",
        "        # Get unprocessed pairs\n",
        "        for _, row in unique_pairs.iterrows():\n",
        "            if (row['EC'], row['Genus']) not in processed_pairs:\n",
        "                pairs_to_process.append((row['EC'], row['Genus']))\n",
        "\n",
        "        pairs_df = pd.DataFrame(pairs_to_process, columns=['EC', 'Genus'])\n",
        "        total_pairs = len(pairs_df)\n",
        "\n",
        "        print(f\"\\nTotal pairs to process: {total_pairs}\")\n",
        "\n",
        "        if total_pairs == 0:\n",
        "            print(\"No new pairs to process!\")\n",
        "            return self.existing_results\n",
        "\n",
        "        for batch_start in range(0, total_pairs, self.batch_size):\n",
        "            batch = pairs_df.iloc[batch_start:batch_start + self.batch_size]\n",
        "            batch_results = []\n",
        "\n",
        "            print(f\"\\nProcessing batch {batch_start//self.batch_size + 1} of {total_pairs//self.batch_size + 1}\")\n",
        "            print(f\"Progress: {batch_start}/{total_pairs} pairs ({(batch_start/total_pairs)*100:.1f}%)\")\n",
        "\n",
        "            current_ec = None\n",
        "            for _, row in batch.iterrows():\n",
        "                if current_ec != row['EC']:\n",
        "                    current_ec = row['EC']\n",
        "                    print(f\"\\nProcessing EC number: {current_ec}\")\n",
        "\n",
        "                result = self.get_uniprot_info(row['EC'], row['Genus'])\n",
        "                if result:\n",
        "                    batch_results.append(result)\n",
        "\n",
        "            if batch_results:\n",
        "                results.extend(batch_results)\n",
        "\n",
        "                # Save progress by combining with existing results\n",
        "                if (batch_start//self.batch_size) % self.save_every == 0:\n",
        "                    combined_results = pd.concat([self.existing_results, pd.DataFrame(results)], ignore_index=True)\n",
        "                    combined_results.to_csv(self.results_file, index=False)\n",
        "                    print(f\"Saved {len(combined_results)} total results to file\")\n",
        "\n",
        "        # Combine final results\n",
        "        final_results = pd.concat([self.existing_results, pd.DataFrame(results)], ignore_index=True)\n",
        "        final_results.to_csv(self.results_file, index=False)\n",
        "        return final_results\n",
        "\n",
        "# Main processing function\n",
        "def continue_enzyme_retrieval(data_file: str, existing_results_file: str):\n",
        "    \"\"\"Main function to continue enzyme data retrieval\"\"\"\n",
        "    # Initialize retriever\n",
        "    retriever = ColabEnzymeRetriever(batch_size=100)\n",
        "\n",
        "    # Load existing results\n",
        "    processed_pairs = retriever.load_existing_results(existing_results_file)\n",
        "\n",
        "    # Load and process data\n",
        "    df = pd.read_csv(data_file)\n",
        "    unique_pairs = df[['EC', 'Genus']].drop_duplicates()\n",
        "    print(f\"Total unique pairs in data: {len(unique_pairs)}\")\n",
        "    print(f\"Already processed pairs: {len(processed_pairs)}\")\n",
        "\n",
        "    # Process remaining pairs\n",
        "    results_df = retriever.process_remaining_pairs(unique_pairs, processed_pairs)\n",
        "\n",
        "    # Save and download final results\n",
        "    results_df.to_csv('uniprot_results_final.csv', index=False)\n",
        "    files.download('uniprot_results_final.csv')\n",
        "\n",
        "    return results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1sJyminuvgT",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Usage (after uploading files to Colab), ECcontri_otu was made in colab because it was too big to upload after transformed\n",
        "# results = continue_enzyme_retrieval('ECcontri_otu', 'uniprot_results.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UM7HpQiHuvgT"
      },
      "source": [
        "have to be modify after first run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTO90O2duvgT",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import time\n",
        "from pathlib import Path\n",
        "import logging\n",
        "from typing import Set, Tuple, Optional\n",
        "import json\n",
        "\n",
        "class ColabEnzymeRetriever:\n",
        "    def __init__(self, batch_size=100, save_every=5):\n",
        "        self.uniprot_api = \"https://rest.uniprot.org/uniprotkb/search\"\n",
        "        self.batch_size = batch_size\n",
        "        self.save_every = save_every\n",
        "        self.results_file = Path('uniprot_results.tsv')\n",
        "        self.state_file = Path('retrieval_state.json')\n",
        "        self.processed_pairs: Set[Tuple[str, str]] = set()\n",
        "        self.existing_results = None\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "        self.logger.setLevel(logging.INFO)\n",
        "\n",
        "        if not self.logger.handlers:\n",
        "            handler = logging.StreamHandler()\n",
        "            formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
        "            handler.setFormatter(formatter)\n",
        "            self.logger.addHandler(handler)\n",
        "\n",
        "    def load_existing_results(self, file_path: Path) -> pd.DataFrame:\n",
        "        \"\"\"Load and validate existing results\"\"\"\n",
        "        if file_path.exists():\n",
        "            try:\n",
        "                self.existing_results = pd.read_csv(file_path, sep='\\t')\n",
        "                self.logger.info(f\"Loaded {len(self.existing_results)} existing results\")\n",
        "\n",
        "                # Build set of processed pairs\n",
        "                self.processed_pairs = set()\n",
        "                for _, row in self.existing_results.iterrows():\n",
        "                    if pd.notna(row['ec_number']) and pd.notna(row['organism']):\n",
        "                        ec_num = str(row['ec_number']).strip()\n",
        "                        org = str(row['organism']).split()[0].strip()\n",
        "                        self.processed_pairs.add((ec_num, org))\n",
        "\n",
        "                return self.existing_results\n",
        "            except Exception as e:\n",
        "                self.logger.error(f\"Error loading results file: {e}\")\n",
        "                self.existing_results = pd.DataFrame(\n",
        "                    columns=['uniprot_id', 'ec_number', 'protein_name', 'organism', 'score']\n",
        "                )\n",
        "                return self.existing_results\n",
        "\n",
        "        self.existing_results = pd.DataFrame(\n",
        "            columns=['uniprot_id', 'ec_number', 'protein_name', 'organism', 'score']\n",
        "        )\n",
        "        return self.existing_results\n",
        "\n",
        "    def get_uniprot_info(self, ec: str, organism: str) -> Optional[dict]:\n",
        "        \"\"\"Get UniProt information for a specific EC-organism pair\"\"\"\n",
        "        if (ec, organism) in self.processed_pairs:\n",
        "            return None\n",
        "\n",
        "        query = f'({ec}) AND (organism_name:\"{organism}*\")'\n",
        "        params = {\n",
        "            'query': query,\n",
        "            'format': 'tsv',\n",
        "            'fields': 'id,ec,protein_name,organism_name',\n",
        "            'size': 10\n",
        "        }\n",
        "\n",
        "        max_retries = 3\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                response = requests.get(self.uniprot_api, params=params)\n",
        "                response.raise_for_status()\n",
        "                time.sleep(0.5)\n",
        "\n",
        "                lines = response.text.strip().split('\\n')\n",
        "                if len(lines) < 2:\n",
        "                    return None\n",
        "\n",
        "                best_match = None\n",
        "                best_score = -float('inf')\n",
        "\n",
        "                for line in lines[1:]:\n",
        "                    parts = line.split('\\t')\n",
        "                    if len(parts) < 4:\n",
        "                        continue\n",
        "\n",
        "                    uniprot_id, ec_numbers, protein_name, organism_name = parts\n",
        "\n",
        "                score = 0\n",
        "                if organism_name and isinstance(organism_name, str):\n",
        "                    name_parts = organism_name.split()\n",
        "                    genus = name_parts[0] if name_parts else \"\"\n",
        "\n",
        "                    # Exact genus match gets highest score\n",
        "                    if genus.lower() == organism.lower():\n",
        "                        score += 500\n",
        "                        # Prefer entries with just the genus name\n",
        "                        if len(name_parts) == 1:\n",
        "                            score += 300\n",
        "                        # Heavily penalize strain designations or subspecies\n",
        "                        elif len(name_parts) > 2 or any(char.isdigit() for char in organism_name):\n",
        "                            score -= 400\n",
        "\n",
        "                    if score > -float('inf'):\n",
        "                        if ec.replace('EC:', '') in ec_numbers.split('; '):\n",
        "                            score += 150\n",
        "\n",
        "                            if score > best_score:\n",
        "                                best_score = score\n",
        "                                best_match = {\n",
        "                                    'uniprot_id': uniprot_id,\n",
        "                                    'ec_number': ec,\n",
        "                                    'protein_name': protein_name,\n",
        "                                    'organism': organism_name,\n",
        "                                    'score': score\n",
        "                                }\n",
        "\n",
        "                return best_match if best_match else None\n",
        "\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                if attempt < max_retries - 1:\n",
        "                    time.sleep(2 ** attempt)\n",
        "                    continue\n",
        "                self.logger.error(f\"Error fetching data from UniProt: {e}\")\n",
        "                return None\n",
        "\n",
        "    def process_remaining_pairs(self, unique_pairs: pd.DataFrame, start_ec: str) -> pd.DataFrame:\n",
        "        \"\"\"Process remaining pairs with enforced starting point\"\"\"\n",
        "        # Ensure EC format consistency\n",
        "        if not start_ec.startswith('EC:'):\n",
        "            start_ec = f\"EC:{start_ec.replace('EC:', '')}\"\n",
        "\n",
        "        # Sort and filter pairs\n",
        "        unique_pairs = unique_pairs.sort_values(['EC', 'Genus']).reset_index(drop=True)\n",
        "        unique_pairs = unique_pairs[unique_pairs['EC'] >= start_ec].reset_index(drop=True)\n",
        "\n",
        "        if len(unique_pairs) == 0:\n",
        "            self.logger.warning(f\"No EC numbers found after {start_ec}\")\n",
        "            return self.existing_results\n",
        "\n",
        "        self.logger.info(f\"Starting processing from {unique_pairs.iloc[0]['EC']}\")\n",
        "        total_pairs = len(unique_pairs)\n",
        "\n",
        "        results = []\n",
        "        for idx in range(0, total_pairs, self.batch_size):\n",
        "            batch = unique_pairs.iloc[idx:idx + self.batch_size]\n",
        "            batch_results = []\n",
        "\n",
        "            self.logger.info(f\"\\nProcessing batch {idx//self.batch_size + 1} of {total_pairs//self.batch_size + 1}\")\n",
        "            self.logger.info(f\"Progress: {idx}/{total_pairs} pairs ({(idx/total_pairs)*100:.1f}%)\")\n",
        "\n",
        "            current_ec = None\n",
        "            for _, row in batch.iterrows():\n",
        "                if current_ec != row['EC']:\n",
        "                    current_ec = row['EC']\n",
        "                    self.logger.info(f\"\\nProcessing EC number: {current_ec}\")\n",
        "\n",
        "                if (row['EC'], row['Genus']) not in self.processed_pairs:\n",
        "                    result = self.get_uniprot_info(row['EC'], row['Genus'])\n",
        "                    if result:\n",
        "                        batch_results.append(result)\n",
        "                        self.processed_pairs.add((row['EC'], row['Genus']))\n",
        "\n",
        "            if batch_results:\n",
        "                results.extend(batch_results)\n",
        "\n",
        "                # Save progress periodically\n",
        "                if (idx//self.batch_size) % self.save_every == 0:\n",
        "                    combined_results = pd.concat(\n",
        "                        [self.existing_results, pd.DataFrame(results)],\n",
        "                        ignore_index=True\n",
        "                    )\n",
        "                    combined_results.to_csv(self.results_file, sep='\\t', index=False)\n",
        "                    self.logger.info(f\"Saved {len(combined_results)} total results to file\")\n",
        "\n",
        "        # Final save\n",
        "        final_results = pd.concat(\n",
        "            [self.existing_results, pd.DataFrame(results)],\n",
        "            ignore_index=True\n",
        "        )\n",
        "        final_results.to_csv(self.results_file, sep='\\t', index=False)\n",
        "\n",
        "        return final_results\n",
        "\n",
        "def continue_enzyme_retrieval(unique_pairs: pd.DataFrame, existing_results_file: Path, start_ec: str):\n",
        "    \"\"\"Main function to continue enzyme data retrieval\"\"\"\n",
        "    retriever = ColabEnzymeRetriever(batch_size=100)\n",
        "\n",
        "    # Load existing results\n",
        "    retriever.load_existing_results(existing_results_file)\n",
        "\n",
        "    # Ensure input data is properly formatted\n",
        "    if isinstance(unique_pairs, str):\n",
        "        unique_pairs = pd.read_csv(unique_pairs, sep='\\t')\n",
        "    elif isinstance(unique_pairs, pd.DataFrame):\n",
        "        unique_pairs = unique_pairs.copy()\n",
        "    else:\n",
        "        raise ValueError(\"Input must be either a file path or a pandas DataFrame\")\n",
        "\n",
        "    # Validate and prepare input data\n",
        "    required_columns = ['EC', 'Genus']\n",
        "    if not all(col in unique_pairs.columns for col in required_columns):\n",
        "        raise ValueError(f\"Input data must contain columns: {required_columns}\")\n",
        "\n",
        "    unique_pairs['EC'] = unique_pairs['EC'].astype(str).apply(lambda x: f\"EC:{x.replace('EC:', '')}\")\n",
        "    unique_pairs['Genus'] = unique_pairs['Genus'].astype(str).str.strip()\n",
        "    unique_pairs = unique_pairs[['EC', 'Genus']].drop_duplicates()\n",
        "\n",
        "    # Process remaining pairs\n",
        "    results_df = retriever.process_remaining_pairs(unique_pairs, start_ec)\n",
        "\n",
        "    # Save final results\n",
        "    final_path = Path('uniprot_results_final.tsv')\n",
        "    results_df.to_csv(final_path, sep='\\t', index=False)\n",
        "\n",
        "    return results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mowtkQwJuvgT",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "'''uniprot_results_path = Path(base_dir '/uniprot_results.tsv')\n",
        "# Usage (after uploading files to Colab), ECcontri_otu was made in colab because it was too big to upload after transformed\n",
        "results = continue_enzyme_retrieval(ECcontri_otu, uniprot_results_path, start_ec=\"x.3.1.12\" )'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nARoxMBKuvgT"
      },
      "source": [
        "## 8.6. Cleaning and Preparing Retrieved Data to integrate to ECContri"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9r_F4kMtuvgU",
        "outputId": "b8b291ca-0132-46a4-cdb3-716020a8c583",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "37015 39042 9064 2827\n"
          ]
        }
      ],
      "source": [
        "df_1_path = Path(base_dir / \"uniprot_1_4_sorted.tsv\") # First file retrieved on first run 4 am\n",
        "df_2_path = Path(base_dir / \"uniprot_2_1.38_sorted.tsv\") # Same file retrieven when corrupted around 1:38 following day\n",
        "df_3_path = Path(base_dir / \"uniprot_3_sorted.tsv\") # Rerun done trying to get following EC numbers\n",
        "df_4_path = Path(base_dir / \"uniprot_4_missing_sorted.tsv\") # Final run in missing data\n",
        "\n",
        "df_1 = pd.read_csv(df_1_path, sep='\\t')\n",
        "df_2 = pd.read_csv(df_2_path, sep='\\t')\n",
        "df_3 = pd.read_csv(df_3_path, sep='\\t')\n",
        "df_4 = pd.read_csv(df_4_path, sep='\\t')\n",
        "print(len(df_1), len(df_2), len(df_3), len(df_4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9a2RbXR1uvgU",
        "outputId": "a11da5db-794a-4551-ebae-4221d3747435",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "51152"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "# extract EC and Genus from the Retrieved files, so I need to join them first\n",
        "retrieved = pd.concat([df_1, df_2, df_3, df_4], axis = 0)\n",
        "unique_pairs = ECcontri_otu[['EC', 'Genus']].drop_duplicates()\n",
        "len(unique_pairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dACO91ciYel5",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        },
        "outputId": "30e40cdb-d643-4d3b-f604-63fd9880b70b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          uniprot_id   ec_number  \\\n",
              "0        TERPD_PSESP  EC:1.1.1.1   \n",
              "1   A0A9D7J4U7_9ACTN  EC:1.1.1.1   \n",
              "2   A0A212K4D7_9BACT  EC:1.1.1.1   \n",
              "3   A0AA95FFE5_9FLAO  EC:1.1.1.1   \n",
              "4   A0A4D7AY54_9HYPH  EC:1.1.1.1   \n",
              "5   A0A949KZ85_9BACT  EC:1.1.1.1   \n",
              "6   A0A355D6P5_CLOSP  EC:1.1.1.1   \n",
              "7   A0A7W6D7B0_9HYPH  EC:1.1.1.1   \n",
              "8   A0A1H7Y3R7_9BACT  EC:1.1.1.1   \n",
              "9       D2MP74_9FIRM  EC:1.1.1.1   \n",
              "10  A0A943X7Y3_9SPIR  EC:1.1.1.1   \n",
              "11  A0A554XA12_9BURK  EC:1.1.1.1   \n",
              "12  A0A6N3DIC7_ENTCA  EC:1.1.1.1   \n",
              "13  A0A8I1RUE4_9BRAD  EC:1.1.1.1   \n",
              "14         ADH_CUPNE  EC:1.1.1.1   \n",
              "15  A0A1X6WTC8_9MICO  EC:1.1.1.1   \n",
              "16  A0A163QKV6_9CELL  EC:1.1.1.1   \n",
              "17  A0A212K4D7_9BACT  EC:1.1.1.1   \n",
              "18        ADH1_GEOSE  EC:1.1.1.1   \n",
              "19  A0A212K4D7_9BACT  EC:1.1.1.1   \n",
              "\n",
              "                                         protein_name  \\\n",
              "0         Probable alcohol dehydrogenase (EC 1.1.1.1)   \n",
              "1                  alcohol dehydrogenase (EC 1.1.1.1)   \n",
              "2    Fe-containing alcohol dehydrogenase (EC 1.1.1.1)   \n",
              "3   Iron-containing alcohol dehydrogenase (EC 1.1....   \n",
              "4                  alcohol dehydrogenase (EC 1.1.1.1)   \n",
              "5   Iron-containing alcohol dehydrogenase (EC 1.1....   \n",
              "6                  alcohol dehydrogenase (EC 1.1.1.1)   \n",
              "7                  Alcohol dehydrogenase (EC 1.1.1.1)   \n",
              "8                  alcohol dehydrogenase (EC 1.1.1.1)   \n",
              "9   Alcohol dehydrogenase, iron-dependent (EC 1.1....   \n",
              "10  Iron-containing alcohol dehydrogenase (EC 1.1....   \n",
              "11                 Alcohol dehydrogenase (EC 1.1.1.1)   \n",
              "12                 Alcohol dehydrogenase (EC 1.1.1.1)   \n",
              "13                 alcohol dehydrogenase (EC 1.1.1.1)   \n",
              "14  Alcohol dehydrogenase (EC 1.1.1.1) (EC 1.1.1.4...   \n",
              "15                 Alcohol dehydrogenase (EC 1.1.1.1)   \n",
              "16                 alcohol dehydrogenase (EC 1.1.1.1)   \n",
              "17   Fe-containing alcohol dehydrogenase (EC 1.1.1.1)   \n",
              "18         Alcohol dehydrogenase (EC 1.1.1.1) (ADH-T)   \n",
              "19   Fe-containing alcohol dehydrogenase (EC 1.1.1.1)   \n",
              "\n",
              "                                             organism  score  \n",
              "0                                      Pseudomonas sp    450  \n",
              "1                                    Tessaracoccus sp    450  \n",
              "2                         uncultured Desulfovibrio sp    450  \n",
              "3                                 Chryseobacterium sp    450  \n",
              "4                               Phreatobacter stygius    250  \n",
              "5                                 Desulfomicrobium sp    450  \n",
              "6                                      Clostridium sp    450  \n",
              "7                               Mycoplana azooxidifex    250  \n",
              "8                                Syntrophus gentianae    250  \n",
              "9                            Bulleidia extructa W1219    150  \n",
              "10                                       Treponema sp    450  \n",
              "11                              Tepidimonas charontis    250  \n",
              "12  Enterococcus casseliflavus (Enterococcus flave...    250  \n",
              "13                                          Afipia sp    450  \n",
              "14  Cupriavidus necator (Alcaligenes eutrophus) (R...    250  \n",
              "15                      Brachybacterium nesterenkovii    250  \n",
              "16                              Oerskovia enterophila    250  \n",
              "17                        uncultured Desulfovibrio sp    450  \n",
              "18  Geobacillus stearothermophilus (Bacillus stear...    250  \n",
              "19                        uncultured Desulfovibrio sp    450  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fd1c81c8-af4c-4ae7-a95c-1194e56520ff\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>uniprot_id</th>\n",
              "      <th>ec_number</th>\n",
              "      <th>protein_name</th>\n",
              "      <th>organism</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TERPD_PSESP</td>\n",
              "      <td>EC:1.1.1.1</td>\n",
              "      <td>Probable alcohol dehydrogenase (EC 1.1.1.1)</td>\n",
              "      <td>Pseudomonas sp</td>\n",
              "      <td>450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A0A9D7J4U7_9ACTN</td>\n",
              "      <td>EC:1.1.1.1</td>\n",
              "      <td>alcohol dehydrogenase (EC 1.1.1.1)</td>\n",
              "      <td>Tessaracoccus sp</td>\n",
              "      <td>450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A0A212K4D7_9BACT</td>\n",
              "      <td>EC:1.1.1.1</td>\n",
              "      <td>Fe-containing alcohol dehydrogenase (EC 1.1.1.1)</td>\n",
              "      <td>uncultured Desulfovibrio sp</td>\n",
              "      <td>450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A0AA95FFE5_9FLAO</td>\n",
              "      <td>EC:1.1.1.1</td>\n",
              "      <td>Iron-containing alcohol dehydrogenase (EC 1.1....</td>\n",
              "      <td>Chryseobacterium sp</td>\n",
              "      <td>450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A0A4D7AY54_9HYPH</td>\n",
              "      <td>EC:1.1.1.1</td>\n",
              "      <td>alcohol dehydrogenase (EC 1.1.1.1)</td>\n",
              "      <td>Phreatobacter stygius</td>\n",
              "      <td>250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>A0A949KZ85_9BACT</td>\n",
              "      <td>EC:1.1.1.1</td>\n",
              "      <td>Iron-containing alcohol dehydrogenase (EC 1.1....</td>\n",
              "      <td>Desulfomicrobium sp</td>\n",
              "      <td>450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>A0A355D6P5_CLOSP</td>\n",
              "      <td>EC:1.1.1.1</td>\n",
              "      <td>alcohol dehydrogenase (EC 1.1.1.1)</td>\n",
              "      <td>Clostridium sp</td>\n",
              "      <td>450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>A0A7W6D7B0_9HYPH</td>\n",
              "      <td>EC:1.1.1.1</td>\n",
              "      <td>Alcohol dehydrogenase (EC 1.1.1.1)</td>\n",
              "      <td>Mycoplana azooxidifex</td>\n",
              "      <td>250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>A0A1H7Y3R7_9BACT</td>\n",
              "      <td>EC:1.1.1.1</td>\n",
              "      <td>alcohol dehydrogenase (EC 1.1.1.1)</td>\n",
              "      <td>Syntrophus gentianae</td>\n",
              "      <td>250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>D2MP74_9FIRM</td>\n",
              "      <td>EC:1.1.1.1</td>\n",
              "      <td>Alcohol dehydrogenase, iron-dependent (EC 1.1....</td>\n",
              "      <td>Bulleidia extructa W1219</td>\n",
              "      <td>150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>A0A943X7Y3_9SPIR</td>\n",
              "      <td>EC:1.1.1.1</td>\n",
              "      <td>Iron-containing alcohol dehydrogenase (EC 1.1....</td>\n",
              "      <td>Treponema sp</td>\n",
              "      <td>450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>A0A554XA12_9BURK</td>\n",
              "      <td>EC:1.1.1.1</td>\n",
              "      <td>Alcohol dehydrogenase (EC 1.1.1.1)</td>\n",
              "      <td>Tepidimonas charontis</td>\n",
              "      <td>250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>A0A6N3DIC7_ENTCA</td>\n",
              "      <td>EC:1.1.1.1</td>\n",
              "      <td>Alcohol dehydrogenase (EC 1.1.1.1)</td>\n",
              "      <td>Enterococcus casseliflavus (Enterococcus flave...</td>\n",
              "      <td>250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>A0A8I1RUE4_9BRAD</td>\n",
              "      <td>EC:1.1.1.1</td>\n",
              "      <td>alcohol dehydrogenase (EC 1.1.1.1)</td>\n",
              "      <td>Afipia sp</td>\n",
              "      <td>450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>ADH_CUPNE</td>\n",
              "      <td>EC:1.1.1.1</td>\n",
              "      <td>Alcohol dehydrogenase (EC 1.1.1.1) (EC 1.1.1.4...</td>\n",
              "      <td>Cupriavidus necator (Alcaligenes eutrophus) (R...</td>\n",
              "      <td>250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>A0A1X6WTC8_9MICO</td>\n",
              "      <td>EC:1.1.1.1</td>\n",
              "      <td>Alcohol dehydrogenase (EC 1.1.1.1)</td>\n",
              "      <td>Brachybacterium nesterenkovii</td>\n",
              "      <td>250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>A0A163QKV6_9CELL</td>\n",
              "      <td>EC:1.1.1.1</td>\n",
              "      <td>alcohol dehydrogenase (EC 1.1.1.1)</td>\n",
              "      <td>Oerskovia enterophila</td>\n",
              "      <td>250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>A0A212K4D7_9BACT</td>\n",
              "      <td>EC:1.1.1.1</td>\n",
              "      <td>Fe-containing alcohol dehydrogenase (EC 1.1.1.1)</td>\n",
              "      <td>uncultured Desulfovibrio sp</td>\n",
              "      <td>450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>ADH1_GEOSE</td>\n",
              "      <td>EC:1.1.1.1</td>\n",
              "      <td>Alcohol dehydrogenase (EC 1.1.1.1) (ADH-T)</td>\n",
              "      <td>Geobacillus stearothermophilus (Bacillus stear...</td>\n",
              "      <td>250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>A0A212K4D7_9BACT</td>\n",
              "      <td>EC:1.1.1.1</td>\n",
              "      <td>Fe-containing alcohol dehydrogenase (EC 1.1.1.1)</td>\n",
              "      <td>uncultured Desulfovibrio sp</td>\n",
              "      <td>450</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fd1c81c8-af4c-4ae7-a95c-1194e56520ff')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-fd1c81c8-af4c-4ae7-a95c-1194e56520ff button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-fd1c81c8-af4c-4ae7-a95c-1194e56520ff');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-2b7fd35a-de7a-4fe2-9c53-0e361df4ec8c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2b7fd35a-de7a-4fe2-9c53-0e361df4ec8c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-2b7fd35a-de7a-4fe2-9c53-0e361df4ec8c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "retrieved",
              "summary": "{\n  \"name\": \"retrieved\",\n  \"rows\": 87948,\n  \"fields\": [\n    {\n      \"column\": \"uniprot_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 45393,\n        \"samples\": [\n          \"A0A3B9FDP0_9BACT\",\n          \"D5X7Y7_THEPJ\",\n          \"A0A0D0HV06_9BACI\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ec_number\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1653,\n        \"samples\": [\n          \"EC:1.3.1.9\",\n          \"EC:1.1.1.370\",\n          \"EC:2.3.1.59\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"protein_name\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5766,\n        \"samples\": [\n          \"Citrate lyase beta subunit (EC 4.1.3.24)\",\n          \"Phosphohexose mutases (EC 5.4.2.2, EC 5.4.2.8)\",\n          \"Ribokinase (EC 2.7.1.15)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"organism\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1924,\n        \"samples\": [\n          \"Desulfosporosinus sp. Tol-M\",\n          \"Novosphingobium silvae\",\n          \"Hydrogenibacillus schlegelii (Bacillus schlegelii)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 133,\n        \"min\": -50,\n        \"max\": 650,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          450,\n          250,\n          650\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "retrieved.head(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwbF_HFmuvgU"
      },
      "source": [
        "Extracting the Genus from the retrieved_pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jfKMErDWuvgU",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Function to extract the Genus from the organism str\n",
        "def extract_genus(organism_str):\n",
        "    # Assumes Genus is the first word that starts with an uppercase letter.\n",
        "    match = re.search(r'([A-Z][a-z]+)', organism_str)\n",
        "    return match.group(1) if match else None\n",
        "# Creating a Genus column in the retrieved dataframe.\n",
        "retrieved['Genus'] = retrieved['organism'].astype(str).apply(extract_genus)\n",
        "\n",
        "# if there are duplicates, we want the best entry based on score:\n",
        "retrieved_unique = retrieved.sort_values('score', ascending=False)\\\n",
        "                            .drop_duplicates(subset=['ec_number', 'Genus'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoQasxuauvgU",
        "outputId": "60a17ca6-5378-4bde-88f2-8572f6bcc219",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1491288, 16)\n"
          ]
        }
      ],
      "source": [
        "# Merging using a left join on the two keys (EC_number and Genus). Plus an indicator of missing data.\n",
        "ECcontri_Uniprot  = pd.merge(\n",
        "    ECcontri_otu,\n",
        "    retrieved_unique[['ec_number', 'Genus', 'protein_name', 'score', 'uniprot_id']],\n",
        "    left_on=['EC', 'Genus'],\n",
        "    right_on=['ec_number', 'Genus'],\n",
        "    how='left',\n",
        "    suffixes=('', '_retr')\n",
        ")\n",
        "print(ECcontri_Uniprot.shape) # Very slow 1 minute, can kill the kernel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elCz1V27O-E3",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "ECcontri_Uniprot = ECcontri_Uniprot.drop(columns = [\"OTU\",\t\"EC\",\t\"npath\", \"ec_number\",\t\"score\",\t\"uniprot_id\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qA_nz2qFuvgU"
      },
      "source": [
        "ECcontri_uniprot_info is the final df mixed and is keep for reference only purposes. With the missing unique df I will retrive again the rest of the missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6iKlSuHuvgU",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "a175d2ab-a8d3-4eee-a66c-82a52478899b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'#Rows with no match from retrieved_unique will have \\'_merge\\' value of \\'left_only\\'\\nmerged_unique = pd.merge(\\n    unique_pairs,\\n    retrieved_unique,\\n    left_on=[\\'EC\\', \\'Genus\\'],\\n    right_on=[\\'ec_number\\', \\'Genus\\'],\\n    how=\\'left\\',\\n    indicator=True\\n)\\n\\n# Filter unique pairs missing from retrieved data\\nECcontri_missing = merged_unique[merged_unique[\\'_merge\\'] == \\'left_only\\']\\nprint(\"Missing unique pairs count:\", ECcontri_missing.shape[0])\\nECcontri_missing = ECcontri_missing[[\"EC\", \"Genus\"]]\\nfile_path = os.path.join(output_dir, \"ECcontri_missing.tsv\")\\nECcontri_missing.to_csv(file_path, sep=\\'\\t\\', index=False)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "'''#Rows with no match from retrieved_unique will have '_merge' value of 'left_only'\n",
        "merged_unique = pd.merge(\n",
        "    unique_pairs,\n",
        "    retrieved_unique,\n",
        "    left_on=['EC', 'Genus'],\n",
        "    right_on=['ec_number', 'Genus'],\n",
        "    how='left',\n",
        "    indicator=True\n",
        ")\n",
        "\n",
        "# Filter unique pairs missing from retrieved data\n",
        "ECcontri_missing = merged_unique[merged_unique['_merge'] == 'left_only']\n",
        "print(\"Missing unique pairs count:\", ECcontri_missing.shape[0])\n",
        "ECcontri_missing = ECcontri_missing[[\"EC\", \"Genus\"]]\n",
        "file_path = os.path.join(output_dir, \"ECcontri_missing.tsv\")\n",
        "ECcontri_missing.to_csv(file_path, sep='\\t', index=False)'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVnGj7D_-PJL"
      },
      "source": [
        "### Cleaning anc collecting garbage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayi-4RIN-PJL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "92cbf479-2ac1-4c65-8d7f-1e87f40065b5"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'record' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-645d681019e1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#del results_EPA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#del results_SEPP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mreact\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mreaction_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'record' is not defined"
          ]
        }
      ],
      "source": [
        "#del results_EPA\n",
        "#del results_SEPP\n",
        "del record\n",
        "del react\n",
        "del reaction_list\n",
        "del consistency_check\n",
        "del mean_abundances_react\n",
        "del EC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2rdKHvKj-PJM"
      },
      "outputs": [],
      "source": [
        "del picrust_long\n",
        "del retrieved\n",
        "del retrieved_unique\n",
        "del unique_pairs\n",
        "del df_1\n",
        "del df_2\n",
        "del df_3\n",
        "del df_4\n",
        "del ECcontri\n",
        "del ECcontri_agg_site\n",
        "del ECcontri_otu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwyO_mme-PJM",
        "outputId": "9cc84499-424b-4895-df0b-364b4c18af20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8971"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d2KfaHxuvgU"
      },
      "source": [
        "### Data Retrieval Completion Note\n",
        "After multiple retrieval attempts, 12,656 pairs remain unmapped out of approximately 1,500,000 total rows (0.84%). Given this small percentage and the diminishing returns from further retrieval attempts, we concluded that this level of completeness is acceptable for analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QY8Rxk24uvgU"
      },
      "source": [
        "## 8.7. Making an Integrated Picrust Result df: genera_matrix\n",
        "the data is found now in a long format, for the next plotting we will be grouping it and making it onto a new df. We can group by Site and Genus (or even by protein_name) to look at the overall enzymatic contributions and how they correlate with the risk categories. The pathways information is actually very dense, it has some entries upto 300 pathway for site. So in this way we do a pathway chart but wont be bringing it to the next visualisations. The aim being to identify which genera or enzymes are most abundant in high-risk sites, assess if the presence of certain metal-specific enzymes (like Fe-dependent dehydrogenases) is linked to corrosion risk, ultimately potentially filter out common background organisms that are less relevant and perhaps just endemic part of the water systems in general and no specific to corrosion. So a table will be created with following information\n",
        "|Sites|Genus|protein_name|norm_abund_contri|*Category|\n",
        "|--|--|--|--|--|\n",
        "\n",
        "*where category will be utilised for colouring purposes.   \n",
        "\n",
        "__Pivoting on Two Variables:__\n",
        "Using a pivot table with both Genus and protein_name as column levels is attempted in the next snipped, in order to capture contributions at that level. This will:\n",
        "- Generate a quantitative view of normalized abundance contributions per site\n",
        "- Maintain the hierarchical relationship between genera and their enzymes\n",
        "- Create a separate metabolic information matrix for pathway interpretation\n",
        "This structure allows to analyze both taxonomic patterns and specific enzyme contributions while maintaining the ability to link back to corrosion risk categories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T09:55:00.338798Z",
          "iopub.status.busy": "2025-02-19T09:55:00.338377Z",
          "iopub.status.idle": "2025-02-19T09:55:00.346377Z",
          "shell.execute_reply": "2025-02-19T09:55:00.344664Z",
          "shell.execute_reply.started": "2025-02-19T09:55:00.338766Z"
        },
        "id": "s_4Fb88NuvgV",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Define category dict outside so that all charts can use same dict\n",
        "category_dict = Integrated_T.T.iloc[0, 0:-1].astype(int).to_dict()\n",
        "\n",
        "# Define colors and categories\n",
        "category_colors = {1: '#008800',  # Dark green\n",
        "                   2: '#FF8C00',  # Dark orange\n",
        "                   3: '#FF0000'}   # Red\n",
        "\n",
        "categories_labels = {1: 'Normal Operation',\n",
        "              2: 'Early Warning',\n",
        "              3: 'System Failure'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpwCWh1JYel5"
      },
      "source": [
        "### Cleaning Proteins Names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjkVRoV9Yel5",
        "outputId": "629a2e7b-e97f-4ad7-f3e6-4928fc75e15e",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing cleaning function:\n",
            "\n",
            "Original:  1,4-alpha-glucan branching enzyme GlgB -glucan branching enzyme\n",
            "Cleaned:   1,4-alpha-glucan branching enzyme GlgB -glucan\n",
            "\n",
            "Original:  Gluconate 5-dehydrogenase enzyme (EC 1.1.1.69) dehydrogenase\n",
            "Cleaned:   Gluconate 5-dehydrogenase enzyme dehydrogenase\n",
            "\n",
            "Original:  synthase protein synthase (EC 2.2.1.6) protein\n",
            "Cleaned:   synthase protein\n"
          ]
        }
      ],
      "source": [
        "def clean_protein_name(name):\n",
        "    \"\"\"\n",
        "    Enhanced protein name cleaning:\n",
        "    1. Remove EC numbers unless it's the only information\n",
        "    2. Remove redundant information in parentheses\n",
        "    3. Remove duplicated terms\n",
        "    4. Handle special cases\n",
        "    \"\"\"\n",
        "    if pd.isna(name):\n",
        "        return \"Uncharacterized protein\"\n",
        "\n",
        "    # If the name is just an EC number in any format, return it\n",
        "    if re.match(r'^[\\s\\(\\)]*EC\\s*[\\d\\.]+[\\s\\(\\)]*$', name):\n",
        "        return name.strip()\n",
        "\n",
        "    # Remove EC numbers and content in parentheses\n",
        "    name = re.sub(r'\\(EC\\s*[\\d\\.]+\\)', '', name)\n",
        "    name = re.sub(r'\\([^)]*\\)', '', name)\n",
        "\n",
        "    # Split into words and remove duplicates while preserving order\n",
        "    words = name.split()\n",
        "    seen = set()\n",
        "    unique_words = []\n",
        "    for word in words:\n",
        "        # Convert to lowercase for comparison but keep original case in result\n",
        "        lower_word = word.lower()\n",
        "        if lower_word not in seen:\n",
        "            seen.add(lower_word)\n",
        "            unique_words.append(word)\n",
        "\n",
        "    # Rejoin words\n",
        "    name = ' '.join(unique_words)\n",
        "\n",
        "    # Remove specific redundant patterns\n",
        "    redundant_patterns = [\n",
        "        (r'enzyme\\s+enzyme', 'enzyme'),\n",
        "        (r'synthase\\s+synthase', 'synthase'),\n",
        "        (r'transferase\\s+transferase', 'transferase'),\n",
        "        (r'-glucan\\s+glucan', 'glucan'),\n",
        "        (r'protein\\s+protein', 'protein')\n",
        "    ]\n",
        "\n",
        "    for pattern, replacement in redundant_patterns:\n",
        "        name = re.sub(pattern, replacement, name, flags=re.IGNORECASE)\n",
        "\n",
        "    return name.strip()\n",
        "\n",
        "def check_cleaning(df, n_samples=10):\n",
        "    \"\"\"\n",
        "    Check the cleaning results with before/after comparison\n",
        "    \"\"\"\n",
        "    sample_names = df['protein_name'].dropna().sample(n=n_samples)\n",
        "    cleaned_names = sample_names.apply(clean_protein_name)\n",
        "\n",
        "    print(\"Sample of name cleaning results:\")\n",
        "    for orig, cleaned in zip(sample_names, cleaned_names):\n",
        "        print(f\"\\nOriginal:  {orig}\")\n",
        "        print(f\"Cleaned:   {cleaned}\")\n",
        "\n",
        "# Test\n",
        "test_names = [\n",
        "    \"1,4-alpha-glucan branching enzyme GlgB -glucan branching enzyme\",\n",
        "    \"Gluconate 5-dehydrogenase enzyme (EC 1.1.1.69) dehydrogenase\",\n",
        "    \"synthase protein synthase (EC 2.2.1.6) protein\",\n",
        "]\n",
        "\n",
        "print(\"Testing cleaning function:\")\n",
        "for name in test_names:\n",
        "    cleaned = clean_protein_name(name)\n",
        "    print(f\"\\nOriginal:  {name}\")\n",
        "    print(f\"Cleaned:   {cleaned}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0hwxPe7Yel5"
      },
      "source": [
        "### Creating Basics Matrixes for Normalise and Relative abundances\n",
        "\n",
        "> Add blockquote\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jkv-id-auvgV",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def create_matrix(df):\n",
        "    \"\"\"\n",
        "    Creates the base matrix with cleaned protein names\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    check_cleaning(df)\n",
        "\n",
        "    df['protein_name'] = df['protein_name'].apply(clean_protein_name)\n",
        "\n",
        "    # Create pivot table\n",
        "    base_matrix = df.pivot_table(\n",
        "        values='norm_abund_contri',\n",
        "        index='Sites',\n",
        "        columns=['Genus', 'protein_name'],\n",
        "        aggfunc='first',\n",
        "        fill_value=0,\n",
        "        observed=True\n",
        "    )\n",
        "\n",
        "    return base_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhbnsAc4Yel5",
        "outputId": "ef422076-eff2-4350-b1ae-f00729ed9db6",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample of name cleaning results:\n",
            "\n",
            "Original:  Alpha-1,4 glucan phosphorylase (EC 2.4.1.1)\n",
            "Cleaned:   Alpha-1,4 glucan phosphorylase\n",
            "\n",
            "Original:  Quinone oxidoreductase 1 (EC 1.6.5.5)\n",
            "Cleaned:   Quinone oxidoreductase 1\n",
            "\n",
            "Original:  Succinate-semialdehyde dehydrogenase I, NADP-dependent (EC 1.2.1.16)\n",
            "Cleaned:   Succinate-semialdehyde dehydrogenase I, NADP-dependent\n",
            "\n",
            "Original:  Cadmium-translocating P-type ATPase (EC 3.6.3.3)\n",
            "Cleaned:   Cadmium-translocating P-type ATPase\n",
            "\n",
            "Original:  Pyruvate kinase (EC 2.7.1.40)\n",
            "Cleaned:   Pyruvate kinase\n",
            "\n",
            "Original:  Adenylate kinase (AK) (EC 2.7.4.3) (ATP-AMP transphosphorylase) (ATP:AMP phosphotransferase) (Adenylate monophosphate kinase)\n",
            "Cleaned:   Adenylate kinase\n",
            "\n",
            "Original:  dTDP-4-dehydrorhamnose reductase (EC 1.1.1.133)\n",
            "Cleaned:   dTDP-4-dehydrorhamnose reductase\n",
            "\n",
            "Original:  Glycine dehydrogenase (decarboxylating) (EC 1.4.4.2) (Glycine cleavage system P-protein) (Glycine decarboxylase) (Glycine dehydrogenase (aminomethyl-transferring))\n",
            "Cleaned:   Glycine dehydrogenase )\n",
            "\n",
            "Original:  tRNA pseudouridine synthase B (EC 5.4.99.25) (tRNA pseudouridine(55) synthase) (Psi55 synthase) (tRNA pseudouridylate synthase) (tRNA-uridine isomerase)\n",
            "Cleaned:   tRNA pseudouridine synthase B synthase)\n",
            "\n",
            "Original:  Tryptophan synthase beta chain (EC 4.2.1.20)\n",
            "Cleaned:   Tryptophan synthase beta chain\n"
          ]
        }
      ],
      "source": [
        "# Create and save the matrix, this is the killer kernel operation number one takes over 1-4 min if works\n",
        "base_matrix = create_matrix(ECcontri_Uniprot)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZKLqOyV2XC2"
      },
      "source": [
        "# Creating a Matrix for relative Abundance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-02-19T09:53:38.875032Z",
          "iopub.status.busy": "2025-02-19T09:53:38.874631Z",
          "iopub.status.idle": "2025-02-19T09:53:38.913843Z",
          "shell.execute_reply": "2025-02-19T09:53:38.912614Z",
          "shell.execute_reply.started": "2025-02-19T09:53:38.875001Z"
        },
        "id": "QVoGKd6EYel5",
        "outputId": "e1ff5c9b-d383-4d28-a0b8-8ea7b6ed1835",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample of name cleaning results:\n",
            "\n",
            "Original:  Tryptophan 2,3-dioxygenase (TDO) (EC 1.13.11.11) (Tryptamin 2,3-dioxygenase) (Tryptophan oxygenase) (TO) (TRPO) (Tryptophan pyrrolase) (Tryptophanase)\n",
            "Cleaned:   Tryptophan 2,3-dioxygenase\n",
            "\n",
            "Original:  Glucose-6-phosphate isomerase (EC 5.3.1.9)\n",
            "Cleaned:   Glucose-6-phosphate isomerase\n",
            "\n",
            "Original:  UDP-N-acetylmuramate--L-alanine ligase (EC 6.3.2.8) (UDP-N-acetylmuramoyl-L-alanine synthetase)\n",
            "Cleaned:   UDP-N-acetylmuramate--L-alanine ligase\n",
            "\n",
            "Original:  Lipoyl synthase (EC 2.8.1.8) (Lip-syn) (LS) (Lipoate synthase) (Lipoic acid synthase) (Sulfur insertion protein LipA)\n",
            "Cleaned:   Lipoyl synthase\n",
            "\n",
            "Original:  Geranyltranstransferase (EC 2.5.1.1, EC 2.5.1.10)\n",
            "Cleaned:   Geranyltranstransferase\n",
            "\n",
            "Original:  Peptide deformylase (PDF) (EC 3.5.1.88) (Polypeptide deformylase)\n",
            "Cleaned:   Peptide deformylase\n",
            "\n",
            "Original:  Vanillate O-demethylase monooxygenase subunit (EC 1.14.13.82)\n",
            "Cleaned:   Vanillate O-demethylase monooxygenase subunit\n",
            "\n",
            "Original:  Transketolase (EC 2.2.1.1)\n",
            "Cleaned:   Transketolase\n",
            "\n",
            "Original:  Acyl-[acyl-carrier-protein]--UDP-N-acetylglucosamine O-acyltransferase (UDP-N-acetylglucosamine acyltransferase) (EC 2.3.1.129)\n",
            "Cleaned:   Acyl-[acyl-carrier-protein]--UDP-N-acetylglucosamine O-acyltransferase\n",
            "\n",
            "Original:  Probable dual-specificity RNA methyltransferase RlmN (EC 2.1.1.192) (23S rRNA (adenine(2503)-C(2))-methyltransferase) (23S rRNA m2A2503 methyltransferase) (Ribosomal RNA large subunit methyltransferase N) (tRNA (adenine(37)-C(2))-methyltransferase) (tRNA m2A37 methyltransferase)\n",
            "Cleaned:   Probable dual-specificity RNA methyltransferase RlmN -C)-methyltransferase)\n",
            "Matrix shape: (70, 37089)\n",
            "Index type: <class 'pandas.core.indexes.base.Index'>\n",
            "Index name: Sites\n"
          ]
        }
      ],
      "source": [
        "def create_matrix_rel(df):\n",
        "    \"\"\"\n",
        "    Creates the base matrix with relative abundance with cleaned protein names\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "\n",
        "    check_cleaning(df)\n",
        "\n",
        "    df['protein_name'] = df['protein_name'].apply(clean_protein_name)\n",
        "\n",
        "    # Create pivot table\n",
        "    base_matrix_relative = df.pivot_table(\n",
        "        values='rel_abund_raw',\n",
        "        index='Sites',\n",
        "        columns=['Genus', 'protein_name'],\n",
        "        aggfunc='first',\n",
        "        fill_value=0,\n",
        "        observed=True\n",
        "    )\n",
        "\n",
        "    # Ensure clean single-level index\n",
        "    if isinstance(base_matrix_relative.index, pd.MultiIndex):\n",
        "        base_matrix_relative = base_matrix_relative.reset_index()\n",
        "        base_matrix_relative = base_matrix_relative.set_index('Sites')\n",
        "\n",
        "    print(f\"Matrix shape: {base_matrix_relative.shape}\")\n",
        "    print(f\"Index type: {type(base_matrix_relative.index)}\")\n",
        "    print(f\"Index name: {base_matrix_relative.index.name}\")\n",
        "\n",
        "    return base_matrix_relative\n",
        "\n",
        "base_matrix_relative = create_matrix_rel(ECcontri_Uniprot)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fk6wmJJ9Yel5"
      },
      "source": [
        "### Saving by Parts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EudW9WXbKN7j",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "'''# Save components separately\n",
        "def save_matrix_components(base_matrix, base_path='matrix'):\n",
        "    \"\"\"\n",
        "    Save matrix components separately:\n",
        "    - Index (Sites)\n",
        "    - Column levels (Genus and protein_name)\n",
        "    - Values\n",
        "    \"\"\"\n",
        "    # Save index (Sites)\n",
        "    pd.Series(base_matrix.index).to_csv(f\"{base_path}_sites.csv\")\n",
        "\n",
        "    # Save column levels separately\n",
        "    for i, name in enumerate(base_matrix.columns.names):\n",
        "        level_values = base_matrix.columns.get_level_values(i)\n",
        "        pd.Series(level_values).to_csv(f\"{base_path}_columns_{name}.csv\")\n",
        "\n",
        "    # Save the actual values as numpy array\n",
        "    np.save(f\"{base_path}_values.npy\", base_matrix.values)\n",
        "\n",
        "    print(\"Components saved:\")\n",
        "    print(f\"- Sites: {base_path}_sites.csv\")\n",
        "    print(f\"- Genus: {base_path}_columns_Genus.csv\")\n",
        "    print(f\"- Proteins: {base_path}_columns_protein_name.csv\")\n",
        "    print(f\"- Values: {base_path}_values.npy\")'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMV77p_Q-PJM"
      },
      "outputs": [],
      "source": [
        "'''base_matrix_path = output_dir / \"base_matrix\"\n",
        "# Save the components\n",
        "save_matrix_components(base_matrix_path, base_path=base_matrix_path)'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieVnJDEkYel6"
      },
      "source": [
        "### Introducing Risk Category for Plotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-02-19T09:53:51.013159Z",
          "iopub.status.busy": "2025-02-19T09:53:51.012759Z",
          "iopub.status.idle": "2025-02-19T09:53:51.072579Z",
          "shell.execute_reply": "2025-02-19T09:53:51.070761Z",
          "shell.execute_reply.started": "2025-02-19T09:53:51.013128Z"
        },
        "id": "1nCWQxjkYel6",
        "outputId": "83cb09f9-5bb8-4673-82d9-9f47a7dfe3fa",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cell executed at: 2025-02-25 13:18:42.644375\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "print(f\"Cell executed at: {datetime.now()}\")\n",
        "\n",
        "# First ensure sites are properly ordered numerically. Get current index and sort it\n",
        "current_index = base_matrix.index\n",
        "sorted_index = sorted(current_index, key=lambda x: int(x.split('_')[1]))\n",
        "base_matrix = base_matrix.reindex(sorted_index)\n",
        "\n",
        "# Now add the category mapping\n",
        "if category_dict is not None:\n",
        "    category_mapping = pd.Series(base_matrix.index.map(category_dict),\n",
        "                               index=base_matrix.index,\n",
        "                               name='Category')\n",
        "    base_matrix.index = pd.MultiIndex.from_arrays([base_matrix.index, category_mapping],\n",
        "                                                 names=['Sites', 'Category'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T09:52:47.158796Z",
          "iopub.status.busy": "2025-02-19T09:52:47.158361Z",
          "iopub.status.idle": "2025-02-19T09:52:47.167198Z",
          "shell.execute_reply": "2025-02-19T09:52:47.165854Z",
          "shell.execute_reply.started": "2025-02-19T09:52:47.158763Z"
        },
        "id": "UopuTKppYel6",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# First ensure sites are properly ordered numerically. Get current index and sort it\n",
        "current_index = base_matrix_relative.index\n",
        "sorted_index = sorted(current_index, key=lambda x: int(x.split('_')[1]))\n",
        "base_matrix_relative = base_matrix_relative.reindex(sorted_index)\n",
        "\n",
        "# Now add the category mapping\n",
        "if category_dict is not None:\n",
        "    category_mapping = pd.Series(base_matrix_relative.index.map(category_dict),\n",
        "                               index=base_matrix_relative.index,\n",
        "                               name='Category')\n",
        "    base_matrix_relative.index = pd.MultiIndex.from_arrays([base_matrix_relative.index, category_mapping],\n",
        "                                                 names=['Sites', 'Category'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LOdsv2xYel6",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "'''# Saving Colab or VSCode\n",
        "file_path = os.path.join(output_dir, \"category_mapping\")\n",
        "\n",
        "category_mapping.to_csv(file_path, sep='\\t', index=False)'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_o2VvH3Yel6"
      },
      "source": [
        "## 8.8. Making Metabolic Sites information df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T10:08:51.95245Z",
          "iopub.status.busy": "2025-02-19T10:08:51.95208Z",
          "iopub.status.idle": "2025-02-19T10:08:51.959001Z",
          "shell.execute_reply": "2025-02-19T10:08:51.957396Z",
          "shell.execute_reply.started": "2025-02-19T10:08:51.952425Z"
        },
        "id": "EQAqPzKuIyhU",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def metabolic_sites_info(df):\n",
        "    \"\"\"\n",
        "    Create metabolic information DataFrame with site-specific aggregation.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : pandas.Data Input ECcontri_Uniprot DataFrame with 'Sites', 'Genus', 'pathway', 'protein_name' columns\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    pandas.DataFrame Aggregated metabolic information with sites preserved\n",
        "    \"\"\"\n",
        "    def safe_join(x):\n",
        "        return ', '.join(sorted(set(x.dropna().astype(str))))\n",
        "\n",
        "    # Group by both Sites and Genus to preserve site information\n",
        "    metabolic_info = df.groupby(['Sites', 'Genus'], observed=True).agg({\n",
        "        'pathway': safe_join,\n",
        "        'protein_name': safe_join,\n",
        "        'norm_abund_contri': 'sum'  # Add abundance information\n",
        "    }).rename(columns={\n",
        "        'pathway': 'Pathways',\n",
        "        'protein_name': 'Protein_Names',\n",
        "        'norm_abund_contri': 'norm_abund_contri'\n",
        "    })\n",
        "\n",
        "    return metabolic_info\n",
        "\n",
        "metabolic_sites_info = metabolic_sites_info(ECcontri_Uniprot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T10:08:55.017135Z",
          "iopub.status.busy": "2025-02-19T10:08:55.016713Z",
          "iopub.status.idle": "2025-02-19T10:11:13.307708Z",
          "shell.execute_reply": "2025-02-19T10:11:13.306289Z",
          "shell.execute_reply.started": "2025-02-19T10:08:55.017101Z"
        },
        "id": "mGkhsxDpYel6",
        "trusted": true
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T10:11:13.310193Z",
          "iopub.status.busy": "2025-02-19T10:11:13.309714Z",
          "iopub.status.idle": "2025-02-19T10:11:13.324335Z",
          "shell.execute_reply": "2025-02-19T10:11:13.322739Z",
          "shell.execute_reply.started": "2025-02-19T10:11:13.310147Z"
        },
        "id": "-oiPF4s6Yel6",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "metabolic_sites_info.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Js54enJiYel6",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "base_matrix.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "buF9K_ksYel6",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "print(metabolic_sites_info.shape, base_matrix.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()"
      ],
      "metadata": {
        "id": "bST2L4pyBbiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlhGQdAB-PJN"
      },
      "source": [
        "### Cleaning and collecting garbage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTM64OQ1-PJN"
      },
      "outputs": [],
      "source": [
        "'''del ECcontri_Uniprot\n",
        "\n",
        "gc.collect()'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPA8kjB2IENR",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "'''# Saving on Colab or VSCode\n",
        "metabolic_sites_info= metabolic_sites_info(ECcontri_Uniprot)\n",
        "# Saving just in case\n",
        "file_path = os.path.join(output_dir, \"metabolic_sites_info.tsv\")\n",
        "metabolic_sites_info.to_csv(file_path, sep='\\t', index=False)'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUmCgiWWAx9A"
      },
      "source": [
        "## 8.9. Reading the Files\n",
        "The local machine struggled to work with the files, and killed the kernel, so Colab was continued to be used, however even with the high memory availbable it was struggling with memory fragmentation, it is believed the complexity of the dataa make the problem, chucking didnt improve problem, so the notebook was continued to be worked in Kaggle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9OF_EJwYel6",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "'''# Reading for Kaggle\n",
        "import shutil\n",
        "\n",
        "output_dir = Path(\"/kaggle/working/\")\n",
        "# Path to your uploaded dataset\n",
        "dataset_path = Path(\"/kaggle/input/results-basic\")\n",
        "\n",
        "try:\n",
        "    shutil.copytree(dataset_path, output_dir / dataset_path.name, dirs_exist_ok=True) # Copies the entire directory structure\n",
        "    print(f\"Directory '{dataset_path.name}' copied successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while copying the directory: {e}\")'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pw5qVLUcOuTl",
        "outputId": "ec469117-307a-4d3e-b714-d10143ecf289",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "System Memory Details:\n",
            "Total: 50.99 GB\n",
            "Available: 47.57 GB\n",
            "Used: 2.82 GB\n",
            "Free: 40.96 GB\n",
            "Percent used: 6.7%\n",
            "\n",
            "Process Memory Details:\n",
            "RSS (Physical): 1.74 GB\n",
            "VMS (Virtual): 7.37 GB\n"
          ]
        }
      ],
      "source": [
        "import psutil\n",
        "import os\n",
        "\n",
        "def detailed_memory_check():\n",
        "    # Get memory info\n",
        "    mem = psutil.virtual_memory()\n",
        "\n",
        "    # Get process memory info\n",
        "    process = psutil.Process(os.getpid())\n",
        "    process_mem = process.memory_info()\n",
        "\n",
        "    print(\"System Memory Details:\")\n",
        "    print(f\"Total: {mem.total/1024**3:.2f} GB\")\n",
        "    print(f\"Available: {mem.available/1024**3:.2f} GB\")\n",
        "    print(f\"Used: {mem.used/1024**3:.2f} GB\")\n",
        "    print(f\"Free: {mem.free/1024**3:.2f} GB\")\n",
        "    print(f\"Percent used: {mem.percent}%\")\n",
        "\n",
        "    print(\"\\nProcess Memory Details:\")\n",
        "    print(f\"RSS (Physical): {process_mem.rss/1024**3:.2f} GB\")\n",
        "    print(f\"VMS (Virtual): {process_mem.vms/1024**3:.2f} GB\")\n",
        "\n",
        "detailed_memory_check()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jx4tPQEG-PJN"
      },
      "source": [
        "without scrpy\n",
        "System Memory Details:\n",
        "Total: 2.91 GB\n",
        "Available: 0.42 GB\n",
        "Used: 2.32 GB\n",
        "Free: 0.36 GB\n",
        "Percent used: 85.6%\n",
        "\n",
        "Process Memory Details:\n",
        "RSS (Physical): 0.85 GB\n",
        "VMS (Virtual): 2.14 GB\n",
        "\n",
        "with scrpy but without all other called cells just the necesary , this is without deleting another massive df which is intriguing\n",
        "System Memory Details:\n",
        "Total: 2.91 GB\n",
        "Available: 0.57 GB\n",
        "Used: 2.16 GB\n",
        "Free: 0.48 GB\n",
        "Percent used: 80.3%\n",
        "\n",
        "Process Memory Details:\n",
        "RSS (Physical): 0.86 GB\n",
        "VMS (Virtual): 2.22 GB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZ1D49QFYel6",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "'''# Reading\n",
        "metabolic_info_path  = Path(output_dir / \"metabolic_sites_info.tsv\")\n",
        "metabolic_sites_info = pd.read_csv(metabolic_info_path, sep='\\t',  low_memory=False)'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsyxkAFuuvgV"
      },
      "source": [
        "# 9. Analysign the Dominant Protein Enzymes, Pathways and Genes with the Principal Component Loadings\n",
        "Following script analyse the dominant Protein Enzymes, Pathways and Genes contributing to the first two PCs comming from section 7.1. The risk label is use here to color code the hue.\n",
        "## 9.1. Principal Components of Genera vs Risk Category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T10:11:29.641918Z",
          "iopub.status.busy": "2025-02-19T10:11:29.641549Z",
          "iopub.status.idle": "2025-02-19T10:11:30.984997Z",
          "shell.execute_reply": "2025-02-19T10:11:30.983614Z",
          "shell.execute_reply.started": "2025-02-19T10:11:29.641883Z"
        },
        "id": "IorbqV1wZ4ng",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def prepare_genera_pca(base_matrix, category_mapping=None):\n",
        "    \"\"\"\n",
        "    Prepare genera data for PCA with handling of multi-index categories\n",
        "\n",
        "    Parameters: base_matrix : pandas.DataFrame, Matrix with multi-index (Sites, Category)\n",
        "                category_mapping : pandas.Series,  Category mapping\n",
        "\n",
        "    Returns:    X_pca : numpy.ndarray, PCA transformed data\n",
        "                explained_variance_ratio : numpy.ndarray  Explained variance ratios\n",
        "                loadings : pandas.DataFrame, PCA loadings\n",
        "                categories : pandas.Series,  Categories for each site\n",
        "    \"\"\"\n",
        "    # Extract categories if they're in the multi-index\n",
        "    if isinstance(base_matrix.index, pd.MultiIndex):\n",
        "        categories = base_matrix.index.get_level_values('Category')\n",
        "        # No need to drop category as it's in the index\n",
        "        X = base_matrix\n",
        "    else:\n",
        "        # Use provided category mapping or None\n",
        "        categories = category_mapping\n",
        "        X = base_matrix\n",
        "\n",
        "    # No need for iloc[1:] as we don't have enzyme names as first row anymore\n",
        "    X_for_scaling = X.astype(float)\n",
        "\n",
        "    # Standardize\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X_for_scaling)\n",
        "\n",
        "    # PCA\n",
        "    pca = PCA(n_components=2)\n",
        "    X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "    # Create loadings DataFrame with proper multi-index columns\n",
        "    loadings = pd.DataFrame(\n",
        "        pca.components_.T,\n",
        "        index=X.columns,  # preserving multi-index columns (Genus, protein_name)\n",
        "        columns=['PC1', 'PC2']\n",
        "    )\n",
        "\n",
        "    return X_pca, pca.explained_variance_ratio_, loadings, categories\n",
        "\n",
        "def plot_pca_results(X_pca, explained_variance, Category, title, category_colors, categories_labels,\n",
        "                     pc1_idx=0, pc2_idx=1):  # Add parameters for component indices\n",
        "    \"\"\"\n",
        "    Plot PCA with risk categories\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    X_pca : numpy array   PCA transformed data\n",
        "    explained_variance : numpy array        Explained variance ratios\n",
        "    Category : array-like  Category labels for each sample\n",
        "    title : str   Plot title\n",
        "    category_colors : dict  Mapping of categories to colors\n",
        "    categories_labels : dict  Mapping of categories to display labels\n",
        "    pc1_idx : int        Index of the first PC to plot (default 0 for PC1)\n",
        "    pc2_idx : int        Index of the second PC to plot (default 1 for PC2)\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 8))\n",
        "\n",
        "    # Plot using specified components\n",
        "    for category in sorted(set(Category)):\n",
        "        mask = Category == category\n",
        "        plt.scatter(\n",
        "            X_pca[mask, pc1_idx],  # Specified PC for x-axis\n",
        "            X_pca[mask, pc2_idx],  # Specified PC for y-axis\n",
        "            c=category_colors[category],\n",
        "            label=categories_labels[category],\n",
        "            alpha=0.7,\n",
        "            s=100\n",
        "        )\n",
        "\n",
        "    plt.xlabel(f'PC{pc1_idx+1} ({explained_variance[pc1_idx]:.1%} variance explained)')\n",
        "    plt.ylabel(f'PC{pc2_idx+1} ({explained_variance[pc2_idx]:.1%} variance explained)')\n",
        "    plt.title(title)\n",
        "    plt.legend(title='Risk Category')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# For genera PCA\n",
        "X_pca_genera, var_ratio_genera, loadings_genera, categories = prepare_genera_pca(base_matrix, category_mapping)\n",
        "# PC1 vs PC2 (default)\n",
        "plot_pca_results(X_pca_genera, var_ratio_genera, categories.values, \"PC1 vs PC2\",\n",
        "                 category_colors, categories_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T10:11:49.450275Z",
          "iopub.status.busy": "2025-02-19T10:11:49.449889Z",
          "iopub.status.idle": "2025-02-19T10:11:51.503161Z",
          "shell.execute_reply": "2025-02-19T10:11:51.501945Z",
          "shell.execute_reply.started": "2025-02-19T10:11:49.450245Z"
        },
        "id": "FUe9thnSe7cf",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def prepare_flexible_pca(data_matrix, categories=None, n_components=None):\n",
        "    \"\"\"\n",
        "    Prepare PCA with flexible number of components\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    data_matrix : pandas DataFrame   Input data with samples as rows and features as columns\n",
        "    categories : array-like,   Category labels for each sample\n",
        "    n_components : int,   Number of components to calculate (None for all possible)\n",
        "    n_plot : int   Number of components to return for plotting\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    X_pca : numpy array   PCA transformed data (first n_plot components)\n",
        "    explained_variance : numpy array  Explained variance ratios for all components\n",
        "    loadings : pandas DataFrame  PCA loadings with feature names as index\n",
        "    \"\"\"\n",
        "    # Standardize\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(data_matrix)\n",
        "\n",
        "    # PCA\n",
        "    pca = PCA(n_components=n_components)\n",
        "    X_pca_full = pca.fit_transform(X_scaled)\n",
        "\n",
        "    # Get loadings for all components\n",
        "    loadings = pd.DataFrame(\n",
        "        pca.components_.T,\n",
        "        index=data_matrix.columns,\n",
        "        columns=[f'PC{i+1}' for i in range(pca.n_components_)]\n",
        "    )\n",
        "\n",
        "    # Return only requested components for plotting\n",
        "    X_pca = X_pca_full\n",
        "\n",
        "    # Return all calculated components\n",
        "    return X_pca_full, pca.explained_variance_ratio_, loadings\n",
        "\n",
        "# Calculate PCA with all components\n",
        "X_pca_all, var_ratio_all, loadings_all = prepare_flexible_pca(base_matrix_relative)\n",
        "\n",
        "# Plot different component combinations\n",
        "# PC1 vs PC2 (default)\n",
        "plot_pca_results(X_pca_all, var_ratio_all, categories.values, \"PC1 vs PC2\",\n",
        "                 category_colors, categories_labels)\n",
        "\n",
        "# PC2 vs PC3\n",
        "plot_pca_results(X_pca_all, var_ratio_all, categories.values, \"PC2 vs PC3\",\n",
        "                 category_colors, categories_labels,\n",
        "                 pc1_idx=1, pc2_idx=2)\n",
        "\n",
        "# PC3 vs PC4\n",
        "plot_pca_results(X_pca_all, var_ratio_all, categories.values, \"PC3 vs PC4\",\n",
        "                 category_colors, categories_labels,\n",
        "                 pc1_idx=2, pc2_idx=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7vGk-dvJxXL"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeKymKOe5A3k"
      },
      "source": [
        "## 9.2. Principal Component of Protein by Risk Category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T10:14:45.493097Z",
          "iopub.status.busy": "2025-02-19T10:14:45.492634Z",
          "iopub.status.idle": "2025-02-19T10:18:02.267216Z",
          "shell.execute_reply": "2025-02-19T10:18:02.265636Z",
          "shell.execute_reply.started": "2025-02-19T10:14:45.493066Z"
        },
        "id": "ol9qvarP5jRz",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def prepare_protein_pca(metabolic_info, category_dict):\n",
        "    \"\"\"\n",
        "    Convert protein strings to numeric features for PCA, aggregating by site first\n",
        "    \"\"\"\n",
        "    # First aggregate by Sites\n",
        "    site_protein_data = metabolic_info.groupby('Sites', observed=True).agg({\n",
        "        'Protein_Names': lambda x: ', '.join(x.dropna())\n",
        "    })\n",
        "\n",
        "    # Create set of unique proteins\n",
        "    all_proteins = set()\n",
        "    for proteins_str in site_protein_data['Protein_Names'].dropna():\n",
        "        proteins = [p.strip() for p in proteins_str.split(',')]\n",
        "        all_proteins.update(proteins)\n",
        "\n",
        "    # Create binary matrix at site level\n",
        "    protein_data = {}\n",
        "    for protein in all_proteins:\n",
        "        if protein:\n",
        "            protein_escaped = re.escape(protein)\n",
        "            protein_data[protein] = site_protein_data['Protein_Names'].str.contains(\n",
        "                protein_escaped,\n",
        "                regex=True,\n",
        "                na=False\n",
        "            ).astype(int)\n",
        "\n",
        "    protein_matrix = pd.DataFrame(protein_data, index=site_protein_data.index)\n",
        "\n",
        "    # Run PCA\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(protein_matrix)\n",
        "    pca = PCA(n_components=2)\n",
        "    X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "    loadings = pd.DataFrame(\n",
        "        pca.components_.T,\n",
        "        index=protein_matrix.columns,\n",
        "        columns=['PC1', 'PC2']\n",
        "    )\n",
        "\n",
        "    return X_pca, pca.explained_variance_ratio_, loadings, protein_matrix\n",
        "    # For protein PCA # 4 min\n",
        "X_pca_protein, var_ratio_protein, loadings_protein, protein_matrix = prepare_protein_pca(metabolic_sites_info, category_dict)\n",
        "plot_pca_results(X_pca_protein, var_ratio_protein, categories.values, \"Protein PCA by Risk Category\", category_colors, categories_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p72ly8fYYel7"
      },
      "source": [
        "## 9.3. Top Protein Loadings by Category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T10:20:33.294993Z",
          "iopub.status.busy": "2025-02-19T10:20:33.294612Z",
          "iopub.status.idle": "2025-02-19T10:20:33.319388Z",
          "shell.execute_reply": "2025-02-19T10:20:33.318181Z",
          "shell.execute_reply.started": "2025-02-19T10:20:33.294963Z"
        },
        "id": "0L5F06YaEBiW",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def analyze_protein_loadings(loadings, top_n=20):\n",
        "    \"\"\"\n",
        "    Analyze protein loadings to find most influential proteins\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    loadings : pandas DataFrame\n",
        "        PCA loadings with proteins as index\n",
        "    top_n : int\n",
        "        Number of top proteins to return\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dict with top proteins for each PC and their contributions\n",
        "    \"\"\"\n",
        "    # Calculate magnitude of contribution for each protein\n",
        "    loadings['magnitude'] = np.sqrt(loadings['PC1']**2 + loadings['PC2']**2)\n",
        "\n",
        "    # Get top contributors overall\n",
        "    top_overall = loadings.nlargest(top_n, 'magnitude')\n",
        "\n",
        "    # Get top contributors for each PC\n",
        "    top_pc1_pos = loadings.nlargest(top_n, 'PC1')\n",
        "    top_pc1_neg = loadings.nsmallest(top_n, 'PC1')\n",
        "    top_pc2_pos = loadings.nlargest(top_n, 'PC2')\n",
        "    top_pc2_neg = loadings.nsmallest(top_n, 'PC2')\n",
        "\n",
        "    return {\n",
        "        'top_overall': top_overall,\n",
        "        'top_pc1_positive': top_pc1_pos,\n",
        "        'top_pc1_negative': top_pc1_neg,\n",
        "        'top_pc2_positive': top_pc2_pos,\n",
        "        'top_pc2_negative': top_pc2_neg\n",
        "    }\n",
        "\n",
        "# Use after running PCA:\n",
        "loading_analysis = analyze_protein_loadings(loadings_protein)\n",
        "\n",
        "# Print top contributors\n",
        "print(\"Top 20 proteins contributing to separation:\")\n",
        "print(loading_analysis['top_overall'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnA0pU4EsLXX"
      },
      "source": [
        "A lecture of the top 20 proteins contributing to separation on the exactly the same magnitude make it suspicius to the fact that maybe we not really taking the 20 top but just the 20 first, and indistiguisible will be just be all contributing on the same fashion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEoWk_0x73_U"
      },
      "source": [
        "## 9.5 Retrieving Statistically Significant Groups\n",
        "\n",
        "From notebook 3_Feature_selection the file finalist.xlsx contain the groups worked and that were statistically significant in relation to the risk label. This groups posses interest since the relationship to the label could show better understanding in contrast with the different groups of known bacteria, core taxa, checked bacteria and the mixed groups.\n",
        "The idea is to understand if the core taxa which make up a large influence on the comunities on the water and cooling systems are also influencing corrosion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSgS-kec8Gg6"
      },
      "outputs": [],
      "source": [
        "source_groups = {\n",
        "    \"known_bacteria\": known_bacteria_list,\n",
        "    \"pure_checked\": pure_checked_list,\n",
        "    \"pure_core\": pure_core_list,\n",
        "    \"checked_core\": checked_core_list\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owvu2k118Hyk"
      },
      "outputs": [],
      "source": [
        "Influencers_uniques_path = base_dir / \"finalist_dfs.xlsx\"\n",
        "# Integrated taxa from origin genus as headers with levels 6 for the genera, 7 for the GID, muss be cleaned\n",
        "Influencers_uniques = pd.read_excel(Influencers_uniques_path, sheet_name='Influencers_uniques', header=[0,1,2,3,4,5,6,7], engine ='openpyxl')\n",
        "# Drop first row (index 0) and first column in one chain\n",
        "Influencers_uniques = Influencers_uniques.drop(index=0)\n",
        "Influencers_uniques = Influencers_uniques.drop(Influencers_uniques.columns[0], axis=1)\n",
        "Influencers_uniques = Influencers_uniques.astype({'Sites': str})\n",
        "# Remove 'Unnamed' level names\n",
        "Influencers_uniques.columns = Influencers_uniques.columns.map(lambda x: tuple('' if \"Unnamed\" in str(level) else level for level in x))\n",
        "Influencers_uniques_list= Influencers_uniques.columns.get_level_values(6)\n",
        "Influencers_uniques_list= Influencers_uniques_list[Influencers_uniques_list !='']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3dgDHQr8Xi2"
      },
      "source": [
        "\n",
        "### Updating the groups to visualise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPOjPHN88U8O"
      },
      "outputs": [],
      "source": [
        "source_groups = {\n",
        "    \"known_bacteria\": known_bacteria_list,\n",
        "    \"pure_checked\": pure_checked_list,\n",
        "    \"pure_core\": pure_core_list,\n",
        "    \"checked_core\": checked_core_list,\n",
        "    \"Influencers_uniques\": Influencers_uniques_list,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nXUOBVBJWGk"
      },
      "source": [
        "## 9.6. Analysing Top Proteins by Category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0oorAB98DCg",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def analyze_top_proteins_by_category(base_matrix, category_dict, source_groups, n_top=20):\n",
        "    \"\"\"\n",
        "    Analyze top proteins for each risk category and source group\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    base_matrix : DataFrame with MultiIndex columns (Genus, protein_name)\n",
        "    category_dict : Dict mapping sites to categories (1,2,3)\n",
        "    source_groups : Dict mapping group names to list of genera\n",
        "    n_top : Number of top proteins to show\n",
        "    \"\"\"\n",
        "    # Get sites for each category\n",
        "    sites_by_category = {\n",
        "        cat: [site for site, c in category_dict.items() if c == cat]\n",
        "        for cat in [1, 2, 3]  # Explicitly use categories 1,2,3\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # For each bacteria group (known, pure_checked, etc.)\n",
        "    for group_name, genera in source_groups.items():\n",
        "        print(f\"\\nAnalyzing {group_name}...\")\n",
        "\n",
        "        # Filter for genera in this group\n",
        "        group_cols = [col for col in base_matrix.columns if col[0] in genera]\n",
        "        if not group_cols:\n",
        "            print(f\"No data found for {group_name}\")\n",
        "            continue\n",
        "\n",
        "        group_data = base_matrix[group_cols]\n",
        "\n",
        "        # Analyze each risk category\n",
        "        group_results = {}\n",
        "        for cat, sites in sites_by_category.items():\n",
        "            # Get data for sites in this category\n",
        "            cat_data = group_data.loc[sites]\n",
        "\n",
        "            # Calculate mean abundance for each protein-genus combination\n",
        "            mean_abundances = cat_data.mean()\n",
        "            top_proteins = mean_abundances.nlargest(n_top)\n",
        "\n",
        "            group_results[cat] = top_proteins\n",
        "\n",
        "        results[group_name] = group_results\n",
        "\n",
        "        # Plot results for this group\n",
        "        plt.figure(figsize=(20, 10))\n",
        "        plt.suptitle(f\"Top {n_top} Proteins - {group_name}\", y=1.02, fontsize=14)\n",
        "\n",
        "        for i, cat in enumerate([1, 2, 3], 1):\n",
        "            plt.subplot(1, 3, i)\n",
        "\n",
        "            if cat in group_results:\n",
        "                top = group_results[cat]\n",
        "\n",
        "                # Create labels combining genus and protein\n",
        "                labels = [f\"{genus}\\n{protein[:30]}...\"\n",
        "                         for genus, protein in top.index]\n",
        "\n",
        "                # Plot\n",
        "                sns.barplot(x=top.values,\n",
        "                          y=labels,\n",
        "                          color=category_colors[cat],\n",
        "                          alpha=0.7)\n",
        "\n",
        "                plt.title(f\"{categories_labels[cat]}\")\n",
        "                plt.xlabel(\"Mean Abundance\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    return results\n",
        "\n",
        "# Call the function\n",
        "results = analyze_top_proteins_by_category(base_matrix, category_dict, source_groups)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQ-5nx_6DMvN"
      },
      "source": [
        "## 9.7 Top Genera & Proteins in One Category\n",
        "Subset by category, compute total abundance for each genus, pick the top n. For each of those genera, pick the top 𝑛 n proteins. Stack into a long DataFrame for boxplotting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0t7eYAELEi2x"
      },
      "outputs": [],
      "source": [
        "def pick_top_proteins_for_category(base_matrix, cat, n_top, n_genera, category_level=1):\n",
        "    \"\"\"\n",
        "    Return a DataFrame in long form of the top proteins for the specified category.\n",
        "\n",
        "    Steps:\n",
        "    1) Filter rows by category\n",
        "    2) Identify top n_genera by total abundance\n",
        "    3) For each top genus, pick the n_top most abundant proteins\n",
        "    4) Return a long DataFrame of those columns only\n",
        "    \"\"\"\n",
        "    # 1) Subset rows for the chosen category\n",
        "    cat_data = base_matrix.xs(cat, level=category_level, axis=0, drop_level=False)\n",
        "\n",
        "    # 2) Calculate mean abundance for each (Genus, Protein)\n",
        "    col_means = cat_data.mean(axis=0)  # Series indexed by (Genus, Protein)\n",
        "\n",
        "    # 3) Identify top n_genera by total abundance\n",
        "    genus_sums = col_means.groupby(level=0).sum()  # sum across proteins within each genus\n",
        "    top_genera = genus_sums.nlargest(n_genera).index\n",
        "\n",
        "    # 4) For each genus in top_genera, pick top n_top proteins\n",
        "    all_top_cols = []\n",
        "    for genus in top_genera:\n",
        "        # Get columns belonging to this genus\n",
        "        genus_cols = [col for col in col_means.index if col[0] == genus]\n",
        "        # Among those, pick the n_top highest\n",
        "        top_genus_proteins = col_means[genus_cols].nlargest(n_top).index\n",
        "        all_top_cols.extend(top_genus_proteins)\n",
        "\n",
        "    # Subset cat_data to these selected columns\n",
        "    top_data = cat_data[all_top_cols]\n",
        "\n",
        "    # Convert to long form using stack\n",
        "    df_long = (\n",
        "        top_data\n",
        "        .stack(level=list(range(top_data.columns.nlevels)))  # stack both (Genus, Protein)\n",
        "        .reset_index()\n",
        "    )\n",
        "\n",
        "    # After stacking, columns typically become [<row_idx1>, <row_idx2>, \"Genus\", \"Protein\", 0]\n",
        "    if len(df_long.columns) == 5:\n",
        "        df_long.columns = [\"Site\", \"Category\", \"Genus\", \"Protein\", \"Abundance\"]\n",
        "    else:\n",
        "        df_long.columns = [\"Site\", \"Genus\", \"Protein\", \"Abundance\"]\n",
        "\n",
        "    # Add a combined label\n",
        "    df_long[\"Feature\"] = df_long[\"Genus\"] + \" | \" + df_long[\"Protein\"].str[:30] + \"...\"\n",
        "    # Keep track of which category these data came from\n",
        "    df_long[\"Cat\"] = cat\n",
        "\n",
        "    return df_long"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIAe_wGbICUC"
      },
      "source": [
        "## 9.8. Plotting Top Protein-Genera for each Category\n",
        "The plotting loops over the categories, uses pick_top_proteins_for_category to build a long DataFrame for each category an d creates one subplot per category with a boxplot of the chosen features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VbhkEIADFTd"
      },
      "outputs": [],
      "source": [
        "def plot_top_proteins_across_categories(base_matrix, categories=[1, 2, 3],\n",
        "                                        n_top=5, n_genera=10, category_level=1):\n",
        "    \"\"\"\n",
        "    Create side-by-side boxplots of the top proteins from multiple categories.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    base_matrix : pd.DataFrame\n",
        "        Rows: (Site, Category), Columns: (Genus, Protein)\n",
        "    categories : list\n",
        "        Which category values to plot, e.g. [1,2,3].\n",
        "    n_top : int\n",
        "        Number of top proteins per genus.\n",
        "    n_genera : int\n",
        "        Number of top genera to consider per category.\n",
        "    category_level : int\n",
        "        The level in the row MultiIndex that holds the category.\n",
        "    \"\"\"\n",
        "    # Prepare subplots\n",
        "    fig, axes = plt.subplots(1, len(categories), figsize=(10*len(categories), 15), sharey=True)\n",
        "    if len(categories) == 1:\n",
        "        axes = [axes]  # ensure it's iterable\n",
        "\n",
        "    # For each category, pick top proteins, then plot in its own subplot\n",
        "    for i, cat in enumerate(categories):\n",
        "        df_long = pick_top_proteins_for_category(base_matrix, cat, n_top, n_genera, category_level)\n",
        "\n",
        "        # Boxplot in the ith subplot\n",
        "        sns.boxplot(ax=axes[i], x='Abundance', y='Feature', data=df_long)\n",
        "        axes[i].set_title(f\"Category {cat}: Top {n_top} Proteins\\nfrom Top {n_genera} Genera\")\n",
        "        axes[i].set_xlabel(\"Abundance\")\n",
        "        axes[i].set_ylabel(\"Genus | Protein\")\n",
        "        #axes[i].tick_params(axis='y', rotation=90)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_top_proteins_across_categories(base_matrix, categories=[1, 2, 3],  n_top=5, n_genera=10, category_level=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNRMNDJ9L9yz"
      },
      "outputs": [],
      "source": [
        "plot_top_proteins_across_categories(base_matrix_relative, categories=[1, 2, 3],  n_top=5, n_genera=5, category_level=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "l58SL_DO-PJP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4d273a9-7be2-40b1-e1d8-13ae6da5bd4b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. Building a Dictionary from Databases"
      ],
      "metadata": {
        "id": "5lnzHgflRGFV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m296sDKJQW27"
      },
      "source": [
        "\n",
        "\n",
        "Charts can be overwhelming and difficult to read, in order to be able to compare better, an statistical test will be made in order to elucidate which of this genus-protein combinations have no positive or negative significance with the risk label and therefore wont serve to differentiate for the final model, it is not that they are no important, just that they can no be reliable at the time to decide what microorganism is influencing corrosion.\n",
        "\n",
        "I need to search in the database but at the same time to capture some naming that allow me to classify protein as well as pathways with my references I have on drive or with the table of metabolism on my main doc,\n",
        "using the notebook lm to search into the docs my own literature review? so to find how to profit from this naming to make a bloody table or dictionary with no one but maybe several columns?\n",
        "\n",
        "First the previous code to plot  would be modified to get all the proteins- genus pars and their respective classification into categories so that a statistical analyis could be done contrasting the presence and abundance of each of the combinations, so that will be doable to search for the protein function pathway that are relevant to corosion studies.\n",
        "1. Clasify proteins on their presence in each cat, identify the common in all the categories so to discard them and identify the proteins enriched on cat 2 and 3 in order to keep them for further analyisis.\n",
        "2. Significance test will confirm the differencial abundance by comparing cat 2 and cat 3 usign Kruskal-Wallis. Then we keep only proteins with a significant difference in abundance between 2 and 3 and the inverse.\n",
        "3. That way we can search programaticaly for the meaning of the protein function on the databases.\n",
        "\n",
        "\n",
        "Step\tAction\tRationale\n",
        "Step 1: Normalize Protein/EC Data\tEnsure EC numbers, KO numbers, and reaction numbers are cleaned and deduplicated\tPrevent mismatches when querying databases\n",
        "Step 2: Query Metabolic Databases\tMap EC/KO/Reactions → Pathway & Metabolism using KEGG, MetaCyc, BioCyc\tIdentify metabolic roles\n",
        "Step 3: Identify Metal-Related Proteins\tCross-check with BRENDA (cofactor data), MetalPDB, TransportDB\tFind direct metal-binding or metal-transporting proteins\n",
        "Step 4: Corrosion-Specific Filtering\tFlag proteins involved in electron transfer, biofilm formation, sulfate reduction, iron oxidation, etc.\tDirect relevance to corrosion microbi, Polysaccharide synthesis/export proteins\n",
        "Adhesins and biofilm formation proteins\n",
        "\n",
        "Final Dictionary Assembly   \n",
        "\n",
        "|Protein\t|EC/KO|\tMetabolism\t|Pathway|\tMetal Interaction|\tMIC function|\n",
        "|--|---|---|---|---|---|\n",
        "|Hydrogenase|\tEC 1.12.1.2\t|Energy Metabolism|\tHydrogen oxidation|\tFe-S cluster|\tElectron donor in MIC|\n",
        "|Cytochrome c\t|EC 1.9.3.1|Electron Transfer|\tRespiration|\tHeme-Fe|\tKey in Fe(III) reduction|\n",
        "|Sulfate Reductase|\tEC 1.8.99.5|\tSulfur Metabolism|\tSulfate reduction|\tMo/Fe\t|MIC agent|\n",
        "|Exopolysaccharide Synthase|\tKO K00710\t|Biofilm Formation\t|Polysaccharide metabolism|\tNo direct metal binding\t|Biofilm stabilization|\n",
        "|Manganese Oxidase|\tEC 1.16.3.3|\tMetal Oxidation|\tMn(II) oxidation|\tMn binding|\tForms protective layer (biogenic passivation)|\n",
        "\n",
        "Once the dictionary is built, filter for:\n",
        "\n",
        "Filtering Strategy\n",
        "Step 1: Identify the most abundant proteins in each MIC category.\n",
        "\n",
        "separate classification for redox proteins:\n",
        "Electron transport proteins Hydrogenases (key for cathodic depolarization)\n",
        "Cytochromes (electron transfer)\n",
        "Oxidoreductases (particularly those using Fe/Mn as cofactors)\n",
        "\n",
        "Sulfate reducers (directly linked to MIC)\n",
        "Iron/Manganese oxidizers (biogenic corrosion layers)\n",
        " specific metabolic markers for MIC mechanisms:\n",
        "\n",
        "Classify proteins by their role in specific corrosion mechanisms\n",
        "Tag proteins known to be upregulated in field-verified MIC cases\n",
        "\n",
        "Add environmental context relevance:\n",
        "\n",
        "Flag proteins known to function in industrial water systems\n",
        "Identify proteins that function under different oxygen conditions (aerobic/anaerobic)\n",
        "\n",
        "Step 2: Perform Permutation Tests instead of Kruskal-Wallis or Kruskal-Wallis is appropriate, but follow with post-hoc tests (Dunn's test) to identify which categories differ\n",
        "Step 3: Use False Discovery Rate (FDR) correction (Benjamini-Hochberg) or Consider adding fold-change thresholds for biological significance\n",
        "\n",
        "\n",
        "No all proteins involved in the metal metabolism influence corrosion, so it is necesary to check for corrosion mechanims types:\n",
        "Direct electron transfer, metabolite mediated electron transfer (MET),\n",
        "Hydrogen Metabolism for MIC e transfer, sulfur metabolism, biofilm formation genes (EPS production), metal binging proteins (corrosion acceleration), acid production, oxygen consumption (creating differential aereation cells)\n",
        "\n",
        "## 10.1 Retrieving the Databases"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_paths():\n",
        "    \"\"\"Set up paths for database access\"\"\"\n",
        "\n",
        "    # Database paths\n",
        "    db_paths = {\n",
        "        'enzyme': db_dir / 'enzyme',\n",
        "        'enzyme_class': db_dir / 'enzclass.txt',\n",
        "        'enzyme_brenda' : db_dir/ 'brenda_2024.txt',\n",
        "        'ko': db_dir / 'ko',\n",
        "        'ko_hierarchy': db_dir / 'ko_hierarchy.txt',\n",
        "        'pathway': db_dir / 'pathway',\n",
        "        'module': db_dir / 'module',\n",
        "        'reaction': db_dir / 'reaction',\n",
        "        'compound': db_dir / 'compound',\n",
        "        'metalpdb': db_dir / 'flat_db_file.xml'\n",
        "    }\n",
        "\n",
        "    return db_paths\n",
        "\n",
        "#  Calling the paths\n",
        "if __name__ == \"__main__\":\n",
        "    paths = setup_paths()\n",
        "    # Print paths to verify\n",
        "    for db_name, path in paths.items():\n",
        "        print(f\"{db_name}: {path}\")\n",
        "        print(f\"Exists: {path.exists()}\")"
      ],
      "metadata": {
        "id": "0efKvfwuHc0f",
        "outputId": "493ec655-2c90-406e-fda7-b884de16a9d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enzyme: /content/drive/MyDrive/MIC/Databases/enzyme\n",
            "Exists: True\n",
            "enzyme_class: /content/drive/MyDrive/MIC/Databases/enzclass.txt\n",
            "Exists: True\n",
            "enzyme_brenda: /content/drive/MyDrive/MIC/Databases/brenda_2024.txt\n",
            "Exists: True\n",
            "ko: /content/drive/MyDrive/MIC/Databases/ko\n",
            "Exists: True\n",
            "ko_hierarchy: /content/drive/MyDrive/MIC/Databases/ko_hierarchy.txt\n",
            "Exists: True\n",
            "pathway: /content/drive/MyDrive/MIC/Databases/pathway\n",
            "Exists: True\n",
            "module: /content/drive/MyDrive/MIC/Databases/module\n",
            "Exists: True\n",
            "reaction: /content/drive/MyDrive/MIC/Databases/reaction\n",
            "Exists: True\n",
            "compound: /content/drive/MyDrive/MIC/Databases/compound\n",
            "Exists: True\n",
            "metalpdb: /content/drive/MyDrive/MIC/Databases/flat_db_file.xml\n",
            "Exists: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Brenda Enzyme Parse Brenda\n",
        "\n",
        "https://www.brenda-enzymes.org/download.php\n",
        "\n",
        "Chang A., Jeske L., Ulbrich S., Hofmann J., Koblitz J., Schomburg I., Neumann-Schaal M., Jahn D., Schomburg D.\n",
        "BRENDA, the ELIXIR core data resource in 2021: new developments and updates. (2021), Nucleic Acids Res., 49:D498-D508.\n",
        "DOI: 10.1093/nar/gkaa1025 PubMed: 33211880"
      ],
      "metadata": {
        "id": "7P2TUJLNWX3B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_brenda_file():\n",
        "    \"\"\"Parse BRENDA database file for detailed enzyme information\"\"\"\n",
        "    paths = setup_paths()\n",
        "    enzyme_brenda_path = paths['enzyme_brenda']\n",
        "\n",
        "    ec_detailed_info = {}\n",
        "    current_ec = None\n",
        "    in_enzyme_entry = False\n",
        "\n",
        "    try:\n",
        "        with open(enzyme_brenda_path, 'r') as f:\n",
        "          for line in f:\n",
        "              line = line.strip()\n",
        "\n",
        "              # Skip empty lines\n",
        "              if not line:\n",
        "                  continue\n",
        "\n",
        "              # Check for the end of an entry\n",
        "              if line == \"///\":\n",
        "                  current_ec = None\n",
        "                  in_enzyme_entry = False\n",
        "                  continue\n",
        "\n",
        "              # Process ID line - identify enzyme entries\n",
        "              if line.startswith('ID\\t'):\n",
        "                  current_ec = line.split('\\t')[1]\n",
        "\n",
        "                  # Skip \"spontaneous\" and other non-EC entries\n",
        "                  if not any(c.isdigit() for c in current_ec):\n",
        "                      current_ec = None\n",
        "                      in_enzyme_entry = False\n",
        "                      continue\n",
        "\n",
        "                  # Initialize proper EC entry\n",
        "                  ec_detailed_info[current_ec] = {\n",
        "                      'metals': [],\n",
        "                      'cofactors': [],\n",
        "                      'reactions': [],\n",
        "                      'substrates': [],\n",
        "                      'inhibitors': []\n",
        "                  }\n",
        "                  in_enzyme_entry = True\n",
        "\n",
        "              # Only process other lines if we're in a valid enzyme entry\n",
        "              elif in_enzyme_entry and current_ec:\n",
        "                  if line.startswith('ME\\t'):\n",
        "                      # Extract metal information\n",
        "                      metal_info = line.split('\\t')[1]\n",
        "                      ec_detailed_info[current_ec]['metals'].append(metal_info)\n",
        "\n",
        "                  elif line.startswith('CF\\t'):\n",
        "                      # Extract cofactor information\n",
        "                      cofactor_info = line.split('\\t')[1]\n",
        "                      ec_detailed_info[current_ec]['cofactors'].append(cofactor_info)\n",
        "\n",
        "                  elif line.startswith('RE\\t'):\n",
        "                      # Extract detailed reaction information\n",
        "                      reaction_info = line.split('\\t')[1]\n",
        "                      ec_detailed_info[current_ec]['reactions'].append(reaction_info)\n",
        "\n",
        "                  elif line.startswith('SP\\t') or line.startswith('NSP\\t'):\n",
        "                      # Extract substrate information\n",
        "                      substrate_info = line.split('\\t')[1]\n",
        "                      ec_detailed_info[current_ec]['substrates'].append(substrate_info)\n",
        "\n",
        "                  elif line.startswith('IN\\t'):\n",
        "                      # Extract inhibitor information\n",
        "                      inhibitor_info = line.split('\\t')[1]\n",
        "                      ec_detailed_info[current_ec]['inhibitors'].append(inhibitor_info)\n",
        "\n",
        "        # Verify we have valid EC numbers\n",
        "        ec_detailed_info = {ec: info for ec, info in ec_detailed_info.items()\n",
        "                            if ec.count('.') == 3 and all(part.isdigit() for part in ec.split('.'))}\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing BRENDA file: {e}\")\n",
        "        return {}\n",
        "    return ec_detailed_info\n",
        "\n",
        "brenda_data = parse_brenda_file()\n",
        "#brenda_data"
      ],
      "metadata": {
        "id": "mkW7GPMhvlL3"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_brenda_data(brenda_data):\n",
        "    \"\"\"Process BRENDA data to extract clean metal information while keeping other data intact\"\"\"\n",
        "    processed_data = {}\n",
        "\n",
        "    # Common metal ions to look for\n",
        "    metal_patterns = {'iron': ['Fe2+', 'Fe3+', 'iron', 'ferrous', 'ferric'],\n",
        "        'manganese': ['Mn2+', 'manganese'],\n",
        "        'copper': ['Cu+', 'Cu2+', 'copper'],\n",
        "        'nickel': ['Ni2+', 'nickel'],\n",
        "        'cobalt': ['Co2+', 'cobalt'],\n",
        "        'magnesium': ['Mg2+', 'magnesium'],\n",
        "        'calcium': ['Ca2+', 'calcium'],\n",
        "        'Mo': ['Mo', 'molybdenum'],\n",
        "        'V5+': ['V5+', 'vanadium'],\n",
        "        'Al3+': ['Al3+', 'aluminum'],\n",
        "        'Cr3+': ['Cr3+', 'chromium'],\n",
        "        'zinc': ['Zn2+', 'zinc'],\n",
        "        'sodium': ['Na+', 'sodium', 'NaCl'],\n",
        "        'potassium': ['K+', 'potassium', 'KCl'],\n",
        "        'selenium': ['selenium', 'Se'],\n",
        "        'barium': ['Ba2+', 'barium']\n",
        "    }\n",
        "    # Pathway categories collecting all terms\n",
        "    pathway_categories = {\n",
        "        'organic_acid_metabolism': [\n",
        "            'acetate', 'acetic acid', 'acetyl',\n",
        "            'oxalate', 'oxalic acid',\n",
        "            'organic acid', 'fatty acid',\n",
        "            'carboxylic acid'\n",
        "        ],\n",
        "        'metal_organic_interaction': [\n",
        "            'siderophore', 'metal binding',\n",
        "            'iron complex', 'metal transport',\n",
        "            'metallophore', 'metal organic'\n",
        "        ],\n",
        "        'biofilm_formation': [\n",
        "            'biofilm', 'exopolysaccharide',\n",
        "            'extracellular matrix', 'adhesion'\n",
        "        ],\n",
        "        'halogen_related': [\n",
        "            'halogen', 'chloride', 'bromide',\n",
        "            'halide', 'dehalogenation'\n",
        "        ],\n",
        "        'sulfur': [\n",
        "            'sulfur', 'sulfate', 'sulfide',\n",
        "            'thiosulfate', 'sulfite', 'sulfonate'\n",
        "        ],\n",
        "        'electron_transfer': [\n",
        "            'cytochrome', 'electron transport',\n",
        "            'oxidoreductase', 'redox'\n",
        "        ],\n",
        "        'carbon_metabolism': [\n",
        "            'carbon fixation', 'carbon utilization',\n",
        "            'carbohydrate metabolism'\n",
        "        ],\n",
        "        'ph_modulation': [\n",
        "            'acid', 'alkaline', 'proton pump',\n",
        "            'pH homeostasis'\n",
        "        ],\n",
        "        'temp_response': [\n",
        "            'heat shock', 'cold shock',\n",
        "            'temperature response'\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    # Define organic matter categories\n",
        "    organic_categories = {\n",
        "        'degradation': ['degradation', 'breakdown', 'catabolism'],\n",
        "        'synthesis': ['biosynthesis', 'anabolism', 'synthesis'],\n",
        "        'transport': ['transport', 'uptake', 'export'],\n",
        "        'modification': ['modification', 'conversion', 'transformation']\n",
        "    }\n",
        "\n",
        "\n",
        "    for ec_number, data in brenda_data.items():\n",
        "        processed_data[ec_number] = {\n",
        "            'cofactors': data.get('cofactors', []),\n",
        "            'reactions': data.get('reactions', []),\n",
        "            'substrates': data.get('substrates', []),\n",
        "            'inhibitors': data.get('inhibitors', []),\n",
        "            'raw_metals': data.get('metals', []),\n",
        "            'clean_metals': []\n",
        "        }\n",
        "\n",
        "        # Extract clean metal names\n",
        "        for entry in data.get('metals', []):\n",
        "            for metal in metal_patterns:\n",
        "                if metal in entry:\n",
        "                    if metal not in processed_data[ec_number]['clean_metals']:\n",
        "                        processed_data[ec_number]['clean_metals'].append(metal)\n",
        "\n",
        "        # Create a single text string for pathway searching\n",
        "        all_text = ' '.join([\n",
        "            ' '.join(data.get('reactions', [])),\n",
        "            ' '.join(data.get('substrates', [])),\n",
        "            ' '.join(data.get('cofactors', []))\n",
        "        ]).lower()\n",
        "\n",
        "        # Add corrosion relevance information\n",
        "        processed_data[ec_number]['corrosion_relevant_metals'] = [\n",
        "            metal for metal in processed_data[ec_number]['clean_metals']\n",
        "            if metal in ['Fe2+', 'Fe3+', 'iron', 'Mn2+', 'manganese', 'Cu+', 'Cu2+',\n",
        "                         'copper', 'Ni2+', 'nickel', 'Co2+', 'cobalt']\n",
        "        ]\n",
        "        ## Add pathway relevance information, would it no be better to search for this relevance on the pathway database?\n",
        "        for category, terms in pathway_categories.items():\n",
        "            if any(term.lower() in all_text for term in terms):\n",
        "                ec_enriched_data[ec_num]['pathway_categories'][category] = True\n",
        "\n",
        "        # Check for organic matter processes\n",
        "        for category, terms in organic_categories.items():\n",
        "            if any(term.lower() in all_text for term in terms):\n",
        "                ec_enriched_data[ec_num]['organic_processes'][category] = True\n",
        "\n",
        "\n",
        "        # Calculate corrosion relevance score based on metals\n",
        "        if processed_data[ec_number]['corrosion_relevant_metals']:\n",
        "            processed_data[ec_number]['corrosion_relevance'] = 'high'\n",
        "        elif processed_data[ec_number]['clean_metals']:\n",
        "            processed_data[ec_number]['corrosion_relevance'] = 'medium'\n",
        "        else:\n",
        "            processed_data[ec_number]['corrosion_relevance'] = 'low'\n",
        "\n",
        "    return processed_data\n",
        "\n",
        "brenda_en= process_brenda_data(paths['enzyme_brenda'])\n",
        "brenda_en"
      ],
      "metadata": {
        "id": "d2S3JREa6rg7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "outputId": "87f6cff6-df2e-473c-9381-5c01427817a7"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'PosixPath' object has no attribute 'items'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-ad779ec5b91c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mprocessed_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m \u001b[0mbrenda_en\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mprocess_brenda_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'enzyme_brenda'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0mbrenda_en\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-46-ad779ec5b91c>\u001b[0m in \u001b[0;36mprocess_brenda_data\u001b[0;34m(brenda_data)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mec_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbrenda_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         processed_data[ec_number] = {\n\u001b[1;32m     77\u001b[0m             \u001b[0;34m'cofactors'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cofactors'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'PosixPath' object has no attribute 'items'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_brenda_data(brenda_data):\n",
        "    \"\"\"Process BRENDA data to extract clean metal information while keeping other data intact\"\"\"\n",
        "    processed_data = {}\n",
        "\n",
        "    # Common metal ions to look for\n",
        "    metal_patterns = {'iron': ['Fe2+', 'Fe3+', 'iron', 'ferrous', 'ferric'],\n",
        "        'manganese': ['Mn2+', 'manganese'],\n",
        "        'copper': ['Cu+', 'Cu2+', 'copper'],\n",
        "        'nickel': ['Ni2+', 'nickel'],\n",
        "        'cobalt': ['Co2+', 'cobalt'],\n",
        "        'magnesium': ['Mg2+', 'magnesium'],\n",
        "        'calcium': ['Ca2+', 'calcium'],\n",
        "        'Mo': ['Mo', 'molybdenum'],\n",
        "        'V5+': ['V5+', 'vanadium'],\n",
        "        'Al3+': ['Al3+', 'aluminum'],\n",
        "        'Cr3+': ['Cr3+', 'chromium'],\n",
        "        'zinc': ['Zn2+', 'zinc'],\n",
        "        'sodium': ['Na+', 'sodium', 'NaCl'],\n",
        "        'potassium': ['K+', 'potassium', 'KCl'],\n",
        "        'selenium': ['selenium', 'Se'],\n",
        "        'barium': ['Ba2+', 'barium']\n",
        "    }\n",
        "    # Categorize metals by corrosion relevance\n",
        "    high_relevance_metals = ['iron', 'manganese', 'copper', 'nickel', 'cobalt', 'chromium']\n",
        "    medium_relevance_metals = ['zinc', 'selenium',  'magnesium', 'calcium', 'molybdenum', 'aluminium']\n",
        "    low_relevance_metals = ['sodium', 'potassium', 'barium']\n",
        "\n",
        "    for ec_number, data in brenda_data.items():\n",
        "        processed_data[ec_number] = {\n",
        "            'reactions': data.get('reactions', []),\n",
        "            'substrates': data.get('substrates', []),\n",
        "            'metal_by_type': data.get('metal_by_type', []),\n",
        "            'raw_metals': data.get('metals', []),\n",
        "            'clean_metals': []\n",
        "        }\n",
        "\n",
        "        # Extract clean metal names\n",
        "        for entry in data.get('metals', []):\n",
        "            for metal in metal_patterns:\n",
        "                if metal in entry:\n",
        "                    if metal not in processed_data[ec_number]['clean_metals']:\n",
        "                        processed_data[ec_number]['clean_metals'].append(metal)\n",
        "\n",
        "        # Add corrosion relevance information\n",
        "        processed_data[ec_number]['corrosion_relevant_metals'] = [\n",
        "            metal for metal in processed_data[ec_number]['clean_metals']\n",
        "            if metal in ['Fe2+', 'Fe3+', 'iron', 'Mn2+', 'manganese', 'Cu+', 'Cu2+',\n",
        "                         'copper', 'Ni2+', 'nickel', 'Co2+', 'cobalt']]\n",
        "\n",
        "        # Calculate corrosion relevance score based on metals\n",
        "        if processed_data[ec_number]['corrosion_relevant_metals']:\n",
        "            processed_data[ec_number]['corrosion_relevance'] = 'high'\n",
        "        elif processed_data[ec_number]['clean_metals']:\n",
        "            processed_data[ec_number]['corrosion_relevance'] = 'medium'\n",
        "        else:\n",
        "            processed_data[ec_number]['corrosion_relevance'] = 'low'\n",
        "\n",
        "    return processed_data\n",
        "\n",
        "brenda_en= process_brenda_data(paths['enzyme_brenda'])\n",
        "#brenda_en"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "bv67uXfHWavj",
        "outputId": "7025e270-0d8b-464b-ffa9-7e4225b983e6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'PosixPath' object has no attribute 'items'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-6070582b158d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mprocessed_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0mbrenda_en\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mprocess_brenda_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'enzyme_brenda'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;31m#brenda_en\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-6070582b158d>\u001b[0m in \u001b[0;36mprocess_brenda_data\u001b[0;34m(brenda_data)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mlow_relevance_metals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'sodium'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'potassium'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'barium'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mec_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbrenda_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         processed_data[ec_number] = {\n\u001b[1;32m     30\u001b[0m             \u001b[0;34m'reactions'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'reactions'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'PosixPath' object has no attribute 'items'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enzyme names\n",
        "The database containing enzyme names and EC numbers\n",
        "\n",
        "wget https://www.enzyme-database.org/downloads/enzyme-database.sql.gz"
      ],
      "metadata": {
        "id": "EA-pSl0dvd0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_enzyme_names():\n",
        "    \"\"\"Read and parse enzyme file to get EC numbers and their names\"\"\"\n",
        "    paths = setup_paths()\n",
        "    enzyme_path = paths['enzyme']\n",
        "\n",
        "    ec_to_names = {}  # More descriptive name\n",
        "    with open(enzyme_path, 'r') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split('\\t')\n",
        "            if len(parts) >= 2:\n",
        "                ec_number = parts[0]\n",
        "                names = parts[1].split('; ')\n",
        "                ec_to_names[ec_number] = names\n",
        "\n",
        "    return ec_to_names\n",
        "ec_to_names = read_enzyme_names()\n",
        "#ec_to_names"
      ],
      "metadata": {
        "id": "5JWrGEQYH1JM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Enzyme Class\n",
        "The enzyme classification system (text-based hierarchy)."
      ],
      "metadata": {
        "id": "aUUP7OAF9iaM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_enzyme_class():\n",
        "    paths = setup_paths()\n",
        "    ec_file_path = paths['enzyme_class']\n",
        "\n",
        "    enzyme_class = {}\n",
        "\n",
        "    with open(ec_file_path, 'r') as f:\n",
        "        for line in f:\n",
        "            # Format is like \"1. 1. 1.-    With NAD(+) or NADP(+) as acceptor.\"\n",
        "            if line.strip() and any(line.startswith(str(i)) for i in range(1, 7)):\n",
        "                parts = line.strip().split('  ')\n",
        "                if len(parts) >= 2:\n",
        "                    ec_id = parts[0].replace(' ', '')\n",
        "                    desc = parts[1].strip()\n",
        "                    enzyme_class[ec_id] = desc\n",
        "    return enzyme_class\n",
        "en_class = read_enzyme_class()\n",
        "#"
      ],
      "metadata": {
        "id": "6FTH4E8gZfdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ko Database\n",
        " A new variable mapping  KO numbers to EC numbers.\n",
        "rsync -avz rsync://rest.kegg.jp/kegg/pathway/ ."
      ],
      "metadata": {
        "id": "CBtayQh4eQ82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_ko_data():\n",
        "    \"\"\"Read and parse KEGG KO file\"\"\"\n",
        "    paths = setup_paths()\n",
        "    ko_file_path = paths['ko']\n",
        "\n",
        "    ko_info = {}\n",
        "    with open(ko_file_path, 'r') as f:\n",
        "        for line in f:\n",
        "            if line.startswith('K'):\n",
        "                parts = line.strip().split('\\t')\n",
        "                if len(parts) > 1:\n",
        "                    ko_info[parts[0]] = {\n",
        "                        'definition': parts[1],\n",
        "                        'pathway': parts[2] if len(parts) > 2 else ''\n",
        "                    }\n",
        "    return ko_info\n",
        "\n",
        "ko_ec =read_ko_data()"
      ],
      "metadata": {
        "id": "Nz9K72kaKM52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ko Hierarchi Database\n",
        "Hierarchy of KO numbers (helps in pathway mapping).\n"
      ],
      "metadata": {
        "id": "iBwbp_UkeTI6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_ko_hierarchy():\n",
        "    paths = setup_paths()\n",
        "    ko_path = paths['ko_hierarchy']\n",
        "\n",
        "    hierarchy = {\n",
        "        'A': {},  # Top level\n",
        "        'B': {},  # Category\n",
        "        'C': {},  # Pathway\n",
        "        'D': {}   # KO/Enzyme\n",
        "    }\n",
        "\n",
        "    current = {'A': None, 'B': None, 'C': None}\n",
        "\n",
        "    with open(ko_path, 'r') as f:\n",
        "        for line in f:\n",
        "            if line.startswith('A'):\n",
        "                parts = line.strip().split()\n",
        "                id = parts[1]\n",
        "                name = ' '.join(parts[2:])\n",
        "                hierarchy['A'][id] = name\n",
        "                current['A'] = id\n",
        "\n",
        "            elif line.startswith('B'):\n",
        "                parts = line.strip().split()\n",
        "                id = parts[1]\n",
        "                name = ' '.join(parts[2:])\n",
        "                hierarchy['B'][id] = {'name': name, 'parent': current['A']}\n",
        "                current['B'] = id\n",
        "\n",
        "            elif line.startswith('C'):\n",
        "                parts = line.strip().split()\n",
        "                id = parts[1]\n",
        "                name = ' '.join(parts[2:])\n",
        "                if '[PATH:' in name:\n",
        "                    path_parts = name.split('[PATH:')\n",
        "                    name = path_parts[0].strip()\n",
        "                    path_id = path_parts[1].split(']')[0]\n",
        "                else:\n",
        "                    path_id = None\n",
        "\n",
        "                hierarchy['C'][id] = {\n",
        "                    'name': name,\n",
        "                    'parent': current['B'],\n",
        "                    'path_id': path_id\n",
        "                }\n",
        "                current['C'] = id\n",
        "\n",
        "            elif line.startswith('D'):\n",
        "                parts = line.strip().split()\n",
        "                ko_id = parts[1]\n",
        "                name = ' '.join(parts[2:])\n",
        "\n",
        "                # Extract EC numbers if present\n",
        "                ec_numbers = []\n",
        "                if '[EC:' in name:\n",
        "                    ec_part = name.split('[EC:')[1].split(']')[0]\n",
        "                    ec_numbers = ec_part.split()\n",
        "                    name = name.split('[EC:')[0].strip()\n",
        "\n",
        "                hierarchy['D'][ko_id] = {\n",
        "                    'name': name,\n",
        "                    'parent': current['C'],\n",
        "                    'ec_numbers': ec_numbers\n",
        "                }\n",
        "\n",
        "    return hierarchy\n",
        "\n",
        "ko_hierarchy = read_ko_hierarchy()\n",
        "#ko_hierarchy"
      ],
      "metadata": {
        "id": "poI6QC92e3QM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reaction Data\n",
        " Reaction-level information.\n",
        "\n",
        "!wget -c \"ftp://ftp.genome.jp/pub/kegg/reaction/reaction.tar.gz\""
      ],
      "metadata": {
        "id": "4-kOhcGCcn-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_reaction_data():\n",
        "    paths = setup_paths()\n",
        "    reaction_file_path = paths['reaction']\n",
        "\n",
        "    reaction_info = {}\n",
        "\n",
        "    with open(reaction_file_path, 'r') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "\n",
        "            parts = line.split(None, 1)  # Split on first whitespace\n",
        "            if len(parts) >= 2:\n",
        "                rxn_id = parts[0]\n",
        "                desc_parts = parts[1].split(';')\n",
        "\n",
        "                # First part is reaction name\n",
        "                name = desc_parts[0].strip()\n",
        "\n",
        "                # Rest might contain equation\n",
        "                equation = desc_parts[1].strip() if len(desc_parts) > 1 else \"\"\n",
        "\n",
        "                reaction_info[rxn_id] = {\n",
        "                    'name': name,\n",
        "                    'equation': equation\n",
        "                }\n",
        "\n",
        "    return reaction_info\n",
        "\n",
        "reaction_equation = read_reaction_data()\n",
        "#reaction_equation"
      ],
      "metadata": {
        "id": "s5Pd6vUTLR4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pathway Database\n",
        "hemical compounds database.\n",
        "\n",
        "wget https://biocyc.org/download.shtml\n",
        "\n",
        "wget https://www.brenda-enzymes.org/download.php\n",
        "\n"
      ],
      "metadata": {
        "id": "LIyE7Tx5chQ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_pathway_data():\n",
        "    paths = setup_paths()\n",
        "    pathway_path = paths['pathway']\n",
        "\n",
        "    pathway_info = {}\n",
        "    with open(pathway_path, 'r') as f:\n",
        "          for line in f:\n",
        "              parts = line.strip().split('\\t')\n",
        "              if len(parts) >= 2:\n",
        "                  pathway_id = parts[0]\n",
        "                  pathway_name = parts[1]\n",
        "                  pathway_info[pathway_id] = pathway_name\n",
        "    return pathway_info\n",
        "\n",
        "pathway_data = read_pathway_data()\n",
        "#pathway_data"
      ],
      "metadata": {
        "id": "xu5sqGIPc0op"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Module Database"
      ],
      "metadata": {
        "id": "zxYkP8l0dCXo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_module_data():\n",
        "    paths = setup_paths()\n",
        "    module_path = paths['module']\n",
        "\n",
        "    module_info = {}\n",
        "    with open(module_path, 'r') as f:\n",
        "      for line in f:\n",
        "          parts = line.strip().split('\\t')\n",
        "          if len(parts) >= 2:\n",
        "              module_id = parts[0]\n",
        "              module_desc = parts[1]\n",
        "              module_info[module_id] = module_desc\n",
        "    return module_info\n",
        "\n",
        "module_info = read_module_data()\n",
        "#module_info"
      ],
      "metadata": {
        "id": "0Ue-7KZpdEfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compound Database"
      ],
      "metadata": {
        "id": "W2GClcq8dmrP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_compound_data():\n",
        "    paths = setup_paths()\n",
        "    compound_path = paths['compound']\n",
        "\n",
        "    compound_info = {}\n",
        "    with open(compound_path, 'r') as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split('\\t')\n",
        "                if len(parts) >= 2:\n",
        "                    compound_id = parts[0]\n",
        "                    compound_names = parts[1].split('; ')\n",
        "                    compound_info[compound_id] = {\n",
        "                        'name': compound_names[0],\n",
        "                        'synonyms': compound_names[1:] if len(compound_names) > 1 else []\n",
        "                    }\n",
        "    return compound_info\n",
        "\n",
        "compound_info = read_compound_data()\n",
        "# compound_info"
      ],
      "metadata": {
        "id": "K9riHOnadqSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compound_info"
      ],
      "metadata": {
        "id": "BaOKQxnV47cA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Metal pdb\n",
        "MetalPDB in 2018: a database of metal sites in biological macromolecular structures.\n",
        "Putignano V., Rosato A., Banci L., Andreini C.\n",
        "Nucleic Acids Res. 2018 Jan;46(D1):D459-D464. [PMID: 29077942]\n"
      ],
      "metadata": {
        "id": "hi9MNHn55BT3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_metalpdb_xml():\n",
        "    \"\"\"Parse MetalPDB XML file to extract metal-binding information\"\"\"\n",
        "    paths = setup_paths()\n",
        "    metalpdb_path = paths['metalpdb']\n",
        "\n",
        "    metal_binding_data = {}\n",
        "\n",
        "    try:\n",
        "        # Use a more tolerant parser\n",
        "        parser = etree.XMLParser(recover=True)\n",
        "        tree = etree.parse(metalpdb_path, parser)\n",
        "        root = tree.getroot()\n",
        "\n",
        "        # Process each site\n",
        "        for site in root.findall('.//site'):\n",
        "            # Extract site information\n",
        "            site_name = site.findtext('site_name')\n",
        "            pdb_code = site.findtext('pdb_code')\n",
        "            site_nuclearity = site.findtext('site_nuclearity')\n",
        "\n",
        "            # Process each metal in the site\n",
        "            for metal in site.findall('.//metal'):\n",
        "                metal_symbol = metal.findtext('periodic_symbol')\n",
        "                metal_name = metal.findtext('periodic_name')\n",
        "                coordination_number = metal.findtext('coordination_number')\n",
        "                geometry = metal.findtext('geometry')\n",
        "\n",
        "                # Process ligands\n",
        "                ligands = []\n",
        "                for ligand in metal.findall('.//ligand'):\n",
        "                    residue_name = ligand.findtext('residue_name')\n",
        "                    residue_num = ligand.findtext('residue_pdb_number')\n",
        "                    chain = ligand.findtext('chain_letter')\n",
        "                    binding_type = ligand.findtext('endo_exo')\n",
        "\n",
        "                    # Process donor atoms\n",
        "                    donors = []\n",
        "                    for donor in ligand.findall('.//donor'):\n",
        "                        distance = donor.findtext('distance')\n",
        "                        atom_name = donor.findtext('atom_pdb_name')\n",
        "                        atom_symbol = donor.findtext('atom_symbol')\n",
        "                        interaction_type = donor.findtext('interaction_type')\n",
        "\n",
        "                        donors.append({\n",
        "                            'distance': distance,\n",
        "                            'atom_name': atom_name,\n",
        "                            'atom_symbol': atom_symbol,\n",
        "                            'interaction_type': interaction_type\n",
        "                        })\n",
        "\n",
        "                    ligands.append({\n",
        "                        'residue_name': residue_name,\n",
        "                        'residue_number': residue_num,\n",
        "                        'chain': chain,\n",
        "                        'binding_type': binding_type,\n",
        "                        'donors': donors\n",
        "                    })\n",
        "\n",
        "                # Get the protein/molecule information\n",
        "                site_chains = []\n",
        "                for chain in site.findall('.//site_chain'):\n",
        "                    molecule_name = chain.findtext('molecule_name')\n",
        "                    molecule_type = chain.findtext('molecule_type')\n",
        "                    chain_letter = chain.findtext('letter')\n",
        "\n",
        "                    site_chains.append({\n",
        "                        'molecule_name': molecule_name,\n",
        "                        'molecule_type': molecule_type,\n",
        "                        'chain_letter': chain_letter\n",
        "                    })\n",
        "\n",
        "                # Create a unique key for this metal site\n",
        "                metal_site_key = f\"{pdb_code}_{site_name}_{metal_symbol}\"\n",
        "\n",
        "                # Store the data\n",
        "                metal_binding_data[metal_site_key] = {\n",
        "                    'pdb_code': pdb_code,\n",
        "                    'site_name': site_name,\n",
        "                    'site_nuclearity': site_nuclearity,\n",
        "                    'metal': {\n",
        "                        'symbol': metal_symbol,\n",
        "                        'name': metal_name,\n",
        "                        'coordination_number': coordination_number,\n",
        "                        'geometry': geometry\n",
        "                    },\n",
        "                    'ligands': ligands,\n",
        "                    'site_chains': site_chains\n",
        "                }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing MetalPDB XML: {e}\")\n",
        "        return {}\n",
        "\n",
        "    return metal_binding_data\n",
        "metal_binding_data = parse_metalpdb_xml()"
      ],
      "metadata": {
        "id": "MhoKLqYQihT8"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extracting metal binding patters from metal binding data"
      ],
      "metadata": {
        "id": "7ScqMYjZ3dIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_metal_coordination_patterns(metal_binding_data):\n",
        "    \"\"\"Extract metal coordination patterns from MetalPDB data\"\"\"\n",
        "\n",
        "    # Track metal coordination patterns\n",
        "    metal_coordination = {}\n",
        "    metal_residue_binding = {}\n",
        "\n",
        "    for site_key, site_data in metal_binding_data.items():\n",
        "        metal_symbol = site_data['metal']['symbol']\n",
        "\n",
        "        # Track coordination environments\n",
        "        coord_num = site_data['metal']['coordination_number']\n",
        "        geometry = site_data['metal']['geometry']\n",
        "        coord_key = f\"{metal_symbol}_{coord_num}_{geometry}\"\n",
        "\n",
        "        if coord_key not in metal_coordination:\n",
        "            metal_coordination[coord_key] = 0\n",
        "        metal_coordination[coord_key] += 1\n",
        "\n",
        "        # Track metal-residue binding\n",
        "        if metal_symbol not in metal_residue_binding:\n",
        "            metal_residue_binding[metal_symbol] = {}\n",
        "\n",
        "        for ligand in site_data['ligands']:\n",
        "            residue = ligand['residue_name']\n",
        "            if residue not in metal_residue_binding[metal_symbol]:\n",
        "                metal_residue_binding[metal_symbol][residue] = 0\n",
        "            metal_residue_binding[metal_symbol][residue] += 1\n",
        "\n",
        "    return {\n",
        "        'coordination': metal_coordination,\n",
        "        'residue_binding': metal_residue_binding\n",
        "    }"
      ],
      "metadata": {
        "id": "2SwP7fDN3so5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cWllweaD68Le"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EC to reaction Mapping"
      ],
      "metadata": {
        "id": "jdl9jF7TikIR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_ec_to_reaction_mapping():\n",
        "    # Get EC to enzyme names mapping\n",
        "    ec_to_names = read_enzyme_names()\n",
        "\n",
        "    # Get reaction data\n",
        "    reaction_info = read_reaction_data()\n",
        "\n",
        "    # Create a mapping from EC to reactions\n",
        "    ec_to_reaction = {}\n",
        "\n",
        "    # Use string pattern matching to find EC numbers in reaction names\n",
        "    for rxn_id, rxn_info in reaction_info.items():\n",
        "        rxn_name = rxn_info['name'].lower()\n",
        "\n",
        "        # Look through all EC numbers and their names\n",
        "        for ec, names in ec_to_names.items():\n",
        "            enzyme_text = ' '.join(names).lower()\n",
        "\n",
        "            # Check for common significant words\n",
        "            if any(word in rxn_name for word in enzyme_text.split() if len(word) > 4):\n",
        "                if ec not in ec_to_reaction:\n",
        "                    ec_to_reaction[ec] = []\n",
        "                if rxn_id not in ec_to_reaction[ec]:\n",
        "                    ec_to_reaction[ec].append(rxn_id)\n",
        "\n",
        "    return ec_to_reaction\n",
        "\n",
        "ec_to_reaction = create_ec_to_reaction_mapping()\n",
        "#ec_to_reaction"
      ],
      "metadata": {
        "id": "FH9jMXKKSJM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Search Metabolic Pathway"
      ],
      "metadata": {
        "id": "gcWY_04y56yF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10.2 Creating an Integrated Database"
      ],
      "metadata": {
        "id": "QQ3q5YlPYZWJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Smqh2fy8-PJP"
      },
      "outputs": [],
      "source": [
        "def create_metabolism_database():\n",
        "    # Read all necessary files\n",
        "    ec_to_names = read_enzyme_names()\n",
        "    en_class = read_enzyme_class()\n",
        "    reaction_equation = read_reaction_data()\n",
        "    ko_ec = read_ko_data()\n",
        "    ko_hierarchy = read_ko_hierarchy()\n",
        "    pathway_data = read_pathway_data()\n",
        "    module_info = read_module_data()\n",
        "    compound_info = read_compound_data()\n",
        "    brenda_en = read_brenda_enzymes()\n",
        "\n",
        "    # Enhanced metal and corrosion keywords\n",
        "    metal_terms = {\n",
        "        'iron': ['iron', 'iron reduction','fe', 'ferrous', 'ferric', 'heme', 'iron-sulfur', 'Fe2+', 'Fe3+'],\n",
        "        'sulfur': ['sulfate', 'sulfide', 'thiosulfate', 'S-S', 'sulfur'],\n",
        "        'hydrogen': ['hydrogen', 'hydrogenase', 'h2'],\n",
        "        'manganese': ['Mn2+', 'manganese', 'mn', 'manganous', 'manganic', 'manganese oxidation', 'metal oxide'],\n",
        "        'biofilm': ['exopolysaccharide', 'biofilm', 'adhesin', 'eps', 'polysaccharide'],\n",
        "        'copper': ['Cu+', 'Cu2+', 'copper'],\n",
        "        'nickel': ['Ni2+', 'nickel'],\n",
        "        'cobalt': ['Co2+', 'cobalt'],\n",
        "        'calcium': ['Ca2+', 'calcium'],\n",
        "        'Mo': ['Mo', 'molybdenum'],\n",
        "        'V5+': ['V5+', 'vanadium'],\n",
        "        'Al3+': ['Al3+', 'aluminum'],\n",
        "        'Cr3+': ['Cr3+', 'chromium'],\n",
        "        'zinc': ['Zn2+', 'zinc'],\n",
        "        'sodium': ['Na+', 'sodium', 'NaCl'],\n",
        "        'potassium': ['K+', 'potassium', 'KCl'],\n",
        "        'selenium': ['selenium', 'Se'],\n",
        "        'barium': ['Ba2+', 'barium']\n",
        "    }\n",
        "\n",
        "    # Corrosion mechanism classification\n",
        "    corrosion_mechanisms = {\n",
        "          'direct_eet': ['cytochrome', 'electron transfer', 'conductive pili', 'nanowire', 'mtrABC', 'omcS','oxidoreductase', 'redox', 'reductase', 'oxidase'],\n",
        "          'indirect_eet': ['shuttle', 'mediator', 'redox mediator'],\n",
        "          'acid_production': ['acid', 'acidification', 'fermentation', 'lactic acid', 'formic acid', 'acetic acid', 'oxalic acid', 'organic acid'],\n",
        "          'h2_consumption': ['hydrogenase', 'hydrogen uptake', 'hydrogen consumption', 'h2', 'H2 oxidation', 'H2ase'],\n",
        "          'o2_consumption': ['oxidase', 'oxygen reduction', 'aerobic respiration','oxygen reduc', 'aerobic respiration', 'oxygen consum'],\n",
        "          'biofilm_formation': ['polysaccharide', 'adhesin', 'biofilm', 'EPS', 'extracellular polymeric substance', 'curli', 'exopolymer',],\n",
        "          'sulfur_metabolism': ['sulfate reduc', 'sulfide', 'sulfite', 'thiosulfate', 'sulfur oxidation', 'SRB'],\n",
        "          'metal_transformation': ['iron reduction', 'manganese oxidation', 'metal oxide'],\n",
        "          'iron_metabolism': ['iron reduc', 'ferric reduc', 'iron oxid', 'ferrous oxid'],\n",
        "    }\n",
        "\n",
        "    # Get EC to reaction mapping\n",
        "    ec_to_rxn = create_ec_to_reaction_mapping()\n",
        "\n",
        "    # Create EC number metadata\n",
        "    ec_metadata = {}\n",
        "\n",
        "    # Add basic enzyme names\n",
        "    for ec_num, names in ec_to_names.items():\n",
        "        ec_metadata[ec_num] = {\n",
        "            'names': names,\n",
        "            'pathways': ko_ec.get(ec_num, []),\n",
        "            'hierarchy': [],\n",
        "            'ko': [ko for ko, data in ko_ec.items() if f\"[EC:{ec_num}]\" in data['definition']],\n",
        "            'reactions': [{'id':rxn, 'name':reaction_equation[rxn]['equation']} for rxn in ec_to_rxn.get(ec_num, [])]}\n",
        "\n",
        "        # Add BRENDA metal information if available\n",
        "        if brenda_en and ec_num in brenda_en:\n",
        "            ec_metadata[ec_num]['metals_from_brenda'] = brenda_en[ec_num].get('clean_metals', [])\n",
        "            ec_metadata[ec_num]['corrosion_relevant_metals'] = brenda_en[ec_num].get('corrosion_relevant_metals', [])\n",
        "\n",
        "    for ec_num, metadata in ec_metadata.items():\n",
        "        all_text = ' '.join(metadata['names'] + metadata.get('class', ''))\n",
        "        # Check for metal involvement with more detail\n",
        "        lower_text = all_text.lower()\n",
        "        metadata['metals_involved'] = {metal: any(term in lower_text for term in terms)\n",
        "            for metal, terms in metal_terms.items()}\n",
        "\n",
        "        # Classify by corrosion mechanism\n",
        "        metadata['corrosion_mechanisms'] = [mechanism for mechanism, terms in corrosion_mechanisms.items()\n",
        "                if any(term in lower_text for term in terms)]\n",
        "\n",
        "    # Add KO information and hierarchy\n",
        "    for ko, info in ko_hierarchy['D'].items():\n",
        "        for ec in info['ec_numbers']:\n",
        "            if ec in ec_metadata:\n",
        "                # Add pathway hierarchy\n",
        "                parent_c = info['parent']\n",
        "                if parent_c in ko_hierarchy['C']:\n",
        "                    pathway_info = ko_hierarchy['C'][parent_c] # Local variable\n",
        "                    parent_b = pathway_info['parent']\n",
        "                    if parent_b in ko_hierarchy['B']:\n",
        "                        category = ko_hierarchy['B'][parent_b]['name']\n",
        "                        pathway = pathway_info['name']\n",
        "\n",
        "                        ec_metadata[ec]['pathways'].append(pathway) if pathway not in ec_metadata[ec]['pathways'] else None\n",
        "                        hierarchy = f\"{category} > {pathway}\"\n",
        "                        ec_metadata[ec]['hierarchy'].append(hierarchy) if hierarchy not in ec_metadata[ec]['hierarchy'] else None\n",
        "\n",
        "    # Add enzyme class information\n",
        "    for ec_num in ec_metadata:\n",
        "        # Find matching class (first 2 parts of EC number)\n",
        "        ec_prefix = '.'.join(ec_num.split('.')[:2])\n",
        "        if ec_prefix in en_class:\n",
        "            ec_metadata[ec_num]['class'] = en_class[ec_prefix]\n",
        "\n",
        "    for ec_num, metadata in ec_metadata.items():\n",
        "        all_text = ' '.join(metadata['names'])\n",
        "\n",
        "    return ec_metadata\n",
        "#ec_metadata = create_metabolism_database()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G_pX1Jqj4KNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Enrich EC_metadata with Brenda Data"
      ],
      "metadata": {
        "id": "MFcsjSBJyn-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###def enrich_ec_metadata_with_brenda(ec_metadata, brenda_processed_data):\n",
        "    \"\"\"Integrate BRENDA metal information with existing EC metadata\"\"\"\n",
        "\n",
        "    for ec_num, metadata in ec_metadata.items():\n",
        "        if ec_num in brenda_processed_data:\n",
        "            # Add BRENDA metal information\n",
        "            metadata['metals_from_brenda'] = brenda_processed_data[ec_num].get('clean_metals', [])\n",
        "            metadata['corrosion_metal_relevance'] = brenda_processed_data[ec_num].get('corrosion_relevance', 'unknown')\n",
        "\n",
        "            # Enhance metal detection with BRENDA data\n",
        "            if 'has_metal' in metadata:\n",
        "                metadata['has_metal'] = metadata['has_metal'] or len(metadata['metals_from_brenda']) > 0\n",
        "            else:\n",
        "                metadata['has_metal'] = len(metadata['metals_from_brenda']) > 0\n",
        "\n",
        "            # Add other BRENDA data if it exists\n",
        "            for field in ['cofactors', 'reactions', 'substrates', 'inhibitors']:\n",
        "                if field in brenda_processed_data[ec_num]:\n",
        "                    metadata[f'brenda_{field}'] = brenda_processed_data[ec_num][field]\n",
        "\n",
        "    return ec_metadata\n",
        "\n",
        "def add_corrosion_relevance_score(ec_metadata):\n",
        "    \"\"\"Add a comprehensive corrosion relevance score to EC metadata\"\"\"\n",
        "\n",
        "    # Define corrosion-relevant reaction patterns\n",
        "    redox_patterns = ['oxid', 'reduc', 'electron', 'transfer']\n",
        "    sulfur_patterns = ['sulfate', 'sulfide', 'thiosulfate', 'sulfur']\n",
        "    biofilm_patterns = ['polysaccharide', 'biofilm', 'exopolymer']\n",
        "\n",
        "    for ec_num, metadata in ec_metadata.items():\n",
        "        score = 0\n",
        "\n",
        "        # Score based on metal involvement\n",
        "        if metadata.get('has_metal', False):\n",
        "            score += 1\n",
        "\n",
        "        # Additional points for high-relevance metals\n",
        "        if metadata.get('corrosion_metal_relevance') == 'high':\n",
        "            score += 2\n",
        "        elif metadata.get('corrosion_metal_relevance') == 'medium':\n",
        "            score += 1\n",
        "\n",
        "        # Score based on reaction patterns\n",
        "        reactions = []\n",
        "        if 'brenda_reactions' in metadata:\n",
        "            reactions.extend(metadata['brenda_reactions'])\n",
        "\n",
        "        reaction_text = ' '.join(reactions).lower()\n",
        "        if any(pattern in reaction_text for pattern in redox_patterns):\n",
        "            score += 2\n",
        "        if any(pattern in reaction_text for pattern in sulfur_patterns):\n",
        "            score += 2\n",
        "        if any(pattern in reaction_text for pattern in biofilm_patterns):\n",
        "            score += 1\n",
        "\n",
        "        # Score based on pathway relevance\n",
        "        pathways = metadata.get('pathways', [])\n",
        "        pathway_text = ' '.join(pathways).lower()\n",
        "\n",
        "        if 'sulfur' in pathway_text or 'sulfate' in pathway_text:\n",
        "            score += 2\n",
        "        if 'iron' in pathway_text or 'metal' in pathway_text:\n",
        "            score += 2\n",
        "        if 'electron' in pathway_text or 'transport' in pathway_text:\n",
        "            score += 1\n",
        "\n",
        "        # Add the final score\n",
        "        metadata['corrosion_relevance_score'] = score\n",
        "\n",
        "    return ec_metadata\n",
        "\n",
        "def filter_ec_by_corrosion_relevance(ec_contribution_data, ec_metadata, min_score=3):\n",
        "    \"\"\"Filter EC contribution data to focus on corrosion-relevant enzymes\"\"\"\n",
        "    relevant_ecs = [ec for ec, metadata in ec_metadata.items()\n",
        "                   if metadata.get('corrosion_relevance_score', 0) >= min_score]\n",
        "\n",
        "    # Filter contribution data to include only relevant ECs\n",
        "    relevant_columns = [col for col in ec_contribution_data.columns\n",
        "                       if any(ec in col for ec in relevant_ecs)]\n",
        "\n",
        "    return ec_contribution_data[relevant_columns]"
      ],
      "metadata": {
        "id": "V7yFtgUhylQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Enrich ec_metadata with metal binding patterns form dpb"
      ],
      "metadata": {
        "id": "Omt5Btd52qY6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def enrich_ec_metadata_with_metal_binding(ec_metadata, metal_binding_patterns):\n",
        "    \"\"\"Enhance EC metadata with metal binding information from MetalPDB\"\"\"\n",
        "\n",
        "    # Corrosion-relevant metals\n",
        "    corrosion_metals = ['Fe', 'Mn', 'Cu', 'Ni', 'Co', 'Zn']\n",
        "    metal_mapping = {\n",
        "    'iron': 'Fe',\n",
        "    'manganese': 'Mn',\n",
        "    'copper': 'Cu',\n",
        "    'nickel': 'Ni',\n",
        "    'cobalt': 'Co',\n",
        "    'zinc': 'Zn',\n",
        "    'calcium': 'Ca',\n",
        "    'magnesium': 'Mg',\n",
        "    'sodium': 'Na',\n",
        "    'potassium': 'K',\n",
        "    'selenium': 'Se',\n",
        "    'molybdenum': 'Mo',\n",
        "    'vanadium': 'V',\n",
        "    'aluminum': 'Al',\n",
        "    'chromium': 'Cr',\n",
        "    'barium': 'Ba'\n",
        "    }\n",
        "    # Update EC metadata with metal binding potential\n",
        "    for ec_num, metadata in ec_metadata.items():\n",
        "        # Initialize metal binding fields\n",
        "        metadata['metal_binding_potential'] = {}\n",
        "\n",
        "        # Check if we have brenda cofactor information\n",
        "        if 'metals_from_brenda' in metadata:\n",
        "            for metal in metadata['metals_from_brenda']:\n",
        "                # Standardize metal symbol\n",
        "                metal_symbol = next((m for m in metal_binding_patterns['metal_preferences'] if metal.startswith(m)), metal[:2].strip())\n",
        "\n",
        "                if metal_symbol in metal_binding_patterns['metal_preferences']:\n",
        "                    # Add metal binding potential information\n",
        "                    metadata['metal_binding_potential'][metal_symbol] = {\n",
        "                        'potential': 'high',\n",
        "                        'common_residues': list(metal_binding_patterns['metal_preferences'][metal_symbol].keys())[:5]\n",
        "                    }\n",
        "\n",
        "        # Update corrosion relevance score based on metal binding\n",
        "        binding_score = sum(\n",
        "            1 for metal in corrosion_metals\n",
        "            if metal in metadata['metal_binding_potential']\n",
        "        )\n",
        "\n",
        "        if 'corrosion_relevance_score' in metadata:\n",
        "            metadata.setdefault('corrosion_relevance_score', 0)\n",
        "            metadata['corrosion_relevance_score'] += binding_score\n",
        "        else:\n",
        "            metadata['corrosion_relevance_score'] = binding_score\n",
        "\n",
        "    return ec_metadata"
      ],
      "metadata": {
        "id": "bhTV7ynq2q4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add relevance metal scores really?"
      ],
      "metadata": {
        "id": "WJ1i-wc-24Ny"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "joEZMlU42790"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving the database"
      ],
      "metadata": {
        "id": "yCE1edxRX7Sf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''import pickle\n",
        "\n",
        "def save_ec_metadata(ec_metadata, filename=\"ec_metadata.pkl\"):\n",
        "    # Full path to save file\n",
        "    full_path = db_dir / filename\n",
        "\n",
        "    # Save the file\n",
        "    with open(full_path, 'wb') as f:\n",
        "        pickle.dump(ec_metadata, f)\n",
        "\n",
        "    print(f\"Saved EC metadata with {len(ec_metadata)} entries to {full_path}\")\n",
        "    return full_path\n",
        "\n",
        "# Usage\n",
        "save_ec_metadata(ec_metadata)'''"
      ],
      "metadata": {
        "id": "ETGWSCZYX9nI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. 3  Filter Enriched Dataframe of ECcontri\n"
      ],
      "metadata": {
        "id": "6zqpU2g8Xpkv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def enrich_eccontri_data(eccontri_df, enriched_ec_metadata):\n",
        "    \"\"\"\n",
        "    Enrich the ECcontri_Uniprot dataframe with complete information from the database\n",
        "    \"\"\"\n",
        "    # Make a copy to avoid modifying the original\n",
        "    enriched_df = eccontri_df.copy()\n",
        "\n",
        "    # Extract EC numbers from protein_name column\n",
        "    enriched_df['EC_number'] = enriched_df['protein_name'].str.extract(r'EC (\\d+\\.\\d+\\.\\d+\\.\\d+)')\n",
        "\n",
        "    # Add all metadata columns as needed\n",
        "    metadata_columns = ['enzyme_names', 'enzyme_class', 'pathways', 'has_metal', 'metal_types', 'hierarchy']\n",
        "    for col in metadata_columns:\n",
        "        enriched_df[col] = None\n",
        "\n",
        "    # Detailed metal analysis\n",
        "    metal_types = {\n",
        "        'iron': ['iron', 'fe', 'heme', 'ferr'],\n",
        "        'zinc': ['zinc', 'zn'],\n",
        "        'copper': ['copper', 'cu'],\n",
        "        'manganese': ['manganese', 'mn'],\n",
        "        'nickel': ['nickel', 'ni'],\n",
        "        'cobalt': ['cobalt', 'co'],\n",
        "        'molybdenum': ['molybdenum', 'mo'],\n",
        "        'aluminium': ['aluminium', 'al'],\n",
        "        'chromium': ['chromium', 'cr']\n",
        "    }\n",
        "\n",
        "    # Add metadata based on EC number\n",
        "    for idx, row in enriched_df.iterrows():\n",
        "        ec_num = row['EC_number']\n",
        "        if pd.notna(ec_num) and ec_num in ec_metadata:\n",
        "            metadata = ec_metadata[ec_num]\n",
        "\n",
        "            # Add all available metadata\n",
        "            enriched_df.at[idx, 'enzyme_names'] = '; '.join(metadata['names'])\n",
        "            if 'class' in metadata:  # This matches the key in create_metabolism_database\n",
        "                enriched_df.at[idx, 'enzyme_class'] = metadata['class']\n",
        "            if 'pathways' in metadata:\n",
        "                enriched_df.at[idx, 'pathways'] = '; '.join(metadata['pathways'])\n",
        "            if 'hierarchy' in metadata:\n",
        "                enriched_df.at[idx, 'hierarchy'] = '; '.join(metadata['hierarchy'])\n",
        "\n",
        "            # Detailed metal analysis\n",
        "            enriched_df.at[idx, 'has_metal'] = metadata['has_metal']\n",
        "\n",
        "            # Which specific metals are involved\n",
        "            found_metals = []\n",
        "            all_text = ' '.join(metadata['names']).lower()\n",
        "            for metal, keywords in metal_types.items():\n",
        "                if any(keyword in all_text for keyword in keywords):\n",
        "                    found_metals.append(metal)\n",
        "\n",
        "            if found_metals:\n",
        "                enriched_df.at[idx, 'metal_types'] = '; '.join(found_metals)\n",
        "\n",
        "    def filter_MIC_relevant(protein_dict):\n",
        "      \"\"\"\n",
        "      Retains only proteins directly involved in MIC processes.\n",
        "      \"\"\"\n",
        "      MIC_keywords = ['electron transfer', 'sulfate reduction', 'biofilm', 'hydrogen oxidation', 'metal transport']\n",
        "\n",
        "      return {prot: data for prot, data in protein_dict.items() if any(term in data['Pathway'] for term in MIC_keywords)}\n",
        "\n",
        "\n",
        "    return enriched_df\n",
        "\n",
        "enriched_df = enrich_eccontri_data(ECcontri_Uniprot, ec_metadata)\n"
      ],
      "metadata": {
        "id": "--jwekk9SHE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''def save_enriched_df(enriched_df, filename=\"enriched_eccontri.csv\"):\n",
        "    # Full path to save file\n",
        "    full_path = db_dir / filename\n",
        "\n",
        "    # Save the file\n",
        "    enriched_df.to_csv(full_path, index=False)\n",
        "\n",
        "    print(f\"Saved enriched dataframe with {len(enriched_df)} rows to {full_path}\")\n",
        "    return full_path\n",
        "\n",
        "# Saving df with the enriched data\n",
        "save_enriched_df(enriched_df)'''"
      ],
      "metadata": {
        "id": "hH6bsSelrm3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filtering\n",
        "can be with  identify_corrosion_marker_proteins(ec_contribution_data, risk_categories, ec_metadata):\n",
        "    \"\"\"Identify statistically significant marker proteins across corrosion risk categories\"\n",
        "\n",
        "or maybe"
      ],
      "metadata": {
        "id": "XGog1uriHCI0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def identify_corrosion_marker_proteins(ec_contribution_data, risk_categories, ec_metadata):\n",
        "    \"\"\"Identify statistically significant marker proteins across corrosion risk categories\"\"\"\n",
        "    import scipy.stats as stats\n",
        "    import pandas as pd\n",
        "\n",
        "    # Step 1: Normalize data to account for sequencing depth\n",
        "    normalized_data = ec_contribution_data.div(ec_contribution_data.sum(axis=1), axis=0)\n",
        "\n",
        "    # Step 2: Group by risk category\n",
        "    grouped_data = {\n",
        "        cat: normalized_data[normalized_data.index.isin(samples)]\n",
        "        for cat, samples in risk_categories.items()\n",
        "    }\n",
        "\n",
        "    # Step 3: Calculate differential abundance\n",
        "    significant_proteins = {}\n",
        "\n",
        "    for ec_id in ec_contribution_data.columns:\n",
        "        # Skip if no metadata available\n",
        "        if ec_id not in ec_metadata:\n",
        "            continue\n",
        "\n",
        "        # Extract abundance values by category\n",
        "        cat_values = {\n",
        "            cat: group[ec_id].values\n",
        "            for cat, group in grouped_data.items()\n",
        "        }\n",
        "\n",
        "        # Perform Kruskal-Wallis test\n",
        "        if len(cat_values) >= 2:\n",
        "            try:\n",
        "                h_stat, p_value = stats.kruskal(*cat_values.values())\n",
        "\n",
        "                # If significant difference exists\n",
        "                if p_value < 0.05:\n",
        "                    # Calculate fold changes between categories\n",
        "                    mean_values = {cat: values.mean() for cat, values in cat_values.items()}\n",
        "\n",
        "                    # Create fold change matrix\n",
        "                    categories = sorted(cat_values.keys())\n",
        "                    fold_changes = {}\n",
        "\n",
        "                    for i, cat1 in enumerate(categories):\n",
        "                        for cat2 in categories[i+1:]:\n",
        "                            # Avoid division by zero\n",
        "                            if mean_values[cat1] > 0 and mean_values[cat2] > 0:\n",
        "                                fold_changes[f'{cat1}_vs_{cat2}'] = mean_values[cat2] / mean_values[cat1]\n",
        "\n",
        "                    # Store significant results\n",
        "                    significant_proteins[ec_id] = {\n",
        "                        'p_value': p_value,\n",
        "                        'mean_values': mean_values,\n",
        "                        'fold_changes': fold_changes,\n",
        "                        'corrosion_score': ec_metadata[ec_id].get('corrosion_relevance_score', 0)\n",
        "                    }\n",
        "            except:\n",
        "                # Handle cases with insufficient data\n",
        "                pass\n",
        "\n",
        "    # Create DataFrame for easier analysis\n",
        "    results_df = pd.DataFrame.from_dict(significant_proteins, orient='index')\n",
        "\n",
        "    # Filter for proteins with both statistical significance and corrosion relevance\n",
        "    corrosion_markers = results_df[results_df['corrosion_score'] >= 2].sort_values(\n",
        "        by=['p_value', 'corrosion_score'], ascending=[True, False]\n",
        "    )\n",
        "\n",
        "    return corrosion_markers"
      ],
      "metadata": {
        "id": "yINdeebdHEIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4I8la_Y-PJP"
      },
      "source": [
        "## Classifying the Dataframes depending on the MIC Containing Proteins/Pathways"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_for_statistical_analysis(ec_with_genera, ko_with_genera, sample_metadata):\n",
        "    \"\"\"\n",
        "    Prepare data for statistical analysis by adding risk categories\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    ec_with_genera : DataFrame\n",
        "        EC data with genera information\n",
        "    ko_with_genera : DataFrame\n",
        "        KO data with genera information\n",
        "    sample_metadata : DataFrame\n",
        "        Contains sample IDs and their risk categories\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    ec_analysis_ready : DataFrame\n",
        "        EC data ready for statistical analysis\n",
        "    ko_analysis_ready : DataFrame\n",
        "        KO data ready for statistical analysis\n",
        "    \"\"\"\n",
        "    # Assuming sample_metadata has columns 'sample' and 'risk_category'\n",
        "\n",
        "    # Add risk categories to EC data\n",
        "    ec_analysis_ready = ec_with_genera.merge(\n",
        "        sample_metadata,\n",
        "        on='sample',\n",
        "        how='left'\n",
        "    )\n",
        "\n",
        "    # Add risk categories to KO data\n",
        "    ko_analysis_ready = ko_with_genera.merge(\n",
        "        sample_metadata,\n",
        "        on='sample',\n",
        "        how='left'\n",
        "    )\n",
        "\n",
        "    # Create genus-function pairs for easier analysis\n",
        "    ec_analysis_ready['genus_function'] = ec_analysis_ready['Genus'] + '_' + ec_analysis_ready['function']\n",
        "    ko_analysis_ready['genus_function'] = ko_analysis_ready['Genus'] + '_' + ko_analysis_ready['function']\n",
        "\n",
        "    return ec_analysis_ready, ko_analysis_ready"
      ],
      "metadata": {
        "id": "3UU-TLNggPk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dq8C9vSywZjZ"
      },
      "source": [
        "### Filtering pairs Bacteria-Protein by significance to the risk category by claude\n",
        "It was thought to filter the data by Bacteria, however if this point of view is stablish, some of the pionier species will be neglected and maybe some bystanders will be left. So we are going to filter at the protein level instead of the genus level, which means that for a data to continue it has to have some sort of involvement with corrosion phenomena."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQbYGLCDGd64"
      },
      "outputs": [],
      "source": [
        "'''import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import kruskal\n",
        "from typing import List, Tuple, Dict\n",
        "\n",
        "def classify_proteins_by_category(base_matrix, category_level: int = 1) -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Classify proteins based on their presence and abundance patterns across categories.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    base_matrix : pd.DataFrame\n",
        "        MultiIndex DataFrame with (Site, Category) as rows and (Genus, Protein) as columns\n",
        "    category_level : int\n",
        "        Level in the row MultiIndex that contains categories\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    Dict[str, pd.DataFrame]: Dictionary containing classified proteins with their statistics\n",
        "    \"\"\"\n",
        "    # Get unique categories\n",
        "    categories = sorted(base_matrix.index.get_level_values(category_level).unique())\n",
        "\n",
        "    # Create dictionary to store presence/absence matrices for each category\n",
        "    cat_data = {}\n",
        "    for cat in categories:\n",
        "        cat_data[cat] = base_matrix.xs(cat, level=category_level, axis=0)\n",
        "\n",
        "    # Calculate mean abundance for each protein in each category\n",
        "    cat_means = {cat: data.mean() for cat, data in cat_data.items()}\n",
        "\n",
        "    # Calculate presence (where abundance > 0) for each protein in each category\n",
        "    cat_presence = {cat: (data > 0).any() for cat, data in cat_data.items()}\n",
        "\n",
        "    # Create classification DataFrame\n",
        "    classification_data = []\n",
        "    for col in base_matrix.columns:\n",
        "        genus, protein = col\n",
        "        presence_pattern = tuple(cat_presence[cat][col] for cat in categories)\n",
        "        means = [cat_means[cat][col] for cat in categories]\n",
        "\n",
        "        # Determine abundance pattern\n",
        "        abundance_pattern = \"increasing\" if all(means[i] <= means[i+1] for i in range(len(means)-1)) else \\\n",
        "                          \"decreasing\" if all(means[i] >= means[i+1] for i in range(len(means)-1)) else \\\n",
        "                          \"mixed\"\n",
        "\n",
        "        classification_data.append({\n",
        "            'Genus': genus,\n",
        "            'Protein': protein,\n",
        "            'Presence_Pattern': presence_pattern,\n",
        "            'Abundance_Pattern': abundance_pattern,\n",
        "            **{f'Mean_Cat_{cat}': means[i] for i, cat in enumerate(categories)}\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(classification_data)\n",
        "\n",
        "def perform_statistical_tests(base_matrix, classifications: pd.DataFrame,\n",
        "                            category_level: int = 1, alpha: float = 0.05) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Perform Kruskal-Wallis tests and post-hoc analysis on classified proteins.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    base_matrix : pd.DataFrame\n",
        "        Original data matrix\n",
        "    classifications : pd.DataFrame\n",
        "        Output from classify_proteins_by_category\n",
        "    category_level : int\n",
        "        Level in the row MultiIndex that contains categories\n",
        "    alpha : float\n",
        "        Significance level for statistical tests\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    pd.DataFrame: Statistical test results\n",
        "    \"\"\"\n",
        "    categories = sorted(base_matrix.index.get_level_values(category_level).unique())\n",
        "\n",
        "    # Prepare results storage\n",
        "    stat_results = []\n",
        "\n",
        "    for _, row in classifications.iterrows():\n",
        "        genus, protein = row['Genus'], row['Protein']\n",
        "        col = (genus, protein)\n",
        "\n",
        "        # Get data for each category\n",
        "        cat_data = [base_matrix.xs(cat, level=category_level)[col] for cat in categories]\n",
        "\n",
        "        # Perform Kruskal-Wallis test\n",
        "        h_stat, p_val = kruskal(*cat_data)\n",
        "\n",
        "        # Calculate effect sizes (difference between medians)\n",
        "        medians = [data.median() for data in cat_data]\n",
        "        effect_sizes = [medians[i+1] - medians[i] for i in range(len(medians)-1)]\n",
        "\n",
        "        # Check if pattern meets criteria (cat3 > cat1 and consistent increases)\n",
        "        valid_pattern = (medians[-1] > medians[0]) and all(eff >= 0 for eff in effect_sizes)\n",
        "\n",
        "        if p_val < alpha and valid_pattern:\n",
        "            stat_results.append({\n",
        "                'Genus': genus,\n",
        "                'Protein': protein,\n",
        "                'H_statistic': h_stat,\n",
        "                'p_value': p_val,\n",
        "                'Pattern_Valid': valid_pattern,\n",
        "                **{f'Median_Cat_{cat}': med for cat, med in zip(categories, medians)},\n",
        "                **{f'Effect_Size_{i+1}_to_{i+2}': eff for i, eff in enumerate(effect_sizes)}\n",
        "            })\n",
        "\n",
        "    return pd.DataFrame(stat_results).sort_values('p_value')\n",
        "\n",
        "def analyze_protein_patterns(base_matrix, category_level: int = 1, alpha: float = 0.05):\n",
        "    \"\"\"\n",
        "    Main function to analyze protein patterns across categories.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    base_matrix : pd.DataFrame\n",
        "        Input data matrix\n",
        "    category_level : int\n",
        "        Level in the row MultiIndex that contains categories\n",
        "    alpha : float\n",
        "        Significance level for statistical tests\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    Tuple[pd.DataFrame, pd.DataFrame]: Classifications and statistical results\n",
        "    \"\"\"\n",
        "    # Step 1: Classify proteins\n",
        "    classifications = classify_proteins_by_category(base_matrix, category_level)\n",
        "\n",
        "    # Step 2: Perform statistical tests\n",
        "    significant_results = perform_statistical_tests(base_matrix, classifications,\n",
        "                                                 category_level, alpha)\n",
        "\n",
        "    return classifications, significant_results\n",
        "# Run the analysis\n",
        "classifications, significant_results = analyze_protein_patterns(base_matrix, category_level=1, alpha=0.05)'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qKPfQ-TpBedk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lq8qP93d-PJP"
      },
      "outputs": [],
      "source": [
        "significant_results.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4jMs0B6-PJP"
      },
      "outputs": [],
      "source": [
        "all_genera =classifications['Genus'].unique()\n",
        "significant_genera = significant_results['Genus'].unique()\n",
        "genera_removed = set(all_genera) - set(significant_genera)\n",
        "genera_removed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7AigABQiIH9"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2P2IgJdFjM2"
      },
      "outputs": [],
      "source": [
        "# View classifications\n",
        "print(\"Protein Classifications:\")\n",
        "print(classifications.head())\n",
        "\n",
        "# View significant results\n",
        "print(\"\\nSignificant Results:\")\n",
        "print(significant_results.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_830Bc_bGY3w"
      },
      "source": [
        "## Adapting for Source Groups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JaOwWSY1DFP9"
      },
      "outputs": [],
      "source": [
        "# Suppose known_bacteria_list is your list of genera\n",
        "group_cols_known = [col for col in base_matrix.columns if col[0] in known_bacteria_list]\n",
        "base_matrix_known = base_matrix.loc[:, group_cols_known]\n",
        "\n",
        "plot_top_proteins_across_categories(\n",
        "    base_matrix_known,\n",
        "    categories=[1,2,3],\n",
        "    n_top=5,\n",
        "    n_genera=10,\n",
        "    category_level=1\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tLvr9eYYel7"
      },
      "source": [
        "# 10. Pathways Analysis\n",
        "\n",
        "## 10.1. Pathways distribution by Risk Category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T10:23:06.776683Z",
          "iopub.status.busy": "2025-02-19T10:23:06.776256Z",
          "iopub.status.idle": "2025-02-19T10:23:13.31924Z",
          "shell.execute_reply": "2025-02-19T10:23:13.31809Z",
          "shell.execute_reply.started": "2025-02-19T10:23:06.776651Z"
        },
        "id": "dakf67H0Yel7",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def prepare_pathway_pca(metabolic_info, use_col='Pathways'):\n",
        "    \"\"\"\n",
        "    Convert pathway strings to numeric features for PCA\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    metabolic_info : pandas.DataFrame,   DataFrame with 'Pathways' column containing comma-separated pathway strings\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    X_pca : numpy.ndarray,    PCA transformed data\n",
        "    explained_variance_ratio : numpy.ndarray,     Explained variance ratios\n",
        "    loadings : pandas.DataFrame,   PCA loadings with pathway names as index\n",
        "    pathway_matrix : pandas.Dataframe,  Binary matrix of pathway presence/absence (useful for further analysis)\n",
        "    \"\"\"\n",
        "    # Handle NaN values first\n",
        "    valid_data = metabolic_info[metabolic_info[use_col].notna()]\n",
        "\n",
        "    # Create set of unique items with explicit string handling\n",
        "    all_items = set()\n",
        "    for item_str in valid_data[use_col]:\n",
        "        if isinstance(item_str, str):  # Ensure it's a string\n",
        "            items = [i.strip() for i in item_str.strip('[]').split(',') if i.strip()]\n",
        "            all_items.update(items)\n",
        "\n",
        "    # Create binary matrix with explicit index preservation\n",
        "    data_dict = {}\n",
        "    original_index = metabolic_info.index\n",
        "\n",
        "    for item in all_items:\n",
        "        if item:  # Skip empty strings\n",
        "            item_escaped = re.escape(item)\n",
        "            data_dict[item] = metabolic_info[use_col].str.contains(\n",
        "                item_escaped,\n",
        "                regex=True,\n",
        "                na=False\n",
        "            ).astype(int)\n",
        "\n",
        "    data_matrix = pd.DataFrame(data_dict, index=original_index)\n",
        "\n",
        "    # Print debug info\n",
        "    print(f\"Created matrix with {data_matrix.shape[1]} features\")\n",
        "    print(f\"Non-zero entries: {data_matrix.astype(bool).sum().sum()}\")\n",
        "\n",
        "    # Run PCA with explicit scaling\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(data_matrix)\n",
        "    pca = PCA(n_components=2)\n",
        "    X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "    loadings = pd.DataFrame(\n",
        "        pca.components_.T,\n",
        "        index=data_matrix.columns,\n",
        "        columns=['PC1', 'PC2']\n",
        "    )\n",
        "\n",
        "    return X_pca, pca.explained_variance_ratio_, loadings, data_matrix\n",
        "\n",
        "def plot_metabolic_pca_results(X_pca, explained_variance, metabolic_sites_info, category_dict, title, category_colors, categories_labels):\n",
        "    \"\"\"\n",
        "    Plot PCA results for pathways with risk categories\n",
        "\n",
        "    Parameters:     X_pca : numpy array  PCA transformed coordinates\n",
        "                    explained_variance : numpy array   Explained variance ratios\n",
        "                    metabolic_info : pandas DataFrame   The metabolic info DataFrame with Sites index\n",
        "                    category_dict : dict     Mapping of sites to categories\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 8))\n",
        "\n",
        "    # Get categories for each site in metabolic_info\n",
        "    if isinstance(metabolic_sites_info.index, pd.MultiIndex):\n",
        "        sites = metabolic_sites_info.index.get_level_values('Sites')\n",
        "    else:\n",
        "        sites = metabolic_sites_info.index\n",
        "\n",
        "    plot_categories = pd.Series(sites).map(category_dict)\n",
        "\n",
        "    # Plot each category\n",
        "    for category in sorted(set(plot_categories)):\n",
        "        mask = plot_categories == category\n",
        "        plt.scatter( X_pca[mask, 0], X_pca[mask, 1], c=category_colors[category],\n",
        "            label=categories_labels[category], alpha=0.7, s=100)\n",
        "\n",
        "    plt.xlabel(f'PC1 ({explained_variance[0]:.1%} variance explained)')\n",
        "    plt.ylabel(f'PC2 ({explained_variance[1]:.1%} variance explained)')\n",
        "    plt.title(title)\n",
        "    plt.legend(title='Risk Category')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "# For pathway PCA\n",
        "X_pca_path, var_ratio_path, loadings_path, pathway_matrix = prepare_pathway_pca(metabolic_sites_info, use_col='Pathways')\n",
        "\n",
        "plot_metabolic_pca_results( X_pca_path, var_ratio_path,  metabolic_sites_info, category_dict, \"Pathways PCA by Risk Category\", category_colors, categories_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqmkIJeaYel7"
      },
      "source": [
        "## 10.2. Top Pathways Loadings by Category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T10:27:14.136175Z",
          "iopub.status.busy": "2025-02-19T10:27:14.13575Z",
          "iopub.status.idle": "2025-02-19T10:27:15.194506Z",
          "shell.execute_reply": "2025-02-19T10:27:15.193297Z",
          "shell.execute_reply.started": "2025-02-19T10:27:14.136142Z"
        },
        "id": "gY-sQA13Yel7",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def plot_pca_loadings_heatmap(loadings, top_n=20):\n",
        "    \"\"\"Plot a heatmap of pathway loadings for PC1 and PC2.\n",
        "       Parameters:     loadings: DataFrame with PCA loadings\n",
        "       top_n: Number of top pathways to display     \"\"\"\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    # Select top pathways based on absolute contribution to PC1 and PC2\n",
        "    top_pathways = (loadings[['PC1', 'PC2']].abs().sum(axis=1).nlargest(top_n).index)\n",
        "    # Filter the loadings dataframe\n",
        "    heatmap_data = loadings.loc[top_pathways, ['PC1', 'PC2']]\n",
        "    sns.heatmap(heatmap_data, annot=True, cmap='coolwarm', center=0)\n",
        "    plt.title('Top Pathway Contributions to PC1 and PC2')\n",
        "    plt.xlabel('Principal Components')\n",
        "    plt.ylabel('Pathways')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_pca_loadings_heatmap(loadings_genera)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXRurjWFuvgW"
      },
      "source": [
        "## 10.3. Pathways patterns by source groups"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GH3EfeFrYel8"
      },
      "source": [
        "|Sites|---|site_1|site_1|site_1|site_2|site_2|site_2|site_2|\n",
        "|---|---|---|---|---|---|---|---|---|\n",
        "|Genus|---|genus_1|genus_2|genus3|genus_2|genus_70|genus_154|genus_520|\n",
        "|Pathways|---|---|---|---|---|---|---|---|\n",
        "|pathway_1|---|---|---|---|---|---|---|---|\n",
        "|pathway_2|---|---|---|---|---|---|---|---|\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-19T13:18:06.432257Z",
          "iopub.status.busy": "2025-02-19T13:18:06.431835Z",
          "iopub.status.idle": "2025-02-19T13:18:06.44645Z",
          "shell.execute_reply": "2025-02-19T13:18:06.445098Z",
          "shell.execute_reply.started": "2025-02-19T13:18:06.432202Z"
        },
        "id": "G0i9EqyPYel8",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def analyze_bacterial_groups(base_matrix, metabolic_sites_info, source_groups):\n",
        "    \"\"\"\n",
        "    Analyze relationships between bacterial groups and functional patterns.\n",
        "\n",
        "    Parameters:\n",
        "    - base_matrix: DataFrame with sites and functional data.\n",
        "      (Columns are multi-indexed (Site, Genus) or similar structure.)\n",
        "    - metabolic_sites_info: DataFrame with site-genus level information.\n",
        "    - source_groups: dict with group names as keys and list of genera as values.\n",
        "\n",
        "    Returns:\n",
        "    - results: dict with analysis results for each group.\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "\n",
        "    for source_name, genus_list in source_groups.items():\n",
        "\n",
        "        # Filter columns where the first level (e.g., site or genus) is in the group list.\n",
        "        group_cols = [col for col in base_matrix.columns if col[0] in genus_list]\n",
        "        group_data = base_matrix.loc[:, group_cols]\n",
        "\n",
        "        # Standardize the data\n",
        "        scaler = MinMaxScaler() # Changing from standard scaler to robustscaler\n",
        "        scaled_data = scaler.fit_transform(group_data)\n",
        "\n",
        "        # PCA analysis\n",
        "        pca = PCA(n_components=5)\n",
        "        pca_result = pca.fit_transform(scaled_data)\n",
        "\n",
        "        print(\"\\nPCA Variance Explained:\")\n",
        "        for i, var in enumerate(pca.explained_variance_ratio_):\n",
        "            print(f\"PC{i+1}: {var:.2%}\")\n",
        "        print(f\"Total variance explained: {sum(pca.explained_variance_ratio_):.2%}\")\n",
        "\n",
        "        # UMAP analysis\n",
        "        reducer = umap.UMAP(random_state=42)\n",
        "        umap_result = reducer.fit_transform(scaled_data)\n",
        "\n",
        "        # Save results for current group\n",
        "        results[source_name] = {\n",
        "            'pca': pca_result,\n",
        "            'umap': umap_result,\n",
        "            'pca_explained': pca.explained_variance_ratio_,\n",
        "            'data': group_data\n",
        "        }\n",
        "\n",
        "        # Plottinextract categories from base_matrix index if available.\n",
        "        try:\n",
        "            categories = base_matrix.index.get_level_values('Category')\n",
        "        except (KeyError, AttributeError):\n",
        "            # If no 'Category' level, assign a default category (e.g., all 1)\n",
        "            categories = pd.Series(np.ones(group_data.shape[0]), index=group_data.index)\n",
        "\n",
        "        # PCA plot\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "        for cat in sorted(set(categories)):\n",
        "            mask = categories == cat\n",
        "            ax1.scatter(pca_result[mask, 0],\n",
        "                        pca_result[mask, 1],\n",
        "                        c=category_colors.get(cat, '#000000'),\n",
        "                        label=categories_labels.get(cat, f'Cat {cat}'),\n",
        "                        alpha=0.7)\n",
        "        ax1.set_title(f'PCA - {source_name}')\n",
        "        ax1.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
        "        ax1.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
        "        ax1.legend()\n",
        "\n",
        "        # UMAP plot\n",
        "        for cat in sorted(set(categories)):\n",
        "            mask = categories == cat\n",
        "            ax2.scatter(umap_result[mask, 0],\n",
        "                        umap_result[mask, 1],\n",
        "                        c=category_colors.get(cat, '#000000'),\n",
        "                        label=categories_labels.get(cat, f'Cat {cat}'),\n",
        "                        alpha=0.7)\n",
        "        ax2.set_title(f'UMAP - {source_name}')\n",
        "        ax2.set_xlabel('UMAP 1')\n",
        "        ax2.set_ylabel('UMAP 2')\n",
        "        ax2.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # PCA Explained Variance plot\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(range(1, len(pca.explained_variance_ratio_) + 1),\n",
        "                 np.cumsum(pca.explained_variance_ratio_), 'bo-')\n",
        "        plt.xlabel('Number of Components')\n",
        "        plt.ylabel('Cumulative Explained Variance Ratio')\n",
        "        plt.title(f'PCA Explained Variance - {source_name}')\n",
        "        plt.grid(True)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BjwgGTo22fJB"
      },
      "outputs": [],
      "source": [
        "print(f\"{known_bacteria}: group_data.shape = {group_data.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNhqT9G92y19"
      },
      "outputs": [],
      "source": [
        "known_bacteria_list = ['Clostridium', 'Corynebacterium', 'Novosphingobium', 'Streptococcus', 'Thiobacillus', 'Acetobacterium', 'Bacillus', 'Desulfotomaculum', 'Desulfovibrio', 'Micrococcus', 'Propionibacterium',\n",
        " 'Pseudomonas', 'Staphylococcus', 'Desulfobacterium', 'Desulfobulbus', 'Gallionella', 'Shewanella']\n",
        "\n",
        "group_cols_known = [col for col in base_matrix.columns if col[0] in known_bacteria_list]\n",
        "group_data_known = base_matrix.loc[:, group_cols_known]\n",
        "\n",
        "group_data_known.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJteNaoZ46f7"
      },
      "outputs": [],
      "source": [
        "print(group_data_known.head(), group_data_known.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWvbkrMc6_Yu"
      },
      "outputs": [],
      "source": [
        "for gname, glist in source_groups.items():\n",
        "    group_cols = [col for col in base_matrix.columns if col[0] in glist]\n",
        "    tmp_data = base_matrix.loc[:, group_cols]\n",
        "    print(gname, tmp_data.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ad6mjcN7MhN"
      },
      "outputs": [],
      "source": [
        "corr_matrix = group_data_known.corr()\n",
        "corr_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z55hs_rh7ULF"
      },
      "outputs": [],
      "source": [
        "corr_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eH_ifJ0wnnLo"
      },
      "source": [
        "## Bacterial Groups Analysis Component"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IsAIvUBYntEu"
      },
      "outputs": [],
      "source": [
        "def analyze_combined_groups(base_matrix, source_groups, group_names=['checked_core', 'Influencers_uniques']):\n",
        "    \"\"\"\n",
        "    Analyze combined bacterial groups while preserving their individual contributions.\n",
        "\n",
        "    Parameters:\n",
        "    - base_matrix: DataFrame with sites and functional data\n",
        "    - source_groups: dict with group names as keys and list of genera as values\n",
        "    - group_names: list of group names to combine\n",
        "\n",
        "    Returns:\n",
        "    - Combined analysis results including PCA, UMAP and variance explained\n",
        "    \"\"\"\n",
        "    # Filter for selected groups\n",
        "    selected_genera = []\n",
        "    for group in group_names:\n",
        "        selected_genera.extend(source_groups[group])\n",
        "\n",
        "    # Remove duplicates while preserving order\n",
        "    selected_genera = list(dict.fromkeys(selected_genera))\n",
        "\n",
        "    # Filter columns for selected genera\n",
        "    group_cols = [col for col in base_matrix.columns if col[0] in selected_genera]\n",
        "    combined_data = base_matrix.loc[:, group_cols]\n",
        "\n",
        "    # Remove zero columns\n",
        "    combined_data = combined_data.loc[:, (combined_data != 0).any(axis=0)]\n",
        "\n",
        "    # Standardize\n",
        "    scaler = StandardScaler()\n",
        "    scaled_data = scaler.fit_transform(combined_data)\n",
        "\n",
        "    # PCA\n",
        "    pca = PCA(n_components=3)\n",
        "    pca_result = pca.fit_transform(scaled_data)\n",
        "\n",
        "    # UMAP\n",
        "    reducer = umap.UMAP(random_state=42)\n",
        "    umap_result = reducer.fit_transform(scaled_data)\n",
        "\n",
        "    results = {\n",
        "        'pca': pca_result,\n",
        "        'umap': umap_result,\n",
        "        'pca_explained': pca.explained_variance_ratio_,\n",
        "        'data': combined_data,\n",
        "        'genera': selected_genera\n",
        "    }\n",
        "\n",
        "    # Plottinextract categories from base_matrix index if available.\n",
        "    try:\n",
        "        categories = base_matrix.index.get_level_values('Category')\n",
        "    except (KeyError, AttributeError):\n",
        "        # If no 'Category' level, assign a default category (e.g., all 1)\n",
        "        categories = pd.Series(np.ones(combined_data.shape[0]), index=combined_data.index)\n",
        "\n",
        "    # PCA plot\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "    for cat in sorted(set(categories)):\n",
        "        mask = categories == cat\n",
        "        ax1.scatter(pca_result[mask, 0],\n",
        "                    pca_result[mask, 1],\n",
        "                    c=category_colors.get(cat, '#000000'),\n",
        "                    label=categories_labels.get(cat, f'Cat {cat}'),\n",
        "                    alpha=0.7)\n",
        "    ax1.set_title(f'PCA - {combined_data}')\n",
        "    ax1.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
        "    ax1.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
        "    ax1.legend()\n",
        "\n",
        "    # UMAP plot\n",
        "    for cat in sorted(set(categories)):\n",
        "        mask = categories == cat\n",
        "        ax2.scatter(umap_result[mask, 0],\n",
        "                    umap_result[mask, 1],\n",
        "                    c=category_colors.get(cat, '#000000'),\n",
        "                    label=categories_labels.get(cat, f'Cat {cat}'),\n",
        "                    alpha=0.7)\n",
        "    ax2.set_title(f'UMAP - {group_names}')\n",
        "    ax2.set_xlabel('UMAP 1')\n",
        "    ax2.set_ylabel('UMAP 2')\n",
        "    ax2.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # PCA Explained Variance plot\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(range(1, len(pca.explained_variance_ratio_) + 1),\n",
        "              np.cumsum(pca.explained_variance_ratio_), 'bo-')\n",
        "    plt.xlabel('Number of Components')\n",
        "    plt.ylabel('Cumulative Explained Variance Ratio')\n",
        "    plt.title(f'PCA Explained Variance - {group_names}')\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Voj9pVqlAR3E"
      },
      "outputs": [],
      "source": [
        "results = analyze_combined_groups(base_matrix, source_groups, group_names=['checked_core', 'Influencers_uniques'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuOHFlIQJ1YD"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJhbme4LoEgY"
      },
      "outputs": [],
      "source": [
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ytCoE1icYel8",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "from statsmodels.stats.multitest import multipletests\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def analyze_category_enrichment(base_matrix, category_dict, source_groups):\n",
        "    \"\"\"\n",
        "    Analyze pathway enrichment within each risk category.\n",
        "    Uses relative abundance and statistical testing to identify\n",
        "    significantly enriched proteins in each category.\n",
        "    \"\"\"\n",
        "    # Get the sites and categories from the MultiIndex\n",
        "    sites_categories = pd.Series(\n",
        "        base_matrix.index.get_level_values('Category'),\n",
        "        index=base_matrix.index.get_level_values('Sites')\n",
        "    )\n",
        "\n",
        "    def get_enrichment_for_group(group_data, category):\n",
        "        # Get data for this category\n",
        "        cat_mask = group_data.index.get_level_values(\"Category\")  == category\n",
        "        cat_data = group_data[cat_mask]\n",
        "        other_data= group_data[~cat_mask]\n",
        "\n",
        "        # Calculate mean abundances\n",
        "        cat_means = cat_data.mean()\n",
        "        other_means = other_data.mean()\n",
        "\n",
        "        # Calculate fold change\n",
        "        fold_change = np.log2(cat_means / other_means)\n",
        "\n",
        "        # Perform statistical test (Mann-Whitney U)\n",
        "        pvalues = []\n",
        "        for col in group_data.columns:\n",
        "            stat, pval = stats.mannwhitneyu(\n",
        "                cat_data[col],\n",
        "                other_data[col],\n",
        "                alternative='greater'\n",
        "            )\n",
        "            pvalues.append(pval)\n",
        "\n",
        "        # Create results DataFrame\n",
        "        results = pd.DataFrame({\n",
        "            'fold_change': fold_change,\n",
        "            'pvalue': pvalues,\n",
        "            'mean_abundance': cat_means\n",
        "        })\n",
        "\n",
        "        # Add multiple testing correction\n",
        "        results['padj'] = multipletests(results['pvalue'], method='fdr_bh')[1]\n",
        "\n",
        "        return results\n",
        "\n",
        "    enrichment_results = {}\n",
        "\n",
        "    # Analyze each source group\n",
        "    for group_name, genera in source_groups.items():\n",
        "        print(f\"\\nAnalyzing {group_name}...\")\n",
        "\n",
        "        # Filter for genera in this group\n",
        "        group_cols = [col for col in base_matrix.columns if col[0] in genera]\n",
        "        if not group_cols:\n",
        "            continue\n",
        "\n",
        "        group_data = base_matrix[group_cols]\n",
        "\n",
        "        # Get enrichment for each category\n",
        "        group_results = {}\n",
        "        for cat in [1, 2, 3]:\n",
        "            results = get_enrichment_for_group(group_data, cat)\n",
        "            group_results[cat] = results\n",
        "\n",
        "        enrichment_results[group_name] = group_results\n",
        "\n",
        "        # Plot volcano plots for each category\n",
        "        plt.figure(figsize=(15, 5))\n",
        "        plt.suptitle(f\"Protein Enrichment Analysis - {group_name}\", y=1.05)\n",
        "\n",
        "        for i, cat in enumerate([1, 2, 3], 1):\n",
        "            results = group_results[cat]\n",
        "\n",
        "            plt.subplot(1, 3, i)\n",
        "\n",
        "            # Create volcano plot\n",
        "            plt.scatter(\n",
        "                results['fold_change'],\n",
        "                -np.log10(results['padj']),\n",
        "                alpha=0.6,\n",
        "                c=category_colors[cat],\n",
        "                s= results['mean_abundance']*1000\n",
        "            )\n",
        "\n",
        "            # Add significance lines\n",
        "            plt.axhline(-np.log10(0.05), color='red', linestyle='--', alpha=0.3)\n",
        "            plt.axvline(0, color='black', linestyle='--', alpha=0.3)\n",
        "\n",
        "            plt.title(f\"{categories_labels[cat]}\")\n",
        "            plt.xlabel(\"Log2 Fold Change\")\n",
        "            plt.ylabel(\"-log10(adjusted p-value)\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Print top enriched proteins\n",
        "        for cat in [1, 2, 3]:\n",
        "            results = group_results[cat]\n",
        "            significant = results[results['padj'] < 0.05].sort_values('fold_change', ascending=False)\n",
        "\n",
        "            print(f\"\\nTop enriched proteins in {categories_labels[cat]} for {group_name}:\")\n",
        "            if len(significant) > 0:\n",
        "                print(significant.head(10))\n",
        "            else:\n",
        "                print(\"No significantly enriched proteins found\")\n",
        "\n",
        "    return enrichment_results\n",
        "\n",
        "# Run the analysis\n",
        "enrichment_results = analyze_category_enrichment(base_matrix, category_dict, source_groups)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpDMQg0fZVGn"
      },
      "outputs": [],
      "source": [
        "def create_comparison_table(enrichment_results):\n",
        "    \"\"\"\n",
        "    Create a structured comparison table from enrichment results\n",
        "    \"\"\"\n",
        "    # Create empty list to store rows\n",
        "    comparison_rows = []\n",
        "\n",
        "    for group_name, group_results in enrichment_results.items():\n",
        "        for category in [1, 2, 3]:\n",
        "            if category in group_results:\n",
        "                results = group_results[category]\n",
        "                significant = results[results['padj'] < 0.05]\n",
        "\n",
        "                if len(significant) > 0:\n",
        "                    for idx, row in significant.head(10).iterrows():\n",
        "                        comparison_rows.append({\n",
        "                            'Group': group_name,\n",
        "                            'Category': categories_labels[category],\n",
        "                            'Genus': idx[0],\n",
        "                            'Protein': idx[1],\n",
        "                            'Fold_Change': row['fold_change'],\n",
        "                            'Padj': row['padj'],\n",
        "                            'Mean_Abundance': row['mean_abundance']\n",
        "                        })\n",
        "\n",
        "    # Create DataFrame\n",
        "    comparison_df = pd.DataFrame(comparison_rows)\n",
        "\n",
        "    # Save to CSV with proper formatting\n",
        "    comparison_df.to_csv('enrichment_comparison.csv', index=False)\n",
        "\n",
        "    return comparison_df\n",
        "\n",
        "# Create comparison table\n",
        "comparison_table = create_comparison_table(enrichment_results)\n",
        "\n",
        "# Display formatted table\n",
        "print(\"\\nComparison Table Preview:\")\n",
        "print(comparison_table.to_string())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zoPuDB4Yel8"
      },
      "source": [
        "__________________________________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxMZgylFpBX4"
      },
      "source": [
        "https://www.youtube.com/watch?v=jQVNsyAnDMo\n",
        "\n",
        "https://microreact.org/\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWCQOr4KuvgW",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def create_integrated_visualization(df, results, metadata=None):\n",
        "    \"\"\"\n",
        "    Create an integrated visualization combining PCA, clustering, and metadata\n",
        "\n",
        "    Parameters:\n",
        "    df: Original pathway data\n",
        "    results: Results from explore_pathway_patterns\n",
        "    metadata: DataFrame with risk labels, materials, etc.\n",
        "    \"\"\"\n",
        "    fig = plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # 1. PCA with clustering\n",
        "    pca_data = results['pca']['components']\n",
        "    clusters = results['clustering'][5]['kmeans']  # Using k=5 clusters\n",
        "\n",
        "    plt.subplot(2, 2, 1)\n",
        "    scatter = plt.scatter(pca_data[:, 0], pca_data[:, 1],\n",
        "                         c=clusters, cmap='Set2', alpha=0.6)\n",
        "    plt.title('PCA Components with Clusters')\n",
        "    plt.xlabel('PC1')\n",
        "    plt.ylabel('PC2')\n",
        "    plt.colorbar(scatter, label='Cluster')\n",
        "\n",
        "    # 2. Top pathway contributions\n",
        "    plt.subplot(2, 2, 2)\n",
        "    top_loadings = abs(results['pca']['loadings']['PC1']).nlargest(10)\n",
        "    sns.barplot(x=top_loadings.values, y=top_loadings.index)\n",
        "    plt.title('Top 10 Pathways Contributing to PC1')\n",
        "    plt.xlabel('Absolute Loading')\n",
        "\n",
        "    # 3. Correlation structure summary\n",
        "    plt.subplot(2, 2, 3)\n",
        "    corr_summary = results['correlation'].abs().mean()\n",
        "    sns.histplot(corr_summary, bins=50)\n",
        "    plt.title('Distribution of Mean Correlation Strengths')\n",
        "    plt.xlabel('Mean |Correlation|')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZvr6vjWuvgW",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "create_integrated_visualization(base_matrix, results_patterns, metadata=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bk9_ZGhfuvgW"
      },
      "source": [
        "## 9.3 Analysing Pathways Organic Fate\n",
        "\n",
        "Now the task is to identify the most abundant pathways in the samples, focusing specifically on organic matter-related metabolism. Ultimately creating visualizations to understand pathway distributions and analyze correlations between pathways."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11HYL0iNuvgW",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def analyze_metabolic_pathways(df):\n",
        "    \"\"\"\n",
        "    Analyze metabolic pathways from PICRUSt output\n",
        "\n",
        "    Parameters:\n",
        "    df: pandas DataFrame with pathways as index and samples as columns\n",
        "    \"\"\"\n",
        "    # Calculate mean abundance across samples for each pathway\n",
        "    mean_abundance = df.mean(axis=1).sort_values(ascending=False)\n",
        "\n",
        "    # Get top 20 most abundant pathways\n",
        "    top_pathways = mean_abundance.head(20)\n",
        "\n",
        "    # Create heatmap of top pathways across samples\n",
        "    plt.figure(figsize=(15, 8))\n",
        "    sns.heatmap(df.loc[top_pathways.index],\n",
        "                cmap='YlOrRd',\n",
        "                center=0,\n",
        "                robust=True,\n",
        "                xticklabels=True,\n",
        "                yticklabels=True)\n",
        "    plt.title('Top 20 Most Abundant Pathways Across Samples')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Filter for organic matter metabolism related pathways\n",
        "    organic_terms = ['carbon', 'carbohydrate', 'lipid', 'fatty acid',\n",
        "                    'organic acid', 'amino acid', 'degradation']\n",
        "\n",
        "    organic_pathways = df.index[df.index.str.lower().str.contains('|'.join(organic_terms))]\n",
        "    organic_data = df.loc[organic_pathways]\n",
        "\n",
        "    # Calculate summary statistics for organic matter pathways\n",
        "    pathway_stats = pd.DataFrame({\n",
        "        'mean_abundance': organic_data.mean(axis=1),\n",
        "        'std_abundance': organic_data.std(axis=1),\n",
        "        'cv': organic_data.std(axis=1) / organic_data.mean(axis=1) * 100\n",
        "    }).sort_values('mean_abundance', ascending=False)\n",
        "\n",
        "    return pathway_stats, organic_data\n",
        "\n",
        "def plot_pathway_distribution(pathway_stats):\n",
        "    \"\"\"Plot distribution of pathway abundances\"\"\"\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.boxplot(data=pathway_stats.reset_index(),\n",
        "                x='mean_abundance',\n",
        "                y='index',\n",
        "                order=pathway_stats.index[:15])\n",
        "    plt.title('Top 15 Organic Matter Related Pathways')\n",
        "    plt.xlabel('Mean Abundance')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Calling the function\n",
        "stats, organic_data = analyze_metabolic_pathways(Picrust_Result)\n",
        "plot_pathway_distribution(stats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZkQ1j5X5fHL",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def analyze_pathway_patterns(df):\n",
        "    \"\"\"\n",
        "    Analyze pathway patterns using sites vs pathways abundances\n",
        "    \"\"\"\n",
        "    # Create the correct matrix: sites vs pathways with abundances\n",
        "    pathway_matrix = df.pivot_table(\n",
        "        values='norm_abund_contri',\n",
        "        index='Sites',          # Sites as rows\n",
        "        columns='Pathways',     # Pathways as columns\n",
        "        aggfunc='sum',          # Sum abundances\n",
        "        fill_value=0\n",
        "    )\n",
        "\n",
        "    print(\"Matrix shape:\", pathway_matrix.shape)\n",
        "\n",
        "    # Standardize data\n",
        "    scaler = StandardScaler()\n",
        "    scaled_data = scaler.fit_transform(pathway_matrix)\n",
        "\n",
        "    # PCA\n",
        "    pca = PCA(n_components=5)\n",
        "    X_pca = pca.fit_transform(scaled_data)\n",
        "\n",
        "    # UMAP\n",
        "    reducer = umap.UMAP(random_state=42)\n",
        "    umap_result = reducer.fit_transform(scaled_data)\n",
        "\n",
        "    # Get categories for sites\n",
        "    categories = pd.Series(pathway_matrix.index).map(lambda x: category_dict[x])\n",
        "\n",
        "    # Plot both PCA and UMAP\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
        "\n",
        "    # PCA explained variance\n",
        "    ax1.plot(range(1, 6), pca.explained_variance_ratio_, 'bo-')\n",
        "    print(\"\\nPCA Variance Explained:\")\n",
        "    print(f\"Total (5 components): {sum(pca.explained_variance_ratio_):.2%}\")\n",
        "    ax1.set_title('PCA Explained Variance')\n",
        "    ax1.set_xlabel('Component')\n",
        "    ax1.set_ylabel('Explained Variance Ratio')\n",
        "\n",
        "    # PCA scatter\n",
        "    for cat in category_colors.keys():\n",
        "        mask = categories == cat\n",
        "        ax2.scatter(X_pca[mask, 0],\n",
        "                   X_pca[mask, 1],\n",
        "                   c=category_colors[cat],\n",
        "                   label=categories_labels[cat],\n",
        "                   alpha=0.7)\n",
        "\n",
        "    ax2.set_title('PCA First Two Components')\n",
        "    ax2.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
        "    ax2.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
        "    ax2.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # UMAP plot\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    for cat in category_colors.keys():\n",
        "        mask = categories == cat\n",
        "        plt.scatter(umap_result[mask, 0],\n",
        "                   umap_result[mask, 1],\n",
        "                   c=category_colors[cat],\n",
        "                   label=categories_labels[cat],\n",
        "                   alpha=0.7)\n",
        "\n",
        "    plt.title('UMAP Projection of Pathways')\n",
        "    plt.xlabel('UMAP1')\n",
        "    plt.ylabel('UMAP2')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Run the analysis\n",
        "results = analyze_pathway_patterns(metabolic_sites_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9GJgu3yuvgW",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# To analyze specific pathways of interest:\n",
        "def analyze_specific_pathways(df, pathway_list):\n",
        "    \"\"\"\n",
        "    Analyze specific pathways of interest\n",
        "\n",
        "    Parameters:\n",
        "    df: DataFrame with pathway data\n",
        "    pathway_list: list of pathway names to analyze\n",
        "    \"\"\"\n",
        "    specific_data = df.loc[df.index.str.contains('|'.join(pathway_list), case=False)]\n",
        "\n",
        "    # Create correlation matrix for these pathways\n",
        "    corr = specific_data.T.corr()\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(corr, annot=True, cmap='coolwarm', center=0)\n",
        "    plt.title('Correlation between Selected Pathways')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return specific_data.describe()\n",
        "\n",
        "# Calling the funtion\n",
        "Description = analyze_specific_pathways(Picrust_Result, Picrust_Result.index.tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HEIwON2uvgW"
      },
      "source": [
        "## 9.4. Pathways Relevant to Corrosion\n",
        "This code witll categorise pathways into key groups: sulfur metabolism (critical for sulfate-reducing bacteria), Metal-related pathways (iron, manganese, etc.); organic acid production (which can influence local pH); biofilm formation (important for corrosion processes) and electron transfer mechanisms. Then it would analyse correlations between these different categories to understand potential synergistic effects, identifying the most abundant pathways in each category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8FN-ChvuvgW",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def analyze_corrosion_pathways(df):\n",
        "    \"\"\"\n",
        "    Analyze pathways relevant to microbially influenced corrosion (MIC)\n",
        "\n",
        "    Parameters:\n",
        "    df: pandas DataFrame with pathways as index and samples as columns\n",
        "    \"\"\"\n",
        "    # Define relevant pathway terms for different corrosion mechanisms\n",
        "    pathway_categories = {\n",
        "        'sulfur': ['sulfur', 'sulfate', 'sulfide', 'thiosulfate', 'sulfite', 'sulfonate'],\n",
        "        'metal': ['iron', 'metal', 'Fe', 'manganese', 'chromium', 'nickel'],\n",
        "        'organic_acid': ['organic acid', 'acetate', 'formate', 'lactate', 'pyruvate'],\n",
        "        'biofilm': ['biofilm', 'exopolysaccharide', 'EPS', 'adhesion'],\n",
        "        'electron_transfer': ['cytochrome', 'electron transport', 'oxidoreductase']\n",
        "    }\n",
        "\n",
        "    # Function to filter pathways by category\n",
        "    def get_category_pathways(terms):\n",
        "        return df.index[df.index.str.lower().str.contains('|'.join(terms), regex=True)]\n",
        "\n",
        "    # Analyze each category\n",
        "    category_data = {}\n",
        "    category_stats = {}\n",
        "\n",
        "    for category, terms in pathway_categories.items():\n",
        "        pathways = get_category_pathways(terms)\n",
        "        if len(pathways) > 0:\n",
        "            category_data[category] = df.loc[pathways]\n",
        "            category_stats[category] = pd.DataFrame({\n",
        "                'mean_abundance': category_data[category].mean(axis=1),\n",
        "                'std_abundance': category_data[category].std(axis=1),\n",
        "                'cv': category_data[category].std(axis=1) / category_data[category].mean(axis=1) * 100\n",
        "            }).sort_values('mean_abundance', ascending=False)\n",
        "\n",
        "    return category_data, category_stats\n",
        "\n",
        "def plot_corrosion_pathways(category_data, category_stats):\n",
        "    \"\"\"\n",
        "    Create visualizations for corrosion-related pathways\n",
        "    \"\"\"\n",
        "    # Plot top pathways for each category\n",
        "    for category, data in category_stats.items():\n",
        "        if len(data) > 0:\n",
        "            plt.figure(figsize=(12, min(6, max(3, len(data)*0.3))))\n",
        "            sns.barplot(data=data.head(10).reset_index(),\n",
        "                       x='mean_abundance',\n",
        "                       y='index',\n",
        "                       palette='YlOrRd')\n",
        "            plt.title(f'Top {min(10, len(data))} {category.replace(\"_\", \" \").title()} Related Pathways')\n",
        "            plt.xlabel('Mean Abundance')\n",
        "            plt.ylabel('Pathway')\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "    # Create correlation heatmap between categories\n",
        "    category_means = pd.DataFrame({\n",
        "        cat: data.mean(axis=1) for cat, data in category_data.items()\n",
        "    })\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(category_means.corr(),\n",
        "                annot=True,\n",
        "                cmap='coolwarm',\n",
        "                center=0,\n",
        "                vmin=-1,\n",
        "                vmax=1)\n",
        "    plt.title('Correlation between Pathway Categories')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def analyze_pathway_interactions(df, category_data):\n",
        "    \"\"\"\n",
        "    Analyze interactions between different pathway categories\n",
        "    \"\"\"\n",
        "    # Calculate mean abundance for each category\n",
        "    category_abundances = pd.DataFrame({\n",
        "        category: data.mean(axis=0)\n",
        "        for category, data in category_data.items()\n",
        "    })\n",
        "\n",
        "    # Calculate correlations between categories\n",
        "    correlations = category_abundances.corr()\n",
        "\n",
        "    # Identify potential synergistic relationships\n",
        "    high_correlations = correlations.unstack()\n",
        "    high_correlations = high_correlations[high_correlations != 1.0]\n",
        "    high_correlations = high_correlations[abs(high_correlations) > 0.5]\n",
        "\n",
        "    return category_abundances, correlations, high_correlations.sort_values(ascending=False)\n",
        "\n",
        "# Analysing Corrosion Pathways\n",
        "category_data, category_stats = analyze_corrosion_pathways(Picrust_Result)\n",
        "plot_corrosion_pathways(category_data, category_stats)\n",
        "abundances, correlations, high_corr = analyze_pathway_interactions(Picrust_Result, category_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bi4wuyN7uvgW"
      },
      "source": [
        "## 9.5. Heating and Cooling Systems Pathway Analysis\n",
        "Creating independent analyses:\n",
        "\n",
        "Failure analysis (based on human assessment/estimation)\n",
        "Microbiological analysis (16S rRNA)\n",
        "Physicochemical parameters\n",
        "\n",
        "\n",
        "Using physicochemical parameters as labels/indicators of corrosion state - this is quite clever because it gives you an objective measure without directly mixing in the biological data\n",
        "Then planning to correlate the microbial communities with these states through machine learning\n",
        "\n",
        "And now you want to use PICRUSt's functional predictions to validate your assumptions about organic matter metabolism. This is very valuable because:\n",
        "\n",
        "It can help confirm if the bacteria you've identified through correlations actually have the metabolic capacity to influence corrosion\n",
        "It might reveal unexpected metabolic pathways that could explain the correlations you're seeing\n",
        "The following script will Validate your organic matter assumptions by:\n",
        "\n",
        "Breaking down different types of organic matter processing\n",
        "Looking at both degradation and synthesis pathways\n",
        "Identifying transport mechanisms\n",
        "\n",
        "Connect with your physicochemical parameters by analyzing pathways that could influence:\n",
        "\n",
        "pH modulation\n",
        "Temperature response\n",
        "Metal interactions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWBttrFbuvgW",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def analyze_system_pathways(df):\n",
        "    \"\"\"\n",
        "    Analyze pathways relevant to heating/cooling system corrosion\n",
        "\n",
        "    Parameters:\n",
        "    df: pandas DataFrame with pathways as index and samples as columns\n",
        "    \"\"\"\n",
        "    # Define pathway categories relevant to system conditions\n",
        "    pathway_categories = {\n",
        "        # Water chemistry influence\n",
        "        'ph_modulation': ['acid', 'alkaline', 'proton pump', 'pH homeostasis'],\n",
        "\n",
        "        # Temperature adaptation\n",
        "        'temp_response': ['heat shock', 'cold shock', 'temperature response'],\n",
        "\n",
        "        # Organic matter processing\n",
        "        'carbon_metabolism': [\n",
        "            'carbon fixation', 'carbon utilization',\n",
        "            'organic acid', 'fatty acid',\n",
        "            'carbohydrate metabolism'\n",
        "        ],\n",
        "\n",
        "        # Corrosion-related\n",
        "        'metal_interaction': [\n",
        "            'iron', 'metal', 'oxidation-reduction',\n",
        "            'electron transport', 'metal binding'\n",
        "        ],\n",
        "\n",
        "        # Biofilm formation\n",
        "        'surface_attachment': [\n",
        "            'biofilm', 'adhesion', 'exopolysaccharide',\n",
        "            'extracellular matrix'\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    # Filter and analyze pathways\n",
        "    def get_category_pathways(terms):\n",
        "        return df.index[df.index.str.lower().str.contains('|'.join(terms), regex=True)]\n",
        "\n",
        "    category_data = {}\n",
        "    category_stats = {}\n",
        "\n",
        "    for category, terms in pathway_categories.items():\n",
        "        pathways = get_category_pathways(terms)\n",
        "        if len(pathways) > 0:\n",
        "            category_data[category] = df.loc[pathways]\n",
        "\n",
        "            # Calculate basic statistics\n",
        "            category_stats[category] = pd.DataFrame({\n",
        "                'mean_abundance': category_data[category].mean(axis=1),\n",
        "                'std_abundance': category_data[category].std(axis=1),\n",
        "                'cv': category_data[category].std(axis=1) / category_data[category].mean(axis=1) * 100,\n",
        "                'presence': (category_data[category] > 0).mean(axis=1) * 100  # % of samples with pathway\n",
        "            }).sort_values('mean_abundance', ascending=False)\n",
        "\n",
        "    return category_data, category_stats\n",
        "\n",
        "def analyze_organic_matter_pathways(df):\n",
        "    \"\"\"\n",
        "    Detailed analysis of organic matter related pathways\n",
        "    \"\"\"\n",
        "    # Specific organic matter categories\n",
        "    organic_categories = {\n",
        "        'degradation': ['degradation', 'breakdown', 'catabolism'],\n",
        "        'synthesis': ['biosynthesis', 'anabolism', 'synthesis'],\n",
        "        'transport': ['transport', 'uptake', 'export'],\n",
        "        'modification': ['modification', 'conversion', 'transformation']\n",
        "    }\n",
        "\n",
        "    organic_data = {}\n",
        "\n",
        "    for category, terms in organic_categories.items():\n",
        "        pathways = df.index[df.index.str.lower().str.contains(\n",
        "            '|'.join(terms), regex=True\n",
        "        ) & df.index.str.lower().str.contains(\n",
        "            'organic|carbon|fatty acid|lipid|protein|amino acid'\n",
        "        )]\n",
        "        if len(pathways) > 0:\n",
        "            organic_data[category] = df.loc[pathways]\n",
        "\n",
        "    return organic_data\n",
        "\n",
        "def plot_pathway_distributions(category_stats, category_data):\n",
        "    \"\"\"\n",
        "    Create visualizations for pathway distributions\n",
        "    \"\"\"\n",
        "    for category, stats in category_stats.items():\n",
        "        if len(stats) > 0:\n",
        "            # Create subplot with dual axis\n",
        "            fig, ax1 = plt.subplots(figsize=(12, min(8, max(4, len(stats)*0.3))))\n",
        "            ax2 = ax1.twinx()\n",
        "\n",
        "            # Plot mean abundance\n",
        "            sns.barplot(data=stats.head(10).reset_index(),\n",
        "                       x='mean_abundance',\n",
        "                       y='index',\n",
        "                       color='skyblue',\n",
        "                       ax=ax1)\n",
        "\n",
        "            # Plot presence percentage\n",
        "            stats.head(10)['presence'].plot(\n",
        "                marker='o',\n",
        "                color='red',\n",
        "                ax=ax2\n",
        "            )\n",
        "\n",
        "            ax1.set_title(f'{category.replace(\"_\", \" \").title()} Pathways')\n",
        "            ax1.set_xlabel('Mean Abundance')\n",
        "            ax2.set_xlabel('Presence (%)')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "# Calling the analysis\n",
        "category_data, category_stats = analyze_system_pathways(Picrust_Result)\n",
        "organic_data = analyze_organic_matter_pathways(Picrust_Result)\n",
        "plot_pathway_distributions(category_stats, category_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFrF4oTRuvgX"
      },
      "source": [
        "I have a big gap on the cation anion account and then used mackensy, 2012 method from the usgs to check ec measured Vs calculated and cation Vs ions. It is a big gap still, but I have a lot of OM so I could no assume as normally that OM is CH4 so I attribute it to small organic acids and put acetate and oxalate as OM representatives, I have a small study of small acids form on failure analysis and also report of a mass that has a magnetic consistency, so I infere that those muss be some organic metalic compound but only accounted for AC- and Ox-2, I thought better to chose this other compounds Fe rich but I don't know how to do it actually. So in my bacteria I actually found lots of them with Ac- metabolism whiles I was looking at the families I realise no only oxobacter accendants, but others similar, also got important biofilm formers, there is also halogen related and should be, big deal of difference make the material and location cause water treatment, unfortunately the annotations are no to be taken as parameters but can serve as annotations you understand the difference?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXMFmbSruvgX"
      },
      "source": [
        "validate assumptions about:\n",
        "\n",
        "Organic acid presence (by showing metabolic capability)\n",
        "Metal-organic complex formation (through siderophore and metal-binding pathways)\n",
        "Biofilm formation potential (which can influence local chemistry)\n",
        "\n",
        "Validate acetate/oxalate assumptions by showing if these metabolic pathways are actually present\n",
        "Look for other potential organic acid pathways might want to consider\n",
        "Identify metal-organic interaction pathways that could explain magnetic mass observation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3sujtH1uvgX",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def analyze_organic_metal_pathways(df):\n",
        "    \"\"\"\n",
        "    Analyze pathways related to organic acid metabolism and metal interactions\n",
        "\n",
        "    Parameters:\n",
        "    df: pandas DataFrame with pathways as index and samples as columns\n",
        "    \"\"\"\n",
        "    # Define specific pathway categories\n",
        "    pathway_categories = {\n",
        "        'organic_acid_metabolism': [\n",
        "            'acetate', 'acetic acid', 'acetyl',\n",
        "            'oxalate', 'oxalic acid',\n",
        "            'organic acid', 'fatty acid',\n",
        "            'carboxylic acid'\n",
        "        ],\n",
        "\n",
        "        'metal_organic_interaction': [\n",
        "            'siderophore', 'metal binding',\n",
        "            'iron complex', 'metal transport',\n",
        "            'metallophore', 'metal organic'\n",
        "        ],\n",
        "\n",
        "        'biofilm_formation': [\n",
        "            'biofilm', 'exopolysaccharide',\n",
        "            'extracellular matrix', 'adhesion'\n",
        "        ],\n",
        "\n",
        "        'halogen_related': [\n",
        "            'halogen', 'chloride', 'bromide',\n",
        "            'halide', 'dehalogenation'\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    # Analyze each category\n",
        "    def get_category_pathways(terms):\n",
        "        return df.index[df.index.str.lower().str.contains('|'.join(terms), regex=True)]\n",
        "\n",
        "    pathway_data = {}\n",
        "    pathway_stats = {}\n",
        "\n",
        "    for category, terms in pathway_categories.items():\n",
        "        pathways = get_category_pathways(terms)\n",
        "        if len(pathways) > 0:\n",
        "            pathway_data[category] = df.loc[pathways]\n",
        "\n",
        "            # Calculate comprehensive statistics\n",
        "            pathway_stats[category] = pd.DataFrame({\n",
        "                'mean_abundance': pathway_data[category].mean(axis=1),\n",
        "                'std_abundance': pathway_data[category].std(axis=1),\n",
        "                'cv': pathway_data[category].std(axis=1) / pathway_data[category].mean(axis=1) * 100,\n",
        "                'presence': (pathway_data[category] > 0).mean(axis=1) * 100,  # % of samples with pathway\n",
        "                'relative_abundance': pathway_data[category].mean(axis=1) / df.mean(axis=1).mean() * 100\n",
        "            }).sort_values('mean_abundance', ascending=False)\n",
        "\n",
        "    return pathway_data, pathway_stats\n",
        "\n",
        "def analyze_pathway_relationships(pathway_data):\n",
        "    \"\"\"\n",
        "    Analyze relationships between different pathway categories\n",
        "    \"\"\"\n",
        "    # Calculate mean abundance for each category across samples\n",
        "    category_means = pd.DataFrame({\n",
        "        category: data.mean(axis=0)\n",
        "        for category, data in pathway_data.items()\n",
        "    })\n",
        "\n",
        "    # Calculate correlations\n",
        "    correlations = category_means.corr()\n",
        "\n",
        "    # Identify potential functional relationships\n",
        "    high_correlations = correlations.unstack()\n",
        "    high_correlations = high_correlations[high_correlations != 1.0]\n",
        "    high_correlations = high_correlations[abs(high_correlations) > 0.5]\n",
        "\n",
        "    return category_means, correlations, high_correlations.sort_values(ascending=False)\n",
        "\n",
        "def plot_pathway_analysis(pathway_stats, pathway_data):\n",
        "    \"\"\"\n",
        "    Create visualizations for pathway analysis\n",
        "    \"\"\"\n",
        "    for category, stats in pathway_stats.items():\n",
        "        if len(stats) > 0:\n",
        "            # Create subplot with dual axis\n",
        "            fig, ax1 = plt.subplots(figsize=(12, min(8, max(4, len(stats)*0.3))))\n",
        "            ax2 = ax1.twinx()\n",
        "\n",
        "            # Plot abundance and relative abundance\n",
        "            sns.barplot(data=stats.head(10).reset_index(),\n",
        "                       x='relative_abundance',\n",
        "                       y='index',\n",
        "                       color='skyblue',\n",
        "                       ax=ax1)\n",
        "\n",
        "            stats.head(10)['presence'].plot(\n",
        "                marker='o',\n",
        "                color='red',\n",
        "                ax=ax2\n",
        "            )\n",
        "\n",
        "            ax1.set_title(f'{category.replace(\"_\", \" \").title()} Pathways')\n",
        "            ax1.set_xlabel('Relative Abundance (%)')\n",
        "            ax2.set_xlabel('Presence (%)')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "# calling the function\n",
        "pathway_data, pathway_stats = analyze_organic_metal_pathways(Picrust_Result)category_means, correlations, high_corr = analyze_pathway_relationships(pathway_data)\n",
        "plot_pathway_analysis(pathway_stats, pathway_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvV8Sor3uvgX"
      },
      "source": [
        "## 9.6. Corrosion Relevant Pathways\n",
        "\n",
        "Focus on corrosion-relevant pathways by categorizing them into:\n",
        "\n",
        "Organic acid metabolism (relevant to your acetate/oxalate observations)\n",
        "Sulfur metabolism\n",
        "Metal interactions\n",
        "Biofilm formation\n",
        "\n",
        "\n",
        "Handle the high-dimensional data by:\n",
        "\n",
        "Using dimensionality reduction (PCA)\n",
        "Calculating summary statistics\n",
        "Visualizing key patterns\n",
        "\n",
        "\n",
        "Address your specific interests:\n",
        "\n",
        "Organic matter metabolism pathways\n",
        "Metal-organic interactions\n",
        "Correlations with physicochemical parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EThn4vkFuvgX",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def analyze_corrosion_pathways(df):\n",
        "    \"\"\"\n",
        "    Analyze pathways relevant to microbially influenced corrosion\n",
        "    \"\"\"\n",
        "    # Define pathway categories relevant to corrosion\n",
        "    pathway_categories = {\n",
        "        'organic_acid': [\n",
        "            'CENTFERM-PWY',  # Central fermentation pathways\n",
        "            'FERMENTATION-PWY',  # Mixed acid fermentation\n",
        "            'GLYCOLYSIS',  # Glucose fermentation\n",
        "            'PWY-5100',  # Pyruvate fermentation\n",
        "            'GALACTUROCAT-PWY'  # Galacturonate degradation\n",
        "        ],\n",
        "        'sulfur': [\n",
        "            'PWY-6932',  # Sulfate reduction\n",
        "            'SO4ASSIM-PWY',  # Sulfate assimilation\n",
        "            'SULFATE-CYS-PWY'  # Sulfate to cysteine\n",
        "        ],\n",
        "        'metal_interaction': [\n",
        "            'PWY-7219',  # Iron oxidation\n",
        "            'PWY-7221',  # Iron reduction\n",
        "            'HEME-BIOSYNTHESIS-II',  # Iron-containing compounds\n",
        "            'P125-PWY'  # Metal resistance\n",
        "        ],\n",
        "        'biofilm': [\n",
        "            'COLANSYN-PWY',  # Colanic acid (biofilm)\n",
        "            'EXOPOLYSACC-PWY',  # Exopolysaccharide\n",
        "            'GLUCOSE1PMETAB-PWY'  # UDP-glucose synthesis\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    # Extract relevant pathways and their abundances\n",
        "    relevant_pathways = {}\n",
        "    for category, pathways in pathway_categories.items():\n",
        "        category_data = df[df.index.isin(pathways)]\n",
        "        if not category_data.empty:\n",
        "            relevant_pathways[category] = category_data\n",
        "\n",
        "    # Calculate summary statistics\n",
        "    summary_stats = {}\n",
        "    for category, data in relevant_pathways.items():\n",
        "        summary_stats[category] = {\n",
        "            'mean_abundance': data.mean().mean(),\n",
        "            'std_abundance': data.mean().std(),\n",
        "            'present_in_samples': (data > 0).mean().mean() * 100,\n",
        "            'pathways_found': len(data)\n",
        "        }\n",
        "\n",
        "    # Dimension reduction for visualization\n",
        "    if df.shape[0] > 0:\n",
        "        # Standardize the data\n",
        "        scaler = StandardScaler()\n",
        "        scaled_data = scaler.fit_transform(df.T)\n",
        "\n",
        "        # PCA\n",
        "        pca = PCA(n_components=2)\n",
        "        pca_result = pca.fit_transform(scaled_data)\n",
        "\n",
        "        return relevant_pathways, summary_stats, pca_result, pca.explained_variance_ratio_\n",
        "\n",
        "    return relevant_pathways, summary_stats, None, None\n",
        "\n",
        "def plot_pathway_analysis(relevant_pathways, summary_stats, pca_result=None, explained_variance=None):\n",
        "    \"\"\"\n",
        "    Create visualizations for pathway analysis\n",
        "    \"\"\"\n",
        "    # Plot mean abundances by category\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    categories = list(summary_stats.keys())\n",
        "    means = [stats['mean_abundance'] for stats in summary_stats.values()]\n",
        "    presence = [stats['present_in_samples'] for stats in summary_stats.values()]\n",
        "\n",
        "    ax1 = plt.gca()\n",
        "    ax2 = ax1.twinx()\n",
        "\n",
        "    bars = ax1.bar(categories, means, alpha=0.6, color='skyblue')\n",
        "    ax1.set_ylabel('Mean Abundance')\n",
        "\n",
        "    line = ax2.plot(categories, presence, 'ro-', label='Presence %')\n",
        "    ax2.set_ylabel('Presence in Samples (%)')\n",
        "\n",
        "    plt.title('Pathway Categories Overview')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # If PCA results available, plot them\n",
        "    if pca_result is not None:\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        plt.scatter(pca_result[:, 0], pca_result[:, 1], alpha=0.6)\n",
        "        plt.xlabel(f'PC1 ({explained_variance[0]*100:.1f}%)')\n",
        "        plt.ylabel(f'PC2 ({explained_variance[1]*100:.1f}%)')\n",
        "        plt.title('PCA of Pathway Abundances')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# Calling the functions\n",
        "relevant_pathways, summary_stats, pca_result, explained_variance = analyze_corrosion_pathways(Picrust_Result)\n",
        "plot_pathway_analysis(relevant_pathways, summary_stats, pca_result, explained_variance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZgXC50euvgX"
      },
      "source": [
        "## 9.7. Functional Pathway Clustering Analysis\n",
        "Hierarchical Clustering:\n",
        "\n",
        "Groups pathways based on their abundance patterns\n",
        "Creates a dendrogram to visualize relationships\n",
        "Automatically determines optimal number of clusters\n",
        "\n",
        "\n",
        "Correlation-based Analysis:\n",
        "\n",
        "Identifies pathways that behave similarly across samples\n",
        "Creates correlation heatmap to visualize relationships\n",
        "Helps identify functional modules\n",
        "\n",
        "\n",
        "Feature Creation:\n",
        "\n",
        "Generates new features based on cluster statistics:\n",
        "\n",
        "Mean abundance per cluster\n",
        "Total abundance per cluster\n",
        "Pathway diversity within clusters\n",
        "\n",
        "Reduce dimensionality while maintaining biological meaning\n",
        "Identify functional modules that might be working together\n",
        "Create more robust features for your ML analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6NsHQCquvgX",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def cluster_pathways(df, n_clusters=None, corr_threshold=0.7):\n",
        "    \"\"\"\n",
        "    Cluster pathways based on their functional similarity\n",
        "\n",
        "    Parameters:\n",
        "    df: DataFrame with pathways as rows and samples as columns\n",
        "    n_clusters: Number of clusters (if None, determined automatically)\n",
        "    corr_threshold: Correlation threshold for considering pathways related\n",
        "    \"\"\"\n",
        "    # Standardize the data\n",
        "    scaler = StandardScaler()\n",
        "    scaled_data = scaler.fit_transform(df.T).T\n",
        "\n",
        "    # Calculate correlation matrix\n",
        "    corr_matrix = np.corrcoef(scaled_data)\n",
        "\n",
        "    # Create linkage matrix for hierarchical clustering\n",
        "    linkage_matrix = hierarchy.linkage(pdist(scaled_data), method='ward')\n",
        "\n",
        "    if n_clusters is None:\n",
        "        # Automatically determine number of clusters using elbow method\n",
        "        last = linkage_matrix[-10:, 2]\n",
        "        acceleration = np.diff(last, 2)\n",
        "        n_clusters = len(last) - np.argmax(acceleration) + 1\n",
        "\n",
        "    # Perform clustering\n",
        "    clustering = AgglomerativeClustering(n_clusters=n_clusters)\n",
        "    cluster_labels = clustering.fit_predict(scaled_data)\n",
        "\n",
        "    # Create cluster summary\n",
        "    cluster_summary = pd.DataFrame({\n",
        "        'pathway': df.index,\n",
        "        'cluster': cluster_labels\n",
        "    })\n",
        "\n",
        "    return cluster_labels, linkage_matrix, corr_matrix, cluster_summary\n",
        "\n",
        "def analyze_pathway_clusters(df, cluster_labels):\n",
        "    \"\"\"\n",
        "    Analyze the characteristics of each pathway cluster\n",
        "    \"\"\"\n",
        "    cluster_stats = {}\n",
        "\n",
        "    for cluster in np.unique(cluster_labels):\n",
        "        # Get pathways in this cluster\n",
        "        cluster_paths = df.index[cluster_labels == cluster]\n",
        "        cluster_data = df.loc[cluster_paths]\n",
        "\n",
        "        # Calculate statistics\n",
        "        cluster_stats[cluster] = {\n",
        "            'size': len(cluster_paths),\n",
        "            'mean_abundance': cluster_data.mean().mean(),\n",
        "            'std_abundance': cluster_data.mean().std(),\n",
        "            'pathways': list(cluster_paths),\n",
        "            'correlation': np.corrcoef(cluster_data),\n",
        "            'total_abundance': cluster_data.sum().mean()\n",
        "        }\n",
        "\n",
        "    return cluster_stats\n",
        "\n",
        "def plot_pathway_clusters(df, linkage_matrix, corr_matrix, cluster_labels, cluster_stats):\n",
        "    \"\"\"\n",
        "    Create visualizations for pathway clusters\n",
        "    \"\"\"\n",
        "    # Plot dendrogram\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    plt.title('Pathway Clustering Dendrogram')\n",
        "    hierarchy.dendrogram(linkage_matrix, labels=df.index, leaf_rotation=90)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Plot correlation heatmap\n",
        "    plt.figure(figsize=(12, 12))\n",
        "    sns.heatmap(pd.DataFrame(corr_matrix, index=df.index, columns=df.index),\n",
        "                cmap='coolwarm', center=0, vmin=-1, vmax=1)\n",
        "    plt.title('Pathway Correlation Heatmap')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Plot cluster sizes and abundances\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    clusters = list(cluster_stats.keys())\n",
        "    sizes = [stats['size'] for stats in cluster_stats.values()]\n",
        "    abundances = [stats['mean_abundance'] for stats in cluster_stats.values()]\n",
        "\n",
        "    ax1 = plt.gca()\n",
        "    ax2 = ax1.twinx()\n",
        "\n",
        "    ax1.bar(clusters, sizes, alpha=0.6, color='skyblue')\n",
        "    ax1.set_ylabel('Number of Pathways')\n",
        "\n",
        "    ax2.plot(clusters, abundances, 'ro-')\n",
        "    ax2.set_ylabel('Mean Abundance')\n",
        "\n",
        "    plt.title('Cluster Sizes and Abundances')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def create_cluster_features(df, cluster_labels):\n",
        "    \"\"\"\n",
        "    Create new features based on pathway clusters\n",
        "    \"\"\"\n",
        "    n_clusters = len(np.unique(cluster_labels))\n",
        "    cluster_features = pd.DataFrame(index=df.columns)\n",
        "\n",
        "    for cluster in range(n_clusters):\n",
        "        # Get pathways in this cluster\n",
        "        cluster_paths = df.index[cluster_labels == cluster]\n",
        "\n",
        "        # Calculate mean abundance for cluster\n",
        "        cluster_features[f'cluster_{cluster}'] = df.loc[cluster_paths].mean()\n",
        "\n",
        "        # Calculate total abundance for cluster\n",
        "        cluster_features[f'cluster_{cluster}_total'] = df.loc[cluster_paths].sum()\n",
        "\n",
        "        # Calculate diversity within cluster\n",
        "        cluster_features[f'cluster_{cluster}_diversity'] = (df.loc[cluster_paths] > 0).sum()\n",
        "\n",
        "    return cluster_features\n",
        "\n",
        "# Calling the fUNCTION\n",
        "cluster_labels, linkage_matrix, corr_matrix, cluster_summary = cluster_pathways(Picrust_Result)\n",
        "cluster_stats = analyze_pathway_clusters(Picrust_Result, cluster_labels)\n",
        "plot_pathway_clusters(Picrust_Result, linkage_matrix, corr_matrix, cluster_labels, cluster_stats)\n",
        "cluster_features = create_cluster_features(Picrust_Result, cluster_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Os65js9EuvgX"
      },
      "source": [
        "# 13 Organic Metal Pathways"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vgw9Pwp8uvgX",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "def analyze_organic_metal_pathways(df):\n",
        "    \"\"\"\n",
        "    Analyze pathways related to organic acid metabolism and metal interactions\n",
        "\n",
        "    Parameters:\n",
        "    df: pandas DataFrame with pathways as index and samples as columns\n",
        "    \"\"\"\n",
        "    # Define specific pathway categories\n",
        "    pathway_categories = {\n",
        "        'organic_acid_metabolism': [\n",
        "            'acetate', 'acetic acid', 'acetyl',\n",
        "            'oxalate', 'oxalic acid',\n",
        "            'organic acid', 'fatty acid',\n",
        "            'carboxylic acid'\n",
        "        ],\n",
        "\n",
        "        'metal_organic_interaction': [\n",
        "            'siderophore', 'metal binding',\n",
        "            'iron complex', 'metal transport',\n",
        "            'metallophore', 'metal organic'\n",
        "        ],\n",
        "\n",
        "        'biofilm_formation': [\n",
        "            'biofilm', 'exopolysaccharide',\n",
        "            'extracellular matrix', 'adhesion'\n",
        "        ],\n",
        "\n",
        "        'halogen_related': [\n",
        "            'halogen', 'chloride', 'bromide',\n",
        "            'halide', 'dehalogenation'\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    # Analyze each category\n",
        "    def get_category_pathways(terms):\n",
        "        return df.index[df.index.str.lower().str.contains('|'.join(terms), regex=True)]\n",
        "\n",
        "    pathway_data = {}\n",
        "    pathway_stats = {}\n",
        "\n",
        "    for category, terms in pathway_categories.items():\n",
        "        pathways = get_category_pathways(terms)\n",
        "        if len(pathways) > 0:\n",
        "            pathway_data[category] = df.loc[pathways]\n",
        "\n",
        "            # Calculate comprehensive statistics\n",
        "            pathway_stats[category] = pd.DataFrame({\n",
        "                'mean_abundance': pathway_data[category].mean(axis=1),\n",
        "                'std_abundance': pathway_data[category].std(axis=1),\n",
        "                'cv': pathway_data[category].std(axis=1) / pathway_data[category].mean(axis=1) * 100,\n",
        "                'presence': (pathway_data[category] > 0).mean(axis=1) * 100,  # % of samples with pathway\n",
        "                'relative_abundance': pathway_data[category].mean(axis=1) / df.mean(axis=1).mean() * 100\n",
        "            }).sort_values('mean_abundance', ascending=False)\n",
        "\n",
        "    return pathway_data, pathway_stats\n",
        "\n",
        "def analyze_pathway_relationships(pathway_data):\n",
        "    \"\"\"\n",
        "    Analyze relationships between different pathway categories\n",
        "    \"\"\"\n",
        "    # Calculate mean abundance for each category across samples\n",
        "    category_means = pd.DataFrame({\n",
        "        category: data.mean(axis=0)\n",
        "        for category, data in pathway_data.items()\n",
        "    })\n",
        "\n",
        "    # Calculate correlations\n",
        "    correlations = category_means.corr()\n",
        "\n",
        "    # Identify potential functional relationships\n",
        "    high_correlations = correlations.unstack()\n",
        "    high_correlations = high_correlations[high_correlations != 1.0]\n",
        "    high_correlations = high_correlations[abs(high_correlations) > 0.5]\n",
        "\n",
        "    return category_means, correlations, high_correlations.sort_values(ascending=False)\n",
        "\n",
        "def plot_pathway_analysis(pathway_stats, pathway_data):\n",
        "    \"\"\"\n",
        "    Create visualizations for pathway analysis\n",
        "    \"\"\"\n",
        "    for category, stats in pathway_stats.items():\n",
        "        if len(stats) > 0:\n",
        "            # Create subplot with dual axis\n",
        "            fig, ax1 = plt.subplots(figsize=(12, min(8, max(4, len(stats)*0.3))))\n",
        "            ax2 = ax1.twinx()\n",
        "\n",
        "            # Plot abundance and relative abundance\n",
        "            sns.barplot(data=stats.head(10).reset_index(),\n",
        "                       x='relative_abundance',\n",
        "                       y='index',\n",
        "                       color='skyblue',\n",
        "                       ax=ax1)\n",
        "\n",
        "            stats.head(10)['presence'].plot(\n",
        "                marker='o',\n",
        "                color='red',\n",
        "                ax=ax2\n",
        "            )\n",
        "\n",
        "            ax1.set_title(f'{category.replace(\"_\", \" \").title()} Pathways')\n",
        "            ax1.set_xlabel('Relative Abundance (%)')\n",
        "            ax2.set_xlabel('Presence (%)')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Le_mzeY_uvgX",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Calling the function\n",
        "pathway_data, pathway_stats = analyze_organic_metal_pathways(Picrust_Result)\n",
        "category_means, correlations, high_corr = analyze_pathway_relationships(pathway_data)\n",
        "plot_pathway_analysis(pathway_stats, pathway_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wf76SqphuvgX"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EQUT3fwuvgX"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djNOlMWkuvgX",
        "trusted": true
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "databundleVersionId": 11132286,
          "datasetId": 6686232,
          "sourceId": 10776455,
          "sourceType": "datasetVersion"
        },
        {
          "databundleVersionId": 11151079,
          "datasetId": 6677417,
          "sourceId": 10793499,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30886,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}