{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Analysis\n",
    "This book comprises a cluster analyis of the complete dataset (Complete_data) and its derived datasets (Influencers_taxa and Selected_data), as defined in earlier notebooks. The pipeline applied is described below:\n",
    "\n",
    "__Data Preparation:__  \n",
    "    Only scaling and normalization were applied, as data cleaning was already performed in earlier steps.  \n",
    "__Dimensionality Reduction:__  \n",
    "    Principal Component Analysis (PCA) was used to reduce dimensionality while retaining approximately 90% of the variance.  \n",
    "    This step resulted in transformed datasets with reduced feature dimensions, which were subsequently used for clustering.  \n",
    "__Clustering Analysis:__  \n",
    "    Clustering was performed on the transformed datasets using:   \n",
    "    K-Means, with k=5 determined by the elbow method.  \n",
    "    DBSCAN, for detecting clusters and outliers.  \n",
    "    Gaussian Mixture Models (GMM), to capture potential non-linear structures.  \n",
    "__Performance evaluation included:__   \n",
    "    Internal metrics, such as Silhouette Score and Davies-Bouldin Index.  \n",
    "    External metrics (where labels were available), including Adjusted Rand Index (ARI) and Homogeneity Score.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score, davies_bouldin_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MinMaxScaler is the denominated scaler that owe to be used with this data since doesnt have gaussian distribution, however it does only work well for DBSCAN, so the StandarScaler is use in conjuction with the KMeans cluster analysis. First pipeline Scales, and reduce the dimensionality with PCA to 90% varianza, then uses 3 clustering methods( K-Means, DBSCAN, and GMM) applied on PCA-reduced data. Evaluation of the methods uses following metrics: Silhouette Score and Davies-Bouldin Index for internal evaluation. Adjusted Rand Index (ARI) for external evaluation since true labels are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the data \n",
    "pd.options.display.float_format = '{:.4f}'.format\n",
    "# Read the excel file\n",
    "Jointax = pd.read_excel('data/Jointax.xlsx', sheet_name='Biotot_jointax', header=[0,1,2,3,4,5,6,7] , dtype={**{i: str for i in range(0,2)},\n",
    "                                                                        **{i:float for i in range(2, 884)}},  skiprows=[8]) # Somehow it was showing an empty row, so skiprow deletes it\n",
    "\n",
    "# Making sure the sites and categories get read as they should\n",
    "Jointax[\"Sites\"]= Jointax[\"Sites\"].astype(str)\n",
    "Jointax[\"Category\"]= Jointax[\"Category\"].values.astype(int)\n",
    "#Drop level of Kindom since it is boring\n",
    "Jointax.columns = Jointax.columns.droplevel(1)\n",
    "Jointax = Jointax.reset_index(drop=True)\n",
    "#Setting the sites as index\n",
    "Jointax = Jointax.set_index(\"Sites\").reset_index()\n",
    "# Deleting headers names of unnamed levels\n",
    "Jointax.columns = Jointax.columns.map(lambda x: tuple('' if 'Unnamed' in str(level) else level for level in x))\n",
    "#Drop column 1\n",
    "Jointax =Jointax.drop(Jointax.columns[1], axis=1)\n",
    "#Correcting the Tuple-like Index\n",
    "Jointax['Sites'] = Jointax['Sites'].map(lambda x: x[0] if isinstance(x, tuple) else x)\n",
    "Jointax = Jointax.set_index(\"Sites\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We working only with the values in this notebook, still keeping the taxa \n",
    "Jointax.columns = Jointax.columns.droplevel([0,1,2,3,4,5])\n",
    "abundance_all = Jointax.reset_index(drop=False)\n",
    "#Reset the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "if abundance_all.columns[1] == \"\":\n",
    "    abundance_all.rename(columns = {abundance_all.columns[1]: \"Category\"}, inplace=True)\n",
    "abundance_all= abundance_all.set_index(\"Sites\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>873</th>\n",
       "      <th>874</th>\n",
       "      <th>875</th>\n",
       "      <th>876</th>\n",
       "      <th>877</th>\n",
       "      <th>878</th>\n",
       "      <th>879</th>\n",
       "      <th>880</th>\n",
       "      <th>881</th>\n",
       "      <th>882</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sites</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>site_1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4308</td>\n",
       "      <td>0.5170</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0215</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0215</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>site_2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0190</td>\n",
       "      <td>0.3415</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0190</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0190</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>site_3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0246</td>\n",
       "      <td>0.3192</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0123</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0123</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0246</td>\n",
       "      <td>0.0123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>site_4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0154</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0176</td>\n",
       "      <td>0.2512</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0066</td>\n",
       "      <td>0.0022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>site_5</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0037</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0221</td>\n",
       "      <td>0.5098</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0037</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0037</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.0037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 883 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Category      1      2      3      4      5      6      7      8  \\\n",
       "Sites                                                                      \n",
       "site_1         3 0.0000 0.0000 0.0000 0.0000 0.0000 0.4308 0.5170 0.0000   \n",
       "site_2         1 0.0000 0.0000 0.0000 0.0000 0.0000 0.0190 0.3415 0.0000   \n",
       "site_3         1 0.0000 0.0000 0.0000 0.0000 0.0000 0.0246 0.3192 0.0000   \n",
       "site_4         1 0.0000 0.0000 0.0154 0.0000 0.0000 0.0176 0.2512 0.0000   \n",
       "site_5         1 0.0000 0.0000 0.0037 0.0000 0.0000 0.0221 0.5098 0.0000   \n",
       "\n",
       "            9  ...    873    874    875    876    877    878    879    880  \\\n",
       "Sites          ...                                                           \n",
       "site_1 0.0000  ... 0.0000 0.0000 0.0000 0.0215 0.0000 0.0000 0.0000 0.0000   \n",
       "site_2 0.0000  ... 0.0190 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000   \n",
       "site_3 0.0000  ... 0.0123 0.0000 0.0000 0.0123 0.0000 0.0000 0.0000 0.0000   \n",
       "site_4 0.0000  ... 0.0022 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000   \n",
       "site_5 0.0000  ... 0.0037 0.0000 0.0037 0.0000 0.0000 0.0000 0.0000 0.0000   \n",
       "\n",
       "          881    882  \n",
       "Sites                 \n",
       "site_1 0.0215 0.0000  \n",
       "site_2 0.0190 0.0000  \n",
       "site_3 0.0246 0.0123  \n",
       "site_4 0.0066 0.0022  \n",
       "site_5 0.0110 0.0037  \n",
       "\n",
       "[5 rows x 883 columns]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abundance_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering_pipeline_all(df, n_clusters=5, eps=0.5, min_samples=5, n_components=2):\n",
    "    \"\"\"\n",
    "    Performs clustering using K-Means, DBSCAN, and GMM with PCA for dimensionality reduction.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: Input DataFrame (features only, no labels).\n",
    "    - n_clusters: Number of clusters for K-Means and GMM (default=5).\n",
    "    - eps: DBSCAN's epsilon parameter (default=0.5).\n",
    "    - min_samples: Minimum samples for DBSCAN (default=5).\n",
    "    - n_components: Number of components for PCA (default=2).\n",
    "    \n",
    "    Returns:\n",
    "    - results: Dictionary with clustering results for K-Means, DBSCAN, and GMM, as well as PCA data and metrics.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    df = df.drop(columns=['Category'])  # Drop any non-numeric columns\n",
    "\n",
    "    # Step 1: Scaling the data\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(df)\n",
    "    \n",
    "    # Step 2: PCA for dimensionality reduction\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca_data = pca.fit_transform(scaled_data)\n",
    "    results['explained_variance'] = pca.explained_variance_ratio_\n",
    "    \n",
    "    # Step 3: K-Means Clustering\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans_labels = kmeans.fit_predict(pca_data)\n",
    "    kmeans_silhouette = silhouette_score(pca_data, kmeans_labels)\n",
    "    kmeans_db_score = davies_bouldin_score(pca_data, kmeans_labels)\n",
    "    results['kmeans'] = {\n",
    "        'cluster_labels': kmeans_labels,\n",
    "        'silhouette_score': kmeans_silhouette,\n",
    "        'davies_bouldin_score': kmeans_db_score\n",
    "    }\n",
    "    \n",
    "    # Step 4: DBSCAN Clustering\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    dbscan_labels = dbscan.fit_predict(pca_data)\n",
    "    valid_indices = dbscan_labels != -1\n",
    "    if len(set(dbscan_labels[valid_indices])) > 1:\n",
    "        dbscan_silhouette = silhouette_score(pca_data[valid_indices], dbscan_labels[valid_indices])\n",
    "        dbscan_db_score = davies_bouldin_score(pca_data[valid_indices], dbscan_labels[valid_indices])\n",
    "    else:\n",
    "        dbscan_silhouette = None\n",
    "        dbscan_db_score = None\n",
    "    results['dbscan'] = {\n",
    "        'cluster_labels': dbscan_labels,\n",
    "        'silhouette_score': dbscan_silhouette,\n",
    "        'davies_bouldin_score': dbscan_db_score\n",
    "    }\n",
    "    \n",
    "    # Step 5: GMM Clustering\n",
    "    gmm = GaussianMixture(n_components=n_clusters, random_state=42)\n",
    "    gmm_labels = gmm.fit_predict(pca_data)\n",
    "    gmm_silhouette = silhouette_score(pca_data, gmm_labels)\n",
    "    gmm_db_score = davies_bouldin_score(pca_data, gmm_labels)\n",
    "    results['gmm'] = {\n",
    "        'cluster_labels': gmm_labels,\n",
    "        'silhouette_score': gmm_silhouette,\n",
    "        'davies_bouldin_score': gmm_db_score\n",
    "    }\n",
    "    \n",
    "    # Store PCA-transformed data\n",
    "    results['pca_data'] = pca_data\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running the pipeline for DataFrames: df1, df2, df3\n",
    "dataframes = [abundance_all]\n",
    "\n",
    "# Running the  pipeline for each DataFrame\n",
    "clustering_results = [clustering_pipeline_all(df, n_clusters=5, eps=0.5, min_samples=5, n_components=2) for df in dataframes]\n",
    "\n",
    "# K-Means silhouette score for abundance_all\n",
    "kmeans_silhouette_abundance_all = clustering_results[0]['kmeans']['silhouette_score']\n",
    "\n",
    "kmeans_labels_abundance_all = clustering_results[0]['kmeans']['cluster_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 0 1 2 1 1 1 2 2 1 1 2 2\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 3 1 4 4 1 4 4 4 1 1 1 1 1 1 1] 0.7610374381507409\n"
     ]
    }
   ],
   "source": [
    "print(kmeans_labels_abundance_all , kmeans_silhouette_abundance_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[208], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Example: Plot K-Means Clusters\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m pca_data \u001b[38;5;241m=\u001b[39m \u001b[43mclustering_results\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpca_data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     27\u001b[0m kmeans_labels \u001b[38;5;241m=\u001b[39m clustering_results[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkmeans\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcluster_labels\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     29\u001b[0m plot_clusters(pca_data, kmeans_labels, method_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mK-Means Clustering\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_clusters(pca_data, cluster_labels, method_name=\"Clustering\"):\n",
    "    \"\"\"\n",
    "    Visualize clusters using PCA-transformed data.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(\n",
    "        x=pca_data[:, 0],\n",
    "        y=pca_data[:, 1],\n",
    "        hue=cluster_labels,\n",
    "        palette=\"viridis\",\n",
    "        s=50,\n",
    "        alpha=0.8,\n",
    "        edgecolor=\"k\"\n",
    "    )\n",
    "    plt.title(f\"{method_name} Visualization (PCA Reduced)\", fontsize=16)\n",
    "    plt.xlabel(\"Principal Component 1\")\n",
    "    plt.ylabel(\"Principal Component 2\")\n",
    "    plt.legend(title=\"Cluster\", loc=\"best\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Example: Plot K-Means Clusters\n",
    "pca_data = clustering_results['pca_data']\n",
    "kmeans_labels = clustering_results[0]['kmeans']['cluster_labels']\n",
    "\n",
    "plot_clusters(pca_data, kmeans_labels, method_name=\"K-Means Clustering\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second pipeline will use both scalers MinMaxScaler better suitable for DBSCAN and StandarScaler better for KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define scalers and clustering methods\n",
    "scalers = [(\"StandardScaler\", StandardScaler()), (\"MinMaxScaler\", MinMaxScaler())]\n",
    "clustering_methods = [\n",
    "    (\"DBSCAN\", DBSCAN(eps=0.5, min_samples=5)),\n",
    "    (\"KMeans\", KMeans(n_clusters=5, random_state=42))\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "# Loop through scalers and clustering methods\n",
    "for scaler_name, scaler in scalers:\n",
    "    for cluster_name, clusterer in clustering_methods:\n",
    "        pipeline = Pipeline([\n",
    "            (\"scaler\", scaler),\n",
    "            (\"clustering\", clusterer)\n",
    "        ])\n",
    "        # Fit the pipeline\n",
    "        pipeline.fit(df.drop(columns=[\"categories\", \"Sites\"]))\n",
    "\n",
    "        # Retrieve cluster labels\n",
    "        labels = pipeline[\"clustering\"].labels_\n",
    "\n",
    "        # Filter noise for DBSCAN\n",
    "        if cluster_name == \"DBSCAN\" and -1 in labels:\n",
    "            labels = labels[labels != -1]\n",
    "\n",
    "        # Evaluate metrics if clustering is valid\n",
    "        if len(np.unique(labels)) > 1:  # Ensure we have more than one cluster\n",
    "            silhouette = silhouette_score(df.drop(columns=[\"categories\", \"Sites\"]), labels)\n",
    "            davies_bouldin = davies_bouldin_score(df.drop(columns=[\"categories\", \"Sites\"]), labels)\n",
    "        else:\n",
    "            silhouette, davies_bouldin = None, None\n",
    "\n",
    "        # Store results\n",
    "        results.append({\n",
    "            \"Scaler\": scaler_name,\n",
    "            \"Clustering Method\": cluster_name,\n",
    "            \"Silhouette Score\": silhouette,\n",
    "            \"Davies-Bouldin Index\": davies_bouldin\n",
    "        })\n",
    "\n",
    "# Display results\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
