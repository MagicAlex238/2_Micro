{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDED5j4zCqR_"
      },
      "source": [
        "# Sequence Analysis and Functional Prediction Pipeline\n",
        "\n",
        "## 1. Introduction\n",
        "This notebook analyzes the functional and sequence relationships between newly identified bacteria and known corrosion-influencing microorganisms. The analysis builds upon previous findings where:\n",
        "- Statistical significance was established between the selected bacteria and corrosion risk (Notebook 3)\n",
        "- Literature validation confirmed corrosion influence for many bacteria (Notebook 4)\n",
        "- Evolutionary relationships were mapped through phylogenetic analysis (Notebook 5)\n",
        "\n",
        "The study focuses on bacteria from operational heating and cooling water systems, primarily in Germany. Using 16S rRNA data (bootstrap-validated from Notebook 5), this analysis employs PICRUSt2 to predict metabolic functions and compare functional profiles between different bacterial groups.\n",
        "\n",
        "### Analysis Approaches\n",
        "We implement two classification strategies:\n",
        "\n",
        "1. Simple Classification:\n",
        "   - Known corrosion-causing bacteria (usual_taxa)\n",
        "   - Other bacteria (combining checked_taxa and core_taxa)\n",
        "\n",
        "2. Detailed Classification:\n",
        "   - Known corrosion-causing bacteria (usual_taxa)\n",
        "   - Pure checked bacteria (exclusive to checked_taxa)\n",
        "   - Pure core bacteria (exclusive to core_taxa)\n",
        "   - Checked-core bacteria (overlap between checked and core taxa)\n",
        "\n",
        "This detailed approach allows for more nuanced analysis of functional profiles and better understanding of potential corrosion mechanisms across different bacterial groups.\n",
        "\n",
        "### Analysis Goals:\n",
        "- Predict metabolic functions from 16S sequences\n",
        "- Focus on corrosion-relevant pathways (sulfur/iron metabolism)\n",
        "- Compare functional profiles between known corrosion-causing bacteria and newly identified candidates\n",
        "- Validate whether statistical correlations reflect genuine metabolic capabilities associated with corrosion processes\n",
        "\n",
        "### Directory Structure:\n",
        " Following is the structure of the notebook data named data_picrus  \n",
        "data_tree  \n",
        " ├── sequences/  \n",
        " │   ├── known.fasta : sequences of known corrosion-causing bacteria  \n",
        " │   ├── candidate.fasta : sequences of potential new corrosion-causing bacteria  \n",
        " |   └── other files  \n",
        " data_picrus  \n",
        " └── picrust_results/  \n",
        "      ├── known_bacteria/  \n",
        "      |               ├── EC_predictions/       : enzyme predictions  \n",
        "      |               ├── pathway_predictions/  : metabolic pathway abundance  \n",
        "      |               ├── KO_predictions/       : KEGG ortholog predictions  \n",
        "      |               └── other_picrust_files/  \n",
        "      ├── candidate_bacteria/  \n",
        "      |               ├── EC_predictions/       : enzyme predictions  \n",
        "      |               ├── pathway_predictions/  : metabolic pathway abundance  \n",
        "      |               ├── KO_predictions/       : KEGG ortholog predictions  \n",
        "      |               └── other_picrust_files/  : final comparison summary\n",
        "      ├── core_bacteria/\n",
        "      |               ├── EC_predictions/       : enzyme predictions  \n",
        "      |               ├── pathway_predictions/  : metabolic pathway abundance  \n",
        "      |               ├── KO_predictions/       : KEGG ortholog predictions  \n",
        "      |               └── other_picrust_files/  \n",
        "      │      \n",
        "      └── functional_comparison.xlsx  \n",
        "\n",
        "Picrust2 works using its reference database that was installed with the package   \n",
        "~/miniconda3/envs/picrust2/lib/python3.9/site-packages/picrust2/default_files/prokaryotic/pro_ref\n",
        "\n",
        "About picrust2  \n",
        "https://evomics.org/wp-content/uploads/2015/01/presentation_evomics-05-picrust_01-18-15.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeIn7RIBCqSD"
      },
      "source": [
        "# 2. Loading and Preparing the Data\n",
        "\n",
        "## 2.1 Colab Initialisation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "try:\n",
        "    drive.flush_and_unmount()  # Attempt to unmount if already mounted\n",
        "except ValueError:  # Catch the error if it is not mounted\n",
        "    pass\n",
        "\n",
        "# This approach will ensure the mount point directory is empty.\n",
        "!rm -rf /content/drive  # If it still exists, remove everything in it\n",
        "!mkdir /content/drive  # Recreate an empty mount point\n",
        "\n",
        "drive.mount('/content/drive')  # Retry mounting"
      ],
      "metadata": {
        "id": "fy6X03sq5Fzc",
        "outputId": "441b1d3c-7956-4a6d-ee18-edd5d63281c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive not mounted, so nothing to flush and unmount.\n",
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab specific\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#change the path\n",
        "os.chdir('/content/drive/MyDrive/MIC/data_picrust')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5U6gm_R_JQjt",
        "outputId": "ac6ce278-7b6c-42a6-ba44-39cdb592f203"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWqOI6IDD1qW"
      },
      "source": [
        "__Importing PICRUST IN COLAB__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "uKimriI3hmTq",
        "outputId": "b5565f47-532a-48a8-aaa3-a5428635b260",
        "vscode": {
          "languageId": "javascript"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"# Install miniconda and initialize\\n!wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\\n!bash Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local/miniconda3\\n!conda config --add channels defaults\\n!conda config --add channels bioconda\\n!conda config --add channels conda-forge\\n# Imports for colab\\nimport condacolab\\nimport sys\\nsys.path.append('/usr/local/miniconda3/lib/python3.7/site-packages/')\\n\\n# Install PICRUSt2 and its dependencies\\n%conda install -c bioconda -c conda-forge picrust2=2.4.1 -y\\n# Verify installations%\\n%conda list | grep picrust2\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "'''# Install miniconda and initialize\n",
        "!wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
        "!bash Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local/miniconda3\n",
        "!conda config --add channels defaults\n",
        "!conda config --add channels bioconda\n",
        "!conda config --add channels conda-forge\n",
        "# Imports for colab\n",
        "import condacolab\n",
        "import sys\n",
        "sys.path.append('/usr/local/miniconda3/lib/python3.7/site-packages/')\n",
        "\n",
        "# Install PICRUSt2 and its dependencies\n",
        "%conda install -c bioconda -c conda-forge picrust2=2.4.1 -y\n",
        "# Verify installations%\n",
        "%conda list | grep picrust2'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzaNXN9suvgG"
      },
      "source": [
        "### Using Pro colab"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "print([module for module in sys.modules if 'tensorflow' in module])"
      ],
      "metadata": {
        "id": "qF94FGxn2iUL",
        "outputId": "8d59a064-d4c9-43a1-dd03-5d9524f1bc55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['google.colab._tensorflow_magics']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up memory footprint support libraries\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()"
      ],
      "metadata": {
        "id": "3gWJfdx3Ni1f",
        "outputId": "6e34aec1-674a-4b3d-bbe4-0f90de213174",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gputil\n",
            "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gputil: filename=GPUtil-1.4.0-py3-none-any.whl size=7392 sha256=c44615d1168271a0e7e4686fb00ab451277283a4707a5e2cb718cfcd1bc961d4\n",
            "  Stored in directory: /root/.cache/pip/wheels/2b/4d/8f/55fb4f7b9b591891e8d3f72977c4ec6c7763b39c19f0861595\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (5.9.5)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.11/dist-packages (4.11.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwAGNqzHuvgH",
        "outputId": "70632937-aba4-4dd6-b71e-f66fa45014d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 54.8 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KC8v0oJRuvgH",
        "outputId": "8418ad27-5dc7-472d-bae1-a8156b5f536c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting biopython\n",
            "  Downloading biopython-1.85-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from biopython) (1.26.4)\n",
            "Downloading biopython-1.85-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/3.3 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: biopython\n",
            "Successfully installed biopython-1.85\n"
          ]
        }
      ],
      "source": [
        "%pip install biopython\n",
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "# %% [markdown]\n",
        "# %pip install biom-format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZNP94FKuvgH"
      },
      "source": [
        "__Mounting the Drive__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiHOVZiIuvgH"
      },
      "source": [
        "# 2.2. Importing Libraries,  Making Directories and Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l92DnCZ3CqSD"
      },
      "outputs": [],
      "source": [
        "# Standard library imports\n",
        "import os\n",
        "import sys\n",
        "import ast\n",
        "import subprocess\n",
        "import logging\n",
        "from datetime import datetime\n",
        "import shutil\n",
        "from io import StringIO\n",
        "from pathlib import Path\n",
        "# Data processing imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import openpyxl\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "#%matplotlib inline\n",
        "#plt.style.use('seaborn')\n",
        "import seaborn as sns\n",
        "from natsort import natsorted"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install umap-learn\n",
        "import umap\n",
        "# datascience libraries\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.cluster import hierarchy\n",
        "from sklearn.decomposition import PCA, NMF\n",
        "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
        "from scipy.stats import spearmanr\n",
        "from scipy.spatial.distance import pdist\n",
        "import networkx as nx\n",
        "\n",
        "# from Bio\n",
        "from Bio import SeqIO\n",
        "from Bio.Seq import Seq\n",
        "from Bio.SeqRecord import SeqRecord\n",
        "'''\n",
        "# BIOM handling\n",
        "from biom import Table\n",
        "from biom.util import biom_open\n",
        "from biom import load_table\n",
        "\n",
        "#imports retrieval\n",
        "import requests\n",
        "import time\n",
        "import json'''"
      ],
      "metadata": {
        "id": "cLkUWNNbwYbV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "outputId": "2d5ffabf-312e-4acf-d2d7-c14a8e302d86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting umap-learn\n",
            "  Downloading umap_learn-0.5.7-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (1.6.1)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (0.61.0)\n",
            "Collecting pynndescent>=0.5 (from umap-learn)\n",
            "  Downloading pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from umap-learn) (4.67.1)\n",
            "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.2->umap-learn) (0.44.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.11/dist-packages (from pynndescent>=0.5->umap-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22->umap-learn) (3.5.0)\n",
            "Downloading umap_learn-0.5.7-py3-none-any.whl (88 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.8/88.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pynndescent, umap-learn\n",
            "Successfully installed pynndescent-0.5.13 umap-learn-0.5.7\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# BIOM handling\\nfrom biom import Table\\nfrom biom.util import biom_open\\nfrom biom import load_table\\n\\n#imports retrieval\\nimport requests\\nimport time\\nimport json'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "8QDUKMrMtkgC",
        "outputId": "e8d82b41-4c78-42bc-ff2f-cb267cc7bfb7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"# Force sync with Google Drive\\ndrive.flush_and_unmount()\\ndrive.mount('/content/drive')\\nos.chdir('/content/drive/MyDrive/MIC/data_picrust')\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "'''# Force sync with Google Drive\n",
        "drive.flush_and_unmount()\n",
        "drive.mount('/content/drive')\n",
        "os.chdir('/content/drive/MyDrive/MIC/data_picrust')'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jhg73Rb9CqSF"
      },
      "outputs": [],
      "source": [
        "# Set up logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "\n",
        "# Directory Structure Definitions\n",
        "SIMPLE_BASE = {\n",
        "    'known': 'simple_known_mic',\n",
        "    'other': 'simple_candidate_mic'\n",
        "}\n",
        "\n",
        "DETAILED_BASE = {\n",
        "    'known': 'detailed_known_mic',\n",
        "    'pure_checked': 'detailed_pure_checked_mic',\n",
        "    'pure_core': 'detailed_pure_core_mic',\n",
        "    'checked_core': 'detailed_checked_core_mic'\n",
        "}\n",
        "\n",
        "SUBDIRS = [\n",
        "    'EC_predictions',\n",
        "    'pathway_predictions',\n",
        "    'KO_predictions',\n",
        "    'other_picrust_files'\n",
        "]\n",
        "\n",
        "# Base Paths\n",
        "if \"google.colab\" in sys.modules:\n",
        "    base_dir = Path(\"/content/drive/MyDrive/MIC/data_picrust\")\n",
        "else:\n",
        "    base_dir = Path(\"/home/beatriz/MIC/2_Micro/data_picrust\")\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "base_dir.mkdir(parents=True, exist_ok=True)\n",
        "abundance_excel= Path(\"/home/beatriz/MIC/2_Micro/data_Ref/merged_to_sequence.xlsx\")\n",
        "fasta_file_final = Path(\"/home/beatriz/MIC/2_Micro/data_qiime/results_match_gg/final_sequences_gg.fasta\")\n",
        "aligned_fasta = Path(\"/home/beatriz/MIC/2_Micro/data_qiime/results_match_gg/aligned-dna-sequences_gg.fasta\")\n",
        "\n",
        "# for colab\n",
        "# Create output directory if it doesn't exist\n",
        "\n",
        "base_dir.mkdir(parents=True, exist_ok=True)\n",
        "abundance_excel= Path(\"/content/drive/MyDrive/MIC/data_picrust/merged_to_sequence.xlsx\")\n",
        "fasta_file_final = Path(\"/content/drive/MyDrive/MIC/data_picrust/final_sequences_gg.fasta\")\n",
        "aligned_fasta = Path(\"/content/drive/MyDrive/MIC/data_picrust/aligned-dna-sequences_gg.fasta\")\n",
        "results_file = base_dir / \"/content/drive/MyDrive/MIC/data_picrust/functional_comparison.xlsx\"\n",
        "output_dir = base_dir  # Separate output directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COW1kGFZCqSG"
      },
      "source": [
        "The fasta file come from the Alternative Sequences finding from the Greenes Genes Database, from the taxonomy in this study made in section 7 in the 5_Sequences_qiime notebook: final_sequences_gg.fasta. Abundance dataframe come from the data from notebook 4 merged_to_sequence.xlsx sheet=core_check_usual_taxa which is a unified df between 3 different groups explained previously: cora_taxa (>20% 60 abundance features), usual_taxa (17 high literature ranking bacteria influencing corrosion) and checked_taxa (30 statistically significant to the corrosion risk label) in total 85 features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzMCSdlPuvgI"
      },
      "outputs": [],
      "source": [
        "# Integrated taxa from origin genus as headers with levels 6 for the genera, 7 for the GID, muss be cleaned\n",
        "Integrated_T = pd.read_excel(abundance_excel, sheet_name='core_check_usual_taxa', header=[0,1,2,3,4,5,6,7], engine ='openpyxl')\n",
        "# Drop first row (index 0) and first column in one chain\n",
        "Integrated_T = Integrated_T.drop(index=0).drop(Integrated_T.columns[0], axis=1)\n",
        "Integrated_T= Integrated_T.astype({'Sites': str})\n",
        "Integrated_T['Sites'] = Integrated_T['Sites'].fillna('Source')\n",
        "# Remove 'Unnamed' level names\n",
        "Integrated_T.columns = Integrated_T.columns.map(lambda x: tuple('' if 'Unnamed' in str(level) else level for level in x))\n",
        "# Changing dtypes to category whiles respecting structure\n",
        "Integrated_T[\"Category\"] = Integrated_T[\"Category\"].astype(\"Int64\")\n",
        "Integrated_T= Integrated_T.set_index(\"Sites\")\n",
        "pre_Integrated = Integrated_T.T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        },
        "id": "aZSzaNSQuvgI",
        "outputId": "cc522339-0b66-4435-cefc-ec65a89bb46a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        Category Rhodocyclales_Rhodocyclaceae_Azospira  \\\n",
              "                                              Bacteria   \n",
              "                                        Proteobacteria   \n",
              "                                    Betaproteobacteria   \n",
              "                                         Rhodocyclales   \n",
              "                                        Rhodocyclaceae   \n",
              "                                              Azospira   \n",
              "                                                   110   \n",
              "Sites                                                    \n",
              "site_67        3                              0.004886   \n",
              "site_68        3                                     0   \n",
              "site_69        1                                  1.47   \n",
              "site_70        1                                  1.72   \n",
              "Source      <NA>                              chk-core   \n",
              "\n",
              "        Actinomycetales_Dermabacteraceae_Brachybacterium  \\\n",
              "                                                Bacteria   \n",
              "                                          Actinobacteria   \n",
              "                                          Actinobacteria   \n",
              "                                         Actinomycetales   \n",
              "                                        Dermabacteraceae   \n",
              "                                         Brachybacterium   \n",
              "                                                     140   \n",
              "Sites                                                      \n",
              "site_67                                                0   \n",
              "site_68                                         0.021172   \n",
              "site_69                                                0   \n",
              "site_70                                                0   \n",
              "Source                                               chk   \n",
              "\n",
              "        Actinomycetales_Brevibacteriaceae_Brevibacterium  \\\n",
              "                                                Bacteria   \n",
              "                                          Actinobacteria   \n",
              "                                          Actinobacteria   \n",
              "                                         Actinomycetales   \n",
              "                                       Brevibacteriaceae   \n",
              "                                          Brevibacterium   \n",
              "                                                     145   \n",
              "Sites                                                      \n",
              "site_67                                                0   \n",
              "site_68                                                0   \n",
              "site_69                                                0   \n",
              "site_70                                                0   \n",
              "Source                                               chk   \n",
              "\n",
              "        Erysipelotrichales_Erysipelotrichaceae_Bulleidia  \\\n",
              "                                                Bacteria   \n",
              "                                              Firmicutes   \n",
              "                                         Erysipelotrichi   \n",
              "                                      Erysipelotrichales   \n",
              "                                     Erysipelotrichaceae   \n",
              "                                               Bulleidia   \n",
              "                                                     154   \n",
              "Sites                                                      \n",
              "site_67                                                0   \n",
              "site_68                                                0   \n",
              "site_69                                                0   \n",
              "site_70                                                0   \n",
              "Source                                               chk   \n",
              "\n",
              "        Clostridiales_Clostridiaceae_Clostridium  \\\n",
              "                                        Bacteria   \n",
              "                                      Firmicutes   \n",
              "                                      Clostridia   \n",
              "                                   Clostridiales   \n",
              "                                  Clostridiaceae   \n",
              "                                     Clostridium   \n",
              "                                             214   \n",
              "Sites                                              \n",
              "site_67                                        0   \n",
              "site_68                                        0   \n",
              "site_69                                        0   \n",
              "site_70                                        0   \n",
              "Source                               chk-core-us   \n",
              "\n",
              "        Actinomycetales_Corynebacteriaceae_Corynebacterium  \\\n",
              "                                                  Bacteria   \n",
              "                                            Actinobacteria   \n",
              "                                            Actinobacteria   \n",
              "                                           Actinomycetales   \n",
              "                                        Corynebacteriaceae   \n",
              "                                           Corynebacterium   \n",
              "                                                       229   \n",
              "Sites                                                        \n",
              "site_67                                                  0   \n",
              "site_68                                                  0   \n",
              "site_69                                                  0   \n",
              "site_70                                                  0   \n",
              "Source                                         chk-core-us   \n",
              "\n",
              "        Lactobacillales_Enterococcaceae_Enterococcus  \\\n",
              "                                            Bacteria   \n",
              "                                          Firmicutes   \n",
              "                                             Bacilli   \n",
              "                                     Lactobacillales   \n",
              "                                     Enterococcaceae   \n",
              "                                        Enterococcus   \n",
              "                                                 300   \n",
              "Sites                                                  \n",
              "site_67                                            0   \n",
              "site_68                                            0   \n",
              "site_69                                            0   \n",
              "site_70                                            0   \n",
              "Source                                           chk   \n",
              "\n",
              "        Thermoanaerobacterales_Thermoanaerobacteraceae_Gelria  \\\n",
              "                                                     Bacteria   \n",
              "                                                   Firmicutes   \n",
              "                                                   Clostridia   \n",
              "                                       Thermoanaerobacterales   \n",
              "                                      Thermoanaerobacteraceae   \n",
              "                                                       Gelria   \n",
              "                                                          334   \n",
              "Sites                                                           \n",
              "site_67                                                  0      \n",
              "site_68                                                  0      \n",
              "site_69                                                  0      \n",
              "site_70                                                2.3      \n",
              "Source                                                 chk      \n",
              "\n",
              "        Oceanospirillales_Halomonadaceae_Halomonas  ...  \\\n",
              "                                          Bacteria  ...   \n",
              "                                    Proteobacteria  ...   \n",
              "                               Gammaproteobacteria  ...   \n",
              "                                 Oceanospirillales  ...   \n",
              "                                    Halomonadaceae  ...   \n",
              "                                         Halomonas  ...   \n",
              "                                               354  ...   \n",
              "Sites                                               ...   \n",
              "site_67                                   0.942935  ...   \n",
              "site_68                                  28.889007  ...   \n",
              "site_69                                          0  ...   \n",
              "site_70                                          0  ...   \n",
              "Source                                    chk-core  ...   \n",
              "\n",
              "        Actinomycetales_Propionibacteriaceae_Tessaracoccus  \\\n",
              "                                                  Bacteria   \n",
              "                                            Actinobacteria   \n",
              "                                            Actinobacteria   \n",
              "                                           Actinomycetales   \n",
              "                                      Propionibacteriaceae   \n",
              "                                             Tessaracoccus   \n",
              "                                                       715   \n",
              "Sites                                                        \n",
              "site_67                                                  0   \n",
              "site_68                                                  0   \n",
              "site_69                                                  0   \n",
              "site_70                                                  0   \n",
              "Source                                                core   \n",
              "\n",
              "        Clostridiales_Peptococcaceae_Thermincola  \\\n",
              "                                        Bacteria   \n",
              "                                      Firmicutes   \n",
              "                                      Clostridia   \n",
              "                                   Clostridiales   \n",
              "                                  Peptococcaceae   \n",
              "                                     Thermincola   \n",
              "                                             719   \n",
              "Sites                                              \n",
              "site_67                                        0   \n",
              "site_68                                        0   \n",
              "site_69                                     1.63   \n",
              "site_70                                        0   \n",
              "Source                                      core   \n",
              "\n",
              "        Spirochaetales_Spirochaetaceae_Treponema  \\\n",
              "                                        Bacteria   \n",
              "                                    Spirochaetes   \n",
              "                                    Spirochaetes   \n",
              "                                  Spirochaetales   \n",
              "                                 Spirochaetaceae   \n",
              "                                       Treponema   \n",
              "                                             731   \n",
              "Sites                                              \n",
              "site_67                                        0   \n",
              "site_68                                        0   \n",
              "site_69                                        0   \n",
              "site_70                                        0   \n",
              "Source                                      core   \n",
              "\n",
              "        Burkholderiales_Oxalobacteraceae_Oxalobacteraceae_unclassified  \\\n",
              "                                                              Bacteria   \n",
              "                                                        Proteobacteria   \n",
              "                                                    Betaproteobacteria   \n",
              "                                                       Burkholderiales   \n",
              "                                                      Oxalobacteraceae   \n",
              "                                         Oxalobacteraceae_unclassified   \n",
              "                                                                   853   \n",
              "Sites                                                                    \n",
              "site_67                                                  0               \n",
              "site_68                                                  0               \n",
              "site_69                                                  0               \n",
              "site_70                                                  0               \n",
              "Source                                                core               \n",
              "\n",
              "        Burkholderiales_Comamonadaceae_Variovorax  \\\n",
              "                                         Bacteria   \n",
              "                                   Proteobacteria   \n",
              "                               Betaproteobacteria   \n",
              "                                  Burkholderiales   \n",
              "                                   Comamonadaceae   \n",
              "                                       Variovorax   \n",
              "                                              863   \n",
              "Sites                                               \n",
              "site_67                                  0.151456   \n",
              "site_68                                         0   \n",
              "site_69                                         0   \n",
              "site_70                                         0   \n",
              "Source                                       core   \n",
              "\n",
              "        Anaerolineales_Anaerolinaceae_Wchb1-05  \\\n",
              "                                      Bacteria   \n",
              "                                   Chloroflexi   \n",
              "                                  Anaerolineae   \n",
              "                                Anaerolineales   \n",
              "                                Anaerolinaceae   \n",
              "                                      Wchb1-05   \n",
              "                                           867   \n",
              "Sites                                            \n",
              "site_67                                      0   \n",
              "site_68                                      0   \n",
              "site_69                                      0   \n",
              "site_70                                      0   \n",
              "Source                                    core   \n",
              "\n",
              "        Desulfobacterales_Desulfobacteraceae_Desulfobacterium  \\\n",
              "                                                     Bacteria   \n",
              "                                               Proteobacteria   \n",
              "                                          Deltaproteobacteria   \n",
              "                                            Desulfobacterales   \n",
              "                                           Desulfobacteraceae   \n",
              "                                             Desulfobacterium   \n",
              "                                                          264   \n",
              "Sites                                                           \n",
              "site_67                                                  0      \n",
              "site_68                                                  0      \n",
              "site_69                                                  0      \n",
              "site_70                                                  0      \n",
              "Source                                                  us      \n",
              "\n",
              "        Desulfobacterales_Desulfobulbaceae_Desulfobulbus  \\\n",
              "                                                Bacteria   \n",
              "                                          Proteobacteria   \n",
              "                                     Deltaproteobacteria   \n",
              "                                       Desulfobacterales   \n",
              "                                        Desulfobulbaceae   \n",
              "                                           Desulfobulbus   \n",
              "                                                     265   \n",
              "Sites                                                      \n",
              "site_67                                                0   \n",
              "site_68                                                0   \n",
              "site_69                                                0   \n",
              "site_70                                                0   \n",
              "Source                                                us   \n",
              "\n",
              "        Gallionellales_Gallionellaceae_Gallionella  \\\n",
              "                                          Bacteria   \n",
              "                                    Proteobacteria   \n",
              "                                Betaproteobacteria   \n",
              "                                    Gallionellales   \n",
              "                                   Gallionellaceae   \n",
              "                                       Gallionella   \n",
              "                                               332   \n",
              "Sites                                                \n",
              "site_67                                          0   \n",
              "site_68                                          0   \n",
              "site_69                                          0   \n",
              "site_70                                          0   \n",
              "Source                                          us   \n",
              "\n",
              "        Alteromonadales_Shewanellaceae_Shewanella  \n",
              "                                         Bacteria  \n",
              "                                   Proteobacteria  \n",
              "                              Gammaproteobacteria  \n",
              "                                  Alteromonadales  \n",
              "                                   Shewanellaceae  \n",
              "                                       Shewanella  \n",
              "                                              656  \n",
              "Sites                                              \n",
              "site_67                                  0.004886  \n",
              "site_68                                  0.005293  \n",
              "site_69                                         0  \n",
              "site_70                                         0  \n",
              "Source                                         us  \n",
              "\n",
              "[5 rows x 86 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f9167c88-1b24-4f40-a73e-a1e5e133a4a8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr:last-of-type th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>Category</th>\n",
              "      <th>Rhodocyclales_Rhodocyclaceae_Azospira</th>\n",
              "      <th>Actinomycetales_Dermabacteraceae_Brachybacterium</th>\n",
              "      <th>Actinomycetales_Brevibacteriaceae_Brevibacterium</th>\n",
              "      <th>Erysipelotrichales_Erysipelotrichaceae_Bulleidia</th>\n",
              "      <th>Clostridiales_Clostridiaceae_Clostridium</th>\n",
              "      <th>Actinomycetales_Corynebacteriaceae_Corynebacterium</th>\n",
              "      <th>Lactobacillales_Enterococcaceae_Enterococcus</th>\n",
              "      <th>Thermoanaerobacterales_Thermoanaerobacteraceae_Gelria</th>\n",
              "      <th>Oceanospirillales_Halomonadaceae_Halomonas</th>\n",
              "      <th>...</th>\n",
              "      <th>Actinomycetales_Propionibacteriaceae_Tessaracoccus</th>\n",
              "      <th>Clostridiales_Peptococcaceae_Thermincola</th>\n",
              "      <th>Spirochaetales_Spirochaetaceae_Treponema</th>\n",
              "      <th>Burkholderiales_Oxalobacteraceae_Oxalobacteraceae_unclassified</th>\n",
              "      <th>Burkholderiales_Comamonadaceae_Variovorax</th>\n",
              "      <th>Anaerolineales_Anaerolinaceae_Wchb1-05</th>\n",
              "      <th>Desulfobacterales_Desulfobacteraceae_Desulfobacterium</th>\n",
              "      <th>Desulfobacterales_Desulfobulbaceae_Desulfobulbus</th>\n",
              "      <th>Gallionellales_Gallionellaceae_Gallionella</th>\n",
              "      <th>Alteromonadales_Shewanellaceae_Shewanella</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>Bacteria</th>\n",
              "      <th>Bacteria</th>\n",
              "      <th>Bacteria</th>\n",
              "      <th>Bacteria</th>\n",
              "      <th>Bacteria</th>\n",
              "      <th>Bacteria</th>\n",
              "      <th>Bacteria</th>\n",
              "      <th>Bacteria</th>\n",
              "      <th>Bacteria</th>\n",
              "      <th>...</th>\n",
              "      <th>Bacteria</th>\n",
              "      <th>Bacteria</th>\n",
              "      <th>Bacteria</th>\n",
              "      <th>Bacteria</th>\n",
              "      <th>Bacteria</th>\n",
              "      <th>Bacteria</th>\n",
              "      <th>Bacteria</th>\n",
              "      <th>Bacteria</th>\n",
              "      <th>Bacteria</th>\n",
              "      <th>Bacteria</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>Proteobacteria</th>\n",
              "      <th>Actinobacteria</th>\n",
              "      <th>Actinobacteria</th>\n",
              "      <th>Firmicutes</th>\n",
              "      <th>Firmicutes</th>\n",
              "      <th>Actinobacteria</th>\n",
              "      <th>Firmicutes</th>\n",
              "      <th>Firmicutes</th>\n",
              "      <th>Proteobacteria</th>\n",
              "      <th>...</th>\n",
              "      <th>Actinobacteria</th>\n",
              "      <th>Firmicutes</th>\n",
              "      <th>Spirochaetes</th>\n",
              "      <th>Proteobacteria</th>\n",
              "      <th>Proteobacteria</th>\n",
              "      <th>Chloroflexi</th>\n",
              "      <th>Proteobacteria</th>\n",
              "      <th>Proteobacteria</th>\n",
              "      <th>Proteobacteria</th>\n",
              "      <th>Proteobacteria</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>Betaproteobacteria</th>\n",
              "      <th>Actinobacteria</th>\n",
              "      <th>Actinobacteria</th>\n",
              "      <th>Erysipelotrichi</th>\n",
              "      <th>Clostridia</th>\n",
              "      <th>Actinobacteria</th>\n",
              "      <th>Bacilli</th>\n",
              "      <th>Clostridia</th>\n",
              "      <th>Gammaproteobacteria</th>\n",
              "      <th>...</th>\n",
              "      <th>Actinobacteria</th>\n",
              "      <th>Clostridia</th>\n",
              "      <th>Spirochaetes</th>\n",
              "      <th>Betaproteobacteria</th>\n",
              "      <th>Betaproteobacteria</th>\n",
              "      <th>Anaerolineae</th>\n",
              "      <th>Deltaproteobacteria</th>\n",
              "      <th>Deltaproteobacteria</th>\n",
              "      <th>Betaproteobacteria</th>\n",
              "      <th>Gammaproteobacteria</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>Rhodocyclales</th>\n",
              "      <th>Actinomycetales</th>\n",
              "      <th>Actinomycetales</th>\n",
              "      <th>Erysipelotrichales</th>\n",
              "      <th>Clostridiales</th>\n",
              "      <th>Actinomycetales</th>\n",
              "      <th>Lactobacillales</th>\n",
              "      <th>Thermoanaerobacterales</th>\n",
              "      <th>Oceanospirillales</th>\n",
              "      <th>...</th>\n",
              "      <th>Actinomycetales</th>\n",
              "      <th>Clostridiales</th>\n",
              "      <th>Spirochaetales</th>\n",
              "      <th>Burkholderiales</th>\n",
              "      <th>Burkholderiales</th>\n",
              "      <th>Anaerolineales</th>\n",
              "      <th>Desulfobacterales</th>\n",
              "      <th>Desulfobacterales</th>\n",
              "      <th>Gallionellales</th>\n",
              "      <th>Alteromonadales</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>Rhodocyclaceae</th>\n",
              "      <th>Dermabacteraceae</th>\n",
              "      <th>Brevibacteriaceae</th>\n",
              "      <th>Erysipelotrichaceae</th>\n",
              "      <th>Clostridiaceae</th>\n",
              "      <th>Corynebacteriaceae</th>\n",
              "      <th>Enterococcaceae</th>\n",
              "      <th>Thermoanaerobacteraceae</th>\n",
              "      <th>Halomonadaceae</th>\n",
              "      <th>...</th>\n",
              "      <th>Propionibacteriaceae</th>\n",
              "      <th>Peptococcaceae</th>\n",
              "      <th>Spirochaetaceae</th>\n",
              "      <th>Oxalobacteraceae</th>\n",
              "      <th>Comamonadaceae</th>\n",
              "      <th>Anaerolinaceae</th>\n",
              "      <th>Desulfobacteraceae</th>\n",
              "      <th>Desulfobulbaceae</th>\n",
              "      <th>Gallionellaceae</th>\n",
              "      <th>Shewanellaceae</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>Azospira</th>\n",
              "      <th>Brachybacterium</th>\n",
              "      <th>Brevibacterium</th>\n",
              "      <th>Bulleidia</th>\n",
              "      <th>Clostridium</th>\n",
              "      <th>Corynebacterium</th>\n",
              "      <th>Enterococcus</th>\n",
              "      <th>Gelria</th>\n",
              "      <th>Halomonas</th>\n",
              "      <th>...</th>\n",
              "      <th>Tessaracoccus</th>\n",
              "      <th>Thermincola</th>\n",
              "      <th>Treponema</th>\n",
              "      <th>Oxalobacteraceae_unclassified</th>\n",
              "      <th>Variovorax</th>\n",
              "      <th>Wchb1-05</th>\n",
              "      <th>Desulfobacterium</th>\n",
              "      <th>Desulfobulbus</th>\n",
              "      <th>Gallionella</th>\n",
              "      <th>Shewanella</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>110</th>\n",
              "      <th>140</th>\n",
              "      <th>145</th>\n",
              "      <th>154</th>\n",
              "      <th>214</th>\n",
              "      <th>229</th>\n",
              "      <th>300</th>\n",
              "      <th>334</th>\n",
              "      <th>354</th>\n",
              "      <th>...</th>\n",
              "      <th>715</th>\n",
              "      <th>719</th>\n",
              "      <th>731</th>\n",
              "      <th>853</th>\n",
              "      <th>863</th>\n",
              "      <th>867</th>\n",
              "      <th>264</th>\n",
              "      <th>265</th>\n",
              "      <th>332</th>\n",
              "      <th>656</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Sites</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>site_67</th>\n",
              "      <td>3</td>\n",
              "      <td>0.004886</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.942935</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.151456</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.004886</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>site_68</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0.021172</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>28.889007</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.005293</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>site_69</th>\n",
              "      <td>1</td>\n",
              "      <td>1.47</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.63</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>site_70</th>\n",
              "      <td>1</td>\n",
              "      <td>1.72</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2.3</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Source</th>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>chk-core</td>\n",
              "      <td>chk</td>\n",
              "      <td>chk</td>\n",
              "      <td>chk</td>\n",
              "      <td>chk-core-us</td>\n",
              "      <td>chk-core-us</td>\n",
              "      <td>chk</td>\n",
              "      <td>chk</td>\n",
              "      <td>chk-core</td>\n",
              "      <td>...</td>\n",
              "      <td>core</td>\n",
              "      <td>core</td>\n",
              "      <td>core</td>\n",
              "      <td>core</td>\n",
              "      <td>core</td>\n",
              "      <td>core</td>\n",
              "      <td>us</td>\n",
              "      <td>us</td>\n",
              "      <td>us</td>\n",
              "      <td>us</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 86 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f9167c88-1b24-4f40-a73e-a1e5e133a4a8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f9167c88-1b24-4f40-a73e-a1e5e133a4a8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f9167c88-1b24-4f40-a73e-a1e5e133a4a8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f4eaea57-be72-4ff1-9d1c-dfd918590cec\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f4eaea57-be72-4ff1-9d1c-dfd918590cec')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f4eaea57-be72-4ff1-9d1c-dfd918590cec button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "Integrated_T.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdOLY5SPuvgI"
      },
      "source": [
        "## 2.3. Making Sequences for Picrust fasta file\n",
        "\n",
        "Picrust Functional Analyiss requires a biom table with otus as index, samples as headers and abundance as values. The present biom has genus names but is needs instead Otus instead. The other input file for picrust is the representative sequences table that consist of the sequences per genera followed by the frequency of that genera on the whole sample, this is done directly by the software. The fasta file requires the otus instead of the genera names and the sequences non aligned coming from notebook 5. The following scrips will formate the data to picrust."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "cizMvGCGuvgI",
        "outputId": "12abfdd2-5e78-4e9f-c73a-552550547981"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# Read and modify sequences\\nnew_records = []\\nfor record in SeqIO.parse(fasta_file_final, \"fasta\"):\\n    # Get just the OTU number and ignore genus name\\n    otu_id = record.description.split()[1]  # Takes second element (the OTU number)\\n\\n    # Create new record with only OTU as ID\\n    new_record = SeqRecord(\\n        record.seq,\\n        id=otu_id,\\n        description=\"\"  # Empty description to keep only ID\\n    )\\n    new_records.append(new_record)\\n\\n# Write modified FASTA\\noutput_fasta_path = base_dir / \"sequences_for_picrust.fasta\"\\n\\nSeqIO.write(new_records, output_fasta_path, \"fasta\")'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "'''# Read and modify sequences\n",
        "new_records = []\n",
        "for record in SeqIO.parse(fasta_file_final, \"fasta\"):\n",
        "    # Get just the OTU number and ignore genus name\n",
        "    otu_id = record.description.split()[1]  # Takes second element (the OTU number)\n",
        "\n",
        "    # Create new record with only OTU as ID\n",
        "    new_record = SeqRecord(\n",
        "        record.seq,\n",
        "        id=otu_id,\n",
        "        description=\"\"  # Empty description to keep only ID\n",
        "    )\n",
        "    new_records.append(new_record)\n",
        "\n",
        "# Write modified FASTA\n",
        "output_fasta_path = base_dir / \"sequences_for_picrust.fasta\"\n",
        "\n",
        "SeqIO.write(new_records, output_fasta_path, \"fasta\")'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RN-pFp1cuvgI"
      },
      "source": [
        "## 2.4. Making of Dataframes for 2 Different Pipelines\n",
        "The following script is the path to the biom file but also to the Integrate dataframe which create dataframes that discriminate its origin in order to pass then through picrust different pipelines, to know: Simple_Base that compares the known bacteria namely usual_taxa against the other features to understand their relationships on the function of their metabolism, an additional group is put forward as simply_candidate_mic which corresponds to the bacteria no previously linked to corrosion but showing an statistical significance with the risk label, those come from the checked_taxa and in this study are: genera(GID): Bulleida (154); Mycoplana (471), Oxobacter (512) and Oerskovia (). Also as showing an favor behaviour against corrosion are presented: Phenylobacterium (549), Gelria(334), Porphyrobacteria (564) and Tepidimonas (712)\n",
        "SIMPLE_BASE = {'known': 'simple_known_mic', 'other': 'simple_candidate_mic'}\n",
        "The second pipeline comprises a more detailed separation of the bacteria and that is: The Known bacteria as previously, pure_checked corresponding to the statistical significant genera, pure_core correspondent to the core taxa on the systems and the combination of the core and checked taxa.\n",
        "DETAILED_BASE = {'known': 'detailed_known_mic','pure_checked': 'detailed_pure_checked_mic',\n",
        "    'pure_core': 'detailed_pure_core_mic', 'checked_core': 'detailed_checked_core_mic'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akl8MkZjuvgI"
      },
      "source": [
        "__Making the Integrated dataframe__\n",
        "The original dataframe has a column for source, indicating from which df  came from (core, usual, checked), this script proceses that datadrame into individual dfs and the combined preserving the source for further analysis. The Integrated dataframe continues to be process on the next step to become the biom abundance df."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWukq8fUCqSH",
        "outputId": "943a48bd-c5f4-4ead-f37f-a05558fdbad1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Detailed Classification Results:\n",
            "Known corrosion bacteria: 17\n",
            "Pure checked bacteria: 19\n",
            "Pure core bacteria: 46\n",
            "Checked-core bacteria: 3\n",
            "\n",
            "Total classified taxa: 85\n",
            "Total in dataset: 85\n"
          ]
        }
      ],
      "source": [
        "def process_integrated_data(df):\n",
        "    \"\"\"\n",
        "    Process the integrated DataFrame to create a new DataFrame with clear column names\n",
        "    and preserve all values including source information.\n",
        "\n",
        "    Parameters:\n",
        "    df (pandas.DataFrame): Input DataFrame with MultiIndex index and site columns\n",
        "\n",
        "    Returns:\n",
        "    pandas.DataFrame: Processed DataFrame with clear structure\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract genera and GIDs from the index MultiIndex\n",
        "    genera = df.index.get_level_values(6)[1:]  # Skip first row\n",
        "    gids = pd.to_numeric(df.index.get_level_values(7)[1:], errors='coerce')\n",
        "\n",
        "    # Create a new DataFrame with the extracted information\n",
        "    result_df = pd.DataFrame({\n",
        "        'Genus': genera,\n",
        "        'GID': gids\n",
        "    })\n",
        "\n",
        "    # Add the site values from the original DataFrame\n",
        "    for col in df.columns:\n",
        "        result_df[col] = df.iloc[1:][col].values\n",
        "\n",
        "    # Clean up the DataFrame\n",
        "    result_df['GID'] = pd.to_numeric(result_df['GID'], errors='coerce')\n",
        "    result_df = result_df.dropna(subset=['GID'])\n",
        "    result_df['GID'] = result_df['GID'].astype(int)\n",
        "\n",
        "    return result_df\n",
        "\n",
        "def get_taxa_groups(df):\n",
        "    \"\"\"\n",
        "    Separate the processed DataFrame into different taxa groups based on Source column\n",
        "\n",
        "    Parameters:\n",
        "    df (pandas.DataFrame): Processed DataFrame from process_integrated_data()\n",
        "\n",
        "    Returns:\n",
        "    dict: Dictionary containing DataFrames for different taxa groups\n",
        "    \"\"\"\n",
        "    # Split the data into groups based on 'Source' column patterns\n",
        "\n",
        "    # Known corrosion bacteria (any pattern with 'us')\n",
        "    known_bacteria = df[df['Source'].str.contains('us', case=False, na=False)]\n",
        "\n",
        "    # Pure checked bacteria (only 'chk' without 'core' or 'us')\n",
        "    pure_checked = df[\n",
        "        df['Source'].str.contains('chk', case=False, na=False) &\n",
        "        ~df['Source'].str.contains('core|us', case=False, na=False)\n",
        "    ]\n",
        "\n",
        "    # Pure core bacteria (only 'core' without 'chk' or 'us')\n",
        "    pure_core = df[\n",
        "        df['Source'].str.contains('core', case=False, na=False) &\n",
        "        ~df['Source'].str.contains('chk|us', case=False, na=False)\n",
        "    ]\n",
        "\n",
        "    # Checked-core bacteria (contains both 'core' and 'chk' but no 'us')\n",
        "    checked_core = df[\n",
        "        df['Source'].str.contains('chk.*core|core.*chk', case=False, na=False) &\n",
        "        ~df['Source'].str.contains('us', case=False, na=False)\n",
        "    ]\n",
        "\n",
        "    # Create groups dictionary\n",
        "    taxa_groups = {\n",
        "        'known_bacteria': known_bacteria,\n",
        "        'pure_checked': pure_checked,\n",
        "        'pure_core': pure_core,\n",
        "        'checked_core': checked_core\n",
        "    }\n",
        "\n",
        "    # Print summary statistics\n",
        "    print(\"\\nDetailed Classification Results:\")\n",
        "    print(f\"Known corrosion bacteria: {len(known_bacteria)}\")\n",
        "    print(f\"Pure checked bacteria: {len(pure_checked)}\")\n",
        "    print(f\"Pure core bacteria: {len(pure_core)}\")\n",
        "    print(f\"Checked-core bacteria: {len(checked_core)}\")\n",
        "\n",
        "    # Verify total matches expected\n",
        "    total_classified = len(known_bacteria) + len(pure_checked) + len(pure_core) + len(checked_core)\n",
        "    print(f\"\\nTotal classified taxa: {total_classified}\")\n",
        "    print(f\"Total in dataset: {len(df)}\")\n",
        "\n",
        "    return taxa_groups\n",
        "\n",
        "# Usage example:\n",
        "Integrated = process_integrated_data(pre_Integrated)\n",
        "\n",
        "# Get the groups\n",
        "taxa_groups = get_taxa_groups(Integrated)\n",
        "\n",
        "# Access individual groups -\n",
        "known_bacteria = taxa_groups['known_bacteria']\n",
        "pure_core = taxa_groups['pure_core']\n",
        "pure_checked = taxa_groups['pure_checked']\n",
        "checked_core = taxa_groups['checked_core']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g56zbUtiuvgJ"
      },
      "source": [
        "## 2.5. Making the Abundanc Biom dataframe for Picrust\n",
        "\n",
        "The final biom should have as index the Otus numbers no the genera names and a clean formate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qn6xPmvfuvgJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "c5462c89-1903-450b-f606-94ab1a5ad605"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# droping source and genus and putting GID as index\\npre_biom= Integrated.drop(columns=[\"Source\", \"GID\"])\\npre_biom= pre_biom.set_index(\"Genus\").astype(str)\\n# Ensure all data values are float\\npre_biom = pre_biom.astype(float)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "'''# droping source and genus and putting GID as index\n",
        "pre_biom= Integrated.drop(columns=[\"Source\", \"GID\"])\n",
        "pre_biom= pre_biom.set_index(\"Genus\").astype(str)\n",
        "# Ensure all data values are float\n",
        "pre_biom = pre_biom.astype(float)'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mI5nSqwXuvgJ"
      },
      "source": [
        "__changing genera to otus__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amKUleGLuvgJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "4919d2c6-2eaa-4479-efc2-c4901ee733e9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# Create genus to OTU mapping from FASTA headers\\ngenus_to_otu = {}\\nfor record in SeqIO.parse(fasta_file_final, \"fasta\"):\\n    parts = record.description.split()\\n    if len(parts) >= 3:\\n        genus = parts[0]\\n        otu = parts[1]  # We\\'ll use the first OTU number\\n        genus_to_otu[genus] = otu\\n\\n# Print a few mappings to verify\\nprint(\"Sample genus to OTU mappings:\")\\nfor i, (genus, otu) in enumerate(list(genus_to_otu.items())[:5]):\\n    print(f\"{genus} -> {otu}\")\\n\\n# Replace genus with OTU in the index\\npre_biom.index = pre_biom.index.map(lambda x: genus_to_otu.get(x, x))\\n\\n# Remove the \\'Genus\\' name from the index\\npre_biom.index.name = \"OTU\"\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "'''# Create genus to OTU mapping from FASTA headers\n",
        "genus_to_otu = {}\n",
        "for record in SeqIO.parse(fasta_file_final, \"fasta\"):\n",
        "    parts = record.description.split()\n",
        "    if len(parts) >= 3:\n",
        "        genus = parts[0]\n",
        "        otu = parts[1]  # We'll use the first OTU number\n",
        "        genus_to_otu[genus] = otu\n",
        "\n",
        "# Print a few mappings to verify\n",
        "print(\"Sample genus to OTU mappings:\")\n",
        "for i, (genus, otu) in enumerate(list(genus_to_otu.items())[:5]):\n",
        "    print(f\"{genus} -> {otu}\")\n",
        "\n",
        "# Replace genus with OTU in the index\n",
        "pre_biom.index = pre_biom.index.map(lambda x: genus_to_otu.get(x, x))\n",
        "\n",
        "# Remove the 'Genus' name from the index\n",
        "pre_biom.index.name = \"OTU\"\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFYJS2KuuvgJ"
      },
      "source": [
        "__Calculation counts for picrust2__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "bwLLzgWLuvgJ",
        "outputId": "9e6c43fd-a9e8-40ff-ac04-2850a7ad7417"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'scaling_factor = 10000\\n# Multiply by scaling factor and round to nearest integer\\ncount_pre_biom = np.round(pre_biom * scaling_factor).astype(int)\\ncount_pre_biom'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "'''scaling_factor = 10000\n",
        "# Multiply by scaling factor and round to nearest integer\n",
        "count_pre_biom = np.round(pre_biom * scaling_factor).astype(int)\n",
        "count_pre_biom'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gu2uuKLHuvgJ"
      },
      "source": [
        "__Creating the biom table formate__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PsCMsci7w8R7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "5222d6ac-114d-41cf-f43b-eeab9c2a3606"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\'# Create BIOM table with type specification\\nbiom_table = Table(data=count_pre_biom.values,\\n                  observation_ids=count_pre_biom.index.astype(str),\\n                  sample_ids=count_pre_biom.columns.astype(str),\\n                  type=\"OTU table\",\\n                  create_date=datetime.now().isoformat(),\\n                  generated_by=\"BIOM-Format\",\\n                  matrix_type=\"sparse\",\\n                  matrix_element_type=\"float\")\\n\\n# Save with explicit format\\noutput_path = base_dir / \"count_abundance_85.biom\"\\nwith biom_open(output_path, \\'w\\') as f:\\n    biom_table.to_hdf5(f, generated_by=\"BIOM-Format\")'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "''''# Create BIOM table with type specification\n",
        "biom_table = Table(data=count_pre_biom.values,\n",
        "                  observation_ids=count_pre_biom.index.astype(str),\n",
        "                  sample_ids=count_pre_biom.columns.astype(str),\n",
        "                  type=\"OTU table\",\n",
        "                  create_date=datetime.now().isoformat(),\n",
        "                  generated_by=\"BIOM-Format\",\n",
        "                  matrix_type=\"sparse\",\n",
        "                  matrix_element_type=\"float\")\n",
        "\n",
        "# Save with explicit format\n",
        "output_path = base_dir / \"count_abundance_85.biom\"\n",
        "with biom_open(output_path, 'w') as f:\n",
        "    biom_table.to_hdf5(f, generated_by=\"BIOM-Format\")'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "gV5uEFS_uvgJ",
        "outputId": "3bd29c03-f433-4c6c-e40d-646bdd8a7ad3"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "unterminated string literal (detected at line 7) (<ipython-input-20-8d39cf859bbc>, line 7)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-20-8d39cf859bbc>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    !biom summarize-table -i /home/beatriz/MIC/2_Micro/data_picrust/count_abundance_85.biom''''\u001b[0m\n\u001b[0m                                                                                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 7)\n"
          ]
        }
      ],
      "source": [
        "''''# Validate the table structure\n",
        "print(\"\\nValidating table...\")\n",
        "!biom validate-table -i output_path\n",
        "#/home/beatriz/MIC/2_Micro/data_picrust/count_abundance_85.biom\n",
        "\n",
        "# Show table info\n",
        "!biom summarize-table -i /home/beatriz/MIC/2_Micro/data_picrust/count_abundance_85.biom'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmMvQVofuvgK"
      },
      "source": [
        "# 3. Making the representative sequences\n",
        "\n",
        "__Convert Abundance Biom table and the Sequences into a QIIME2 artifact__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXcOOPN7uvgK"
      },
      "outputs": [],
      "source": [
        "def create_rep_seqs_with_freq(sequence_file, pre_biom_df, output_fasta):\n",
        "    \"\"\"\n",
        "    Create representative sequences with frequencies written to output\n",
        "\n",
        "    Args:\n",
        "        sequence_file: Path to FASTA file with OTU sequences\n",
        "        pre_biom_df: DataFrame with abundance data\n",
        "        output_fasta: Path to save sequences with frequencies\n",
        "    \"\"\"\n",
        "    # Calculate total frequency for each OTU\n",
        "    total_frequencies = round(pre_biom_df.sum(axis=1), 2)\n",
        "\n",
        "    with open(output_fasta, 'w') as out:\n",
        "        for record in SeqIO.parse(sequence_file, \"fasta\"):\n",
        "            otu_id = record.id\n",
        "\n",
        "            if otu_id in total_frequencies.index:\n",
        "                freq = total_frequencies[otu_id]\n",
        "                sequence = str(record.seq)\n",
        "\n",
        "                # Write sequence with frequency to FASTA\n",
        "                out.write(f\">{otu_id} {sequence} {freq}\\n\")\n",
        "\n",
        "    # Show example of what was written\n",
        "    print(\"Example of FASTA output format:\")\n",
        "    with open(output_fasta, 'r') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if i < 6:  # Show first 3 sequences (header + sequence lines)\n",
        "                print(line.strip())\n",
        "\n",
        "# Representative sequences\n",
        "'''sequences_for_picrust = Path(\"/home/beatriz/MIC/2_Micro/data_picrust/sequences_for_picrust.fasta\")\n",
        "output_fasta = Path(\"/home/beatriz/MIC/2_Micro/data_picrust/rep_seqs_with_freq.fasta\")\n",
        "create_rep_seqs_with_freq(sequences_for_picrust, pre_biom, output_fasta)'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ku8lUve9uvgK"
      },
      "source": [
        "__Disclamer:__ These notebook was mean to do the analysis of the functional mechanisms of bacteria using picrust2, however the capacity of the laptop was no sufficient to run it, nor colab on public library, nor a virtual machine, that is the reason why the analysis was undertaken in the galaxy website, where the data resides.\n",
        "https://usegalaxy.eu/  \n",
        "username= magicalex238"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zv-yUkfmCqSN"
      },
      "source": [
        "## 3.1. Classifying Bacteria by their Source DataFrame\n",
        "Two distinct classification approaches are implemented to categorize bacteria. The simple approach (get_bacteria_sources_simple) divides bacteria into known corrosion-causers (usual_taxa) and candidates (all others). The detailed approach (get_bacteria_sources_detailed) provides finer categorization by separating bacteria into known corrosion-causers, pure checked taxa, pure core taxa, and those present in both checked and core datasets. Please notice that this function uses df Integrated for source clasification and no abundance.biom which will be used for the picrust2 pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bDVrPwWCqSR"
      },
      "outputs": [],
      "source": [
        "def get_bacteria_sources_simple(Integrated_df):\n",
        "    \"\"\"\n",
        "    Simple classification:\n",
        "    1. Known (anything with 'us')\n",
        "    2. All others (combined chk, core, chk-core)\n",
        "    \"\"\"\n",
        "    # Get genera and gids from column levels 6 and 7\n",
        "    genera = Integrated_df[\"Genus\"]\n",
        "    gids = Integrated_df[\"GID\"]\n",
        "    # Look for Source in the data, not index\n",
        "    sources = Integrated_df['Source'] if 'Source' in Integrated_df.columns else None\n",
        "\n",
        "    known_bacteria = {}     # usual_taxa\n",
        "    other_bacteria = {}     # everything else\n",
        "\n",
        "    sources_found = set()\n",
        "    source ={}\n",
        "    patterns = ['us', 'core-us', 'chk-us', 'chk-core-us']\n",
        "\n",
        "    for i, (genus, gid) in enumerate (zip(genera, gids)):\n",
        "        if source is not None:  # Check if source exists for this genus\n",
        "            source = str(sources.iloc[i]).strip().lower()\n",
        "            sources_found.add(source)\n",
        "\n",
        "            if source in patterns:\n",
        "                known_bacteria[genus] = int(gid) if str(gid).isdigit() else gid\n",
        "            else:\n",
        "                other_bacteria[genus] = int(gid) if str(gid).isdigit() else gid\n",
        "\n",
        "    print(\"\\nSimple Classification Results:\")\n",
        "    print(f\"Known corrosion bacteria: {len(known_bacteria)}\")\n",
        "    print(f\"Other bacteria: {len(other_bacteria)}\")\n",
        "    print(\"\\nSources found:\", sources_found)\n",
        "\n",
        "    return {\n",
        "        'known_bacteria': known_bacteria,\n",
        "        'other_bacteria': other_bacteria\n",
        "    }\n",
        "\n",
        "def get_bacteria_sources_detailed(Integrated_df):\n",
        "    \"\"\"\n",
        "    Detailed classification with all possible combinations:\n",
        "    1. Known (usual_taxa)\n",
        "    2. Pure checked (only 'chk')\n",
        "    3. Pure core (only 'core')\n",
        "    4. Checked-core (overlap 'chk-core')\n",
        "    \"\"\"\n",
        "    # Get genera and gids from column levels 6 and 7\n",
        "    genera = Integrated_df.index.get_level_values(6)[1:]\n",
        "    gids = Integrated_df.index.get_level_values(7)[1:]\n",
        "\n",
        "    sources = Integrated_df['Source'] if 'Source' in Integrated_df.columns else None\n",
        "\n",
        "    known_bacteria = {}      # usual_taxa\n",
        "    pure_checked = {}        # only 'chk' checked_taxa\n",
        "    pure_core = {}          # only 'core' core_taxa\n",
        "    checked_core = {}       # 'chk-core' checked and core taxa\n",
        "    source ={}\n",
        "    sources_found = set()\n",
        "    patterns = ['us', 'core-us', 'chk-us', 'chk-core-us']\n",
        "\n",
        "    for i, (genus, gid) in enumerate (zip(genera, gids)):\n",
        "        if source is not None:  # Check if source exists for this genus\n",
        "            source = str(sources.iloc[i]).strip().lower()\n",
        "            sources_found.add(source)\n",
        "\n",
        "            if source in patterns:\n",
        "                known_bacteria[genus] = int(gid) if str(gid).isdigit() else gid\n",
        "                continue\n",
        "\n",
        "            # Then handle other combinations\n",
        "            if source == 'chk':\n",
        "                pure_checked[genus] = gid\n",
        "            elif source == 'core':\n",
        "                pure_core[genus] = gid\n",
        "            elif 'chk-core' in source:\n",
        "                checked_core[genus] = gid\n",
        "\n",
        "    print(\"\\nDetailed Classification Results:\")\n",
        "    print(f\"Known corrosion bacteria: {len(known_bacteria)}\")\n",
        "    print(f\"Pure checked bacteria: {len(pure_checked)}\")\n",
        "    print(f\"Pure core bacteria: {len(pure_core)}\")\n",
        "    print(f\"Checked-core bacteria: {len(checked_core)}\")\n",
        "    print(\"\\nSources found:\", sources_found)\n",
        "\n",
        "    return {\n",
        "        'known_bacteria': known_bacteria,\n",
        "        'pure_checked': pure_checked,\n",
        "        'pure_core': pure_core,\n",
        "        'checked_core': checked_core\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23P2k1QwCqSR"
      },
      "source": [
        "## 3.2. Prepare picrust data and Creating Directories for PICRUSt2 Input\n",
        "The check_missing_genera function processes the integrated data and handles data quality control. Known problematic genera (e.g., 'Clostridium_sensu_stricto_12', 'Oxalobacteraceae_unclassified') are flagged for exclusion to prevent analysis errors. The function also creates an organized directory structure as outlined in the introduction, with separate paths for different bacterial classifications (known_mic, candidate_mic, etc.) and their respective analysis outputs (EC_predictions, pathway_predictions, KO_predictions). Following function prepares the data for picrust analysis but both dataframes the abundance.biom and Integrated have some bacteria that were no sequenciated mostly cause are no known specimens. So it is necesary to do same procedure to both dfs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNfnbXfKCqSS"
      },
      "outputs": [],
      "source": [
        "def prepare_picrust_data(Integrated_df, aligned_file, function_type='simple'):\n",
        "    \"\"\"\n",
        "    Prepare data for PICRUSt analysis with choice of  function_type method\n",
        "\n",
        "    Args:\n",
        "        Integrated_df: Input DataFrame\n",
        "        aligned_file: Path to aligned sequences\n",
        "        function_type: 'simple' or 'detailed'\n",
        "    \"\"\"\n",
        "    # Get bacteria source_groups based on chosen  function_type\n",
        "    if  function_type == 'simple':\n",
        "        source_groups = get_bacteria_sources_simple(Integrated_df)\n",
        "    else:\n",
        "        source_groups= get_bacteria_sources_detailed(Integrated_df)\n",
        "\n",
        "    # Create appropriate directory structure\n",
        "    create_directory_structure(function_type)\n",
        "\n",
        "    return source_groups\n",
        "\n",
        "def create_directory_structure(function_type='simple'):\n",
        "    \"\"\"Create directory structure for PICRUSt analysis\"\"\"\n",
        "    base_dir = Path(\"/home/beatriz/MIC/2_Micro/data_picrust\")\n",
        "    base_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    if function_type == 'simple':\n",
        "        directories = SIMPLE_BASE\n",
        "    else:\n",
        "        directories = DETAILED_BASE\n",
        "\n",
        "    # Create all required directories\n",
        "    for dir_name in directories.values():\n",
        "        for subdir in SUBDIRS:\n",
        "            (base_dir / dir_name / subdir).mkdir(parents=True, exist_ok=True)\n",
        "    logging.info(\"Directory structure created successfully\")\n",
        "    return True\n",
        "\n",
        "'''except Exception as e:\n",
        "    logging.error(f\"Error creating directory structure: {str(e)}\")\n",
        "    return False'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4v7JVU8uvgK"
      },
      "outputs": [],
      "source": [
        "def verify_input_files():\n",
        "    \"\"\"Verify that input files exist and are readable\"\"\"\n",
        "    missing_files = []\n",
        "\n",
        "    if not fasta_file.exists():\n",
        "        missing_files.append(str(fasta_file))\n",
        "    if not biom_table.exists():\n",
        "        missing_files.append(str(biom_table))\n",
        "\n",
        "    if missing_files:\n",
        "        logging.error(f\"Missing input files: {', '.join(missing_files)}\")\n",
        "        return False\n",
        "\n",
        "    logging.info(\"All input files found\")\n",
        "    return True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUwADMKZCqSS"
      },
      "source": [
        "# 4. PICRUSt Pipeline Definition\n",
        "The pipeline processes the aligned sequence data from notebook 5 that has or not undergo cleaning of the sequences as previously done on section 2. Also processes the biom_table in order to account on this anylsis on abundance. It queries the PICRUSt database to predict potential metabolic pathways for each genus. This prediction is based on evolutionary relationships and known genomic capabilities of related organisms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "srMpS5DkCqSS"
      },
      "outputs": [],
      "source": [
        "def run_picrust2_pipeline(fasta_file, biom_file, output_dir):\n",
        "    \"\"\"\n",
        "    Run the main PICRUSt2 pipeline on input sequences and BIOM table.\n",
        "\n",
        "    Args:\n",
        "        fasta_file: Path to the aligned sequences FASTA file.\n",
        "        biom_file: Path to the BIOM table (without extra columns).\n",
        "        output_dir: Directory for PICRUSt2 output.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Run main PICRUSt2 pipeline\n",
        "        cmd = [\n",
        "            'picrust2_pipeline.py',\n",
        "            '-s', fasta_file,        # Input FASTA file with aligned sequences\n",
        "            '-i', biom_file,         # BIOM table with abundance data\n",
        "            '-o', output_dir,        # Output directory\n",
        "            '--processes', '4',      # Parallel processes\n",
        "            '--verbose',\n",
        "            '--min_align', '0.25'    # Note the split here\n",
        "        ]\n",
        "        subprocess.run(cmd, check=True)\n",
        "\n",
        "        # Add pathway descriptions if the pathway file exists\n",
        "        pathway_file = os.path.join(output_dir, 'pathways_out/path_abun_unstrat.tsv.gz')\n",
        "        if os.path.exists(pathway_file):\n",
        "            cmd_desc = [\n",
        "                'add_descriptions.py',\n",
        "                '-i', pathway_file,\n",
        "                '-m', 'PATHWAY',\n",
        "                '-o', os.path.join(output_dir, 'pathways_with_descriptions.tsv')\n",
        "            ]\n",
        "            subprocess.run(cmd_desc, check=True)\n",
        "\n",
        "        print(f\"PICRUSt2 pipeline completed successfully for {output_dir}\")\n",
        "        return True\n",
        "\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Error running PICRUSt2: {e}\")\n",
        "        return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rx_DyzHbCqSS"
      },
      "source": [
        "# 5. Analysis of Pathways\n",
        "The analysis focuses on metabolic pathways known to be involved in microbially influenced corrosion, including sulfur metabolism, organic acid production, iron metabolism, and biofilm formation. These pathways were selected based on documented mechanisms of known corrosion-inducing bacteria. Separate pipeline runs for simple and detailed classifications ensure proper pathway analysis for each bacterial group."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8eP8MAidCqSS"
      },
      "outputs": [],
      "source": [
        "def analyze_functional_profiles(picrust_output_dir, bacteria_list):\n",
        "    \"\"\"\n",
        "    Analyze functional profiles with focus on corrosion-relevant pathways\n",
        "\n",
        "    Parameters:\n",
        "    picrust_output_dir: directory containing PICRUSt2 output\n",
        "    bacteria_list: list of bacteria names to analyze\n",
        "    \"\"\"\n",
        "    # Define corrosion-relevant pathways\n",
        "    relevant_pathways = [\n",
        "        'Sulfur metabolism',\n",
        "        'Iron metabolism',\n",
        "        'Energy metabolism',\n",
        "        'Biofilm formation',\n",
        "        'Metal transport',\n",
        "        'ochre formation',\n",
        "        'iron oxide deposits',\n",
        "        'iron precipitation',\n",
        "        'rust formation',\n",
        "        'organic acid production',\n",
        "        'acetate production',\n",
        "        'lactate metabolism',\n",
        "        'formate production',\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        # Read PICRUSt2 output\n",
        "        pathway_file = os.path.join(picrust_output_dir, 'pathways_with_descriptions.tsv')\n",
        "        pathways_df = pd.read_csv(pathway_file, sep='\\t')\n",
        "\n",
        "        # Filter for relevant pathways\n",
        "        filtered_pathways = pathways_df[\n",
        "            pathways_df['description'].str.contains('|'.join(relevant_pathways),\n",
        "                                                  case=False,\n",
        "                                                  na=False)]\n",
        "\n",
        "        # Calculate pathway abundances per bacteria\n",
        "        pathway_abundances = filtered_pathways.groupby('description').sum()\n",
        "\n",
        "        # Calculate pathway similarities between bacteria\n",
        "        pathway_similarities = {}\n",
        "        for bacteria in bacteria_list:\n",
        "            if bacteria in pathways_df.columns:\n",
        "                similarities = pathways_df[bacteria].corr(pathways_df[list(bacteria_list)])\n",
        "                pathway_similarities[bacteria] = similarities\n",
        "\n",
        "        # Predict functional potential\n",
        "        functional_predictions = {}\n",
        "        for pathway in relevant_pathways:\n",
        "            pathway_presence = filtered_pathways[\n",
        "                filtered_pathways['description'].str.contains(pathway, case=False)\n",
        "            ]\n",
        "            if not pathway_presence.empty:\n",
        "                functional_predictions[pathway] = {\n",
        "                    'presence': len(pathway_presence),\n",
        "                    'mean_abundance': pathway_presence.mean().mean(),\n",
        "                    'max_abundance': pathway_presence.max().max()\n",
        "                }\n",
        "\n",
        "        # Calculate correlation scores\n",
        "        correlation_scores = {}\n",
        "        for bacteria in bacteria_list:\n",
        "            if bacteria in pathways_df.columns:\n",
        "                correlations = pathways_df[bacteria].corr(\n",
        "                    pathways_df[filtered_pathways.index]\n",
        "                )\n",
        "                correlation_scores[bacteria] = {\n",
        "                    'mean_correlation': correlations.mean(),\n",
        "                    'max_correlation': correlations.max(),\n",
        "                    'key_pathways': correlations.nlargest(5).index.tolist()\n",
        "                }\n",
        "\n",
        "        comparison_results = {\n",
        "            'pathway_similarities': pathway_similarities,\n",
        "            'functional_predictions': functional_predictions,\n",
        "            'correlation_scores': correlation_scores,\n",
        "            'pathway_abundances': pathway_abundances.to_dict()\n",
        "        }\n",
        "\n",
        "        return filtered_pathways, comparison_results\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in pathway analysis: {str(e)}\")\n",
        "        return None, None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-EEla9jCqSS"
      },
      "source": [
        "## 5.2. Testing the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mnwckS6sCqST"
      },
      "outputs": [],
      "source": [
        "'''# ---- RUNNING THE PIPELINE ----\n",
        "\n",
        "# Set paths\n",
        "fasta_file = Path('/home/beatriz/MIC/2_Micro/data_tree/accession_sequences.fasta')\n",
        "abundance_biom_file =  Path('/home/beatriz/MIC/2_Micro/data_picrust/abundance_accession.biom')\n",
        "output_dir = 'picrust_output'\n",
        "\n",
        "# List of bacteria to analyze\n",
        "bacteria_of_interest = ['Azospira', 'Brachybacterium', 'Bulleidia']\n",
        "\n",
        "# Run PICRUSt2\n",
        "if run_picrust2_pipeline(aligned_fasta_file,\n",
        "                         abundance_biom_file,\n",
        "                         output_dir\n",
        "                        ):\n",
        "    # Analyze functional profiles if the pipeline completes successfully\n",
        "    filtered_pathways, abundances = analyze_functional_profiles(output_dir, bacteria_of_interest)'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03KIf3UaCqST"
      },
      "source": [
        "# 6. Functional Analysis\n",
        "## 6.1 Running picrust full pipeline 1\n",
        "The analysis workflow begins by categorizing bacteria into source groups using the classification functions. These categorized data are then processed through the PICRUSt pipeline to predict metabolic capabilities. The functional analysis examines pathway presence, abundance, and correlations between different bacterial groups to identify potential corrosion-related metabolic patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uHpbek-BCqST"
      },
      "outputs": [],
      "source": [
        "def run_functional_analysis(df, Integrated_df, aligned_file, analysis_type='simple'):\n",
        "    \"\"\"\n",
        "    Run complete functional analysis pipeline for either simple or detailed classification\n",
        "\n",
        "    Parameters:\n",
        "    df: Input DataFrame\n",
        "    aligned_file: Path to aligned sequences file\n",
        "    analysis_type: 'simple' or 'detailed'\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"Starting {analysis_type} classification analysis\")\n",
        "        print(f\"{'='*50}\")\n",
        "\n",
        "        # Prepare data and get source groups\n",
        "        print(\"\\nStep 1: Preparing data...\")\n",
        "\n",
        "        source_groups = prepare_picrust_data(Integrated_df, aligned_file, function_type=analysis_type)\n",
        "\n",
        "        if not source_groups:\n",
        "            raise ValueError(\"Failed to prepare data: No source groups returned\")\n",
        "\n",
        "        # Base directory for PICRUSt output\n",
        "        base_dir = Path(\"/home/beatriz/MIC/2_Micro/data_picrust\")\n",
        "\n",
        "        results = {}\n",
        "\n",
        "        if analysis_type == 'simple':\n",
        "            # Run analysis for simple classification\n",
        "            # Known bacteria\n",
        "            known_output_dir = base_dir /SIMPLE_BASE['known']\n",
        "            success_known = run_picrust2_pipeline(aligned_file, df, str(known_output_dir))\n",
        "            if success_known:\n",
        "                results_known = analyze_functional_profiles(str(known_output_dir),\n",
        "                                                        source_groups['known_bacteria'].keys())\n",
        "\n",
        "            # Other bacteria\n",
        "            other_output_dir = base_dir / SIMPLE_BASE['other']\n",
        "            success_other = run_picrust2_pipeline(aligned_file, str(other_output_dir))\n",
        "            if success_other:\n",
        "                results_other = analyze_functional_profiles(str(other_output_dir),\n",
        "                                                        source_groups['other_bacteria'].keys())\n",
        "\n",
        "        else:\n",
        "            # Run analysis for detailed classification\n",
        "            for group, dir_name in DETAILED_BASE.items():\n",
        "\n",
        "                # Known bacteria\n",
        "                known_output_dir = base_dir / DETAILED_BASE['known']\n",
        "                success_known = run_picrust2_pipeline(aligned_file, str(known_output_dir))\n",
        "                if success_known:\n",
        "                    results_known = analyze_functional_profiles(str(known_output_dir),\n",
        "                                                            source_groups['known_bacteria'].keys())\n",
        "\n",
        "                # Pure checked bacteria\n",
        "                checked_output_dir = base_dir /  DETAILED_BASE['pure_checked']\n",
        "                success_checked = run_picrust2_pipeline(aligned_file, str(checked_output_dir))\n",
        "                if success_checked:\n",
        "                    results_checked = analyze_functional_profiles(str(checked_output_dir),\n",
        "                                                            source_groups['pure_checked'].keys())\n",
        "\n",
        "                # Pure core bacteria\n",
        "                core_output_dir = base_dir /DETAILED_BASE['pure_core']\n",
        "                success_core = run_picrust2_pipeline(aligned_file, str(core_output_dir))\n",
        "                if success_core:\n",
        "                    results_core = analyze_functional_profiles(str(core_output_dir),\n",
        "                                                            source_groups['pure_core'].keys())\n",
        "\n",
        "                # Checked-core bacteria\n",
        "                checked_core_output_dir = base_dir /DETAILED_BASE['checked_core']\n",
        "                success_checked_core = run_picrust2_pipeline(aligned_file, str(checked_core_output_dir))\n",
        "                if success_checked_core:\n",
        "                    results_checked_core = analyze_functional_profiles(str(checked_core_output_dir),\n",
        "                                                                    source_groups['checked_core'].keys())\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Error running PICRUSt2: {e}\")\n",
        "\n",
        "        return \"Analysis completed successfully\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3NyEekbBCqST"
      },
      "outputs": [],
      "source": [
        "'''# Run the analysis for both types\n",
        "# Simple source classification\n",
        "simple_results = run_functional_analysis(biom_table, aligned_file, analysis_type='simple') # output_biom\n",
        "\n",
        "# Detailed source classification\n",
        "detailed_results = run_functional_analysis(biom_table, aligned_file, analysis_type='detailed')'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4rP1kdUCqST"
      },
      "source": [
        "## 6.2 Running picrust full pipeline 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5A2a1CNLCqSW"
      },
      "outputs": [],
      "source": [
        "def run_picrust2_pipeline(fasta_file, output_dir, min_align =0.5):\n",
        "    \"\"\"\n",
        "    Run PICRUSt2 pipeline with improved error handling and path management\n",
        "\n",
        "    Args:\n",
        "        fasta_file: Path to aligned sequences fasta file (str or Path)\n",
        "        output_dir: Directory for PICRUSt2 output (str or Path)\n",
        "    \"\"\"\n",
        "    import subprocess\n",
        "    import os\n",
        "    from pathlib import Path\n",
        "\n",
        "    # Convert paths to strings\n",
        "    fasta_file = str(fasta_file)\n",
        "    output_dir = str(output_dir)\n",
        "\n",
        "    try:\n",
        "        # Verify picrust2 is available\n",
        "        picrust_check = subprocess.run(['which', 'picrust2_pipeline.py'],\n",
        "                                     capture_output=True,\n",
        "                                     text=True)\n",
        "        if picrust_check.returncode != 0:\n",
        "            raise RuntimeError(\"picrust2_pipeline.py not found. Please ensure PICRUSt2 is properly installed.\")\n",
        "\n",
        "        # Create output directory\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # Construct command as a single string\n",
        "        cmd = f\"picrust2_pipeline.py -s {fasta_file} -i {fasta_file} -o {output_dir} --processes 1 --verbose\"\n",
        "\n",
        "        # Run pipeline\n",
        "        print(f\"Running command: {cmd}\")\n",
        "        process = subprocess.run(cmd,\n",
        "                               shell=True,  # Use shell to handle command string\n",
        "                               check=True,\n",
        "                               capture_output=True,\n",
        "                               text=True)\n",
        "\n",
        "        print(\"PICRUSt2 Output:\")\n",
        "        print(process.stdout)\n",
        "\n",
        "        if process.stderr:\n",
        "            print(\"Warnings/Errors:\")\n",
        "            print(process.stderr)\n",
        "\n",
        "        # Add descriptions if pathway file exists\n",
        "        pathway_file = os.path.join(output_dir, 'pathways_out/path_abun_unstrat.tsv.gz')\n",
        "        if os.path.exists(pathway_file):\n",
        "            desc_cmd = f\"add_descriptions.py -i {pathway_file} -m PATHWAY -o {os.path.join(output_dir, 'pathways_with_descriptions.tsv')}\"\n",
        "            subprocess.run(desc_cmd, shell=True, check=True)\n",
        "\n",
        "        print(f\"PICRUSt2 pipeline completed successfully for {output_dir}\")\n",
        "        return True\n",
        "\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Error running PICRUSt2 command: {e}\")\n",
        "        print(f\"Command output: {e.output}\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"Error in pipeline: {str(e)}\")\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hbhd5NNkCqSX"
      },
      "outputs": [],
      "source": [
        "'''# For original sequences\n",
        "aligned_file = aligned_fasta\n",
        "output_dir = Path(\"/home/beatriz/MIC/2_Micro/data_picrust/original_results\")\n",
        "success = run_picrust2_pipeline(aligned_file, output_dir)\n",
        "\n",
        "# For improved sequences\n",
        "optimized_file = Path(\"/home/beatriz/MIC/2_Micro/data_picrust/picrust_optimized_sequences.fasta\")\n",
        "optimized_output = Path(\"/home/beatriz/MIC/2_Micro/data_picrust/optimized_results\")\n",
        "success_opt = run_picrust2_pipeline(optimized_file, optimized_output)'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EDOctt8uvgP"
      },
      "source": [
        "# 7. Findings and Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXu7ACOAuvgP"
      },
      "source": [
        "The PICRUSt2 pipeline generated a series of interconnected files revealing the functional potential of the microbial community. These files collectively map metabolic pathways, enzymatic functions, and taxonomic relationships, providing a multi-layered view of microbial functional capabilities across samples. Detailed view of the files found in the folder ~data_picrust are located in the manuscript."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Se5YK4UfuvgP"
      },
      "source": [
        "Picrust_Result_SEPP and Picrust_Result_EPA contain the descriptions, pathways and abundance of the full pipeline of picrust."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aj6MuSmuvgP"
      },
      "source": [
        "Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZ8njtbRuvgP"
      },
      "outputs": [],
      "source": [
        "MetaCyc_EPA_path = Path(base_dir / \"Galaxy19-[PICRUSt2 Add descriptions on data 8].tabular\")\n",
        "Picrust_Result= pd.read_csv(MetaCyc_EPA_path, sep = \"\\t\")\n",
        "Picrust_Result_EPA= pd.read_csv(MetaCyc_EPA_path, sep = \"\\t\")\n",
        "Picrust_Result_EPA.set_index(\"description\", inplace=True)\n",
        "Picrust_Result_EPA = Picrust_Result_EPA.drop(\"pathway\", axis=1)\n",
        "Picrust_Result_EPA.index.name = \"pathway\"\n",
        "'''MetaCyc_SEPP_path = Path(base_dir / \"Galaxy35-[PICRUSt2 Add descriptions on data 14].tabular\")\n",
        "Picrust_Result_SEPP= pd.read_csv(MetaCyc_SEPP_path, sep = \"\\t\")\n",
        "Picrust_Result_SEPP.set_index(\"description\", inplace=True)\n",
        "Picrust_Result_SEPP = Picrust_Result_SEPP.drop(\"pathway\", axis=1)\n",
        "Picrust_Result_SEPP.index.name = \"pathway\"'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H39IXM7-uvgP"
      },
      "source": [
        "## 7.1. Placement Algorithm EPA vs SEPP\n",
        "nsti_SEPP and nsti_EPA Corresponds to a sample-wide measure of how closely related the microbial taxa in that sample are to known reference genomes with two different placement algoritms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TW5Je0oouvgP"
      },
      "outputs": [],
      "source": [
        "'''nsti_path_EPA = Path(base_dir  / \"Galaxy13-[EC_weighted_nsti].tabular\")\n",
        "nsti_EPA= pd.read_csv(nsti_path_EPA, sep = \"\\t\")\n",
        "nsti_path_SEPP = Path(base_dir  / \"Galaxy20-[EC_weighted_nsti].tabular\")\n",
        "nsti_SEPP= pd.read_csv(nsti_path_SEPP, sep = \"\\t\")'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESTXja6nuvgP"
      },
      "outputs": [],
      "source": [
        "'''# Create the scatter plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(nsti_EPA['sample'], nsti_EPA['weighted_NSTI'], alpha=0.5, label= \"EPA\", color=\"blue\")\n",
        "plt.scatter(nsti_SEPP['sample'], nsti_SEPP['weighted_NSTI'], alpha=0.5, label= \"SEPP\", color=\"purple\")\n",
        "\n",
        "# Add the threshold line\n",
        "plt.axhline(y=0.15, color='black', linestyle='--', label='Threshold (0.15)')\n",
        "\n",
        "# Customize the plot\n",
        "plt.title('NSTI Values by Site')\n",
        "plt.xlabel('Site')\n",
        "plt.ylabel('NSTI Value')\n",
        "plt.legend()\n",
        "\n",
        "# Rotate x-axis labels if there are many samples\n",
        "plt.xticks(rotation=90)\n",
        "\n",
        "# Adjust layout and display the plot\n",
        "plt.tight_layout()\n",
        "plt.show()'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_fb468duvgP"
      },
      "source": [
        "Interestingly, the results are no as expected, it was though that the algorithm for placing the sequences more convenient for the present samples was SEPP because it is design specially for 16sRNA samples and diverse microbios communities, however the samples show another story. I fail to realise that the present data has been validated with the greenes genes database with the purpose of finding more compatibility with the picrust2 database, and therefore the EPA algoritm is performing much better on the all of samples using EPA placement algoritm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUPl-r2huvgQ"
      },
      "source": [
        "## 7.2. Explore Pathway Patterns\n",
        "The pathway analysis strategy is to do a preliminar exploration before diving into specific hypotheses about organic matter metabolism and corrosion. It was chosen to start with unbiased exploratory data analysis of the PICRUSt pathways. The aim is to let the data reveal natural patterns without preconceptions. That helps to identify unexpected relationships between pathways, providing a baseline understanding of pathway distributions and relationships. This will guide subsequent targeted analyses of corrosion-relevant pathways.\n",
        "The following script takes multiple perspectives in order to visualise the data without bias and let it reveal itself. We do PCA for linear patterns, NMF for modular organization, UMAP for non-linear relationships and take different clustering approaches. The aim being to look for natural Patterns without predefined categories, so that strong strong correlations can be identified regardless of pathway type. It is visualised the distribution of pathway abundances, correlation structure, hierarchical relationships and non-linear patterns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfLGAWTXuvgQ"
      },
      "source": [
        "__Category Dict__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSQwysg7uvgQ"
      },
      "outputs": [],
      "source": [
        "# Define category dict outside so that all charts can use same dict\n",
        "\n",
        "category_dict = Integrated_T.T.iloc[0, 0:-1].to_dict()\n",
        "\n",
        "# Define colors and categories\n",
        "category_colors = {1: '#008800',  # Dark green\n",
        "                   2: '#FF8C00',  # Dark orange\n",
        "                   3: '#FF0000'}   # Red\n",
        "\n",
        "categories_labels = {1: 'Normal Operation',\n",
        "              2: 'Early Warning',\n",
        "              3: 'System Failure'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvoGFdS5uvgQ"
      },
      "outputs": [],
      "source": [
        "def explore_pathway_patterns(df):\n",
        "    \"\"\"\n",
        "    Explore pathway patterns using multiple analytical approaches\n",
        "    \"\"\"\n",
        "    # Standardize data\n",
        "    scaler = StandardScaler()\n",
        "    scaled_data = scaler.fit_transform(df)\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    def plot_exploration_results(df, results, category_dict, category_colors, categories_labels):\n",
        "        \"\"\"\n",
        "        Create visualizations for the exploratory analysis with consistent category colors\n",
        "        \"\"\"\n",
        "        # 1. Distribution of pathway abundances with category colors - side by side\n",
        "        plt.figure(figsize=(12, 6))\n",
        "\n",
        "        # Create dictionary to store abundances by category\n",
        "        category_abundances = {cat_id: [] for cat_id in categories_labels.keys()}\n",
        "\n",
        "        # Group abundances by category\n",
        "        for site_col in df.columns:\n",
        "            if site_col.startswith('site_'):\n",
        "                site_num = int(site_col.split('_')[1])\n",
        "                category = category_dict.get(f'site_{site_num}', 0)\n",
        "                if category in category_abundances:\n",
        "                    category_abundances[category].extend(df[site_col].values)\n",
        "\n",
        "        # Plot distribution for each category side by side\n",
        "        for category_id in categories_labels.keys():\n",
        "            sns.histplot(data=category_abundances[category_id],\n",
        "                        bins=50,\n",
        "                        color=category_colors[category_id],\n",
        "                        label=categories_labels[category_id],\n",
        "                        alpha=0.6,\n",
        "                        multiple=\"layer\")\n",
        "\n",
        "        plt.title('Distribution of Pathway Abundances by Category')\n",
        "        plt.xlabel('Abundance')\n",
        "        plt.yscale('log')\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    # 2. PCA Dimensionality Reduction\n",
        "    pca = PCA(n_components=5)\n",
        "    X_pca = pca.fit_transform(scaled_data)\n",
        "    results['pca'] = {\n",
        "        'components': X_pca,\n",
        "        'explained_variance': pca.explained_variance_ratio_,\n",
        "        'loadings': pd.DataFrame(\n",
        "            pca.components_.T,\n",
        "            index=df.columns,\n",
        "            columns=[f'PC{i+1}' for i in range(5)])}\n",
        "\n",
        "    # 3 NMF for pathway modules\n",
        "    nmf = NMF(n_components=5, init='random', random_state=0, max_iter=400)\n",
        "    W = nmf.fit_transform(df.clip(lower=0))\n",
        "    H = nmf.components_\n",
        "    results['nmf'] = {\n",
        "        'W': pd.DataFrame(W, index=df.index, columns=[f'NMF{i+1}' for i in range(5)]), # Pathway contributions\n",
        "        'H': pd.DataFrame(H, columns=df.columns, index=[f'NMF{i+1}' for i in range(5)]), # Sample patterns\n",
        "        'reconstruction_err': nmf.reconstruction_err_ }\n",
        "\n",
        "    # 4 UMAP for non-linear patterns\n",
        "    umap_reducer = umap.UMAP(random_state=0)\n",
        "    umap_result = umap_reducer.fit_transform(scaled_data)\n",
        "    results['umap'] = pd.DataFrame(umap_result, index=df.index, columns=['UMAP1', 'UMAP2'])\n",
        "\n",
        "    # 5. Multiple Clustering Approaches / Hierarchical clustering\n",
        "    linkage_matrix = hierarchy.linkage(scaled_data, method='ward')\n",
        "\n",
        "    # Try different numbers of clusters\n",
        "    cluster_results = {}\n",
        "    for n_clusters in [5, 10, 15]:\n",
        "        # Hierarchical\n",
        "        hc = AgglomerativeClustering(n_clusters=n_clusters)\n",
        "        hc_labels = hc.fit_predict(scaled_data)\n",
        "\n",
        "        # K-means\n",
        "        km = KMeans(n_clusters=n_clusters, random_state=0)\n",
        "        km_labels = km.fit_predict(scaled_data)\n",
        "\n",
        "        cluster_results[n_clusters] = {'hierarchical': pd.Series(hc_labels, index=df.index, name='cluster'),\n",
        "            'kmeans': pd.Series(km_labels, index=df.index, name='cluster')}\n",
        "\n",
        "    results['clustering'] = cluster_results\n",
        "    results['linkage'] = linkage_matrix\n",
        "\n",
        "    # 4. Correlation Analysis/Spearman correlation for non-linear relationships\n",
        "    corr_matrix = spearmanr(df.T)[0]\n",
        "    results['correlation'] = pd.DataFrame(corr_matrix, index=df.index, columns=df.index)\n",
        "\n",
        "    return results, X_pca\n",
        "\n",
        "def plot_exploration_results(df, results, category_dict, category_colors, categories_labels):\n",
        "    \"\"\"\n",
        "    Create visualizations for the exploratory analysis with colored categories_labels\n",
        "    \"\"\"\n",
        "    # Modified PCA visualization with categories\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # Create subplots\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(range(1, 6), results['pca']['explained_variance'], 'bo-')\n",
        "    plt.title('PCA Explained Variance')\n",
        "    plt.xlabel('Component')\n",
        "    plt.ylabel('Explained Variance Ratio')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "\n",
        "    # Get PCA components\n",
        "    pca_data = results['pca']['components']\n",
        "\n",
        "    # Plot each category separately to create the legend\n",
        "    for category_id in categories_labels.keys():\n",
        "        # Get indices for current category\n",
        "        category_mask = [category_dict.get(f'site_{i+1}', 0) == category_id\n",
        "                        for i in range(len(pca_data))]\n",
        "\n",
        "        # Plot points for current category\n",
        "        plt.scatter(pca_data[category_mask, 0],\n",
        "                   pca_data[category_mask, 1],\n",
        "                   c=category_colors[category_id],\n",
        "                   label=categories_labels[category_id],\n",
        "                   alpha=0.6)\n",
        "\n",
        "    plt.title('PCA First Two Components')\n",
        "    plt.xlabel('PC1')\n",
        "    plt.ylabel('PC2')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    # 3. UMAP visualization with categories\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    umap_df = results['umap']\n",
        "\n",
        "    for category_id in categories_labels.keys():\n",
        "        category_mask = [category_dict.get(f'site_{i+1}', 0) == category_id\n",
        "                        for i in range(len(umap_df))]\n",
        "        category_data = umap_df[category_mask]\n",
        "\n",
        "        plt.scatter(category_data['UMAP1'],\n",
        "                   category_data['UMAP2'],\n",
        "                   c=category_colors[category_id],\n",
        "                   label=categories_labels[category_id],\n",
        "                   alpha=0.6)\n",
        "\n",
        "    plt.title('UMAP Projection of Pathways by Category')\n",
        "    plt.xlabel('UMAP1')\n",
        "    plt.ylabel('UMAP2')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # 4. Hierarchical clustering dendrogram - simplified version\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    dendrogram = hierarchy.dendrogram(\n",
        "        results['linkage'],\n",
        "        labels=df.index,  # Use index , columns instead of index\n",
        "        leaf_rotation=90,\n",
        "        leaf_font_size=8\n",
        "    )\n",
        "    plt.title('Pathway Clustering Dendrogram')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # 5. Correlation heatmap\n",
        "    plt.figure(figsize=(12, 12))\n",
        "    mask = np.triu(np.ones_like(results['correlation']))\n",
        "\n",
        "    # Create a custom colormap that uses your category colors\n",
        "    custom_cmap = sns.diverging_palette(220, 20, as_cmap=True)\n",
        "\n",
        "    sns.heatmap(results['correlation'],\n",
        "                mask=mask,\n",
        "                cmap=custom_cmap,\n",
        "                center=0,\n",
        "                vmin=-1,\n",
        "                vmax=1)\n",
        "    plt.title('Pathway Correlation Heatmap')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def identify_key_patterns(df, results):\n",
        "    \"\"\"\n",
        "    Identify and summarize key patterns in the data\n",
        "    \"\"\"\n",
        "    patterns = {}\n",
        "\n",
        "    # Find highly correlated pathway groups\n",
        "    corr = results['correlation']\n",
        "    high_corr = pd.DataFrame(\n",
        "        [(i, j, corr.loc[i,j])\n",
        "         for i in corr.index\n",
        "         for j in corr.index\n",
        "         if i < j and abs(corr.loc[i,j]) > 0.8],\n",
        "        columns=['pathway1', 'pathway2', 'correlation']\n",
        "    ).sort_values('correlation', ascending=False)\n",
        "\n",
        "    # Find pathways with strong PCA loadings\n",
        "    loadings = results['pca']['loadings']\n",
        "    strong_loadings = pd.DataFrame({\n",
        "        'PC1_contribution': abs(loadings['PC1']),\n",
        "        'PC2_contribution': abs(loadings['PC2'])\n",
        "    }).sort_values('PC1_contribution', ascending=False)\n",
        "\n",
        "    patterns['high_correlations'] = high_corr\n",
        "    patterns['strong_loadings'] = strong_loadings\n",
        "\n",
        "    return patterns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TrS9Ii_GuvgQ"
      },
      "outputs": [],
      "source": [
        "'''# Calling the function for the pipeline using EPA algoritm\n",
        "results_SEPP, X_pca_SEPP = explore_pathway_patterns(Picrust_Result_SEPP)\n",
        "plot_exploration_results(Picrust_Result_SEPP, results_SEPP, category_dict, category_colors, categories_labels)\n",
        "patterns_SEPP = identify_key_patterns(Picrust_Result_SEPP, results_SEPP)'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EDmJPqM7uvgQ"
      },
      "outputs": [],
      "source": [
        "'''# Calling the function for the pipeline using EPA algoritm\n",
        "results_EPA, X_pca_EPA = explore_pathway_patterns(Picrust_Result_EPA)\n",
        "plot_exploration_results(Picrust_Result_EPA, results_EPA, category_dict, category_colors, categories_labels)\n",
        "patterns_EPA = identify_key_patterns(Picrust_Result_EPA, results_EPA)'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R59lKZ60uvgQ"
      },
      "source": [
        "__Discussing first results__\n",
        "\n",
        "The distribution of pathway abundances shows a typical microbial community pattern with few dominant pathways, suggesting key metabolic processes are essential across samples. PCA analysis reveals that only two components explain over 80% of the variance, indicating that metabolism in these systems might be driven by two major functional groups. The UMAP visualization confirms this binary pattern through two distinct clusters, demonstrating the robustness of this separation across different dimensional reduction techniques. The hierarchical clustering dendrogram further validates this division by showing two major branches, which notably align with previously observed physicochemical patterns in our Pourbaix plot analysis. The correlation heatmap exhibits strong relationships between specific pathway groups, suggesting coordinated metabolic activities that require detailed pathway mapping for full biological interpretation. EPA sequence placement shows much better differenciation on the pc plot."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RI1NL6_uuvgQ"
      },
      "source": [
        "## 7.3. Distribution of pathway abundances and Heatmap Hierarchies\n",
        "In the following script we map the column pathway on the dataframe Picrust_Result_raw to the actual names provided by the Galaxy website that corresponds to the MetaCyc pathways. We will end up with the original Picrust_Results df with disernible names.After the 20 most abundant pathways will be plotted and the heatmap with the hierarchichal pathways drawn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zczh30NruvgQ"
      },
      "outputs": [],
      "source": [
        "# Define category dict outside so that all charts can use same dict\n",
        "\n",
        "category_dict = Integrated_T.T.iloc[0, 0:-1].to_dict()\n",
        "\n",
        "# Define colors and categories\n",
        "category_colors = {1: '#008800',  # Dark green\n",
        "                   2: '#FF8C00',  # Dark orange\n",
        "                   3: '#FF0000'}   # Red\n",
        "\n",
        "categories_labels = {1: 'Normal Operation',\n",
        "              2: 'Early Warning',\n",
        "              3: 'System Failure'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "praqb_f6uvgQ"
      },
      "outputs": [],
      "source": [
        "def analyze_pathway_patterns(df, mean_abundances, category_dict, top_n=20):\n",
        "    \"\"\"\n",
        "    Create two separate visualizations for pathway analysis:\n",
        "    1. Stacked bar chart of top pathways by system state\n",
        "    2. Correlation heatmap of top pathways\n",
        "\n",
        "    Parameters:\n",
        "    df: DataFrame with pathway data\n",
        "    mean_abundances: Series with pre-calculated mean abundances\n",
        "    category_dict: Dictionary mapping sites to risk categories\n",
        "    top_n: Number of top pathways to display\n",
        "    \"\"\"\n",
        "    # Get top pathways\n",
        "    top_pathways = mean_abundances.nlargest(top_n)\n",
        "\n",
        "    # 1. Stacked Bar Chart\n",
        "    plt.figure(figsize=(15, 8))\n",
        "\n",
        "    # Prepare data for stacking\n",
        "    pathway_data = []\n",
        "    for pathway in top_pathways.index:\n",
        "        cat_means = {}\n",
        "        for cat in [1, 2, 3]:\n",
        "            cat_sites = [site for site, c in category_dict.items() if c == cat]\n",
        "            if cat_sites:\n",
        "                cat_means[cat] = df.loc[pathway, cat_sites].mean()\n",
        "            else:\n",
        "                cat_means[cat] = 0\n",
        "        pathway_data.append((pathway, cat_means))\n",
        "\n",
        "    # Create stacked bars\n",
        "    bottoms = np.zeros(len(top_pathways))\n",
        "    for cat in [1, 2, 3]:\n",
        "        values = [d[1][cat] for d in pathway_data]\n",
        "        plt.bar(range(len(top_pathways)), values, bottom=bottoms,\n",
        "                label=categories_labels[cat], color=category_colors[cat], alpha=0.7)\n",
        "        bottoms += values\n",
        "\n",
        "    plt.title('Top 20 Most Abundant Pathways by System State', fontsize=14, pad=20)\n",
        "    plt.xlabel('Pathway', fontsize=12)\n",
        "    plt.ylabel('Mean Abundance', fontsize=12)\n",
        "    plt.xticks(range(len(top_pathways)), top_pathways.index,\n",
        "               rotation=45, ha='right', fontsize=10)\n",
        "    plt.legend(title='System State', title_fontsize=12, fontsize=10)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # 2. Correlation Heatmap (separate figure)\n",
        "    plt.figure(figsize=(15, 12))\n",
        "    top_data = df.loc[top_pathways.index]\n",
        "    corr = top_data.T.corr()\n",
        "\n",
        "    # Create mask for upper triangle\n",
        "    mask = np.triu(np.ones_like(corr), k=1)\n",
        "\n",
        "    # Create heatmap with improved readability\n",
        "    sns.heatmap(corr,\n",
        "                mask=mask,\n",
        "                cmap='coolwarm',\n",
        "                center=0,\n",
        "                annot=True,\n",
        "                fmt='.2f',\n",
        "                square=True,\n",
        "                cbar_kws={'label': 'Correlation Coefficient'},\n",
        "                annot_kws={'size': 8})\n",
        "\n",
        "    plt.title('Pathway Correlation Heatmap\\n(Top 20 Most Abundant)',\n",
        "              fontsize=14,\n",
        "              pad=20)\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return corr\n",
        "\n",
        "#Calculate mean abundances and run analysis\n",
        "mean_abundances_epa = Picrust_Result_EPA.mean(axis=1)\n",
        "corr_epa = analyze_pathway_patterns(Picrust_Result_EPA, mean_abundances_epa, category_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meleGILNuvgR"
      },
      "source": [
        "__Discussing the 20 biggest metabolisms and their Hierarchical Heatmap__\n",
        "The metabolic pathway analysis reveals aerobic respiration as the dominant metabolism, showing approximately 75% higher abundance than other pathways across all systems. Correlation analysis highlights strong relationships between aerobic respiration and key metabolic processes, including TCA cycles and amino acid biosynthesis pathways, particularly those involved in biofilm formation. While these patterns provide insights into the overall metabolic landscape, a more detailed analysis separating corroded and non-corroded systems, along with integration of physicochemical variables and risk labels, would be necessary for actionable conclusions about corrosion processes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0Sbd6A7uvgR"
      },
      "source": [
        "# 8. Mapping the Pathways back to the Genera\n",
        "\n",
        "The result we obtained from the picrust pipeline contain the following dataframes, here described so it would be possible to parse.\n",
        "| Picrust_Result | Picrust_Result | Picrust_Result | parce | parce | parce | ECcontri | ECcontri | ECcontri | ECcontri | React | React |\n",
        "|---|---|---|---|---|---|---|---|---|---|---|---|\n",
        "| pathway | description | Sites/abund | pathway | RXN | EC number | EC number | varios abundances | Sites | OTU | Sites/abund | Reactions |\n",
        "|366,72|366,72|366,72|574,1|574,1|574,1|1491288, 9|1491288, 9|1491288, 9|1491288, 9| (2955, 71)|(2955, 71)|\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGMCpOrRuvgR"
      },
      "outputs": [],
      "source": [
        "'''large_dir = Path(\"/home/beatriz/MIC/MIC_large\")\n",
        "EC_path = Path(base_dir / \"Galaxy9-[EC_T].tabular\")\n",
        "EC= pd.read_csv(EC_path, sep = \"\\t\")\n",
        "ECcontri and KOcontri files contain sample, function (EC/KO number), taxon (genus/OTU ID), and abundance metrics.\n",
        "ECcontri_path =  Path(large_dir / \"Galaxy26-[EC_pred_metagenome_contrib].tabular\") # for VSCODE'''\n",
        "\n",
        "ECcontri_path =  Path(base_dir / \"Galaxy26-[EC_pred_metagenome_contrib].tabular\") # for Colab\n",
        "ECcontri= pd.read_csv(ECcontri_path, sep = \"\\t\")\n",
        "'''KOcontri_path = Path(large_dir / \"Galaxy30-[KO_pred_metagenome_contrib].tabular\")\n",
        "KOcontri= pd.read_csv(KOcontri_path, sep = \"\\t\")\n",
        "#parsing pathways (PWY) to the reactions (RXN), parce has a single column with 575 rows, that will mean that the patways can be more than once with different reactions\n",
        "parce_path = Path(base_dir / \"Galaxy17-[parsed_mapfile].tabular\")\n",
        "parce= pd.read_csv(parce_path, sep = \"\\t\")\n",
        "# reaction is a regroup file comprises the list of reactions in the index and the sites with abundances, similar to the pathways with abundances master file\n",
        "# whiles pathways has 366 rows (pathway), react has 2956 rows(reactions)\n",
        "react_path =Path(base_dir / \"Galaxy18-[regrouped_infile].tabular\")\n",
        "react= pd.read_csv(react_path, sep = \"\\t\")'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HB57hdTiuvgR"
      },
      "source": [
        "## 8.1. Mapping Genera to Otu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_7z90_iuvgR"
      },
      "outputs": [],
      "source": [
        "# Mapping the Genera to Otu for the Taxonomy assigment requeriment\n",
        "def create_otu_mapping(fasta_file_final):\n",
        "    \"\"\"Creates a DataFrame mapping OTUs to genera from a FASTA file\n",
        "    Args: fasta_file (str): Path to FASTA file\n",
        "    Returns: pd.DataFrame: DataFrame with columns ['Genus', 'OTU']\n",
        "    \"\"\"\n",
        "    mapping_data = []\n",
        "\n",
        "    for record in SeqIO.parse(fasta_file_final, \"fasta\"):\n",
        "        # Split description to get genus and OTU\n",
        "        parts = record.description.split()\n",
        "        genus = parts[0]\n",
        "        otu = parts[1]  # Take first OTU number\n",
        "\n",
        "        mapping_data.append({'Genus': genus,'OTU': otu})\n",
        "\n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame(mapping_data).sort_values('Genus')\n",
        "\n",
        "    return df\n",
        "\n",
        "otu_mapping = create_otu_mapping(fasta_file_final)\n",
        "# Change the name of the Otus since they using taxon\n",
        "otu_mapping = otu_mapping.rename(columns={\"OTU\" : \"taxon\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_HSFhUmuvgR"
      },
      "source": [
        "Now with the parce file where there is info for pathway, reactions and also it is the EC numbers, the EC numbers will be extracted because they are encoded inside the parce file. So that we can link the EC with the pathways in the ECcontri df. Precisely the stratified Pathway Abundance contributions (KO/EC + taxon + Taxon abundance +  others ) = KOcontri, ECcontri will be join by the taxon (which is same as the otus) with the file where is located the taxonomy Assigment = Otus + Genera Falta. Ultimately, the pathway descriptions file = Full pipipeline results(pathways + abundances) + description(human readable pathway) = Picrust_Result will also be join in order to make the visualisations, we doing that in steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SWsDEMsuvgR"
      },
      "source": [
        "### Map pathways to ECcontri\n",
        "\n",
        "Map pathways (Picrust_Result) to Parce → Ensures accurate EC-pathway links.   \n",
        "Map reactions (React) to Parce → Ensures correct RXN-pathway links.   \n",
        "Use these mappings to update ECcontri → Assign pathways to EC numbers.  \n",
        "Handle unmapped EC numbers → Keep them separate for review.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjaxRTlluvgR"
      },
      "source": [
        "## Decomvoluting Parce :\n",
        "separating the contents of the parsing file parce df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7mUFRVcuvgR"
      },
      "outputs": [],
      "source": [
        "'''# Initialize lists\n",
        "pathways = []\n",
        "all_ec_numbers = []\n",
        "reactions = []\n",
        "\n",
        "# Parse parce file\n",
        "for line in parce.iloc[:, 0]:\n",
        "    parts = line.split(' ', 1)\n",
        "    if len(parts) == 2:\n",
        "        pathway = parts[0]\n",
        "        reaction_list = parts[1].strip().split()\n",
        "\n",
        "        # Find EC numbers\n",
        "        ec_nums = []\n",
        "        rxns = []\n",
        "\n",
        "        for rxn in reaction_list:\n",
        "            if rxn.count('.') == 3:\n",
        "                ec_part = rxn.split('-RXN')[0]\n",
        "                if all(p.replace('-','').isdigit() for p in ec_part.split('.')):\n",
        "                    ec_nums.append(ec_part)\n",
        "                    continue\n",
        "            rxns.append(rxn)\n",
        "\n",
        "        # Pad ec_nums list to always have 7 elements\n",
        "        ec_nums.extend([None] * (7 - len(ec_nums)))\n",
        "\n",
        "        # Add to lists\n",
        "        pathways.append(pathway)\n",
        "        all_ec_numbers.append(ec_nums)\n",
        "        reactions.append(' '.join(rxns))\n",
        "\n",
        "# Create DataFrame\n",
        "parce_reference = pd.DataFrame({\n",
        "    'pathway': pathways,\n",
        "    'EC1': [ecs[0] for ecs in all_ec_numbers],\n",
        "    'EC2': [ecs[1] for ecs in all_ec_numbers],\n",
        "    'EC3': [ecs[2] for ecs in all_ec_numbers],\n",
        "    'EC4': [ecs[3] for ecs in all_ec_numbers],\n",
        "    'EC5': [ecs[4] for ecs in all_ec_numbers],\n",
        "    'EC6': [ecs[5] for ecs in all_ec_numbers],\n",
        "    'EC7': [ecs[6] for ecs in all_ec_numbers],\n",
        "    'other_reactions': reactions\n",
        "})\n",
        "\n",
        "# Display first few rows\n",
        "print(\"First 5 rows of parce data with all EC numbers:\")\n",
        "print(parce_reference.head().to_string())\n",
        "\n",
        "# Print example of a pathway with many EC numbers\n",
        "print(\"\\nExample of pathway with many EC numbers:\")\n",
        "print(parce_reference[parce_reference['EC7'].notna()].iloc[0].to_string())\n",
        "\n",
        "parce_reference.to_csv(\"~/MIC/2_Micro/data_picrust/EC_path_parce.csv\", index=False, sep=\"\\t\")'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhNsmPeouvgR"
      },
      "source": [
        "enriched_picrust"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FsuN4hYuvgR"
      },
      "outputs": [],
      "source": [
        "'''# Get list of pathways we need from Picrust_Result\n",
        "picrust_pathways = set(Picrust_Result['pathway'])\n",
        "\n",
        "# Filter parce_reference to only include needed pathways\n",
        "parced_picrust = parce_reference[parce_reference['pathway'].isin(picrust_pathways)]\n",
        "\n",
        "# Print statistics\n",
        "print(\"Matching Statistics:\")\n",
        "print(f\"Total pathways in Picrust_Result: {len(picrust_pathways)}\")\n",
        "print(f\"Matched pathways from parced_picrust: {len(parced_picrust)}\")\n",
        "\n",
        "# Show some examples of the matched data\n",
        "print(\"\\nFirst few matched pathways:\")\n",
        "print(parced_picrust.head().to_string())\n",
        "\n",
        "# Check if we missed any pathways\n",
        "missing_pathways = picrust_pathways - set(parced_picrust['pathway'])\n",
        "if missing_pathways:\n",
        "    print(\"\\nWarning: Some Picrust pathways not found in parce:\")\n",
        "    print(list(missing_pathways)[:5])  # Show first 5 missing pathways if any\n",
        "parced_picrust.to_csv(\"~/MIC/2_Micro/data_picrust/parced_picrust.csv\", index=False, sep=\"\\t\")'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EEKnKNYuvgR"
      },
      "source": [
        "This way of mapping from parce df was no effective, the pathways and EC in parce are no all the ones are on ECcontri. So it was decided to map it from ECcontri directly putting the pathways into the df from Picrust_Result, so just put the two columns description and pathways inside the ECcontri by the column Site, instead that from the column function aka EC numbers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCs2cimyuvgS"
      },
      "source": [
        "## 8.2 Map Econtri to pathways"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEkDJg5zuvgS"
      },
      "source": [
        "We can no directly map the description and the pathway from Picrust_Result into ECcontri because each site can have several pathways, so we reshaping the Picrust_Result to long format and so that each row corresponds to a pathway for a given site. It is no possible to do this on a go using the whole 1491288 rows on ECcontri, so it would have to be done on agreggated data, as suggested by McKinney, 2010.\n",
        "Source: McKinney, W. (2010). Data Structures for Statistical Computing in Python. Retrieved from https://pandas.pydata.org/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npxOZlx8uvgS"
      },
      "outputs": [],
      "source": [
        "# Reshape Picrust_Result to long format: each row now corresponds to a pathway for a given site\n",
        "picrust_long = Picrust_Result.melt(id_vars=['pathway', 'description'],\n",
        "                                   var_name='sample',\n",
        "                                   value_name='abundance')\n",
        "\n",
        "# Filter out rows where the abundance is 0 (assuming that's what you mean by \"pathway present\")\n",
        "picrust_long = picrust_long[picrust_long['abundance'] > 0]\n",
        "\n",
        "# Aggregate pathway info per site\n",
        "mapping = picrust_long.groupby('sample').agg({\n",
        "    'pathway': lambda x: list(x),\n",
        "    'description': lambda x: list(x)\n",
        "}).reset_index()\n",
        "\n",
        "# Merge the aggregated mapping with ECcontri\n",
        "ECcontri_agg_site = pd.merge(ECcontri, mapping, on='sample', how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5EXHu-QeuvgS"
      },
      "outputs": [],
      "source": [
        " # Add genus information from otu_mapping\n",
        "ECcontri_agg_site['taxon'] = ECcontri_agg_site['taxon'].astype(str)\n",
        "otu_mapping['taxon'] = otu_mapping['taxon'].astype(str)\n",
        "\n",
        "ECcontri_otu= pd.merge(ECcontri_agg_site, otu_mapping, on='taxon', how='left', validate='m:1')\n",
        "\n",
        "unmapped = ECcontri_otu['Genus'].isna().sum()\n",
        "if unmapped > 0:\n",
        "    print(f\"Warning: {unmapped} rows could not be mapped to genera\")\n",
        "# Rename columns: here \"description\" becomes \"pathway\" and \"pathway\" becomes \"npath\"\n",
        "ECcontri_otu  = ECcontri_otu.rename(columns={\"sample\":\"Sites\", \"function\": \"EC\", \"taxon\": \"OTU\", \"description\":\"pathway\", \"pathway\":\"npath\",\n",
        "                                     \"taxon_abun\": \"abund_raw\", \"taxon_function_abun\": \"abund_contri\", \"taxon_rel_abun\": \"rel_abund_raw\",\n",
        "                                       \"taxon_rel_function_abun\": \"rel_abund_contri\", \"norm_taxon_function_contrib\" :\"norm_abund_contri\", \"genome_function_count\":\"genome_EC_count\"})\n",
        "# Organize columns in logical groups\n",
        "cols_order = ['Sites', 'Genus', 'OTU', 'EC', # Identification columns\n",
        "              'npath', 'pathway', # Pathway information\n",
        "              'abund_raw', 'rel_abund_raw', # Raw abundance metrics\n",
        "              'genome_EC_count', 'abund_contri', 'rel_abund_contri', 'norm_abund_contri'] # Contribution metrics\n",
        "# Reorder columns, takes like 4 minutes on this slow laptop\n",
        "ECcontri_otu = ECcontri_otu[cols_order]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPaVL4T0uvgS"
      },
      "source": [
        "ECcontri_otu is a comprehensive dataframe that combines site locations, taxonomic information (genera and OTUs), enzyme classifications (ECs), and pathways (code for pathway (npath) and description (pathway)). The associated abundance metrics belong to the original ECcontri. The abundance metrics include:\n",
        "abund_raw: The original count of each organism (OTU) at each site\n",
        "rel_abund_raw: The relative abundance of each organism at each site, expressed as a proportion of total counts\n",
        "genome_function_count represents the predicted number of copies of a particular EC number (enzyme) in an organism's genome. This prediction comes from PICRUSt's hidden-state prediction process, which infers gene family abundances for each organism based on its phylogenetic placement relative to reference genomes\n",
        "abund_contri: The contribution of each organism to a specific enzyme function, calculated by multiplying the raw abundance by the number of copies of that enzyme in the organism's genome\n",
        "rel_abund_contri: The relative contribution of each organism to the enzyme function, accounting for both abundance and genome copy number\n",
        "norm_abund_contri: The normalized contribution metric that allows comparison across different sites and functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xeKQezwyuvgS"
      },
      "outputs": [],
      "source": [
        "'''# Analyze genome_function_count\n",
        "print(\"Genome function count statistics:\")\n",
        "print(\"\\nOverall statistics:\")\n",
        "print(ECcontri_otu['genome_EC_count'].describe())\n",
        "\n",
        "# Look at distribution by EC number\n",
        "print(\"\\nExample EC numbers and their genome counts:\")\n",
        "ec_counts = ECcontri_otu.groupby('EC')['genome_EC_count'].agg(['unique', 'mean', 'max']).head()\n",
        "print(ec_counts)\n",
        "\n",
        "# Check if genome_function_count is consistent for each OTU-EC pair\n",
        "print(\"\\nCheck if genome_EC_count is consistent for OTU-EC combinations:\")\n",
        "consistency_check = ECcontri_otu.groupby(['OTU', 'EC'])['genome_EC_count'].nunique()\n",
        "inconsistent = consistency_check[consistency_check > 1]\n",
        "if len(inconsistent) > 0:\n",
        "    print(f\"Found {len(inconsistent)} OTU-EC pairs with inconsistent genome counts\")\n",
        "else:\n",
        "    print(\"Genome counts are consistent for all OTU-EC pairs\")\n",
        "\n",
        "# Explain the metrics in the dataframe\n",
        "print(\"\\nDataframe Components:\")\n",
        "print(\"1. Abundance Metrics:\")\n",
        "print(\"   - abund_raw: Raw abundance of each organism in each site\")\n",
        "print(\"   - abund_contri: Organism's abundance contribution to function/pathway\")\n",
        "print(\"   - rel_abund_raw: Original relative abundance\")\n",
        "print(\"   - rel_abund_contri: Relative abundance contribution to pathway\")\n",
        "print(\"   - norm_abund_contri: Normalized abundance contribution\")\n",
        "print(\"\\n2. Genome Function Count:\")\n",
        "print(\"   Number of copies of each EC (enzyme) in organism's genome\")'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MA-sQJpQuvgS"
      },
      "source": [
        "Analysis of genome_function_count(genome_EC_count) shows that most organisms typically have just one copy of any given enzyme (EC number) in their genome, with 75% of all cases showing a single copy. However, there is notable variation, with some organisms having up to 10 copies of certain enzymes. The average across all cases is 1.4 copies per enzyme per organism.\n",
        "Some enzymes show more variation than others. For example:\n",
        "\n",
        "EC:1.1.1.1 varies from 1 to 8 copies across different organisms\n",
        "EC:1.1.1.100 shows the widest range, from 1 to 10 copies\n",
        "Many enzymes (like EC:1.1.1.102, 103, 105) consistently appear as single copies\n",
        "\n",
        "Importantly, the copy number is consistent for each organism-enzyme combination across all sites, indicating this is a stable genomic characteristic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTSu-jj9uvgS"
      },
      "source": [
        "_____________________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voefPo-VuvgS"
      },
      "source": [
        "Pathway Mapping Analysis\n",
        "\n",
        "There were identified a discrepancy between EC predictions and pathway abundances. Found 61 pathways with EC number evidence that were not included in final predictions. Total number of reference pathways: 574 (from MetaCyc), total pathways in final predictions: 366, example missing pathway: PWY-6486 supported by EC:4.2.1.41\n",
        "\n",
        "Implications\n",
        "This finding suggests that the pathway prediction pipeline might be filtering out potentially relevant pathways despite having supporting EC evidence. This could impact the biological interpretation of the functional profiles and warrants further investigation.\n",
        "So in this study we mapped the pathways dataframe directly to the parce file and in doing so we have also the reaction information, avoiding the discrepancy with the Picrust_Result missing pathways."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MP4zCAyYuvgS"
      },
      "source": [
        "_____________________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEJ63Uk0uvgS"
      },
      "source": [
        "Now ECcontri_otu has several rows and columns providing information of the EC contribution to the metrics to each enzime aka EC number to the sites, genera combination, however the pathways are from origin link to most of the sites. This is perhaps because the methos infwee dunxriona bAWS ON XOMON sets of reference genomes.  Then, same environment in this case heating and cooling water systems poses similar organisms with similar pathways, the difference being on the abundance. So in order for this data to be usable, it is necesary to parse the EC into human readable information from a external enzyme databases to retrieve functional information about an EC number. Common resources include:\n",
        "\n",
        "UniProt: query UniProt’s REST API to get enzyme details by searching with the EC number.\n",
        "ExPASy Enzyme Database: Provides enzyme information based on EC numbers.\n",
        "BRENDA: A comprehensive enzyme database that can be queried either via its web interface or programmatically (e.g., using the bioservices Python package). Following script creates an EnzymeRetriever class that handles API requests to UniProt, processes unique EC numbers to avoid duplicate requests\n",
        "Adds protein names, functions, and UniProt IDs to ECcontri_otu df and includes rate limiting to avoid API restrictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XksxGYwIuvgS"
      },
      "source": [
        "## 8.3 Retrieving EC from Uniprot Locally"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vTTKHr0buvgS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import time\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "class BatchEnzymeRetriever:\n",
        "    def __init__(self, batch_size=50, save_every=10):\n",
        "        self.uniprot_api = \"https://rest.uniprot.org/uniprotkb/search\"\n",
        "        self.batch_size = batch_size\n",
        "        self.save_every = save_every\n",
        "        self.results_file = 'uniprot_results.csv'\n",
        "        self.progress_file = 'retrieval_progress.json'\n",
        "        self.current_position = 0\n",
        "        self.load_progress()\n",
        "\n",
        "    def load_progress(self):\n",
        "        \"\"\"Load progress from previous run\"\"\"\n",
        "        if Path(self.progress_file).exists():\n",
        "            with open(self.progress_file, 'r') as f:\n",
        "                self.current_position = json.load(f)['position']\n",
        "            print(f\"Resuming from position {self.current_position}\")\n",
        "        else:\n",
        "            print(\"Starting new retrieval process\")\n",
        "\n",
        "    def save_progress(self):\n",
        "        \"\"\"Save current progress\"\"\"\n",
        "        with open(self.progress_file, 'w') as f:\n",
        "            json.dump({'position': self.current_position}, f)\n",
        "\n",
        "    def get_uniprot_info(self, ec: str, organism: str) -> dict:\n",
        "        \"\"\"Get UniProt information for a specific EC-organism pair\"\"\"\n",
        "        query = f\"{ec} {organism}\"\n",
        "\n",
        "        params = {\n",
        "            'query': query,\n",
        "            'format': 'tsv',\n",
        "            'fields': 'id,ec,organism_name,protein_name',\n",
        "            'size': 10\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = requests.get(self.uniprot_api, params=params)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            # Parse results\n",
        "            lines = response.text.strip().split('\\n')\n",
        "            if len(lines) < 2:\n",
        "                return None\n",
        "\n",
        "            # Process results to find best match\n",
        "            best_match = None\n",
        "            best_score = -float('inf')\n",
        "\n",
        "            for line in lines[1:]:  # Skip header\n",
        "                parts = line.split('\\t')\n",
        "                if len(parts) < 4:\n",
        "                    continue\n",
        "\n",
        "                entry_id, ec_numbers, org_name, protein_name = parts\n",
        "\n",
        "                # Calculate score based on organism name simplicity\n",
        "                score = 0\n",
        "                if organism.lower() in org_name.lower():\n",
        "                    score += 100\n",
        "                    if org_name.lower().endswith(' sp') or org_name.lower().endswith(' sp.'):\n",
        "                        score += 200\n",
        "                    if any(char.isdigit() for char in org_name):\n",
        "                        score -= 100\n",
        "\n",
        "                    # Check EC number match\n",
        "                    if ec.replace('EC:', '') in ec_numbers.split('; '):\n",
        "                        score += 150\n",
        "\n",
        "                        if score > best_score:\n",
        "                            best_score = score\n",
        "                            best_match = {\n",
        "                                'uniprot_id': entry_id,\n",
        "                                'ec_number': ec,\n",
        "                                'protein_name': protein_name,\n",
        "                                'organism': org_name,\n",
        "                                'score': score\n",
        "                            }\n",
        "\n",
        "            return best_match\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error retrieving data for {ec} - {organism}: {e}\")\n",
        "            time.sleep(5)\n",
        "            return None\n",
        "\n",
        "    def process_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Process DataFrame in batches, saving progress\"\"\"\n",
        "\n",
        "        # Load existing results if any\n",
        "        results_df = pd.DataFrame()\n",
        "        if Path(self.results_file).exists():\n",
        "            results_df = pd.read_csv(self.results_file)\n",
        "            print(f\"Loaded {len(results_df)} existing results\")\n",
        "\n",
        "        # Get unique EC-organism pairs starting from current position\n",
        "        pairs = df[['EC', 'Genus']].drop_duplicates()\n",
        "        pairs = pairs.iloc[self.current_position:]\n",
        "\n",
        "        print(f\"Processing {len(pairs)} unique EC-organism pairs\")\n",
        "\n",
        "        for batch_start in range(0, len(pairs), self.batch_size):\n",
        "            batch = pairs.iloc[batch_start:batch_start + self.batch_size]\n",
        "            batch_results = []\n",
        "\n",
        "            print(f\"\\nProcessing batch {batch_start//self.batch_size + 1}\")\n",
        "\n",
        "            for _, row in batch.iterrows():\n",
        "                print(f\"Querying {row['EC']} - {row['Genus']}\")\n",
        "                result = self.get_uniprot_info(row['EC'], row['Genus'])\n",
        "\n",
        "                if result:\n",
        "                    batch_results.append(result)\n",
        "                time.sleep(1)  # Rate limiting\n",
        "\n",
        "            # Add batch results to DataFrame\n",
        "            if batch_results:\n",
        "                batch_df = pd.DataFrame(batch_results)\n",
        "                results_df = pd.concat([results_df, batch_df], ignore_index=True)\n",
        "\n",
        "                # Save periodically\n",
        "                if (batch_start//self.batch_size) % self.save_every == 0:\n",
        "                    results_df.to_csv(self.results_file, index=False)\n",
        "                    self.current_position += len(batch)\n",
        "                    self.save_progress()\n",
        "                    print(f\"Saved progress at position {self.current_position}\")\n",
        "\n",
        "        # Final save\n",
        "        if not results_df.empty:\n",
        "            results_df.to_csv(self.results_file, index=False)\n",
        "        return results_df\n",
        "\n",
        "def process_enzymes(input_data, batch_size: int = 50):\n",
        "    \"\"\"Main function to process enzyme data\"\"\"\n",
        "    # Handle both DataFrame and file path inputs\n",
        "    if isinstance(input_data, str):\n",
        "        df = pd.read_csv(input_data)\n",
        "        print(f\"Loaded {len(df)} rows from {input_data}\")\n",
        "    elif isinstance(input_data, pd.DataFrame):\n",
        "        df = input_data\n",
        "        print(f\"Processing DataFrame with {len(df)} rows\")\n",
        "    else:\n",
        "        raise ValueError(\"Input must be either a file path or a pandas DataFrame\")\n",
        "\n",
        "    # Initialize retriever\n",
        "    retriever = BatchEnzymeRetriever(batch_size=batch_size)\n",
        "\n",
        "    # Process data\n",
        "    results = retriever.process_dataframe(df)\n",
        "    print(\"\\nProcessing complete!\")\n",
        "    print(f\"Results saved to {retriever.results_file}\")\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i127PUfOuvgT"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "#  DataFrame loaded:\n",
        "retriever = BatchEnzymeRetriever(batch_size=50)\n",
        "Picrust_Econtri = retriever.process_dataframe(ECcontri_otu)'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQt8CV11uvgT"
      },
      "source": [
        "## 8.4. Colab\n",
        "Made in colab for resource posibility."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-sX1OxiuvgT"
      },
      "source": [
        "original colab code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGPK3BTGuvgT"
      },
      "outputs": [],
      "source": [
        "class ColabEnzymeRetriever:\n",
        "    def __init__(self, batch_size=100, save_every=5):\n",
        "        self.uniprot_api = \"https://rest.uniprot.org/uniprotkb/search\"\n",
        "        self.batch_size = batch_size\n",
        "        self.save_every = save_every\n",
        "        self.results_file = 'uniprot_results.csv'\n",
        "        self.existing_results = None\n",
        "\n",
        "    def load_existing_results(self, file_path):\n",
        "        \"\"\"Load and validate existing results\"\"\"\n",
        "        self.existing_results = pd.read_csv(file_path)\n",
        "        print(f\"Loaded {len(self.existing_results)} existing results\")\n",
        "        return set(zip(self.existing_results['ec_number'],\n",
        "                      [org.split()[0] for org in self.existing_results['organism']]))\n",
        "\n",
        "    def get_uniprot_info(self, ec: str, organism: str) -> dict:\n",
        "        \"\"\"Get UniProt information for a specific EC-organism pair\"\"\"\n",
        "        query = f\"{ec} {organism}\"\n",
        "\n",
        "        params = {\n",
        "            'query': query,\n",
        "            'format': 'tsv',\n",
        "            'fields': 'id,ec,organism_name,protein_name',\n",
        "            'size': 10\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = requests.get(self.uniprot_api, params=params)\n",
        "            response.raise_for_status()\n",
        "            time.sleep(0.5)  # Reduced sleep time for Colab\n",
        "\n",
        "            lines = response.text.strip().split('\\n')\n",
        "            if len(lines) < 2:\n",
        "                return None\n",
        "\n",
        "            best_match = None\n",
        "            best_score = -float('inf')\n",
        "\n",
        "            for line in lines[1:]:\n",
        "                parts = line.split('\\t')\n",
        "                if len(parts) < 4:\n",
        "                    continue\n",
        "\n",
        "                entry_id, ec_numbers, org_name, protein_name = parts\n",
        "\n",
        "                score = 0\n",
        "                if organism.lower() in org_name.lower():\n",
        "                    score += 100\n",
        "                    if org_name.lower().endswith(' sp') or org_name.lower().endswith(' sp.'):\n",
        "                        score += 200\n",
        "                    if any(char.isdigit() for char in org_name):\n",
        "                        score -= 100\n",
        "\n",
        "                    if ec.replace('EC:', '') in ec_numbers.split('; '):\n",
        "                        score += 150\n",
        "\n",
        "                        if score > best_score:\n",
        "                            best_score = score\n",
        "                            best_match = {\n",
        "                                'uniprot_id': entry_id,\n",
        "                                'ec_number': ec,\n",
        "                                'protein_name': protein_name,\n",
        "                                'organism': org_name,\n",
        "                                'score': score\n",
        "                            }\n",
        "\n",
        "            return best_match\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error retrieving data for {ec} - {organism}: {e}\")\n",
        "            time.sleep(2)\n",
        "            return None\n",
        "\n",
        "    def process_remaining_pairs(self, unique_pairs: pd.DataFrame, processed_pairs: set) -> pd.DataFrame:\n",
        "        \"\"\"Process only the pairs that haven't been processed yet\"\"\"\n",
        "        results = []\n",
        "        pairs_to_process = []\n",
        "\n",
        "        # Get unprocessed pairs\n",
        "        for _, row in unique_pairs.iterrows():\n",
        "            if (row['EC'], row['Genus']) not in processed_pairs:\n",
        "                pairs_to_process.append((row['EC'], row['Genus']))\n",
        "\n",
        "        pairs_df = pd.DataFrame(pairs_to_process, columns=['EC', 'Genus'])\n",
        "        total_pairs = len(pairs_df)\n",
        "\n",
        "        print(f\"\\nTotal pairs to process: {total_pairs}\")\n",
        "\n",
        "        if total_pairs == 0:\n",
        "            print(\"No new pairs to process!\")\n",
        "            return self.existing_results\n",
        "\n",
        "        for batch_start in range(0, total_pairs, self.batch_size):\n",
        "            batch = pairs_df.iloc[batch_start:batch_start + self.batch_size]\n",
        "            batch_results = []\n",
        "\n",
        "            print(f\"\\nProcessing batch {batch_start//self.batch_size + 1} of {total_pairs//self.batch_size + 1}\")\n",
        "            print(f\"Progress: {batch_start}/{total_pairs} pairs ({(batch_start/total_pairs)*100:.1f}%)\")\n",
        "\n",
        "            current_ec = None\n",
        "            for _, row in batch.iterrows():\n",
        "                if current_ec != row['EC']:\n",
        "                    current_ec = row['EC']\n",
        "                    print(f\"\\nProcessing EC number: {current_ec}\")\n",
        "\n",
        "                result = self.get_uniprot_info(row['EC'], row['Genus'])\n",
        "                if result:\n",
        "                    batch_results.append(result)\n",
        "\n",
        "            if batch_results:\n",
        "                results.extend(batch_results)\n",
        "\n",
        "                # Save progress by combining with existing results\n",
        "                if (batch_start//self.batch_size) % self.save_every == 0:\n",
        "                    combined_results = pd.concat([self.existing_results, pd.DataFrame(results)], ignore_index=True)\n",
        "                    combined_results.to_csv(self.results_file, index=False)\n",
        "                    print(f\"Saved {len(combined_results)} total results to file\")\n",
        "\n",
        "        # Combine final results\n",
        "        final_results = pd.concat([self.existing_results, pd.DataFrame(results)], ignore_index=True)\n",
        "        final_results.to_csv(self.results_file, index=False)\n",
        "        return final_results\n",
        "\n",
        "# Main processing function\n",
        "def continue_enzyme_retrieval(data_file: str, existing_results_file: str):\n",
        "    \"\"\"Main function to continue enzyme data retrieval\"\"\"\n",
        "    # Initialize retriever\n",
        "    retriever = ColabEnzymeRetriever(batch_size=100)\n",
        "\n",
        "    # Load existing results\n",
        "    processed_pairs = retriever.load_existing_results(existing_results_file)\n",
        "\n",
        "    # Load and process data\n",
        "    df = pd.read_csv(data_file)\n",
        "    unique_pairs = df[['EC', 'Genus']].drop_duplicates()\n",
        "    print(f\"Total unique pairs in data: {len(unique_pairs)}\")\n",
        "    print(f\"Already processed pairs: {len(processed_pairs)}\")\n",
        "\n",
        "    # Process remaining pairs\n",
        "    results_df = retriever.process_remaining_pairs(unique_pairs, processed_pairs)\n",
        "\n",
        "    # Save and download final results\n",
        "    results_df.to_csv('uniprot_results_final.csv', index=False)\n",
        "    files.download('uniprot_results_final.csv')\n",
        "\n",
        "    return results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1sJyminuvgT"
      },
      "outputs": [],
      "source": [
        "# Usage (after uploading files to Colab), ECcontri_otu was made in colab because it was too big to upload after transformed\n",
        "# results = continue_enzyme_retrieval('ECcontri_otu', 'uniprot_results.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UM7HpQiHuvgT"
      },
      "source": [
        "have to be modify after first run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTO90O2duvgT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import time\n",
        "from pathlib import Path\n",
        "import logging\n",
        "from typing import Set, Tuple, Optional\n",
        "import json\n",
        "\n",
        "class ColabEnzymeRetriever:\n",
        "    def __init__(self, batch_size=100, save_every=5):\n",
        "        self.uniprot_api = \"https://rest.uniprot.org/uniprotkb/search\"\n",
        "        self.batch_size = batch_size\n",
        "        self.save_every = save_every\n",
        "        self.results_file = Path('uniprot_results.tsv')\n",
        "        self.state_file = Path('retrieval_state.json')\n",
        "        self.processed_pairs: Set[Tuple[str, str]] = set()\n",
        "        self.existing_results = None\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "        self.logger.setLevel(logging.INFO)\n",
        "\n",
        "        if not self.logger.handlers:\n",
        "            handler = logging.StreamHandler()\n",
        "            formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
        "            handler.setFormatter(formatter)\n",
        "            self.logger.addHandler(handler)\n",
        "\n",
        "    def load_existing_results(self, file_path: Path) -> pd.DataFrame:\n",
        "        \"\"\"Load and validate existing results\"\"\"\n",
        "        if file_path.exists():\n",
        "            try:\n",
        "                self.existing_results = pd.read_csv(file_path, sep='\\t')\n",
        "                self.logger.info(f\"Loaded {len(self.existing_results)} existing results\")\n",
        "\n",
        "                # Build set of processed pairs\n",
        "                self.processed_pairs = set()\n",
        "                for _, row in self.existing_results.iterrows():\n",
        "                    if pd.notna(row['ec_number']) and pd.notna(row['organism']):\n",
        "                        ec_num = str(row['ec_number']).strip()\n",
        "                        org = str(row['organism']).split()[0].strip()\n",
        "                        self.processed_pairs.add((ec_num, org))\n",
        "\n",
        "                return self.existing_results\n",
        "            except Exception as e:\n",
        "                self.logger.error(f\"Error loading results file: {e}\")\n",
        "                self.existing_results = pd.DataFrame(\n",
        "                    columns=['uniprot_id', 'ec_number', 'protein_name', 'organism', 'score']\n",
        "                )\n",
        "                return self.existing_results\n",
        "\n",
        "        self.existing_results = pd.DataFrame(\n",
        "            columns=['uniprot_id', 'ec_number', 'protein_name', 'organism', 'score']\n",
        "        )\n",
        "        return self.existing_results\n",
        "\n",
        "    def get_uniprot_info(self, ec: str, organism: str) -> Optional[dict]:\n",
        "        \"\"\"Get UniProt information for a specific EC-organism pair\"\"\"\n",
        "        if (ec, organism) in self.processed_pairs:\n",
        "            return None\n",
        "\n",
        "        query = f'({ec}) AND (organism_name:\"{organism}*\")'\n",
        "        params = {\n",
        "            'query': query,\n",
        "            'format': 'tsv',\n",
        "            'fields': 'id,ec,protein_name,organism_name',\n",
        "            'size': 10\n",
        "        }\n",
        "\n",
        "        max_retries = 3\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                response = requests.get(self.uniprot_api, params=params)\n",
        "                response.raise_for_status()\n",
        "                time.sleep(0.5)\n",
        "\n",
        "                lines = response.text.strip().split('\\n')\n",
        "                if len(lines) < 2:\n",
        "                    return None\n",
        "\n",
        "                best_match = None\n",
        "                best_score = -float('inf')\n",
        "\n",
        "                for line in lines[1:]:\n",
        "                    parts = line.split('\\t')\n",
        "                    if len(parts) < 4:\n",
        "                        continue\n",
        "\n",
        "                    uniprot_id, ec_numbers, protein_name, organism_name = parts\n",
        "\n",
        "                score = 0\n",
        "                if organism_name and isinstance(organism_name, str):\n",
        "                    name_parts = organism_name.split()\n",
        "                    genus = name_parts[0] if name_parts else \"\"\n",
        "\n",
        "                    # Exact genus match gets highest score\n",
        "                    if genus.lower() == organism.lower():\n",
        "                        score += 500\n",
        "                        # Prefer entries with just the genus name\n",
        "                        if len(name_parts) == 1:\n",
        "                            score += 300\n",
        "                        # Heavily penalize strain designations or subspecies\n",
        "                        elif len(name_parts) > 2 or any(char.isdigit() for char in organism_name):\n",
        "                            score -= 400\n",
        "\n",
        "                    if score > -float('inf'):\n",
        "                        if ec.replace('EC:', '') in ec_numbers.split('; '):\n",
        "                            score += 150\n",
        "\n",
        "                            if score > best_score:\n",
        "                                best_score = score\n",
        "                                best_match = {\n",
        "                                    'uniprot_id': uniprot_id,\n",
        "                                    'ec_number': ec,\n",
        "                                    'protein_name': protein_name,\n",
        "                                    'organism': organism_name,\n",
        "                                    'score': score\n",
        "                                }\n",
        "\n",
        "                return best_match if best_match else None\n",
        "\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                if attempt < max_retries - 1:\n",
        "                    time.sleep(2 ** attempt)\n",
        "                    continue\n",
        "                self.logger.error(f\"Error fetching data from UniProt: {e}\")\n",
        "                return None\n",
        "\n",
        "    def process_remaining_pairs(self, unique_pairs: pd.DataFrame, start_ec: str) -> pd.DataFrame:\n",
        "        \"\"\"Process remaining pairs with enforced starting point\"\"\"\n",
        "        # Ensure EC format consistency\n",
        "        if not start_ec.startswith('EC:'):\n",
        "            start_ec = f\"EC:{start_ec.replace('EC:', '')}\"\n",
        "\n",
        "        # Sort and filter pairs\n",
        "        unique_pairs = unique_pairs.sort_values(['EC', 'Genus']).reset_index(drop=True)\n",
        "        unique_pairs = unique_pairs[unique_pairs['EC'] >= start_ec].reset_index(drop=True)\n",
        "\n",
        "        if len(unique_pairs) == 0:\n",
        "            self.logger.warning(f\"No EC numbers found after {start_ec}\")\n",
        "            return self.existing_results\n",
        "\n",
        "        self.logger.info(f\"Starting processing from {unique_pairs.iloc[0]['EC']}\")\n",
        "        total_pairs = len(unique_pairs)\n",
        "\n",
        "        results = []\n",
        "        for idx in range(0, total_pairs, self.batch_size):\n",
        "            batch = unique_pairs.iloc[idx:idx + self.batch_size]\n",
        "            batch_results = []\n",
        "\n",
        "            self.logger.info(f\"\\nProcessing batch {idx//self.batch_size + 1} of {total_pairs//self.batch_size + 1}\")\n",
        "            self.logger.info(f\"Progress: {idx}/{total_pairs} pairs ({(idx/total_pairs)*100:.1f}%)\")\n",
        "\n",
        "            current_ec = None\n",
        "            for _, row in batch.iterrows():\n",
        "                if current_ec != row['EC']:\n",
        "                    current_ec = row['EC']\n",
        "                    self.logger.info(f\"\\nProcessing EC number: {current_ec}\")\n",
        "\n",
        "                if (row['EC'], row['Genus']) not in self.processed_pairs:\n",
        "                    result = self.get_uniprot_info(row['EC'], row['Genus'])\n",
        "                    if result:\n",
        "                        batch_results.append(result)\n",
        "                        self.processed_pairs.add((row['EC'], row['Genus']))\n",
        "\n",
        "            if batch_results:\n",
        "                results.extend(batch_results)\n",
        "\n",
        "                # Save progress periodically\n",
        "                if (idx//self.batch_size) % self.save_every == 0:\n",
        "                    combined_results = pd.concat(\n",
        "                        [self.existing_results, pd.DataFrame(results)],\n",
        "                        ignore_index=True\n",
        "                    )\n",
        "                    combined_results.to_csv(self.results_file, sep='\\t', index=False)\n",
        "                    self.logger.info(f\"Saved {len(combined_results)} total results to file\")\n",
        "\n",
        "        # Final save\n",
        "        final_results = pd.concat(\n",
        "            [self.existing_results, pd.DataFrame(results)],\n",
        "            ignore_index=True\n",
        "        )\n",
        "        final_results.to_csv(self.results_file, sep='\\t', index=False)\n",
        "\n",
        "        return final_results\n",
        "\n",
        "def continue_enzyme_retrieval(unique_pairs: pd.DataFrame, existing_results_file: Path, start_ec: str):\n",
        "    \"\"\"Main function to continue enzyme data retrieval\"\"\"\n",
        "    retriever = ColabEnzymeRetriever(batch_size=100)\n",
        "\n",
        "    # Load existing results\n",
        "    retriever.load_existing_results(existing_results_file)\n",
        "\n",
        "    # Ensure input data is properly formatted\n",
        "    if isinstance(unique_pairs, str):\n",
        "        unique_pairs = pd.read_csv(unique_pairs, sep='\\t')\n",
        "    elif isinstance(unique_pairs, pd.DataFrame):\n",
        "        unique_pairs = unique_pairs.copy()\n",
        "    else:\n",
        "        raise ValueError(\"Input must be either a file path or a pandas DataFrame\")\n",
        "\n",
        "    # Validate and prepare input data\n",
        "    required_columns = ['EC', 'Genus']\n",
        "    if not all(col in unique_pairs.columns for col in required_columns):\n",
        "        raise ValueError(f\"Input data must contain columns: {required_columns}\")\n",
        "\n",
        "    unique_pairs['EC'] = unique_pairs['EC'].astype(str).apply(lambda x: f\"EC:{x.replace('EC:', '')}\")\n",
        "    unique_pairs['Genus'] = unique_pairs['Genus'].astype(str).str.strip()\n",
        "    unique_pairs = unique_pairs[['EC', 'Genus']].drop_duplicates()\n",
        "\n",
        "    # Process remaining pairs\n",
        "    results_df = retriever.process_remaining_pairs(unique_pairs, start_ec)\n",
        "\n",
        "    # Save final results\n",
        "    final_path = Path('uniprot_results_final.tsv')\n",
        "    results_df.to_csv(final_path, sep='\\t', index=False)\n",
        "\n",
        "    return results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mowtkQwJuvgT"
      },
      "outputs": [],
      "source": [
        "'''uniprot_results_path = Path('/content/drive/MyDrive/MIC/data_picrust/uniprot_results.tsv')\n",
        "# Usage (after uploading files to Colab), ECcontri_otu was made in colab because it was too big to upload after transformed\n",
        "results = continue_enzyme_retrieval(ECcontri_otu, uniprot_results_path, start_ec=\"x.3.1.12\" )'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nARoxMBKuvgT"
      },
      "source": [
        "## 8.5. Cleaning and Preparing Retrieved Data to integrate to ECContri"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9r_F4kMtuvgU"
      },
      "outputs": [],
      "source": [
        "'''df_1_path = Path(base_dir / \"uniprot_1_4_sorted.tsv\") # First file retrieved on first run 4 am\n",
        "df_2_path = Path(base_dir / \"uniprot_2_1.38_sorted.tsv\") # Same file retrieven when corrupted around 1:38 following day\n",
        "df_3_path = Path(base_dir / \"uniprot_3_sorted.tsv\") # Rerun done trying to get following EC numbers\n",
        "df_4_path = Path(base_dir / \"uniprot_4_missing_sorted.tsv\") # Final run in missing data\n",
        "\n",
        "df_1 = pd.read_csv(df_1_path, sep='\\t')\n",
        "df_2 = pd.read_csv(df_2_path, sep='\\t')\n",
        "df_3 = pd.read_csv(df_3_path, sep='\\t')\n",
        "df_4 = pd.read_csv(df_4_path, sep='\\t')\n",
        "print(len(df_1), len(df_2), len(df_3), len(df_4))'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9a2RbXR1uvgU"
      },
      "outputs": [],
      "source": [
        "'''# extract EC and Genus from the Retrieved files, so I need to join them first\n",
        "retrieved = pd.concat([df_1, df_2, df_3, df_4], axis = 0)\n",
        "unique_pairs = ECcontri_otu[['EC', 'Genus']].drop_duplicates()\n",
        "len(unique_pairs)'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FaAUFCMJdRn0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwbF_HFmuvgU"
      },
      "source": [
        "Extracting the Genus from the retrieved_pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jfKMErDWuvgU"
      },
      "outputs": [],
      "source": [
        "'''# Function to extract the Genus from the organism str\n",
        "def extract_genus(organism_str):\n",
        "    # Assumes Genus is the first word that starts with an uppercase letter.\n",
        "    match = re.search(r'([A-Z][a-z]+)', organism_str)\n",
        "    return match.group(1) if match else None\n",
        "# Creating a Genus column in the retrieved dataframe.\n",
        "retrieved['Genus'] = retrieved['organism'].astype(str).apply(extract_genus)\n",
        "\n",
        "# if there are duplicates, we want the best entry based on score:\n",
        "retrieved_unique = retrieved.sort_values('score', ascending=False)\\\n",
        "                            .drop_duplicates(subset=['ec_number', 'Genus'])'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yoQasxuauvgU"
      },
      "outputs": [],
      "source": [
        "'''# Merging using a left join on the two keys (EC_number and Genus). Plus an indicator of missing data.\n",
        "ECcontri_uniprot_info  = pd.merge(\n",
        "    ECcontri_otu,\n",
        "    retrieved_unique[['ec_number', 'Genus', 'protein_name', 'score', 'uniprot_id']],\n",
        "    left_on=['EC', 'Genus'],\n",
        "    right_on=['ec_number', 'Genus'],\n",
        "    how='left',\n",
        "    suffixes=('', '_retr')\n",
        ")\n",
        "print(ECcontri_uniprot_info.shape)'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''ECcontri_uniprot = ECcontri_uniprot_info.drop(columns = [\"OTU\",\t\"EC\",\t\"npath\", \"ec_number\",\t\"score\",\t\"uniprot_id\"])'''"
      ],
      "metadata": {
        "id": "elCz1V27O-E3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''ECcontri_uniprot.head()'''"
      ],
      "metadata": {
        "id": "ehKDu78fOzrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qA_nz2qFuvgU"
      },
      "source": [
        "ECcontri_uniprot_info is the final df mixed and is keep for reference only purposes. With the missing unique df I will retrive again the rest of the missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6iKlSuHuvgU"
      },
      "outputs": [],
      "source": [
        "'''# Rows with no match from retrieved_unique will have '_merge' value of 'left_only'\n",
        "merged_unique = pd.merge(\n",
        "    unique_pairs,\n",
        "    retrieved_unique,\n",
        "    left_on=['EC', 'Genus'],\n",
        "    right_on=['ec_number', 'Genus'],\n",
        "    how='left',\n",
        "    indicator=True\n",
        ")\n",
        "\n",
        "# Filter unique pairs missing from retrieved data\n",
        "ECcontri_missing = merged_unique[merged_unique['_merge'] == 'left_only']\n",
        "print(\"Missing unique pairs count:\", ECcontri_missing.shape[0])\n",
        "ECcontri_missing = ECcontri_missing[[\"EC\", \"Genus\"]]\n",
        "file_path = os.path.join(base_dir, \"ECcontri_missing.tsv\")\n",
        "ECcontri_missing.to_csv(file_path, sep='\\t', index=False)'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d2KfaHxuvgU"
      },
      "source": [
        "### Data Retrieval Completion Note\n",
        "After multiple retrieval attempts, 12,656 pairs remain unmapped out of approximately 1,500,000 total rows (0.84%). Given this small percentage and the diminishing returns from further retrieval attempts, we concluded that this level of completeness is acceptable for analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGlqF-P6h19n"
      },
      "outputs": [],
      "source": [
        "base_dir = Path(\"/content/drive/MyDrive/MIC/data_picrust\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KqSkLxPEuvgU"
      },
      "outputs": [],
      "source": [
        "'''file_path = os.path.join(base_dir, \"ECcontri_uniprot.tsv\")\n",
        "ECcontri_uniprot.to_csv(file_path, sep='\\t', index=False) # This file is too heavy and hence only run when needed, somehow no being save by drive'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QY8Rxk24uvgU"
      },
      "source": [
        "## 8.6. Making an Integrated Picrust Result df: genera_matrix\n",
        "the data is found now in a long format, for the next plotting we will be grouping it and making it onto a new df. We can group by Site and Genus (or even by protein_name) to look at the overall enzymatic contributions and how they correlate with the risk categories. The pathways information is actually very dense, it has some entries upto 300 pathway for site. So in this way we do a pathway chart but wont be bringing it to the next visualisations. The aim being to identify which genera or enzymes are most abundant in high-risk sites, assess if the presence of certain metal-specific enzymes (like Fe-dependent dehydrogenases) is linked to corrosion risk, ultimately potentially filter out common background organisms that are less relevant and perhaps just endemic part of the water systems in general and no specific to corrosion. So a table will be created with following information\n",
        "|Sites|Genus|protein_name|norm_abund_contri|*Category|\n",
        "|--|--|--|--|--|\n",
        "\n",
        "*where category will be utilised for colouring purposes.\n",
        "__Pivoting on Two Variables:__\n",
        "Using a pivot table with both Genus and protein_name as column levels is attempted in the next snipped, in order to capture contributions at that level. This will:\n",
        "- Generate a quantitative view of normalized abundance contributions per site\n",
        "- Maintain the hierarchical relationship between genera and their enzymes\n",
        "- Create a separate metabolic information matrix for pathway interpretation\n",
        "This structure allows to analyze both taxonomic patterns and specific enzyme contributions while maintaining the ability to link back to corrosion risk categories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3uc3CfGRnz9"
      },
      "outputs": [],
      "source": [
        "'''ECcontri_path = Path(base_dir / \"ECcontri_uniprot.tsv\")\n",
        "ECcontri_uniprot = pd.read_csv(ECcontri_path, sep='\\t')'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_4Fb88NuvgV"
      },
      "outputs": [],
      "source": [
        "# Define category dict outside so that all charts can use same dict\n",
        "category_dict = Integrated_T.T.iloc[0, 0:-1].to_dict()\n",
        "\n",
        "# Define colors and categories\n",
        "category_colors = {1: '#008800',  # Dark green\n",
        "                   2: '#FF8C00',  # Dark orange\n",
        "                   3: '#FF0000'}   # Red\n",
        "\n",
        "categories_labels = {1: 'Normal Operation',\n",
        "              2: 'Early Warning',\n",
        "              3: 'System Failure'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jkv-id-auvgV"
      },
      "outputs": [],
      "source": [
        "def create_structured_matrix_v2(df, category_dict=None):\n",
        "    \"\"\"\n",
        "    Creates a structured matrix with optional category mapping in the index.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : pandas.DataFrame\n",
        "        Input DataFrame with required columns\n",
        "    category_dict : dict, optional\n",
        "        Dictionary mapping sites to categories\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    base_matrix : pandas.DataFrame\n",
        "        Pivot table with Sites (and optionally Category) as index and\n",
        "        (Genus, protein_name) as column multi-index\n",
        "    \"\"\"\n",
        "    # Validate inputs\n",
        "    required_cols = {'Sites', 'Genus', 'protein_name', 'norm_abund_contri'}\n",
        "    if not required_cols.issubset(df.columns):\n",
        "        raise ValueError(f\"Missing required columns: {required_cols - set(df.columns)}\")\n",
        "\n",
        "    # Natural sort site names\n",
        "    sorted_sites = natsorted(df['Sites'].unique())\n",
        "    df['Sites'] = pd.Categorical(df['Sites'], categories=sorted_sites, ordered=True)\n",
        "\n",
        "    # Create base abundance matrix with multi-index for columns\n",
        "    base_matrix = df.pivot_table(\n",
        "        values='norm_abund_contri',\n",
        "        index='Sites',\n",
        "        columns=['Genus', 'protein_name'],\n",
        "        aggfunc='sum',\n",
        "        fill_value=0,\n",
        "        observed=False\n",
        "    )\n",
        "\n",
        "    # To add categories to base_matrix for plotting\n",
        "    if category_dict is not None:\n",
        "        # Map categories to index\n",
        "        category_mapping= pd.Series(base_matrix.index.map(category_dict),\n",
        "            index=base_matrix.index, name='Category')\n",
        "\n",
        "        # Modifying the index directly\n",
        "        base_matrix.index = pd.MultiIndex.from_arrays([base_matrix.index, category_mapping],\n",
        "                            names=['Sites', 'Category'])\n",
        "\n",
        "    return base_matrix, category_mapping"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''# Calling the function\n",
        "base_matrix, category_mapping = create_structured_matrix_v2(ECcontri_uniprot, category_dict)'''"
      ],
      "metadata": {
        "id": "EudW9WXbKN7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''file_path = os.path.join(base_dir, \"base_matrix_v2.tsv\")\n",
        "\n",
        "base_matrix.to_csv(file_path, sep='\\t', index=False) # This file is too heavy and hence run once'''"
      ],
      "metadata": {
        "id": "ZTZ6nyeuWoLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def metabolic_sites_info(df):\n",
        "    \"\"\"\n",
        "    Create metabolic information DataFrame with site-specific aggregation.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : pandas.DataFrame\n",
        "        Input DataFrame with 'Sites', 'Genus', 'pathway', 'protein_name' columns\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    pandas.DataFrame\n",
        "        Aggregated metabolic information with sites preserved\n",
        "    \"\"\"\n",
        "    def safe_join(x):\n",
        "        return ', '.join(sorted(set(x.dropna().astype(str))))\n",
        "\n",
        "    # Group by both Sites and Genus to preserve site information\n",
        "    metabolic_info = df.groupby(['Sites', 'Genus']).agg({\n",
        "        'pathway': safe_join,\n",
        "        'protein_name': safe_join,\n",
        "        'norm_abund_contri': 'sum'  # Add abundance information\n",
        "    }).rename(columns={\n",
        "        'pathway': 'Pathways',\n",
        "        'protein_name': 'Protein_Names',\n",
        "        'norm_abund_contri': 'Total_Abundance'\n",
        "    })\n",
        "\n",
        "    return metabolic_info"
      ],
      "metadata": {
        "id": "EQAqPzKuIyhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''metabolic_sites_info= metabolic_sites_info(ECcontri_uniprot)\n",
        "# Saving just in case\n",
        "file_path = os.path.join(base_dir, \"metabolic_sites_info.tsv\")\n",
        "metabolic_sites_info.to_csv(file_path, sep='\\t', index=False)'''"
      ],
      "metadata": {
        "id": "jPA8kjB2IENR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''# Debug your data structure\n",
        "print(\"Base matrix shape:\", base_matrix.shape)\n",
        "print(\"Column levels:\", base_matrix.columns.names)\n",
        "print(\"Index levels:\", base_matrix.index.names)\n",
        "print(\"Sample of first few columns:\\n\", base_matrix.iloc[:, :3])'''"
      ],
      "metadata": {
        "id": "oozjsMtoLEJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.7. Chunking\n",
        "The local machine struggled to read the file and killed the kernel, so Colab was continued to be used, however the high memory availbable it was struggling with memory fragmentation, it is believed the complexity of the dataa make the problem, chucking is promise to improve the performance"
      ],
      "metadata": {
        "id": "lUmCgiWWAx9A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import psutil\n",
        "import os\n",
        "\n",
        "def detailed_memory_check():\n",
        "    # Get memory info\n",
        "    mem = psutil.virtual_memory()\n",
        "\n",
        "    # Get process memory info\n",
        "    process = psutil.Process(os.getpid())\n",
        "    process_mem = process.memory_info()\n",
        "\n",
        "    print(\"System Memory Details:\")\n",
        "    print(f\"Total: {mem.total/1024**3:.2f} GB\")\n",
        "    print(f\"Available: {mem.available/1024**3:.2f} GB\")\n",
        "    print(f\"Used: {mem.used/1024**3:.2f} GB\")\n",
        "    print(f\"Free: {mem.free/1024**3:.2f} GB\")\n",
        "    print(f\"Percent used: {mem.percent}%\")\n",
        "\n",
        "    print(\"\\nProcess Memory Details:\")\n",
        "    print(f\"RSS (Physical): {process_mem.rss/1024**3:.2f} GB\")\n",
        "    print(f\"VMS (Virtual): {process_mem.vms/1024**3:.2f} GB\")\n",
        "\n",
        "detailed_memory_check()"
      ],
      "metadata": {
        "id": "pw5qVLUcOuTl",
        "outputId": "9daaee19-e50f-4f3d-badc-338c9c393471",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "System Memory Details:\n",
            "Total: 50.99 GB\n",
            "Available: 48.02 GB\n",
            "Used: 2.38 GB\n",
            "Free: 42.31 GB\n",
            "Percent used: 5.8%\n",
            "\n",
            "Process Memory Details:\n",
            "RSS (Physical): 1.31 GB\n",
            "VMS (Virtual): 6.86 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_large_matrix_in_chunks(filepath, chunksize=5000):\n",
        "    \"\"\"Read large TSV with self-monitoring\"\"\"\n",
        "    try:\n",
        "        # Initial check\n",
        "        mem = psutil.virtual_memory()\n",
        "        print(f\"Starting memory: {mem.used/1024**3:.2f} GB used\")\n",
        "\n",
        "        # Read the entire file in chunks and concatenate once\n",
        "        df = pd.concat(\n",
        "            pd.read_csv(filepath, sep='\\t', chunksize=chunksize),\n",
        "            ignore_index=True\n",
        "        )\n",
        "\n",
        "        # Final memory check\n",
        "        mem = psutil.virtual_memory()\n",
        "        print(f\"Final memory usage: {mem.used/1024**3:.2f} GB used\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {str(e)}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "hMcpq9EwBR1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = Path(\"/content/drive/MyDrive/MIC/data_picrust\")\n",
        "file_path = os.path.join(base_dir,  \"base_matrix_v2.tsv\")\n",
        "base_matrix = read_large_matrix_in_chunks(file_path, chunksize=5000)"
      ],
      "metadata": {
        "id": "evJARsRPDRMr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "outputId": "a3442eeb-bd7e-46aa-8687-a4e3e66fd04f"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting memory: 3.02 GB used\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-75-d6025fd85005>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;34m\"base_matrix_v2.tsv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mbase_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_large_matrix_in_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-72-65c0cbd3bd69>\u001b[0m in \u001b[0;36mprocess_large_matrix_in_chunks\u001b[0;34m(filepath, chunksize)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Store results temporarily if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0;31m# Perform processing on each chunk here (modify as needed)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mchunk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'new_col'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m  \u001b[0;31m# Example operation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1897\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1898\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1899\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1900\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;31m# Fail here loudly instead of in cython after reading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mimport_optional_dependency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pyarrow\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metabolic_info_path = os.path.join(base_dir,  \"metabolic_sites_info.tsv\")\n",
        "metabolic_info = read_large_matrix_in_chunks(metabolic_info_path, chunksize=5000)"
      ],
      "metadata": {
        "id": "WvksgYbZCTjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WNqbpOc6bn99"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading it when starting from scratch\n",
        "def read_and_process_matrix(filepath, category_dict, chunksize=5000):\n",
        "    \"\"\"\n",
        "    Read and process the matrix while maintaining proper structure\n",
        "    \"\"\"\n",
        "    chunks = []\n",
        "    try:\n",
        "        for chunk in pd.read_csv(filepath, sep='\\t', chunksize=chunksize):\n",
        "            chunks.append(chunk)\n",
        "\n",
        "        df = pd.concat(chunks)\n",
        "        base_matrix, metabolic_info, category_mapping = create_structured_matrix_v2(\n",
        "            df, category_dict\n",
        "        )\n",
        "\n",
        "        # Process with categories if needed\n",
        "        if category_mapping is not None:\n",
        "            processed_matrix = process_with_categories(base_matrix, category_mapping)\n",
        "        else:\n",
        "            processed_matrix = base_matrix\n",
        "\n",
        "        return processed_matrix, metabolic_info\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during processing: {str(e)}\")\n",
        "        return None, None"
      ],
      "metadata": {
        "id": "v5C4Zy4lCjpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsyxkAFuuvgV"
      },
      "source": [
        "# 9. Analysign the Dominant Protein Enzymes, Pathways and Genes with the Principal Component Loadings\n",
        "Following script analyse the dominant Protein Enzymes, Pathways and Genes contributing to the first two PCs comming from section 7.1. The risk label is use here to color code the hue.\n",
        "## 9.1. Principal Components of Genera vs Risk Category"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_genera_pca(base_matrix, category_mapping=None):\n",
        "    \"\"\"\n",
        "    Prepare genera data for PCA with handling of multi-index categories\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    base_matrix : pandas.DataFrame,   Matrix with potential multi-index (Sites, Category)\n",
        "    category_mapping : pandas.Series,  Category mapping\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    X_pca : numpy.ndarray,  PCA transformed data\n",
        "    explained_variance_ratio : numpy.ndarray  Explained variance ratios\n",
        "    loadings : pandas.DataFrame,     PCA loadings\n",
        "    categories : pandas.Series,  Categories for each site\n",
        "    \"\"\"\n",
        "    # Extract categories if they're in the multi-index\n",
        "    if isinstance(base_matrix.index, pd.MultiIndex):\n",
        "        categories = base_matrix.index.get_level_values('Category')\n",
        "        # No need to drop category as it's in the index\n",
        "        X = base_matrix\n",
        "    else:\n",
        "        # Use provided category mapping or None\n",
        "        categories = category_mapping\n",
        "        X = base_matrix\n",
        "\n",
        "    # No need for iloc[1:] as we don't have enzyme names as first row anymore\n",
        "    X_for_scaling = X.astype(float)\n",
        "\n",
        "    # Standardize\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X_for_scaling)\n",
        "\n",
        "    # PCA\n",
        "    pca = PCA(n_components=2)\n",
        "    X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "    # Create loadings DataFrame with proper multi-index columns\n",
        "    loadings = pd.DataFrame(\n",
        "        pca.components_.T,\n",
        "        index=X.columns,  # This will preserve the multi-index columns (Genus, protein_name)\n",
        "        columns=['PC1', 'PC2']\n",
        "    )\n",
        "\n",
        "    return X_pca, pca.explained_variance_ratio_, loadings, categories"
      ],
      "metadata": {
        "id": "IorbqV1wZ4ng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For genera PCA\n",
        "X_pca_genera, var_ratio_genera, loadings_genera, categories = prepare_genera_pca(base_matrix, category_mapping)"
      ],
      "metadata": {
        "id": "7gaitkAh9lxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_pca_results(X_pca, explained_variance, Category, title, category_colors, categories_labels,\n",
        "                     pc1_idx=0, pc2_idx=1):  # Add parameters for component indices\n",
        "    \"\"\"\n",
        "    Plot PCA with risk categories\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    X_pca : numpy array   PCA transformed data\n",
        "    explained_variance : numpy array        Explained variance ratios\n",
        "    Category : array-like  Category labels for each sample\n",
        "    title : str   Plot title\n",
        "    category_colors : dict  Mapping of categories to colors\n",
        "    categories_labels : dict  Mapping of categories to display labels\n",
        "    pc1_idx : int        Index of the first PC to plot (default 0 for PC1)\n",
        "    pc2_idx : int        Index of the second PC to plot (default 1 for PC2)\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 8))\n",
        "\n",
        "    # Plot using specified components\n",
        "    for category in sorted(set(Category)):\n",
        "        mask = Category == category\n",
        "        plt.scatter(\n",
        "            X_pca[mask, pc1_idx],  # Specified PC for x-axis\n",
        "            X_pca[mask, pc2_idx],  # Specified PC for y-axis\n",
        "            c=category_colors[category],\n",
        "            label=categories_labels[category],\n",
        "            alpha=0.7,\n",
        "            s=100\n",
        "        )\n",
        "\n",
        "    plt.xlabel(f'PC{pc1_idx+1} ({explained_variance[pc1_idx]:.1%} variance explained)')\n",
        "    plt.ylabel(f'PC{pc2_idx+1} ({explained_variance[pc2_idx]:.1%} variance explained)')\n",
        "    plt.title(title)\n",
        "    plt.legend(title='Risk Category')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "oc6RFLbT-CCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_pca_results(X_pca_genera, var_ratio_genera, categories.values, \"Genera PCA by Risk Category\", category_colors, categories_labels)"
      ],
      "metadata": {
        "id": "Ohn5kKDk97sv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.2. Principal Components Protein vs Site by Risk Category"
      ],
      "metadata": {
        "id": "TXL_SqsMM_4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_flexible_pca(data_matrix, categories=None, n_components=None):\n",
        "    \"\"\"\n",
        "    Prepare PCA with flexible number of components\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    data_matrix : pandas DataFrame   Input data with samples as rows and features as columns\n",
        "    categories : array-like,   Category labels for each sample\n",
        "    n_components : int,   Number of components to calculate (None for all possible)\n",
        "    n_plot : int   Number of components to return for plotting\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    X_pca : numpy array   PCA transformed data (first n_plot components)\n",
        "    explained_variance : numpy array  Explained variance ratios for all components\n",
        "    loadings : pandas DataFrame  PCA loadings with feature names as index\n",
        "    \"\"\"\n",
        "    # Standardize\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(data_matrix)\n",
        "\n",
        "    # PCA\n",
        "    pca = PCA(n_components=n_components)\n",
        "    X_pca_full = pca.fit_transform(X_scaled)\n",
        "\n",
        "    # Get loadings for all components\n",
        "    loadings = pd.DataFrame(\n",
        "        pca.components_.T,\n",
        "        index=data_matrix.columns,\n",
        "        columns=[f'PC{i+1}' for i in range(pca.n_components_)]\n",
        "    )\n",
        "\n",
        "    # Return only requested components for plotting\n",
        "    X_pca = X_pca_full\n",
        "\n",
        "    # Return all calculated components\n",
        "    return X_pca_full, pca.explained_variance_ratio_, loadings"
      ],
      "metadata": {
        "id": "FUe9thnSe7cf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate PCA with all components\n",
        "X_pca_all, var_ratio_all, loadings_all = prepare_flexible_pca(base_matrix)\n",
        "\n",
        "# Plot different component combinations\n",
        "# PC1 vs PC2 (default)\n",
        "plot_pca_results(X_pca_all, var_ratio_all, categories.values, \"PC1 vs PC2\",\n",
        "                 category_colors, categories_labels)\n",
        "\n",
        "# PC2 vs PC3\n",
        "plot_pca_results(X_pca_all, var_ratio_all, categories.values, \"PC2 vs PC3\",\n",
        "                 category_colors, categories_labels,\n",
        "                 pc1_idx=1, pc2_idx=2)\n",
        "\n",
        "# PC3 vs PC4\n",
        "plot_pca_results(X_pca_all, var_ratio_all, categories.values, \"PC3 vs PC4\",\n",
        "                 category_colors, categories_labels,\n",
        "                 pc1_idx=2, pc2_idx=3)"
      ],
      "metadata": {
        "id": "WeLKg5XIzKbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.4. Pathways distribution by Risk Category"
      ],
      "metadata": {
        "id": "P7vGk-dvJxXL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_pathway_pca(metabolic_info):\n",
        "    \"\"\"\n",
        "    Convert pathway strings to numeric features for PCA\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    metabolic_info : pandas.DataFrame,   DataFrame with 'Pathways' column containing comma-separated pathway strings\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    X_pca : numpy.ndarray,    PCA transformed data\n",
        "    explained_variance_ratio : numpy.ndarray,     Explained variance ratios\n",
        "    loadings : pandas.DataFrame,   PCA loadings with pathway names as index\n",
        "    pathway_matrix : pandas.Dataframe,  Binary matrix of pathway presence/absence (useful for further analysis)\n",
        "    \"\"\"\n",
        "    # Create set of unique pathways (more efficiently)\n",
        "    all_pathways = set()\n",
        "    for pathways_str in metabolic_info['Pathways'].dropna():\n",
        "        all_pathways.update(path.strip() for path in pathways_str.split(','))\n",
        "\n",
        "    # Create binary matrix more efficiently (avoid double loop)\n",
        "    pathway_data = {}\n",
        "    for pathway in all_pathways:\n",
        "        # Use escaped pathway string for exact matches\n",
        "        pathway_escaped = re.escape(pathway)\n",
        "        pathway_data[pathway] = metabolic_info['Pathways'].str.contains(\n",
        "            pathway_escaped,\n",
        "            regex=True,\n",
        "            na=False  # Handle NaN values\n",
        "        ).astype(int)\n",
        "\n",
        "    pathway_matrix = pd.DataFrame(pathway_data, index=metabolic_info.index)\n",
        "\n",
        "    # Run PCA\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(pathway_matrix)\n",
        "    pca = PCA(n_components=2)\n",
        "    X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "    # Create loadings with pathway names\n",
        "    loadings = pd.DataFrame(pca.components_.T, index=pathway_matrix.columns,\n",
        "                                columns=['PC1', 'PC2'])\n",
        "\n",
        "    return X_pca, pca.explained_variance_ratio_, loadings, pathway_matrix\n",
        "\n",
        "def plot_metabolic_pca_results(X_pca, explained_variance, metabolic_sites_info, category_dict, title, category_colors, categories_labels):\n",
        "    \"\"\"\n",
        "    Plot PCA results for pathways with risk categories\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    X_pca : numpy array  PCA transformed coordinates\n",
        "    explained_variance : numpy array   Explained variance ratios\n",
        "    metabolic_info : pandas DataFrame   The metabolic info DataFrame with Sites index\n",
        "    category_dict : dict     Mapping of sites to categories\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 8))\n",
        "\n",
        "    # Get categories for each site in metabolic_info\n",
        "    if isinstance(metabolic_sites_info.index, pd.MultiIndex):\n",
        "        sites = metabolic_sites_info.index.get_level_values('Sites')\n",
        "    else:\n",
        "        sites = metabolic_sites_info.index\n",
        "\n",
        "    plot_categories = pd.Series(sites).map(category_dict)\n",
        "\n",
        "    # Plot each category\n",
        "    for category in sorted(set(plot_categories)):\n",
        "        mask = plot_categories == category\n",
        "        plt.scatter( X_pca[mask, 0], X_pca[mask, 1], c=category_colors[category],\n",
        "            label=categories_labels[category], alpha=0.7, s=100)\n",
        "\n",
        "    plt.xlabel(f'PC1 ({explained_variance[0]:.1%} variance explained)')\n",
        "    plt.ylabel(f'PC2 ({explained_variance[1]:.1%} variance explained)')\n",
        "    plt.title(title)\n",
        "    plt.legend(title='Risk Category')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "zUIVY1O2JvoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For pathway PCA\n",
        "X_pca_path, var_ratio_path, loadings_path, pathway_matrix = prepare_pathway_pca(metabolic_sites_info)\n",
        "\n",
        "plot_metabolic_pca_results( X_pca_path, var_ratio_path,  metabolic_sites_info, category_dict, \"Pathways PCA by Risk Category\", category_colors, categories_labels)"
      ],
      "metadata": {
        "id": "imLbgaY4KBCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.4 Principal Component of Protein by Risk Category"
      ],
      "metadata": {
        "id": "jeKymKOe5A3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First, let's verify your data\n",
        "print(\"Number of samples per category in metabolic_sites_info:\")\n",
        "print(metabolic_sites_info.index.get_level_values('Sites').map(category_dict).value_counts())\n",
        "\n",
        "# Check protein data structure\n",
        "print(\"\\nShape of protein_matrix:\")\n",
        "print(protein_matrix.shape)\n",
        "\n",
        "# Verify the mapping\n",
        "print(\"\\nUnique categories in plot:\")\n",
        "plot_categories = pd.Series(metabolic_sites_info.index.get_level_values('Sites')).map(category_dict)\n",
        "print(plot_categories.value_counts())"
      ],
      "metadata": {
        "id": "YZvYbY2GA_QX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_protein_pca(metabolic_info, category_dict):\n",
        "    \"\"\"\n",
        "    Convert protein strings to numeric features for PCA, aggregating by site first\n",
        "    \"\"\"\n",
        "    # First aggregate by Sites\n",
        "    site_protein_data = metabolic_info.groupby('Sites').agg({\n",
        "        'Protein_Names': lambda x: ', '.join(x.dropna())\n",
        "    })\n",
        "\n",
        "    # Create set of unique proteins\n",
        "    all_proteins = set()\n",
        "    for proteins_str in site_protein_data['Protein_Names'].dropna():\n",
        "        proteins = [p.strip() for p in proteins_str.split(',')]\n",
        "        all_proteins.update(proteins)\n",
        "\n",
        "    # Create binary matrix at site level\n",
        "    protein_data = {}\n",
        "    for protein in all_proteins:\n",
        "        if protein:\n",
        "            protein_escaped = re.escape(protein)\n",
        "            protein_data[protein] = site_protein_data['Protein_Names'].str.contains(\n",
        "                protein_escaped,\n",
        "                regex=True,\n",
        "                na=False\n",
        "            ).astype(int)\n",
        "\n",
        "    protein_matrix = pd.DataFrame(protein_data, index=site_protein_data.index)\n",
        "\n",
        "    # Run PCA\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(protein_matrix)\n",
        "    pca = PCA(n_components=2)\n",
        "    X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "    loadings = pd.DataFrame(\n",
        "        pca.components_.T,\n",
        "        index=protein_matrix.columns,\n",
        "        columns=['PC1', 'PC2']\n",
        "    )\n",
        "\n",
        "    return X_pca, pca.explained_variance_ratio_, loadings, protein_matrix"
      ],
      "metadata": {
        "id": "ol9qvarP5jRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For protein PCA # 4 min\n",
        "X_pca_protein, var_ratio_protein, loadings_protein, protein_matrix = prepare_protein_pca(metabolic_sites_info, category_dict)"
      ],
      "metadata": {
        "id": "f1b-QSBy5qkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_pca_results(X_pca_protein, var_ratio_protein, categories.values, \"Protein PCA by Risk Category\", category_colors, categories_labels)"
      ],
      "metadata": {
        "id": "_LypSvJKGSdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_protein_loadings(loadings, top_n=20):\n",
        "    \"\"\"\n",
        "    Analyze protein loadings to find most influential proteins\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    loadings : pandas DataFrame\n",
        "        PCA loadings with proteins as index\n",
        "    top_n : int\n",
        "        Number of top proteins to return\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dict with top proteins for each PC and their contributions\n",
        "    \"\"\"\n",
        "    # Calculate magnitude of contribution for each protein\n",
        "    loadings['magnitude'] = np.sqrt(loadings['PC1']**2 + loadings['PC2']**2)\n",
        "\n",
        "    # Get top contributors overall\n",
        "    top_overall = loadings.nlargest(top_n, 'magnitude')\n",
        "\n",
        "    # Get top contributors for each PC\n",
        "    top_pc1_pos = loadings.nlargest(top_n, 'PC1')\n",
        "    top_pc1_neg = loadings.nsmallest(top_n, 'PC1')\n",
        "    top_pc2_pos = loadings.nlargest(top_n, 'PC2')\n",
        "    top_pc2_neg = loadings.nsmallest(top_n, 'PC2')\n",
        "\n",
        "    return {\n",
        "        'top_overall': top_overall,\n",
        "        'top_pc1_positive': top_pc1_pos,\n",
        "        'top_pc1_negative': top_pc1_neg,\n",
        "        'top_pc2_positive': top_pc2_pos,\n",
        "        'top_pc2_negative': top_pc2_neg\n",
        "    }\n"
      ],
      "metadata": {
        "id": "0L5F06YaEBiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Use after running PCA:\n",
        "X_pca_protein, var_ratio_protein, loadings_protein, protein_matrix = prepare_protein_pca(metabolic_sites_info, category_dict)\n",
        "loading_analysis = analyze_protein_loadings(loadings_protein)\n",
        "\n",
        "# Print top contributors\n",
        "print(\"Top 20 proteins contributing to separation:\")\n",
        "print(loading_analysis['top_overall'])"
      ],
      "metadata": {
        "id": "AfnmLYJUFbB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oIFI96kf5Riq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_metabolic_pca(metabolic_info, category_dict, n_components=7, n_plot=2):\n",
        "    \"\"\"\n",
        "    Prepare PCA for metabolic_sites_info data to capture the exponential pattern\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    metabolic_info : pandas DataFrame\n",
        "        The metabolic_sites_info DataFrame\n",
        "    category_dict : dict\n",
        "        Mapping of sites to categories\n",
        "    n_components : int\n",
        "        Number of components to calculate\n",
        "    n_plot : int\n",
        "        Number of components to return for plotting\n",
        "    \"\"\"\n",
        "    # Prepare the data matrix using Total_Abundance\n",
        "    X = metabolic_info.pivot_table(\n",
        "        values='Total_Abundance',\n",
        "        index='Sites',\n",
        "        columns='Genus',\n",
        "        fill_value=0\n",
        "    )\n",
        "\n",
        "    # Get categories for sites\n",
        "    categories = pd.Series(X.index).map(category_dict)\n",
        "\n",
        "    # Run PCA\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    pca = PCA(n_components=n_components)\n",
        "    X_pca_full = pca.fit_transform(X_scaled)\n",
        "\n",
        "    # Get loadings\n",
        "    loadings = pd.DataFrame(\n",
        "        pca.components_.T,\n",
        "        index=X.columns,\n",
        "        columns=[f'PC{i+1}' for i in range(pca.n_components_)]\n",
        "    )\n",
        "\n",
        "    # Return only requested components for plotting\n",
        "    X_pca = X_pca_full[:, :n_plot]\n",
        "\n",
        "    return X_pca, pca.explained_variance_ratio_, loadings, categories"
      ],
      "metadata": {
        "id": "sRYLunoQ-y5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calling the function\n",
        "X_pca_met, var_ratio_met, loadings_met, categories_met = prepare_metabolic_pca(metabolic_sites_info, category_dict)\n",
        "\n",
        "# Plot to see the exponential pattern\n",
        "plot_pca_results(X_pca_met, var_ratio_met, categories_met, \"Metabolic PCA by Risk Category\",  category_colors, categories_labels)"
      ],
      "metadata": {
        "id": "yzr4zLAI-5Km"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def identify_outliers(X_pca, base_matrix):\n",
        "    \"\"\"\n",
        "    Identify potential outliers in PCA space\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    X_pca : numpy array        PCA transformed data\n",
        "    base_matrix : pandas DataFrame        Original data with Sites in index\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    pandas Series        5 most extreme samples with their distances\n",
        "    \"\"\"\n",
        "    # Get site names from base_matrix index\n",
        "    sites = base_matrix.index.get_level_values('Sites')\n",
        "\n",
        "    # Calculate distances from origin in PCA space\n",
        "    distances = np.sqrt(np.sum(X_pca**2, axis=1))\n",
        "\n",
        "    # Create Series with sites as index\n",
        "    outliers = pd.Series(distances, index=sites)\n",
        "\n",
        "    return outliers.nlargest(5)  # Return 5 most extreme samples"
      ],
      "metadata": {
        "id": "k0oorAB98DCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outliers= identify_outliers(X_pca_protein, base_matrix)\n",
        "outliers"
      ],
      "metadata": {
        "id": "rDcWpki08Ehi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_pca_loadings_heatmap(loadings, top_n=20):\n",
        "    \"\"\"Plot a heatmap of pathway loadings for PC1 and PC2.\n",
        "       Parameters:     loadings: DataFrame with PCA loadings\n",
        "       top_n: Number of top pathways to display     \"\"\"\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    # Select top pathways based on absolute contribution to PC1 and PC2\n",
        "    top_pathways = (loadings[['PC1', 'PC2']].abs().sum(axis=1).nlargest(top_n).index)\n",
        "    # Filter the loadings dataframe\n",
        "    heatmap_data = loadings.loc[top_pathways, ['PC1', 'PC2']]\n",
        "    sns.heatmap(heatmap_data, annot=True, cmap='coolwarm', center=0)\n",
        "    plt.title('Top Pathway Contributions to PC1 and PC2')\n",
        "    plt.xlabel('Principal Components')\n",
        "    plt.ylabel('Pathways')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "_N6sL465a9H8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oofXwQnF_ZUZ"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_pca_loadings_heatmap(loadings_genera)"
      ],
      "metadata": {
        "id": "5vuPlFa_uJOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.4. Clustering with UMAP"
      ],
      "metadata": {
        "id": "mWpMLswkOfzy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from umap import UMAP\n",
        "\n",
        "def prepare_umap_analysis(data_matrix, n_neighbors=5, min_dist=0.05, scale=True):\n",
        "    \"\"\"\n",
        "    UMAP with customizable parameters for better pattern detection\n",
        "    \"\"\"\n",
        "    if scale:\n",
        "        scaler = StandardScaler()\n",
        "        X = scaler.fit_transform(data_matrix)\n",
        "    else:\n",
        "        X = data_matrix\n",
        "\n",
        "    umap = UMAP(n_neighbors=n_neighbors,\n",
        "                min_dist=min_dist,\n",
        "                random_state=42)\n",
        "    X_umap = umap.fit_transform(X)\n",
        "\n",
        "    return X_umap"
      ],
      "metadata": {
        "id": "jgKrPKZVOdcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_umap = prepare_umap_analysis(base_matrix, n_neighbors=5, min_dist=0.2, scale=True)"
      ],
      "metadata": {
        "id": "pmEGKUmQrX0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_pca_results(X_umap, var_ratio_genera, categories.values, \"Genera PCA UMAP by Risk Category\", category_colors, categories_labels)"
      ],
      "metadata": {
        "id": "_Pds9-5Qrkp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.5. Dimension Reduction Comparison"
      ],
      "metadata": {
        "id": "Duh-WakEPdkf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_dimension_reduction_comparison(X_pca, X_umap, categories, category_colors, category_labels):\n",
        "    \"\"\"\n",
        "    Side-by-side comparison of PCA and UMAP\n",
        "    \"\"\"\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "    # PCA plot\n",
        "    for category in sorted(set(categories)):\n",
        "        mask = categories == category\n",
        "        ax1.scatter(X_pca[mask, 0], X_pca[mask, 1],\n",
        "                   c=category_colors[category],\n",
        "                   label=category_labels[category],\n",
        "                   alpha=0.7)\n",
        "    ax1.set_title('PCA')\n",
        "    ax1.legend()\n",
        "\n",
        "    # UMAP plot\n",
        "    for category in sorted(set(categories)):\n",
        "        mask = categories == category\n",
        "        ax2.scatter(X_umap[mask, 0], X_umap[mask, 1],\n",
        "                   c=category_colors[category],\n",
        "                   label=category_labels[category],\n",
        "                   alpha=0.7)\n",
        "    ax2.set_title('UMAP')\n",
        "    ax2.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "QXb4MnquPeKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_dimension_reduction_comparison(X_pca_genera, X_umap, categories, category_colors, categories_labels)"
      ],
      "metadata": {
        "id": "b-vgkBdr2CGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.7."
      ],
      "metadata": {
        "id": "PRVw3xmDhITd"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXRurjWFuvgW"
      },
      "source": [
        "## 9.2 Integrated Visualitations combining PCA, Clustering and Metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWCQOr4KuvgW"
      },
      "outputs": [],
      "source": [
        "def create_integrated_visualization(df, results, metadata=None):\n",
        "    \"\"\"\n",
        "    Create an integrated visualization combining PCA, clustering, and metadata\n",
        "\n",
        "    Parameters:\n",
        "    df: Original pathway data\n",
        "    results: Results from explore_pathway_patterns\n",
        "    metadata: DataFrame with risk labels, materials, etc.\n",
        "    \"\"\"\n",
        "    fig = plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # 1. PCA with clustering\n",
        "    pca_data = results['pca']['components']\n",
        "    clusters = results['clustering'][5]['kmeans']  # Using k=5 clusters\n",
        "\n",
        "    plt.subplot(2, 2, 1)\n",
        "    scatter = plt.scatter(pca_data[:, 0], pca_data[:, 1],\n",
        "                         c=clusters, cmap='Set2', alpha=0.6)\n",
        "    plt.title('PCA Components with Clusters')\n",
        "    plt.xlabel('PC1')\n",
        "    plt.ylabel('PC2')\n",
        "    plt.colorbar(scatter, label='Cluster')\n",
        "\n",
        "    # 2. Top pathway contributions\n",
        "    plt.subplot(2, 2, 2)\n",
        "    top_loadings = abs(results['pca']['loadings']['PC1']).nlargest(10)\n",
        "    sns.barplot(x=top_loadings.values, y=top_loadings.index)\n",
        "    plt.title('Top 10 Pathways Contributing to PC1')\n",
        "    plt.xlabel('Absolute Loading')\n",
        "\n",
        "    # 3. Correlation structure summary\n",
        "    plt.subplot(2, 2, 3)\n",
        "    corr_summary = results['correlation'].abs().mean()\n",
        "    sns.histplot(corr_summary, bins=50)\n",
        "    plt.title('Distribution of Mean Correlation Strengths')\n",
        "    plt.xlabel('Mean |Correlation|')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZvr6vjWuvgW"
      },
      "outputs": [],
      "source": [
        "create_integrated_visualization(base_matrix, results_patterns, metadata=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bk9_ZGhfuvgW"
      },
      "source": [
        "## 9.3 Analysing Pathways Organic Fate\n",
        "\n",
        "Now the task is to identify the most abundant pathways in the samples, focusing specifically on organic matter-related metabolism. Ultimately creating visualizations to understand pathway distributions and analyze correlations between pathways."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11HYL0iNuvgW"
      },
      "outputs": [],
      "source": [
        "def analyze_metabolic_pathways(df):\n",
        "    \"\"\"\n",
        "    Analyze metabolic pathways from PICRUSt output\n",
        "\n",
        "    Parameters:\n",
        "    df: pandas DataFrame with pathways as index and samples as columns\n",
        "    \"\"\"\n",
        "    # Calculate mean abundance across samples for each pathway\n",
        "    mean_abundance = df.mean(axis=1).sort_values(ascending=False)\n",
        "\n",
        "    # Get top 20 most abundant pathways\n",
        "    top_pathways = mean_abundance.head(20)\n",
        "\n",
        "    # Create heatmap of top pathways across samples\n",
        "    plt.figure(figsize=(15, 8))\n",
        "    sns.heatmap(df.loc[top_pathways.index],\n",
        "                cmap='YlOrRd',\n",
        "                center=0,\n",
        "                robust=True,\n",
        "                xticklabels=True,\n",
        "                yticklabels=True)\n",
        "    plt.title('Top 20 Most Abundant Pathways Across Samples')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Filter for organic matter metabolism related pathways\n",
        "    organic_terms = ['carbon', 'carbohydrate', 'lipid', 'fatty acid',\n",
        "                    'organic acid', 'amino acid', 'degradation']\n",
        "\n",
        "    organic_pathways = df.index[df.index.str.lower().str.contains('|'.join(organic_terms))]\n",
        "    organic_data = df.loc[organic_pathways]\n",
        "\n",
        "    # Calculate summary statistics for organic matter pathways\n",
        "    pathway_stats = pd.DataFrame({\n",
        "        'mean_abundance': organic_data.mean(axis=1),\n",
        "        'std_abundance': organic_data.std(axis=1),\n",
        "        'cv': organic_data.std(axis=1) / organic_data.mean(axis=1) * 100\n",
        "    }).sort_values('mean_abundance', ascending=False)\n",
        "\n",
        "    return pathway_stats, organic_data\n",
        "\n",
        "def plot_pathway_distribution(pathway_stats):\n",
        "    \"\"\"Plot distribution of pathway abundances\"\"\"\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.boxplot(data=pathway_stats.reset_index(),\n",
        "                x='mean_abundance',\n",
        "                y='index',\n",
        "                order=pathway_stats.index[:15])\n",
        "    plt.title('Top 15 Organic Matter Related Pathways')\n",
        "    plt.xlabel('Mean Abundance')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Calling the function\n",
        "stats, organic_data = analyze_metabolic_pathways(Picrust_Result)\n",
        "plot_pathway_distribution(stats)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FZkQ1j5X5fHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9GJgu3yuvgW"
      },
      "outputs": [],
      "source": [
        "# To analyze specific pathways of interest:\n",
        "def analyze_specific_pathways(df, pathway_list):\n",
        "    \"\"\"\n",
        "    Analyze specific pathways of interest\n",
        "\n",
        "    Parameters:\n",
        "    df: DataFrame with pathway data\n",
        "    pathway_list: list of pathway names to analyze\n",
        "    \"\"\"\n",
        "    specific_data = df.loc[df.index.str.contains('|'.join(pathway_list), case=False)]\n",
        "\n",
        "    # Create correlation matrix for these pathways\n",
        "    corr = specific_data.T.corr()\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(corr, annot=True, cmap='coolwarm', center=0)\n",
        "    plt.title('Correlation between Selected Pathways')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return specific_data.describe()\n",
        "\n",
        "# Calling the funtion\n",
        "Description = analyze_specific_pathways(Picrust_Result, Picrust_Result.index.tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HEIwON2uvgW"
      },
      "source": [
        "## 9.4. Pathways Relevant to Corrosion\n",
        "This code witll categorise pathways into key groups: sulfur metabolism (critical for sulfate-reducing bacteria), Metal-related pathways (iron, manganese, etc.); organic acid production (which can influence local pH); biofilm formation (important for corrosion processes) and eectron transfer mechanisms. Then it would analyse correlations between these different categories to understand potential synergistic effects, identifying the most abundant pathways in each category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8FN-ChvuvgW"
      },
      "outputs": [],
      "source": [
        "def analyze_corrosion_pathways(df):\n",
        "    \"\"\"\n",
        "    Analyze pathways relevant to microbially influenced corrosion (MIC)\n",
        "\n",
        "    Parameters:\n",
        "    df: pandas DataFrame with pathways as index and samples as columns\n",
        "    \"\"\"\n",
        "    # Define relevant pathway terms for different corrosion mechanisms\n",
        "    pathway_categories = {\n",
        "        'sulfur': ['sulfur', 'sulfate', 'sulfide', 'thiosulfate', 'sulfite', 'sulfonate'],\n",
        "        'metal': ['iron', 'metal', 'Fe', 'manganese', 'chromium', 'nickel'],\n",
        "        'organic_acid': ['organic acid', 'acetate', 'formate', 'lactate', 'pyruvate'],\n",
        "        'biofilm': ['biofilm', 'exopolysaccharide', 'EPS', 'adhesion'],\n",
        "        'electron_transfer': ['cytochrome', 'electron transport', 'oxidoreductase']\n",
        "    }\n",
        "\n",
        "    # Function to filter pathways by category\n",
        "    def get_category_pathways(terms):\n",
        "        return df.index[df.index.str.lower().str.contains('|'.join(terms), regex=True)]\n",
        "\n",
        "    # Analyze each category\n",
        "    category_data = {}\n",
        "    category_stats = {}\n",
        "\n",
        "    for category, terms in pathway_categories.items():\n",
        "        pathways = get_category_pathways(terms)\n",
        "        if len(pathways) > 0:\n",
        "            category_data[category] = df.loc[pathways]\n",
        "            category_stats[category] = pd.DataFrame({\n",
        "                'mean_abundance': category_data[category].mean(axis=1),\n",
        "                'std_abundance': category_data[category].std(axis=1),\n",
        "                'cv': category_data[category].std(axis=1) / category_data[category].mean(axis=1) * 100\n",
        "            }).sort_values('mean_abundance', ascending=False)\n",
        "\n",
        "    return category_data, category_stats\n",
        "\n",
        "def plot_corrosion_pathways(category_data, category_stats):\n",
        "    \"\"\"\n",
        "    Create visualizations for corrosion-related pathways\n",
        "    \"\"\"\n",
        "    # Plot top pathways for each category\n",
        "    for category, data in category_stats.items():\n",
        "        if len(data) > 0:\n",
        "            plt.figure(figsize=(12, min(6, max(3, len(data)*0.3))))\n",
        "            sns.barplot(data=data.head(10).reset_index(),\n",
        "                       x='mean_abundance',\n",
        "                       y='index',\n",
        "                       palette='YlOrRd')\n",
        "            plt.title(f'Top {min(10, len(data))} {category.replace(\"_\", \" \").title()} Related Pathways')\n",
        "            plt.xlabel('Mean Abundance')\n",
        "            plt.ylabel('Pathway')\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "    # Create correlation heatmap between categories\n",
        "    category_means = pd.DataFrame({\n",
        "        cat: data.mean(axis=1) for cat, data in category_data.items()\n",
        "    })\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(category_means.corr(),\n",
        "                annot=True,\n",
        "                cmap='coolwarm',\n",
        "                center=0,\n",
        "                vmin=-1,\n",
        "                vmax=1)\n",
        "    plt.title('Correlation between Pathway Categories')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def analyze_pathway_interactions(df, category_data):\n",
        "    \"\"\"\n",
        "    Analyze interactions between different pathway categories\n",
        "    \"\"\"\n",
        "    # Calculate mean abundance for each category\n",
        "    category_abundances = pd.DataFrame({\n",
        "        category: data.mean(axis=0)\n",
        "        for category, data in category_data.items()\n",
        "    })\n",
        "\n",
        "    # Calculate correlations between categories\n",
        "    correlations = category_abundances.corr()\n",
        "\n",
        "    # Identify potential synergistic relationships\n",
        "    high_correlations = correlations.unstack()\n",
        "    high_correlations = high_correlations[high_correlations != 1.0]\n",
        "    high_correlations = high_correlations[abs(high_correlations) > 0.5]\n",
        "\n",
        "    return category_abundances, correlations, high_correlations.sort_values(ascending=False)\n",
        "\n",
        "# Analysing Corrosion Pathways\n",
        "category_data, category_stats = analyze_corrosion_pathways(Picrust_Result)\n",
        "plot_corrosion_pathways(category_data, category_stats)\n",
        "abundances, correlations, high_corr = analyze_pathway_interactions(Picrust_Result, category_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bi4wuyN7uvgW"
      },
      "source": [
        "## 9.5. Heating and Cooling Systems Pathway Analysis\n",
        "Creating independent analyses:\n",
        "\n",
        "Failure analysis (based on human assessment/estimation)\n",
        "Microbiological analysis (16S rRNA)\n",
        "Physicochemical parameters\n",
        "\n",
        "\n",
        "Using physicochemical parameters as labels/indicators of corrosion state - this is quite clever because it gives you an objective measure without directly mixing in the biological data\n",
        "Then planning to correlate the microbial communities with these states through machine learning\n",
        "\n",
        "And now you want to use PICRUSt's functional predictions to validate your assumptions about organic matter metabolism. This is very valuable because:\n",
        "\n",
        "It can help confirm if the bacteria you've identified through correlations actually have the metabolic capacity to influence corrosion\n",
        "It might reveal unexpected metabolic pathways that could explain the correlations you're seeing\n",
        "The following script will Validate your organic matter assumptions by:\n",
        "\n",
        "Breaking down different types of organic matter processing\n",
        "Looking at both degradation and synthesis pathways\n",
        "Identifying transport mechanisms\n",
        "\n",
        "Connect with your physicochemical parameters by analyzing pathways that could influence:\n",
        "\n",
        "pH modulation\n",
        "Temperature response\n",
        "Metal interactions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWBttrFbuvgW"
      },
      "outputs": [],
      "source": [
        "def analyze_system_pathways(df):\n",
        "    \"\"\"\n",
        "    Analyze pathways relevant to heating/cooling system corrosion\n",
        "\n",
        "    Parameters:\n",
        "    df: pandas DataFrame with pathways as index and samples as columns\n",
        "    \"\"\"\n",
        "    # Define pathway categories relevant to system conditions\n",
        "    pathway_categories = {\n",
        "        # Water chemistry influence\n",
        "        'ph_modulation': ['acid', 'alkaline', 'proton pump', 'pH homeostasis'],\n",
        "\n",
        "        # Temperature adaptation\n",
        "        'temp_response': ['heat shock', 'cold shock', 'temperature response'],\n",
        "\n",
        "        # Organic matter processing\n",
        "        'carbon_metabolism': [\n",
        "            'carbon fixation', 'carbon utilization',\n",
        "            'organic acid', 'fatty acid',\n",
        "            'carbohydrate metabolism'\n",
        "        ],\n",
        "\n",
        "        # Corrosion-related\n",
        "        'metal_interaction': [\n",
        "            'iron', 'metal', 'oxidation-reduction',\n",
        "            'electron transport', 'metal binding'\n",
        "        ],\n",
        "\n",
        "        # Biofilm formation\n",
        "        'surface_attachment': [\n",
        "            'biofilm', 'adhesion', 'exopolysaccharide',\n",
        "            'extracellular matrix'\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    # Filter and analyze pathways\n",
        "    def get_category_pathways(terms):\n",
        "        return df.index[df.index.str.lower().str.contains('|'.join(terms), regex=True)]\n",
        "\n",
        "    category_data = {}\n",
        "    category_stats = {}\n",
        "\n",
        "    for category, terms in pathway_categories.items():\n",
        "        pathways = get_category_pathways(terms)\n",
        "        if len(pathways) > 0:\n",
        "            category_data[category] = df.loc[pathways]\n",
        "\n",
        "            # Calculate basic statistics\n",
        "            category_stats[category] = pd.DataFrame({\n",
        "                'mean_abundance': category_data[category].mean(axis=1),\n",
        "                'std_abundance': category_data[category].std(axis=1),\n",
        "                'cv': category_data[category].std(axis=1) / category_data[category].mean(axis=1) * 100,\n",
        "                'presence': (category_data[category] > 0).mean(axis=1) * 100  # % of samples with pathway\n",
        "            }).sort_values('mean_abundance', ascending=False)\n",
        "\n",
        "    return category_data, category_stats\n",
        "\n",
        "def analyze_organic_matter_pathways(df):\n",
        "    \"\"\"\n",
        "    Detailed analysis of organic matter related pathways\n",
        "    \"\"\"\n",
        "    # Specific organic matter categories\n",
        "    organic_categories = {\n",
        "        'degradation': ['degradation', 'breakdown', 'catabolism'],\n",
        "        'synthesis': ['biosynthesis', 'anabolism', 'synthesis'],\n",
        "        'transport': ['transport', 'uptake', 'export'],\n",
        "        'modification': ['modification', 'conversion', 'transformation']\n",
        "    }\n",
        "\n",
        "    organic_data = {}\n",
        "\n",
        "    for category, terms in organic_categories.items():\n",
        "        pathways = df.index[df.index.str.lower().str.contains(\n",
        "            '|'.join(terms), regex=True\n",
        "        ) & df.index.str.lower().str.contains(\n",
        "            'organic|carbon|fatty acid|lipid|protein|amino acid'\n",
        "        )]\n",
        "        if len(pathways) > 0:\n",
        "            organic_data[category] = df.loc[pathways]\n",
        "\n",
        "    return organic_data\n",
        "\n",
        "def plot_pathway_distributions(category_stats, category_data):\n",
        "    \"\"\"\n",
        "    Create visualizations for pathway distributions\n",
        "    \"\"\"\n",
        "    for category, stats in category_stats.items():\n",
        "        if len(stats) > 0:\n",
        "            # Create subplot with dual axis\n",
        "            fig, ax1 = plt.subplots(figsize=(12, min(8, max(4, len(stats)*0.3))))\n",
        "            ax2 = ax1.twinx()\n",
        "\n",
        "            # Plot mean abundance\n",
        "            sns.barplot(data=stats.head(10).reset_index(),\n",
        "                       x='mean_abundance',\n",
        "                       y='index',\n",
        "                       color='skyblue',\n",
        "                       ax=ax1)\n",
        "\n",
        "            # Plot presence percentage\n",
        "            stats.head(10)['presence'].plot(\n",
        "                marker='o',\n",
        "                color='red',\n",
        "                ax=ax2\n",
        "            )\n",
        "\n",
        "            ax1.set_title(f'{category.replace(\"_\", \" \").title()} Pathways')\n",
        "            ax1.set_xlabel('Mean Abundance')\n",
        "            ax2.set_xlabel('Presence (%)')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "# Calling the analysis\n",
        "category_data, category_stats = analyze_system_pathways(Picrust_Result)\n",
        "organic_data = analyze_organic_matter_pathways(Picrust_Result)\n",
        "plot_pathway_distributions(category_stats, category_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFrF4oTRuvgX"
      },
      "source": [
        "I have a big gap on the cation anion account and then used mackensy, 2012 method from the usgs to check ec measured Vs calculated and cation Vs ions. It is a big gap still, but I have a lot of OM so I could no assume as normally that OM is CH4 so I attribute it to small organic acids and put acetate and oxalate as OM representatives, I have a small study of small acids form on failure analysis and also report of a mass that has a magnetic consistency, so I infere that those muss be some organic metalic compound but only accounted for AC- and Ox-2, I thought better to chose this other compounds Fe rich but I don't know how to do it actually. So in my bacteria I actually found lots of them with Ac- metabolism whiles I was looking at the families I realise no only oxobacter accendants, but others similar, also got important biofilm formers, there is also halogen related and should be, big deal of difference make the material and location cause water treatment, unfortunately the annotations are no to be taken as parameters but can serve as annotations you understand the difference?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXMFmbSruvgX"
      },
      "source": [
        "validate assumptions about:\n",
        "\n",
        "Organic acid presence (by showing metabolic capability)\n",
        "Metal-organic complex formation (through siderophore and metal-binding pathways)\n",
        "Biofilm formation potential (which can influence local chemistry)\n",
        "\n",
        "Validate acetate/oxalate assumptions by showing if these metabolic pathways are actually present\n",
        "Look for other potential organic acid pathways might want to consider\n",
        "Identify metal-organic interaction pathways that could explain magnetic mass observation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3sujtH1uvgX"
      },
      "outputs": [],
      "source": [
        "def analyze_organic_metal_pathways(df):\n",
        "    \"\"\"\n",
        "    Analyze pathways related to organic acid metabolism and metal interactions\n",
        "\n",
        "    Parameters:\n",
        "    df: pandas DataFrame with pathways as index and samples as columns\n",
        "    \"\"\"\n",
        "    # Define specific pathway categories\n",
        "    pathway_categories = {\n",
        "        'organic_acid_metabolism': [\n",
        "            'acetate', 'acetic acid', 'acetyl',\n",
        "            'oxalate', 'oxalic acid',\n",
        "            'organic acid', 'fatty acid',\n",
        "            'carboxylic acid'\n",
        "        ],\n",
        "\n",
        "        'metal_organic_interaction': [\n",
        "            'siderophore', 'metal binding',\n",
        "            'iron complex', 'metal transport',\n",
        "            'metallophore', 'metal organic'\n",
        "        ],\n",
        "\n",
        "        'biofilm_formation': [\n",
        "            'biofilm', 'exopolysaccharide',\n",
        "            'extracellular matrix', 'adhesion'\n",
        "        ],\n",
        "\n",
        "        'halogen_related': [\n",
        "            'halogen', 'chloride', 'bromide',\n",
        "            'halide', 'dehalogenation'\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    # Analyze each category\n",
        "    def get_category_pathways(terms):\n",
        "        return df.index[df.index.str.lower().str.contains('|'.join(terms), regex=True)]\n",
        "\n",
        "    pathway_data = {}\n",
        "    pathway_stats = {}\n",
        "\n",
        "    for category, terms in pathway_categories.items():\n",
        "        pathways = get_category_pathways(terms)\n",
        "        if len(pathways) > 0:\n",
        "            pathway_data[category] = df.loc[pathways]\n",
        "\n",
        "            # Calculate comprehensive statistics\n",
        "            pathway_stats[category] = pd.DataFrame({\n",
        "                'mean_abundance': pathway_data[category].mean(axis=1),\n",
        "                'std_abundance': pathway_data[category].std(axis=1),\n",
        "                'cv': pathway_data[category].std(axis=1) / pathway_data[category].mean(axis=1) * 100,\n",
        "                'presence': (pathway_data[category] > 0).mean(axis=1) * 100,  # % of samples with pathway\n",
        "                'relative_abundance': pathway_data[category].mean(axis=1) / df.mean(axis=1).mean() * 100\n",
        "            }).sort_values('mean_abundance', ascending=False)\n",
        "\n",
        "    return pathway_data, pathway_stats\n",
        "\n",
        "def analyze_pathway_relationships(pathway_data):\n",
        "    \"\"\"\n",
        "    Analyze relationships between different pathway categories\n",
        "    \"\"\"\n",
        "    # Calculate mean abundance for each category across samples\n",
        "    category_means = pd.DataFrame({\n",
        "        category: data.mean(axis=0)\n",
        "        for category, data in pathway_data.items()\n",
        "    })\n",
        "\n",
        "    # Calculate correlations\n",
        "    correlations = category_means.corr()\n",
        "\n",
        "    # Identify potential functional relationships\n",
        "    high_correlations = correlations.unstack()\n",
        "    high_correlations = high_correlations[high_correlations != 1.0]\n",
        "    high_correlations = high_correlations[abs(high_correlations) > 0.5]\n",
        "\n",
        "    return category_means, correlations, high_correlations.sort_values(ascending=False)\n",
        "\n",
        "def plot_pathway_analysis(pathway_stats, pathway_data):\n",
        "    \"\"\"\n",
        "    Create visualizations for pathway analysis\n",
        "    \"\"\"\n",
        "    for category, stats in pathway_stats.items():\n",
        "        if len(stats) > 0:\n",
        "            # Create subplot with dual axis\n",
        "            fig, ax1 = plt.subplots(figsize=(12, min(8, max(4, len(stats)*0.3))))\n",
        "            ax2 = ax1.twinx()\n",
        "\n",
        "            # Plot abundance and relative abundance\n",
        "            sns.barplot(data=stats.head(10).reset_index(),\n",
        "                       x='relative_abundance',\n",
        "                       y='index',\n",
        "                       color='skyblue',\n",
        "                       ax=ax1)\n",
        "\n",
        "            stats.head(10)['presence'].plot(\n",
        "                marker='o',\n",
        "                color='red',\n",
        "                ax=ax2\n",
        "            )\n",
        "\n",
        "            ax1.set_title(f'{category.replace(\"_\", \" \").title()} Pathways')\n",
        "            ax1.set_xlabel('Relative Abundance (%)')\n",
        "            ax2.set_xlabel('Presence (%)')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "# calling the function\n",
        "pathway_data, pathway_stats = analyze_organic_metal_pathways(Picrust_Result)category_means, correlations, high_corr = analyze_pathway_relationships(pathway_data)\n",
        "plot_pathway_analysis(pathway_stats, pathway_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvV8Sor3uvgX"
      },
      "source": [
        "## 9.6. Corrosion Relevant Pathways\n",
        "\n",
        "Focus on corrosion-relevant pathways by categorizing them into:\n",
        "\n",
        "Organic acid metabolism (relevant to your acetate/oxalate observations)\n",
        "Sulfur metabolism\n",
        "Metal interactions\n",
        "Biofilm formation\n",
        "\n",
        "\n",
        "Handle the high-dimensional data by:\n",
        "\n",
        "Using dimensionality reduction (PCA)\n",
        "Calculating summary statistics\n",
        "Visualizing key patterns\n",
        "\n",
        "\n",
        "Address your specific interests:\n",
        "\n",
        "Organic matter metabolism pathways\n",
        "Metal-organic interactions\n",
        "Correlations with physicochemical parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EThn4vkFuvgX"
      },
      "outputs": [],
      "source": [
        "def analyze_corrosion_pathways(df):\n",
        "    \"\"\"\n",
        "    Analyze pathways relevant to microbially influenced corrosion\n",
        "    \"\"\"\n",
        "    # Define pathway categories relevant to corrosion\n",
        "    pathway_categories = {\n",
        "        'organic_acid': [\n",
        "            'CENTFERM-PWY',  # Central fermentation pathways\n",
        "            'FERMENTATION-PWY',  # Mixed acid fermentation\n",
        "            'GLYCOLYSIS',  # Glucose fermentation\n",
        "            'PWY-5100',  # Pyruvate fermentation\n",
        "            'GALACTUROCAT-PWY'  # Galacturonate degradation\n",
        "        ],\n",
        "        'sulfur': [\n",
        "            'PWY-6932',  # Sulfate reduction\n",
        "            'SO4ASSIM-PWY',  # Sulfate assimilation\n",
        "            'SULFATE-CYS-PWY'  # Sulfate to cysteine\n",
        "        ],\n",
        "        'metal_interaction': [\n",
        "            'PWY-7219',  # Iron oxidation\n",
        "            'PWY-7221',  # Iron reduction\n",
        "            'HEME-BIOSYNTHESIS-II',  # Iron-containing compounds\n",
        "            'P125-PWY'  # Metal resistance\n",
        "        ],\n",
        "        'biofilm': [\n",
        "            'COLANSYN-PWY',  # Colanic acid (biofilm)\n",
        "            'EXOPOLYSACC-PWY',  # Exopolysaccharide\n",
        "            'GLUCOSE1PMETAB-PWY'  # UDP-glucose synthesis\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    # Extract relevant pathways and their abundances\n",
        "    relevant_pathways = {}\n",
        "    for category, pathways in pathway_categories.items():\n",
        "        category_data = df[df.index.isin(pathways)]\n",
        "        if not category_data.empty:\n",
        "            relevant_pathways[category] = category_data\n",
        "\n",
        "    # Calculate summary statistics\n",
        "    summary_stats = {}\n",
        "    for category, data in relevant_pathways.items():\n",
        "        summary_stats[category] = {\n",
        "            'mean_abundance': data.mean().mean(),\n",
        "            'std_abundance': data.mean().std(),\n",
        "            'present_in_samples': (data > 0).mean().mean() * 100,\n",
        "            'pathways_found': len(data)\n",
        "        }\n",
        "\n",
        "    # Dimension reduction for visualization\n",
        "    if df.shape[0] > 0:\n",
        "        # Standardize the data\n",
        "        scaler = StandardScaler()\n",
        "        scaled_data = scaler.fit_transform(df.T)\n",
        "\n",
        "        # PCA\n",
        "        pca = PCA(n_components=2)\n",
        "        pca_result = pca.fit_transform(scaled_data)\n",
        "\n",
        "        return relevant_pathways, summary_stats, pca_result, pca.explained_variance_ratio_\n",
        "\n",
        "    return relevant_pathways, summary_stats, None, None\n",
        "\n",
        "def plot_pathway_analysis(relevant_pathways, summary_stats, pca_result=None, explained_variance=None):\n",
        "    \"\"\"\n",
        "    Create visualizations for pathway analysis\n",
        "    \"\"\"\n",
        "    # Plot mean abundances by category\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    categories = list(summary_stats.keys())\n",
        "    means = [stats['mean_abundance'] for stats in summary_stats.values()]\n",
        "    presence = [stats['present_in_samples'] for stats in summary_stats.values()]\n",
        "\n",
        "    ax1 = plt.gca()\n",
        "    ax2 = ax1.twinx()\n",
        "\n",
        "    bars = ax1.bar(categories, means, alpha=0.6, color='skyblue')\n",
        "    ax1.set_ylabel('Mean Abundance')\n",
        "\n",
        "    line = ax2.plot(categories, presence, 'ro-', label='Presence %')\n",
        "    ax2.set_ylabel('Presence in Samples (%)')\n",
        "\n",
        "    plt.title('Pathway Categories Overview')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # If PCA results available, plot them\n",
        "    if pca_result is not None:\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        plt.scatter(pca_result[:, 0], pca_result[:, 1], alpha=0.6)\n",
        "        plt.xlabel(f'PC1 ({explained_variance[0]*100:.1f}%)')\n",
        "        plt.ylabel(f'PC2 ({explained_variance[1]*100:.1f}%)')\n",
        "        plt.title('PCA of Pathway Abundances')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# Calling the functions\n",
        "relevant_pathways, summary_stats, pca_result, explained_variance = analyze_corrosion_pathways(Picrust_Result)\n",
        "plot_pathway_analysis(relevant_pathways, summary_stats, pca_result, explained_variance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZgXC50euvgX"
      },
      "source": [
        "## 9.7. Functional Pathway Clustering Analysis\n",
        "Hierarchical Clustering:\n",
        "\n",
        "Groups pathways based on their abundance patterns\n",
        "Creates a dendrogram to visualize relationships\n",
        "Automatically determines optimal number of clusters\n",
        "\n",
        "\n",
        "Correlation-based Analysis:\n",
        "\n",
        "Identifies pathways that behave similarly across samples\n",
        "Creates correlation heatmap to visualize relationships\n",
        "Helps identify functional modules\n",
        "\n",
        "\n",
        "Feature Creation:\n",
        "\n",
        "Generates new features based on cluster statistics:\n",
        "\n",
        "Mean abundance per cluster\n",
        "Total abundance per cluster\n",
        "Pathway diversity within clusters\n",
        "\n",
        "Reduce dimensionality while maintaining biological meaning\n",
        "Identify functional modules that might be working together\n",
        "Create more robust features for your ML analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6NsHQCquvgX"
      },
      "outputs": [],
      "source": [
        "def cluster_pathways(df, n_clusters=None, corr_threshold=0.7):\n",
        "    \"\"\"\n",
        "    Cluster pathways based on their functional similarity\n",
        "\n",
        "    Parameters:\n",
        "    df: DataFrame with pathways as rows and samples as columns\n",
        "    n_clusters: Number of clusters (if None, determined automatically)\n",
        "    corr_threshold: Correlation threshold for considering pathways related\n",
        "    \"\"\"\n",
        "    # Standardize the data\n",
        "    scaler = StandardScaler()\n",
        "    scaled_data = scaler.fit_transform(df.T).T\n",
        "\n",
        "    # Calculate correlation matrix\n",
        "    corr_matrix = np.corrcoef(scaled_data)\n",
        "\n",
        "    # Create linkage matrix for hierarchical clustering\n",
        "    linkage_matrix = hierarchy.linkage(pdist(scaled_data), method='ward')\n",
        "\n",
        "    if n_clusters is None:\n",
        "        # Automatically determine number of clusters using elbow method\n",
        "        last = linkage_matrix[-10:, 2]\n",
        "        acceleration = np.diff(last, 2)\n",
        "        n_clusters = len(last) - np.argmax(acceleration) + 1\n",
        "\n",
        "    # Perform clustering\n",
        "    clustering = AgglomerativeClustering(n_clusters=n_clusters)\n",
        "    cluster_labels = clustering.fit_predict(scaled_data)\n",
        "\n",
        "    # Create cluster summary\n",
        "    cluster_summary = pd.DataFrame({\n",
        "        'pathway': df.index,\n",
        "        'cluster': cluster_labels\n",
        "    })\n",
        "\n",
        "    return cluster_labels, linkage_matrix, corr_matrix, cluster_summary\n",
        "\n",
        "def analyze_pathway_clusters(df, cluster_labels):\n",
        "    \"\"\"\n",
        "    Analyze the characteristics of each pathway cluster\n",
        "    \"\"\"\n",
        "    cluster_stats = {}\n",
        "\n",
        "    for cluster in np.unique(cluster_labels):\n",
        "        # Get pathways in this cluster\n",
        "        cluster_paths = df.index[cluster_labels == cluster]\n",
        "        cluster_data = df.loc[cluster_paths]\n",
        "\n",
        "        # Calculate statistics\n",
        "        cluster_stats[cluster] = {\n",
        "            'size': len(cluster_paths),\n",
        "            'mean_abundance': cluster_data.mean().mean(),\n",
        "            'std_abundance': cluster_data.mean().std(),\n",
        "            'pathways': list(cluster_paths),\n",
        "            'correlation': np.corrcoef(cluster_data),\n",
        "            'total_abundance': cluster_data.sum().mean()\n",
        "        }\n",
        "\n",
        "    return cluster_stats\n",
        "\n",
        "def plot_pathway_clusters(df, linkage_matrix, corr_matrix, cluster_labels, cluster_stats):\n",
        "    \"\"\"\n",
        "    Create visualizations for pathway clusters\n",
        "    \"\"\"\n",
        "    # Plot dendrogram\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    plt.title('Pathway Clustering Dendrogram')\n",
        "    hierarchy.dendrogram(linkage_matrix, labels=df.index, leaf_rotation=90)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Plot correlation heatmap\n",
        "    plt.figure(figsize=(12, 12))\n",
        "    sns.heatmap(pd.DataFrame(corr_matrix, index=df.index, columns=df.index),\n",
        "                cmap='coolwarm', center=0, vmin=-1, vmax=1)\n",
        "    plt.title('Pathway Correlation Heatmap')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Plot cluster sizes and abundances\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    clusters = list(cluster_stats.keys())\n",
        "    sizes = [stats['size'] for stats in cluster_stats.values()]\n",
        "    abundances = [stats['mean_abundance'] for stats in cluster_stats.values()]\n",
        "\n",
        "    ax1 = plt.gca()\n",
        "    ax2 = ax1.twinx()\n",
        "\n",
        "    ax1.bar(clusters, sizes, alpha=0.6, color='skyblue')\n",
        "    ax1.set_ylabel('Number of Pathways')\n",
        "\n",
        "    ax2.plot(clusters, abundances, 'ro-')\n",
        "    ax2.set_ylabel('Mean Abundance')\n",
        "\n",
        "    plt.title('Cluster Sizes and Abundances')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def create_cluster_features(df, cluster_labels):\n",
        "    \"\"\"\n",
        "    Create new features based on pathway clusters\n",
        "    \"\"\"\n",
        "    n_clusters = len(np.unique(cluster_labels))\n",
        "    cluster_features = pd.DataFrame(index=df.columns)\n",
        "\n",
        "    for cluster in range(n_clusters):\n",
        "        # Get pathways in this cluster\n",
        "        cluster_paths = df.index[cluster_labels == cluster]\n",
        "\n",
        "        # Calculate mean abundance for cluster\n",
        "        cluster_features[f'cluster_{cluster}'] = df.loc[cluster_paths].mean()\n",
        "\n",
        "        # Calculate total abundance for cluster\n",
        "        cluster_features[f'cluster_{cluster}_total'] = df.loc[cluster_paths].sum()\n",
        "\n",
        "        # Calculate diversity within cluster\n",
        "        cluster_features[f'cluster_{cluster}_diversity'] = (df.loc[cluster_paths] > 0).sum()\n",
        "\n",
        "    return cluster_features\n",
        "\n",
        "# Calling the fUNCTION\n",
        "cluster_labels, linkage_matrix, corr_matrix, cluster_summary = cluster_pathways(Picrust_Result)\n",
        "cluster_stats = analyze_pathway_clusters(Picrust_Result, cluster_labels)\n",
        "plot_pathway_clusters(Picrust_Result, linkage_matrix, corr_matrix, cluster_labels, cluster_stats)\n",
        "cluster_features = create_cluster_features(Picrust_Result, cluster_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Os65js9EuvgX"
      },
      "source": [
        "I dont know hte gfollowinag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vgw9Pwp8uvgX"
      },
      "outputs": [],
      "source": [
        "\n",
        "def analyze_organic_metal_pathways(df):\n",
        "    \"\"\"\n",
        "    Analyze pathways related to organic acid metabolism and metal interactions\n",
        "\n",
        "    Parameters:\n",
        "    df: pandas DataFrame with pathways as index and samples as columns\n",
        "    \"\"\"\n",
        "    # Define specific pathway categories\n",
        "    pathway_categories = {\n",
        "        'organic_acid_metabolism': [\n",
        "            'acetate', 'acetic acid', 'acetyl',\n",
        "            'oxalate', 'oxalic acid',\n",
        "            'organic acid', 'fatty acid',\n",
        "            'carboxylic acid'\n",
        "        ],\n",
        "\n",
        "        'metal_organic_interaction': [\n",
        "            'siderophore', 'metal binding',\n",
        "            'iron complex', 'metal transport',\n",
        "            'metallophore', 'metal organic'\n",
        "        ],\n",
        "\n",
        "        'biofilm_formation': [\n",
        "            'biofilm', 'exopolysaccharide',\n",
        "            'extracellular matrix', 'adhesion'\n",
        "        ],\n",
        "\n",
        "        'halogen_related': [\n",
        "            'halogen', 'chloride', 'bromide',\n",
        "            'halide', 'dehalogenation'\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    # Analyze each category\n",
        "    def get_category_pathways(terms):\n",
        "        return df.index[df.index.str.lower().str.contains('|'.join(terms), regex=True)]\n",
        "\n",
        "    pathway_data = {}\n",
        "    pathway_stats = {}\n",
        "\n",
        "    for category, terms in pathway_categories.items():\n",
        "        pathways = get_category_pathways(terms)\n",
        "        if len(pathways) > 0:\n",
        "            pathway_data[category] = df.loc[pathways]\n",
        "\n",
        "            # Calculate comprehensive statistics\n",
        "            pathway_stats[category] = pd.DataFrame({\n",
        "                'mean_abundance': pathway_data[category].mean(axis=1),\n",
        "                'std_abundance': pathway_data[category].std(axis=1),\n",
        "                'cv': pathway_data[category].std(axis=1) / pathway_data[category].mean(axis=1) * 100,\n",
        "                'presence': (pathway_data[category] > 0).mean(axis=1) * 100,  # % of samples with pathway\n",
        "                'relative_abundance': pathway_data[category].mean(axis=1) / df.mean(axis=1).mean() * 100\n",
        "            }).sort_values('mean_abundance', ascending=False)\n",
        "\n",
        "    return pathway_data, pathway_stats\n",
        "\n",
        "def analyze_pathway_relationships(pathway_data):\n",
        "    \"\"\"\n",
        "    Analyze relationships between different pathway categories\n",
        "    \"\"\"\n",
        "    # Calculate mean abundance for each category across samples\n",
        "    category_means = pd.DataFrame({\n",
        "        category: data.mean(axis=0)\n",
        "        for category, data in pathway_data.items()\n",
        "    })\n",
        "\n",
        "    # Calculate correlations\n",
        "    correlations = category_means.corr()\n",
        "\n",
        "    # Identify potential functional relationships\n",
        "    high_correlations = correlations.unstack()\n",
        "    high_correlations = high_correlations[high_correlations != 1.0]\n",
        "    high_correlations = high_correlations[abs(high_correlations) > 0.5]\n",
        "\n",
        "    return category_means, correlations, high_correlations.sort_values(ascending=False)\n",
        "\n",
        "def plot_pathway_analysis(pathway_stats, pathway_data):\n",
        "    \"\"\"\n",
        "    Create visualizations for pathway analysis\n",
        "    \"\"\"\n",
        "    for category, stats in pathway_stats.items():\n",
        "        if len(stats) > 0:\n",
        "            # Create subplot with dual axis\n",
        "            fig, ax1 = plt.subplots(figsize=(12, min(8, max(4, len(stats)*0.3))))\n",
        "            ax2 = ax1.twinx()\n",
        "\n",
        "            # Plot abundance and relative abundance\n",
        "            sns.barplot(data=stats.head(10).reset_index(),\n",
        "                       x='relative_abundance',\n",
        "                       y='index',\n",
        "                       color='skyblue',\n",
        "                       ax=ax1)\n",
        "\n",
        "            stats.head(10)['presence'].plot(\n",
        "                marker='o',\n",
        "                color='red',\n",
        "                ax=ax2\n",
        "            )\n",
        "\n",
        "            ax1.set_title(f'{category.replace(\"_\", \" \").title()} Pathways')\n",
        "            ax1.set_xlabel('Relative Abundance (%)')\n",
        "            ax2.set_xlabel('Presence (%)')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Le_mzeY_uvgX"
      },
      "outputs": [],
      "source": [
        "# Calling the function\n",
        "pathway_data, pathway_stats = analyze_organic_metal_pathways(Picrust_Result)\n",
        "category_means, correlations, high_corr = analyze_pathway_relationships(pathway_data)\n",
        "plot_pathway_analysis(pathway_stats, pathway_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wf76SqphuvgX"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EQUT3fwuvgX"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djNOlMWkuvgX"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}