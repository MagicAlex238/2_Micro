{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDED5j4zCqR_"
      },
      "source": [
        "# Sequence Analysis and Functional Prediction Pipeline\n",
        "\n",
        "## 1. Introduction\n",
        "This notebook analyzes the functional and sequence relationships between newly identified bacteria and known corrosion-influencing microorganisms. The analysis builds upon previous findings where:\n",
        "- Statistical significance was established between the selected bacteria and corrosion risk (Notebook 3)\n",
        "- Literature validation confirmed corrosion influence for many bacteria (Notebook 4)\n",
        "- Evolutionary relationships were mapped through phylogenetic analysis (Notebook 5)\n",
        "\n",
        "The study focuses on bacteria from operational heating and cooling water systems, primarily in Germany. Using 16S rRNA data (bootstrap-validated from Notebook 5), this analysis employs PICRUSt2 to predict metabolic functions and compare functional profiles between different bacterial groups.\n",
        "\n",
        "### Analysis Approaches\n",
        "We implement two classification strategies:\n",
        "\n",
        "1. Simple Classification:\n",
        "   - Known corrosion-causing bacteria (usual_taxa)\n",
        "   - Other bacteria (combining checked_taxa and core_taxa)\n",
        "\n",
        "2. Detailed Classification:\n",
        "   - Known corrosion-causing bacteria (usual_taxa)\n",
        "   - Pure checked bacteria (exclusive to checked_taxa)\n",
        "   - Pure core bacteria (exclusive to core_taxa)\n",
        "   - Checked-core bacteria (overlap between checked and core taxa)\n",
        "\n",
        "This detailed approach allows for more nuanced analysis of functional profiles and better understanding of potential corrosion mechanisms across different bacterial groups.\n",
        "\n",
        "### Analysis Goals:\n",
        "- Predict metabolic functions from 16S sequences\n",
        "- Focus on corrosion-relevant pathways (sulfur/iron metabolism)\n",
        "- Compare functional profiles between known corrosion-causing bacteria and newly identified candidates\n",
        "- Validate whether statistical correlations reflect genuine metabolic capabilities associated with corrosion processes\n",
        "\n",
        "### Directory Structure:\n",
        " Following is the structure of the notebook data named data_picrus  \n",
        "data_tree  \n",
        " ├── sequences/  \n",
        " │   ├── known.fasta : sequences of known corrosion-causing bacteria  \n",
        " │   ├── candidate.fasta : sequences of potential new corrosion-causing bacteria  \n",
        " |   └── other files  \n",
        " data_picrus  \n",
        " └── picrust_results/  \n",
        "      ├── known_bacteria/  \n",
        "      |               ├── EC_predictions/       : enzyme predictions  \n",
        "      |               ├── pathway_predictions/  : metabolic pathway abundance  \n",
        "      |               ├── KO_predictions/       : KEGG ortholog predictions  \n",
        "      |               └── other_picrust_files/  \n",
        "      ├── candidate_bacteria/  \n",
        "      |               ├── EC_predictions/       : enzyme predictions  \n",
        "      |               ├── pathway_predictions/  : metabolic pathway abundance  \n",
        "      |               ├── KO_predictions/       : KEGG ortholog predictions  \n",
        "      |               └── other_picrust_files/  : final comparison summary\n",
        "      ├── core_bacteria/\n",
        "      |               ├── EC_predictions/       : enzyme predictions  \n",
        "      |               ├── pathway_predictions/  : metabolic pathway abundance  \n",
        "      |               ├── KO_predictions/       : KEGG ortholog predictions  \n",
        "      |               └── other_picrust_files/  \n",
        "      │      \n",
        "      └── functional_comparison.xlsx  \n",
        "\n",
        "Picrust2 works using its reference database that was installed with the package   \n",
        "~/miniconda3/envs/picrust2/lib/python3.9/site-packages/picrust2/default_files/prokaryotic/pro_ref\n",
        "\n",
        "About picrust2  \n",
        "https://evomics.org/wp-content/uploads/2015/01/presentation_evomics-05-picrust_01-18-15.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeIn7RIBCqSD"
      },
      "source": [
        "# 2. Loading and Preparing the Data\n",
        "\n",
        "## 2.1 Colab Initialisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2gkBXEzwKkG",
        "outputId": "91d318ad-ef00-4d60-c1db-27027164214a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "os.path.exists('/content/drive/MyDrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5U6gm_R_JQjt",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Colab specific\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#change the path\n",
        "os.chdir('/content/drive/MyDrive/MIC/data_picrust')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWqOI6IDD1qW"
      },
      "source": [
        "__Importing PICRUST IN COLAB__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKimriI3hmTq",
        "trusted": true,
        "vscode": {
          "languageId": "javascript"
        }
      },
      "outputs": [],
      "source": [
        "'''# Install miniconda and initialize:\n",
        "!wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
        "!bash Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local/miniconda3\n",
        "!conda config --add channels defaults\n",
        "!conda config --add channels bioconda\n",
        "!conda config --add channels conda-forge\n",
        "# Imports for colab\n",
        "import condacolab\n",
        "import sys\n",
        "sys.path.append('/usr/local/miniconda3/lib/python3.7/site-packages/')\n",
        "\n",
        "# Install PICRUSt2 and its dependencies\n",
        "%conda install -c bioconda -c conda-forge picrust2=2.4.1 -y\n",
        "# Verify installations%\n",
        "%conda list | grep picrust2'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzaNXN9suvgG"
      },
      "source": [
        "### Using Pro colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qF94FGxn2iUL",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "'''import sys\n",
        "print([module for module in sys.modules if 'tensorflow' in module])'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-03-16T10:08:51.478873Z",
          "iopub.status.busy": "2025-03-16T10:08:51.478544Z",
          "iopub.status.idle": "2025-03-16T10:09:13.292483Z",
          "shell.execute_reply": "2025-03-16T10:09:13.291140Z",
          "shell.execute_reply.started": "2025-03-16T10:08:51.478815Z"
        },
        "id": "3gWJfdx3Ni1f",
        "outputId": "49d61dad-d54a-422e-c766-2c3c37837c65",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gputil\n",
            "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gputil: filename=GPUtil-1.4.0-py3-none-any.whl size=7392 sha256=3dde68388b8bacb207767862885dbd206653ba9ff56349e61e8a4ad904edd866\n",
            "  Stored in directory: /root/.cache/pip/wheels/2b/4d/8f/55fb4f7b9b591891e8d3f72977c4ec6c7763b39c19f0861595\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (5.9.5)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.11/dist-packages (4.11.0)\n",
            "Collecting memory_profiler\n",
            "  Downloading memory_profiler-0.61.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from memory_profiler) (5.9.5)\n",
            "Downloading memory_profiler-0.61.0-py3-none-any.whl (31 kB)\n",
            "Installing collected packages: memory_profiler\n",
            "Successfully installed memory_profiler-0.61.0\n",
            "Runtime has 13.6 gigabytes of available RAM\n",
            "\n",
            "Not using a high-RAM runtime\n"
          ]
        }
      ],
      "source": [
        "# Set up memory footprint support libraries\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "!pip install memory_profiler\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('Using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTKk2e3eYelt"
      },
      "source": [
        "### Kaggle / Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-03-16T20:13:35.054575Z",
          "iopub.status.busy": "2025-03-16T20:13:35.054151Z",
          "iopub.status.idle": "2025-03-16T20:14:27.868960Z",
          "shell.execute_reply": "2025-03-16T20:14:27.867488Z",
          "shell.execute_reply.started": "2025-03-16T20:13:35.054543Z"
        },
        "id": "KC8v0oJRuvgH",
        "outputId": "65e5ff0e-8f57-439f-ad3d-803b19ed7bfb",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting biopython\n",
            "  Downloading biopython-1.85-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from biopython) (2.0.2)\n",
            "Downloading biopython-1.85-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: biopython\n",
            "Successfully installed biopython-1.85\n",
            "Collecting biom-format\n",
            "  Downloading biom-format-2.1.16.tar.gz (11.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from biom-format) (8.1.8)\n",
            "Requirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.11/dist-packages (from biom-format) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from biom-format) (1.14.1)\n",
            "Requirement already satisfied: pandas>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from biom-format) (2.2.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from biom-format) (3.12.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.20.0->biom-format) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.20.0->biom-format) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.20.0->biom-format) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=0.20.0->biom-format) (1.17.0)\n",
            "Building wheels for collected packages: biom-format\n",
            "  Building wheel for biom-format (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for biom-format: filename=biom_format-2.1.16-cp311-cp311-linux_x86_64.whl size=12182955 sha256=45015a28906f27689ba344685b22aa0d26c51360f02232e0644d6acfc6e82da6\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/6b/58/a879e8fbae2479a3d1a68719f3a062fe62701d6494f1b74f5e\n",
            "Successfully built biom-format\n",
            "Installing collected packages: biom-format\n",
            "Successfully installed biom-format-2.1.16\n",
            "Requirement already satisfied: umap-learn in /usr/local/lib/python3.11/dist-packages (0.5.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (1.14.1)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (1.6.1)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (0.60.0)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (0.5.13)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from umap-learn) (4.67.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.2->umap-learn) (0.43.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.11/dist-packages (from pynndescent>=0.5->umap-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22->umap-learn) (3.6.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (5.3.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (18.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install biopython\n",
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "!pip install biom-format\n",
        "%pip install umap-learn\n",
        "!pip install lxml pandas\n",
        "!pip install pyarrow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiHOVZiIuvgH"
      },
      "source": [
        "# 2.2. Importing Libraries,  Making Directories and Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import plotly.express as px\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "import umap\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-16T20:15:08.114540Z",
          "iopub.status.busy": "2025-03-16T20:15:08.114193Z",
          "iopub.status.idle": "2025-03-16T20:15:52.328071Z",
          "shell.execute_reply": "2025-03-16T20:15:52.327090Z",
          "shell.execute_reply.started": "2025-03-16T20:15:08.114509Z"
        },
        "id": "l92DnCZ3CqSD",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Standard library imports\n",
        "import os\n",
        "import sys\n",
        "import ast\n",
        "import subprocess\n",
        "import logging\n",
        "import time\n",
        "from datetime import datetime\n",
        "import shutil\n",
        "from io import StringIO\n",
        "from pathlib import Path\n",
        "import re\n",
        "import json\n",
        "\n",
        "# Data processing and analysis\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import openpyxl\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import to_rgba, LinearSegmentedColormap\n",
        "import matplotlib.patches as mpatches\n",
        "import seaborn as sns\n",
        "import networkx as nx\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.io as pio\n",
        "\n",
        "# Machine learning and statistical analysis\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
        "from sklearn.decomposition import PCA, NMF\n",
        "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
        "from sklearn.manifold import TSNE\n",
        "import umap\n",
        "from scipy import stats\n",
        "from scipy.cluster import hierarchy\n",
        "import scipy.cluster.hierarchy as sch\n",
        "from statsmodels.stats.multitest import multipletests\n",
        "from scipy.spatial.distance import pdist\n",
        "from scipy.stats import spearmanr, kruskal\n",
        "\n",
        "# Bioinformatics\n",
        "from Bio import SeqIO\n",
        "from Bio.Seq import Seq\n",
        "from Bio.SeqRecord import SeqRecord\n",
        "from biom import Table, load_table\n",
        "from biom.util import biom_open\n",
        "\n",
        "# Web and data retrieval\n",
        "import requests\n",
        "import xml.etree.ElementTree as ET\n",
        "from lxml import etree\n",
        "\n",
        "# Utility libraries\n",
        "import gzip\n",
        "import random\n",
        "from natsort import natsorted\n",
        "from typing import Dict, List, Tuple, Set, Optional\n",
        "import pickle\n",
        "import gc\n",
        "import joblib\n",
        "import h5py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "execution": {
          "iopub.execute_input": "2025-03-16T20:19:50.866883Z",
          "iopub.status.busy": "2025-03-16T20:19:50.866552Z",
          "iopub.status.idle": "2025-03-16T20:19:50.874818Z",
          "shell.execute_reply": "2025-03-16T20:19:50.873600Z",
          "shell.execute_reply.started": "2025-03-16T20:19:50.866858Z"
        },
        "id": "Jhg73Rb9CqSF",
        "outputId": "0060c735-7a17-448b-ca04-4eff3030697d",
        "trusted": true
      },
      "outputs": [
        {
          "ename": "PermissionError",
          "evalue": "[Errno 13] Permission denied: '/content'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "File \u001b[0;32m/usr/lib/python3.10/pathlib.py:1175\u001b[0m, in \u001b[0;36mPath.mkdir\u001b[0;34m(self, mode, parents, exist_ok)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1175\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_accessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/MIC/data_picrust'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "File \u001b[0;32m/usr/lib/python3.10/pathlib.py:1175\u001b[0m, in \u001b[0;36mPath.mkdir\u001b[0;34m(self, mode, parents, exist_ok)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1175\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_accessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/MIC'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "File \u001b[0;32m/usr/lib/python3.10/pathlib.py:1175\u001b[0m, in \u001b[0;36mPath.mkdir\u001b[0;34m(self, mode, parents, exist_ok)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1175\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_accessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "File \u001b[0;32m/usr/lib/python3.10/pathlib.py:1175\u001b[0m, in \u001b[0;36mPath.mkdir\u001b[0;34m(self, mode, parents, exist_ok)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1175\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_accessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[16], line 58\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# for colab\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Create output directory if it doesn't exist\u001b[39;00m\n\u001b[1;32m     57\u001b[0m base_dir \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/content/drive/MyDrive/MIC/data_picrust/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 58\u001b[0m \u001b[43mbase_dir\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m abundance_excel\u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/content/drive/MyDrive/MIC/data_picrust/merged_to_sequence.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     60\u001b[0m fasta_file_final \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/content/drive/MyDrive/MIC/data_picrust/final_sequences_gg.fasta\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m/usr/lib/python3.10/pathlib.py:1179\u001b[0m, in \u001b[0;36mPath.mkdir\u001b[0;34m(self, mode, parents, exist_ok)\u001b[0m\n\u001b[1;32m   1177\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parents \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m   1178\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1179\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1180\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmkdir(mode, parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39mexist_ok)\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m   1182\u001b[0m     \u001b[38;5;66;03m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m     \u001b[38;5;66;03m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n",
            "File \u001b[0;32m/usr/lib/python3.10/pathlib.py:1179\u001b[0m, in \u001b[0;36mPath.mkdir\u001b[0;34m(self, mode, parents, exist_ok)\u001b[0m\n\u001b[1;32m   1177\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parents \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m   1178\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1179\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1180\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmkdir(mode, parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39mexist_ok)\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m   1182\u001b[0m     \u001b[38;5;66;03m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m     \u001b[38;5;66;03m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n",
            "    \u001b[0;31m[... skipping similar frames: Path.mkdir at line 1179 (1 times)]\u001b[0m\n",
            "File \u001b[0;32m/usr/lib/python3.10/pathlib.py:1179\u001b[0m, in \u001b[0;36mPath.mkdir\u001b[0;34m(self, mode, parents, exist_ok)\u001b[0m\n\u001b[1;32m   1177\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parents \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m   1178\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1179\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1180\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmkdir(mode, parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39mexist_ok)\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m   1182\u001b[0m     \u001b[38;5;66;03m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m     \u001b[38;5;66;03m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n",
            "File \u001b[0;32m/usr/lib/python3.10/pathlib.py:1175\u001b[0m, in \u001b[0;36mPath.mkdir\u001b[0;34m(self, mode, parents, exist_ok)\u001b[0m\n\u001b[1;32m   1171\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1172\u001b[0m \u001b[38;5;124;03mCreate a new directory at this given path.\u001b[39;00m\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1174\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1175\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_accessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[1;32m   1177\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parents \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m:\n",
            "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '/content'"
          ]
        }
      ],
      "source": [
        "# Set up logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "\n",
        "# Directory Structure Definitions\n",
        "SIMPLE_BASE = {\n",
        "    'known': 'simple_known_mic',\n",
        "    'other': 'simple_candidate_mic'\n",
        "}\n",
        "\n",
        "DETAILED_BASE = {\n",
        "    'known': 'detailed_known_mic',\n",
        "    'pure_checked': 'detailed_pure_checked_mic',\n",
        "    'pure_core': 'detailed_pure_core_mic',\n",
        "    'checked_core': 'detailed_checked_core_mic'\n",
        "}\n",
        "\n",
        "SUBDIRS = [\n",
        "    'EC_predictions',\n",
        "    'pathway_predictions',\n",
        "    'KO_predictions',\n",
        "    'other_picrust_files'\n",
        "]\n",
        "\n",
        "\n",
        "# Base Paths\n",
        "if \"google.colab\" in sys.modules:\n",
        "    base_dir = Path(\"/content/drive/MyDrive/MIC/data_picrust\")\n",
        "else:\n",
        "    base_dir = Path(\"/home/beatriz/MIC/2_Micro/data_picrust\")\n",
        "\n",
        "#base dir for small files to git\n",
        "base_dir = Path(\"/home/beatriz/MIC/2_Micro/data_picrust\")\n",
        "\n",
        "abundance_excel= Path(\"/home/beatriz/MIC/2_Micro/data_Ref/merged_to_sequence.xlsx\")\n",
        "fasta_file_final = Path(\"/home/beatriz/MIC/2_Micro/data_qiime/results_match_gg/final_sequences_gg.fasta\")\n",
        "aligned_fasta = Path(\"/home/beatriz/MIC/2_Micro/data_qiime/results_match_gg/aligned-dna-sequences_gg.fasta\")\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "output_base = base_dir / \"output_base\"\n",
        "output_base.mkdir(parents=True, exist_ok=True)\n",
        "# large galaxies input and output #large size dir for large files hosted instead in kaggle\n",
        "large_dir = Path(\"/home/beatriz/MIC\")\n",
        "large_dir.mkdir(parents=True, exist_ok=True)\n",
        "# databases\n",
        "db_dir = large_dir / \"Databases\"\n",
        "# input galaxies and uniprots\n",
        "input_galaxy = large_dir / \"data_galaxies\"\n",
        "# Directory to output large files # eccontris, compilated db\n",
        "output_large = large_dir / \"output_large\"\n",
        "output_large.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# for colab\n",
        "# Create output directory if it doesn't exist\n",
        "base_dir = Path(\"/content/drive/MyDrive/MIC/data_picrust/\")\n",
        "base_dir.mkdir(parents=True, exist_ok=True)\n",
        "abundance_excel= Path(\"/content/drive/MyDrive/MIC/data_picrust/merged_to_sequence.xlsx\")\n",
        "fasta_file_final = Path(\"/content/drive/MyDrive/MIC/data_picrust/final_sequences_gg.fasta\")\n",
        "aligned_fasta = Path(\"/content/drive/MyDrive/MIC/data_picrust/aligned-dna-sequences_gg.fasta\")\n",
        "\n",
        "output_base = base_dir  # Separate output directory\n",
        "output_base.mkdir(parents=True, exist_ok=True)\n",
        "large_dir = Path(\"/content/drive/MyDrive/MIC/\")\n",
        "large_dir.mkdir(parents=True, exist_ok=True)\n",
        "db_dir = Path(\"/content/drive/MyDrive/MIC/Databases\")\n",
        "db_dir.mkdir(parents=True, exist_ok=True)\n",
        "input_galaxy = large_dir / \"data_galaxies\"\n",
        "# Directory to output large files # eccontris, compilated db\n",
        "output_large = large_dir / \"output_large\"\n",
        "output_large.mkdir(parents=True, exist_ok=True)\n",
        "'''\n",
        "# For Kaggle work\n",
        "# Input datasets (read-only in Kaggle)\n",
        "base_dir = Path(\"/kaggle/input/new-picrust\") #base dir for small files to git /kaggle/input/new-picrust\n",
        "\n",
        "# Files in small input directory\n",
        "abundance_excel= base_dir / \"merged_to_sequence.xlsx\" # inside input small sizes input\n",
        "fasta_file_final = base_dir  / \"final_sequences_gg.fasta\" # inside input small sizes\n",
        "\n",
        "# Output for small files has to be changed for vscode no to push it to git\n",
        "output_base = Path(\"/kaggle/working/output_base\")\n",
        "output_base.mkdir(parents=True, exist_ok=True)\n",
        "#datasets large galaxies and databases\n",
        "db_dir = Path(\"/kaggle/input/databases/Databases\")\n",
        "input_galaxy = Path(\"/kaggle/input/data-galaxies\")\n",
        "\n",
        "# Directory to output large files # eccontris, compilated db\n",
        "large_dir =  Path(\"/kaggle/working/\")\n",
        "\n",
        "# Directory to output large files # eccontris, compilated db\n",
        "output_large = large_dir / \"output_large\"\n",
        "output_large.mkdir(parents=True, exist_ok=True)'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COW1kGFZCqSG"
      },
      "source": [
        "The fasta file come from the Alternative Sequences finding from the Greenes Genes Database, from the taxonomy in this study made in section 7 in the 5_Sequences_qiime notebook: final_sequences_gg.fasta. Abundance dataframe come from the data from notebook 4 merged_to_sequence.xlsx sheet=core_check_usual_taxa which is a unified df between 3 different groups explained previously: cora_taxa (>20% 60 abundance features), usual_taxa (17 high literature ranking bacteria influencing corrosion) and checked_taxa (30 statistically significant to the corrosion risk label) in total 85 features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-16T20:20:10.673238Z",
          "iopub.status.busy": "2025-03-16T20:20:10.672841Z",
          "iopub.status.idle": "2025-03-16T20:20:10.985475Z",
          "shell.execute_reply": "2025-03-16T20:20:10.984363Z",
          "shell.execute_reply.started": "2025-03-16T20:20:10.673209Z"
        },
        "id": "qzMCSdlPuvgI",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Integrated taxa from origin genus as headers with levels 6 for the genera, 7 for the GID, muss be cleaned\n",
        "Integrated_T = pd.read_excel(abundance_excel, sheet_name='core_check_usual_taxa', header=[0,1,2,3,4,5,6,7], engine ='openpyxl')\n",
        "# Drop first row (index 0) and first column in one chain\n",
        "Integrated_T = Integrated_T.drop(index=0).drop(Integrated_T.columns[0], axis=1)\n",
        "Integrated_T= Integrated_T.astype({'Sites': str})\n",
        "Integrated_T['Sites'] = Integrated_T['Sites'].fillna('Source')\n",
        "# Remove 'Unnamed' level names\n",
        "Integrated_T.columns = Integrated_T.columns.map(lambda x: tuple('' if 'Unnamed' in str(level) else level for level in x))\n",
        "# Changing dtypes to category whiles respecting structure\n",
        "Integrated_T[\"Category\"] = Integrated_T[\"Category\"].astype(\"Int64\")\n",
        "Integrated_T= Integrated_T.set_index(\"Sites\")\n",
        "pre_Integrated = Integrated_T.T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-16T20:20:13.860242Z",
          "iopub.status.busy": "2025-03-16T20:20:13.859868Z",
          "iopub.status.idle": "2025-03-16T20:20:13.911319Z",
          "shell.execute_reply": "2025-03-16T20:20:13.910159Z",
          "shell.execute_reply.started": "2025-03-16T20:20:13.860215Z"
        },
        "id": "aZSzaNSQuvgI",
        "outputId": "5f7cb459-e1f0-4ca7-93e7-2c84217918fa",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr:last-of-type th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>Category</th>\n",
              "      <th>Rhodocyclales_Rhodocyclaceae_Azospira</th>\n",
              "      <th>Actinomycetales_Dermabacteraceae_Brachybacterium</th>\n",
              "      <th>Actinomycetales_Brevibacteriaceae_Brevibacterium</th>\n",
              "      <th>Erysipelotrichales_Erysipelotrichaceae_Bulleidia</th>\n",
              "      <th>Clostridiales_Clostridiaceae_Clostridium</th>\n",
              "      <th>Actinomycetales_Corynebacteriaceae_Corynebacterium</th>\n",
              "      <th>Lactobacillales_Enterococcaceae_Enterococcus</th>\n",
              "      <th>Thermoanaerobacterales_Thermoanaerobacteraceae_Gelria</th>\n",
              "      <th>Oceanospirillales_Halomonadaceae_Halomonas</th>\n",
              "      <th>...</th>\n",
              "      <th>Actinomycetales_Propionibacteriaceae_Tessaracoccus</th>\n",
              "      <th>Clostridiales_Peptococcaceae_Thermincola</th>\n",
              "      <th>Spirochaetales_Spirochaetaceae_Treponema</th>\n",
              "      <th>Burkholderiales_Oxalobacteraceae_Oxalobacteraceae_unclassified</th>\n",
              "      <th>Burkholderiales_Comamonadaceae_Variovorax</th>\n",
              "      <th>Anaerolineales_Anaerolinaceae_Wchb1-05</th>\n",
              "      <th>Desulfobacterales_Desulfobacteraceae_Desulfobacterium</th>\n",
              "      <th>Desulfobacterales_Desulfobulbaceae_Desulfobulbus</th>\n",
              "      <th>Gallionellales_Gallionellaceae_Gallionella</th>\n",
              "      <th>Alteromonadales_Shewanellaceae_Shewanella</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>Bacteria</th>\n",
              "      <th>Bacteria</th>\n",
              "      <th>Bacteria</th>\n",
              "      <th>Bacteria</th>\n",
              "      <th>Bacteria</th>\n",
              "      <th>Bacteria</th>\n",
              "      <th>Bacteria</th>\n",
              "      <th>Bacteria</th>\n",
              "      <th>Bacteria</th>\n",
              "      <th>...</th>\n",
              "      <th>Bacteria</th>\n",
              "      <th>Bacteria</th>\n",
              "      <th>Bacteria</th>\n",
              "      <th>Bacteria</th>\n",
              "      <th>Bacteria</th>\n",
              "      <th>Bacteria</th>\n",
              "      <th>Bacteria</th>\n",
              "      <th>Bacteria</th>\n",
              "      <th>Bacteria</th>\n",
              "      <th>Bacteria</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>Proteobacteria</th>\n",
              "      <th>Actinobacteria</th>\n",
              "      <th>Actinobacteria</th>\n",
              "      <th>Firmicutes</th>\n",
              "      <th>Firmicutes</th>\n",
              "      <th>Actinobacteria</th>\n",
              "      <th>Firmicutes</th>\n",
              "      <th>Firmicutes</th>\n",
              "      <th>Proteobacteria</th>\n",
              "      <th>...</th>\n",
              "      <th>Actinobacteria</th>\n",
              "      <th>Firmicutes</th>\n",
              "      <th>Spirochaetes</th>\n",
              "      <th>Proteobacteria</th>\n",
              "      <th>Proteobacteria</th>\n",
              "      <th>Chloroflexi</th>\n",
              "      <th>Proteobacteria</th>\n",
              "      <th>Proteobacteria</th>\n",
              "      <th>Proteobacteria</th>\n",
              "      <th>Proteobacteria</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>Betaproteobacteria</th>\n",
              "      <th>Actinobacteria</th>\n",
              "      <th>Actinobacteria</th>\n",
              "      <th>Erysipelotrichi</th>\n",
              "      <th>Clostridia</th>\n",
              "      <th>Actinobacteria</th>\n",
              "      <th>Bacilli</th>\n",
              "      <th>Clostridia</th>\n",
              "      <th>Gammaproteobacteria</th>\n",
              "      <th>...</th>\n",
              "      <th>Actinobacteria</th>\n",
              "      <th>Clostridia</th>\n",
              "      <th>Spirochaetes</th>\n",
              "      <th>Betaproteobacteria</th>\n",
              "      <th>Betaproteobacteria</th>\n",
              "      <th>Anaerolineae</th>\n",
              "      <th>Deltaproteobacteria</th>\n",
              "      <th>Deltaproteobacteria</th>\n",
              "      <th>Betaproteobacteria</th>\n",
              "      <th>Gammaproteobacteria</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>Rhodocyclales</th>\n",
              "      <th>Actinomycetales</th>\n",
              "      <th>Actinomycetales</th>\n",
              "      <th>Erysipelotrichales</th>\n",
              "      <th>Clostridiales</th>\n",
              "      <th>Actinomycetales</th>\n",
              "      <th>Lactobacillales</th>\n",
              "      <th>Thermoanaerobacterales</th>\n",
              "      <th>Oceanospirillales</th>\n",
              "      <th>...</th>\n",
              "      <th>Actinomycetales</th>\n",
              "      <th>Clostridiales</th>\n",
              "      <th>Spirochaetales</th>\n",
              "      <th>Burkholderiales</th>\n",
              "      <th>Burkholderiales</th>\n",
              "      <th>Anaerolineales</th>\n",
              "      <th>Desulfobacterales</th>\n",
              "      <th>Desulfobacterales</th>\n",
              "      <th>Gallionellales</th>\n",
              "      <th>Alteromonadales</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>Rhodocyclaceae</th>\n",
              "      <th>Dermabacteraceae</th>\n",
              "      <th>Brevibacteriaceae</th>\n",
              "      <th>Erysipelotrichaceae</th>\n",
              "      <th>Clostridiaceae</th>\n",
              "      <th>Corynebacteriaceae</th>\n",
              "      <th>Enterococcaceae</th>\n",
              "      <th>Thermoanaerobacteraceae</th>\n",
              "      <th>Halomonadaceae</th>\n",
              "      <th>...</th>\n",
              "      <th>Propionibacteriaceae</th>\n",
              "      <th>Peptococcaceae</th>\n",
              "      <th>Spirochaetaceae</th>\n",
              "      <th>Oxalobacteraceae</th>\n",
              "      <th>Comamonadaceae</th>\n",
              "      <th>Anaerolinaceae</th>\n",
              "      <th>Desulfobacteraceae</th>\n",
              "      <th>Desulfobulbaceae</th>\n",
              "      <th>Gallionellaceae</th>\n",
              "      <th>Shewanellaceae</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>Azospira</th>\n",
              "      <th>Brachybacterium</th>\n",
              "      <th>Brevibacterium</th>\n",
              "      <th>Bulleidia</th>\n",
              "      <th>Clostridium</th>\n",
              "      <th>Corynebacterium</th>\n",
              "      <th>Enterococcus</th>\n",
              "      <th>Gelria</th>\n",
              "      <th>Halomonas</th>\n",
              "      <th>...</th>\n",
              "      <th>Tessaracoccus</th>\n",
              "      <th>Thermincola</th>\n",
              "      <th>Treponema</th>\n",
              "      <th>Oxalobacteraceae_unclassified</th>\n",
              "      <th>Variovorax</th>\n",
              "      <th>Wchb1-05</th>\n",
              "      <th>Desulfobacterium</th>\n",
              "      <th>Desulfobulbus</th>\n",
              "      <th>Gallionella</th>\n",
              "      <th>Shewanella</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>110</th>\n",
              "      <th>140</th>\n",
              "      <th>145</th>\n",
              "      <th>154</th>\n",
              "      <th>214</th>\n",
              "      <th>229</th>\n",
              "      <th>300</th>\n",
              "      <th>334</th>\n",
              "      <th>354</th>\n",
              "      <th>...</th>\n",
              "      <th>715</th>\n",
              "      <th>719</th>\n",
              "      <th>731</th>\n",
              "      <th>853</th>\n",
              "      <th>863</th>\n",
              "      <th>867</th>\n",
              "      <th>264</th>\n",
              "      <th>265</th>\n",
              "      <th>332</th>\n",
              "      <th>656</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Sites</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>site_67</th>\n",
              "      <td>3</td>\n",
              "      <td>0.004886</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.942935</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.151456</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.004886</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>site_68</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0.021172</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>28.889007</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.005293</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>site_69</th>\n",
              "      <td>1</td>\n",
              "      <td>1.47</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.63</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>site_70</th>\n",
              "      <td>1</td>\n",
              "      <td>1.72</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2.3</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Source</th>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>chk-core</td>\n",
              "      <td>chk</td>\n",
              "      <td>chk</td>\n",
              "      <td>chk</td>\n",
              "      <td>chk-core-us</td>\n",
              "      <td>chk-core-us</td>\n",
              "      <td>chk</td>\n",
              "      <td>chk</td>\n",
              "      <td>chk-core</td>\n",
              "      <td>...</td>\n",
              "      <td>core</td>\n",
              "      <td>core</td>\n",
              "      <td>core</td>\n",
              "      <td>core</td>\n",
              "      <td>core</td>\n",
              "      <td>core</td>\n",
              "      <td>us</td>\n",
              "      <td>us</td>\n",
              "      <td>us</td>\n",
              "      <td>us</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 86 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        Category Rhodocyclales_Rhodocyclaceae_Azospira  \\\n",
              "                                              Bacteria   \n",
              "                                        Proteobacteria   \n",
              "                                    Betaproteobacteria   \n",
              "                                         Rhodocyclales   \n",
              "                                        Rhodocyclaceae   \n",
              "                                              Azospira   \n",
              "                                                   110   \n",
              "Sites                                                    \n",
              "site_67        3                              0.004886   \n",
              "site_68        3                                     0   \n",
              "site_69        1                                  1.47   \n",
              "site_70        1                                  1.72   \n",
              "Source      <NA>                              chk-core   \n",
              "\n",
              "        Actinomycetales_Dermabacteraceae_Brachybacterium  \\\n",
              "                                                Bacteria   \n",
              "                                          Actinobacteria   \n",
              "                                          Actinobacteria   \n",
              "                                         Actinomycetales   \n",
              "                                        Dermabacteraceae   \n",
              "                                         Brachybacterium   \n",
              "                                                     140   \n",
              "Sites                                                      \n",
              "site_67                                                0   \n",
              "site_68                                         0.021172   \n",
              "site_69                                                0   \n",
              "site_70                                                0   \n",
              "Source                                               chk   \n",
              "\n",
              "        Actinomycetales_Brevibacteriaceae_Brevibacterium  \\\n",
              "                                                Bacteria   \n",
              "                                          Actinobacteria   \n",
              "                                          Actinobacteria   \n",
              "                                         Actinomycetales   \n",
              "                                       Brevibacteriaceae   \n",
              "                                          Brevibacterium   \n",
              "                                                     145   \n",
              "Sites                                                      \n",
              "site_67                                                0   \n",
              "site_68                                                0   \n",
              "site_69                                                0   \n",
              "site_70                                                0   \n",
              "Source                                               chk   \n",
              "\n",
              "        Erysipelotrichales_Erysipelotrichaceae_Bulleidia  \\\n",
              "                                                Bacteria   \n",
              "                                              Firmicutes   \n",
              "                                         Erysipelotrichi   \n",
              "                                      Erysipelotrichales   \n",
              "                                     Erysipelotrichaceae   \n",
              "                                               Bulleidia   \n",
              "                                                     154   \n",
              "Sites                                                      \n",
              "site_67                                                0   \n",
              "site_68                                                0   \n",
              "site_69                                                0   \n",
              "site_70                                                0   \n",
              "Source                                               chk   \n",
              "\n",
              "        Clostridiales_Clostridiaceae_Clostridium  \\\n",
              "                                        Bacteria   \n",
              "                                      Firmicutes   \n",
              "                                      Clostridia   \n",
              "                                   Clostridiales   \n",
              "                                  Clostridiaceae   \n",
              "                                     Clostridium   \n",
              "                                             214   \n",
              "Sites                                              \n",
              "site_67                                        0   \n",
              "site_68                                        0   \n",
              "site_69                                        0   \n",
              "site_70                                        0   \n",
              "Source                               chk-core-us   \n",
              "\n",
              "        Actinomycetales_Corynebacteriaceae_Corynebacterium  \\\n",
              "                                                  Bacteria   \n",
              "                                            Actinobacteria   \n",
              "                                            Actinobacteria   \n",
              "                                           Actinomycetales   \n",
              "                                        Corynebacteriaceae   \n",
              "                                           Corynebacterium   \n",
              "                                                       229   \n",
              "Sites                                                        \n",
              "site_67                                                  0   \n",
              "site_68                                                  0   \n",
              "site_69                                                  0   \n",
              "site_70                                                  0   \n",
              "Source                                         chk-core-us   \n",
              "\n",
              "        Lactobacillales_Enterococcaceae_Enterococcus  \\\n",
              "                                            Bacteria   \n",
              "                                          Firmicutes   \n",
              "                                             Bacilli   \n",
              "                                     Lactobacillales   \n",
              "                                     Enterococcaceae   \n",
              "                                        Enterococcus   \n",
              "                                                 300   \n",
              "Sites                                                  \n",
              "site_67                                            0   \n",
              "site_68                                            0   \n",
              "site_69                                            0   \n",
              "site_70                                            0   \n",
              "Source                                           chk   \n",
              "\n",
              "        Thermoanaerobacterales_Thermoanaerobacteraceae_Gelria  \\\n",
              "                                                     Bacteria   \n",
              "                                                   Firmicutes   \n",
              "                                                   Clostridia   \n",
              "                                       Thermoanaerobacterales   \n",
              "                                      Thermoanaerobacteraceae   \n",
              "                                                       Gelria   \n",
              "                                                          334   \n",
              "Sites                                                           \n",
              "site_67                                                  0      \n",
              "site_68                                                  0      \n",
              "site_69                                                  0      \n",
              "site_70                                                2.3      \n",
              "Source                                                 chk      \n",
              "\n",
              "        Oceanospirillales_Halomonadaceae_Halomonas  ...  \\\n",
              "                                          Bacteria  ...   \n",
              "                                    Proteobacteria  ...   \n",
              "                               Gammaproteobacteria  ...   \n",
              "                                 Oceanospirillales  ...   \n",
              "                                    Halomonadaceae  ...   \n",
              "                                         Halomonas  ...   \n",
              "                                               354  ...   \n",
              "Sites                                               ...   \n",
              "site_67                                   0.942935  ...   \n",
              "site_68                                  28.889007  ...   \n",
              "site_69                                          0  ...   \n",
              "site_70                                          0  ...   \n",
              "Source                                    chk-core  ...   \n",
              "\n",
              "        Actinomycetales_Propionibacteriaceae_Tessaracoccus  \\\n",
              "                                                  Bacteria   \n",
              "                                            Actinobacteria   \n",
              "                                            Actinobacteria   \n",
              "                                           Actinomycetales   \n",
              "                                      Propionibacteriaceae   \n",
              "                                             Tessaracoccus   \n",
              "                                                       715   \n",
              "Sites                                                        \n",
              "site_67                                                  0   \n",
              "site_68                                                  0   \n",
              "site_69                                                  0   \n",
              "site_70                                                  0   \n",
              "Source                                                core   \n",
              "\n",
              "        Clostridiales_Peptococcaceae_Thermincola  \\\n",
              "                                        Bacteria   \n",
              "                                      Firmicutes   \n",
              "                                      Clostridia   \n",
              "                                   Clostridiales   \n",
              "                                  Peptococcaceae   \n",
              "                                     Thermincola   \n",
              "                                             719   \n",
              "Sites                                              \n",
              "site_67                                        0   \n",
              "site_68                                        0   \n",
              "site_69                                     1.63   \n",
              "site_70                                        0   \n",
              "Source                                      core   \n",
              "\n",
              "        Spirochaetales_Spirochaetaceae_Treponema  \\\n",
              "                                        Bacteria   \n",
              "                                    Spirochaetes   \n",
              "                                    Spirochaetes   \n",
              "                                  Spirochaetales   \n",
              "                                 Spirochaetaceae   \n",
              "                                       Treponema   \n",
              "                                             731   \n",
              "Sites                                              \n",
              "site_67                                        0   \n",
              "site_68                                        0   \n",
              "site_69                                        0   \n",
              "site_70                                        0   \n",
              "Source                                      core   \n",
              "\n",
              "        Burkholderiales_Oxalobacteraceae_Oxalobacteraceae_unclassified  \\\n",
              "                                                              Bacteria   \n",
              "                                                        Proteobacteria   \n",
              "                                                    Betaproteobacteria   \n",
              "                                                       Burkholderiales   \n",
              "                                                      Oxalobacteraceae   \n",
              "                                         Oxalobacteraceae_unclassified   \n",
              "                                                                   853   \n",
              "Sites                                                                    \n",
              "site_67                                                  0               \n",
              "site_68                                                  0               \n",
              "site_69                                                  0               \n",
              "site_70                                                  0               \n",
              "Source                                                core               \n",
              "\n",
              "        Burkholderiales_Comamonadaceae_Variovorax  \\\n",
              "                                         Bacteria   \n",
              "                                   Proteobacteria   \n",
              "                               Betaproteobacteria   \n",
              "                                  Burkholderiales   \n",
              "                                   Comamonadaceae   \n",
              "                                       Variovorax   \n",
              "                                              863   \n",
              "Sites                                               \n",
              "site_67                                  0.151456   \n",
              "site_68                                         0   \n",
              "site_69                                         0   \n",
              "site_70                                         0   \n",
              "Source                                       core   \n",
              "\n",
              "        Anaerolineales_Anaerolinaceae_Wchb1-05  \\\n",
              "                                      Bacteria   \n",
              "                                   Chloroflexi   \n",
              "                                  Anaerolineae   \n",
              "                                Anaerolineales   \n",
              "                                Anaerolinaceae   \n",
              "                                      Wchb1-05   \n",
              "                                           867   \n",
              "Sites                                            \n",
              "site_67                                      0   \n",
              "site_68                                      0   \n",
              "site_69                                      0   \n",
              "site_70                                      0   \n",
              "Source                                    core   \n",
              "\n",
              "        Desulfobacterales_Desulfobacteraceae_Desulfobacterium  \\\n",
              "                                                     Bacteria   \n",
              "                                               Proteobacteria   \n",
              "                                          Deltaproteobacteria   \n",
              "                                            Desulfobacterales   \n",
              "                                           Desulfobacteraceae   \n",
              "                                             Desulfobacterium   \n",
              "                                                          264   \n",
              "Sites                                                           \n",
              "site_67                                                  0      \n",
              "site_68                                                  0      \n",
              "site_69                                                  0      \n",
              "site_70                                                  0      \n",
              "Source                                                  us      \n",
              "\n",
              "        Desulfobacterales_Desulfobulbaceae_Desulfobulbus  \\\n",
              "                                                Bacteria   \n",
              "                                          Proteobacteria   \n",
              "                                     Deltaproteobacteria   \n",
              "                                       Desulfobacterales   \n",
              "                                        Desulfobulbaceae   \n",
              "                                           Desulfobulbus   \n",
              "                                                     265   \n",
              "Sites                                                      \n",
              "site_67                                                0   \n",
              "site_68                                                0   \n",
              "site_69                                                0   \n",
              "site_70                                                0   \n",
              "Source                                                us   \n",
              "\n",
              "        Gallionellales_Gallionellaceae_Gallionella  \\\n",
              "                                          Bacteria   \n",
              "                                    Proteobacteria   \n",
              "                                Betaproteobacteria   \n",
              "                                    Gallionellales   \n",
              "                                   Gallionellaceae   \n",
              "                                       Gallionella   \n",
              "                                               332   \n",
              "Sites                                                \n",
              "site_67                                          0   \n",
              "site_68                                          0   \n",
              "site_69                                          0   \n",
              "site_70                                          0   \n",
              "Source                                          us   \n",
              "\n",
              "        Alteromonadales_Shewanellaceae_Shewanella  \n",
              "                                         Bacteria  \n",
              "                                   Proteobacteria  \n",
              "                              Gammaproteobacteria  \n",
              "                                  Alteromonadales  \n",
              "                                   Shewanellaceae  \n",
              "                                       Shewanella  \n",
              "                                              656  \n",
              "Sites                                              \n",
              "site_67                                  0.004886  \n",
              "site_68                                  0.005293  \n",
              "site_69                                         0  \n",
              "site_70                                         0  \n",
              "Source                                         us  \n",
              "\n",
              "[5 rows x 86 columns]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Integrated_T.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdOLY5SPuvgI"
      },
      "source": [
        "## 2.3. Making Sequences for Picrust fasta file\n",
        "\n",
        "Picrust Functional Analyiss requires a biom table with otus as index, samples as headers and abundance as values. The present biom has genus names but is needs instead Otus instead. The other input file for picrust is the representative sequences table that consist of the sequences per genera followed by the frequency of that genera on the whole sample, this is done directly by the software. The fasta file requires the otus instead of the genera names and the sequences non aligned coming from notebook 5. The following scrips will formate the data to picrust."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cizMvGCGuvgI",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Read and modify sequences\n",
        "new_records = []\n",
        "for record in SeqIO.parse(fasta_file_final, \"fasta\"):\n",
        "    match = re.search(r\"\\s(\\d+)\\s\", record.description)  # Look for digits surrounded by spaces\n",
        "    if match:\n",
        "        otu_id = match.group(1)\n",
        "    else:\n",
        "        print(f\"Warning: Could not extract OTU ID from description: {record.description}\")\n",
        "        continue  # Skip this record if OTU ID not found\n",
        "\n",
        "    # Create new record with only OTU as ID\n",
        "    new_record = SeqRecord(\n",
        "        record.seq,\n",
        "        id=otu_id,\n",
        "        description=\"\"  # Empty description to keep only ID\n",
        "    )\n",
        "    new_records.append(new_record)\n",
        "\n",
        "# Write modified FASTA\n",
        "output_fasta_path = output_base / \"sequences_for_picrust.fasta\"\n",
        "\n",
        "SeqIO.write(new_records, output_fasta_path, \"fasta\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RN-pFp1cuvgI"
      },
      "source": [
        "## 2.4. Making of Dataframes for 2 Different Pipelines\n",
        "The following script is the path to the biom file but also to the Integrate dataframe which create dataframes that discriminate its origin in order to pass then through picrust different pipelines, to know: Simple_Base that compares the known bacteria namely usual_taxa against the other features to understand their relationships on the function of their metabolism, an additional group is put forward as simply_candidate_mic which corresponds to the bacteria no previously linked to corrosion but showing an statistical significance with the risk label, those come from the checked_taxa and in this study are: genera(GID): Bulleida (154); Mycoplana (471), Oxobacter (512) and Oerskovia (). Also as showing an favor behaviour against corrosion are presented: Phenylobacterium (549), Gelria(334), Porphyrobacteria (564) and Tepidimonas (712)\n",
        "SIMPLE_BASE = {'known': 'simple_known_mic', 'other': 'simple_candidate_mic'}\n",
        "The second pipeline comprises a more detailed separation of the bacteria and that is: The Known bacteria as previously, pure_checked corresponding to the statistical significant genera, pure_core correspondent to the core taxa on the systems and the combination of the core and checked taxa.\n",
        "DETAILED_BASE = {'known': 'detailed_known_mic','pure_checked': 'detailed_pure_checked_mic',\n",
        "    'pure_core': 'detailed_pure_core_mic', 'checked_core': 'detailed_checked_core_mic'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akl8MkZjuvgI"
      },
      "source": [
        "__Making the Integrated dataframe__\n",
        "The original dataframe has a column for source, indicating from which df  came from (core, usual, checked), this script proceses that datadrame into individual dfs and the combined preserving the source for further analysis. The Integrated dataframe continues to be process on the next step to become the biom abundance df."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWukq8fUCqSH",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def process_integrated_data(df):\n",
        "    \"\"\"\n",
        "    Process the integrated DataFrame to create a new DataFrame with clear column names\n",
        "    and preserve all values including source information.\n",
        "\n",
        "    Parameters:\n",
        "    df (pandas.DataFrame): Input DataFrame with MultiIndex index and site columns\n",
        "\n",
        "    Returns:\n",
        "    pandas.DataFrame: Processed DataFrame with clear structure\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract genera and GIDs from the index MultiIndex\n",
        "    genera = df.index.get_level_values(6)[1:]  # Skip first row\n",
        "    gids = pd.to_numeric(df.index.get_level_values(7)[1:], errors='coerce')\n",
        "\n",
        "    # Create a new DataFrame with the extracted information\n",
        "    result_df = pd.DataFrame({\n",
        "        'Genus': genera,\n",
        "        'GID': gids\n",
        "    })\n",
        "\n",
        "    # Add the site values from the original DataFrame\n",
        "    for col in df.columns:\n",
        "        result_df[col] = df.iloc[1:][col].values\n",
        "\n",
        "    # Clean up the DataFrame\n",
        "    result_df['GID'] = pd.to_numeric(result_df['GID'], errors='coerce')\n",
        "    result_df = result_df.dropna(subset=['GID'])\n",
        "    result_df['GID'] = result_df['GID'].astype(int)\n",
        "\n",
        "    return result_df\n",
        "\n",
        "def get_taxa_groups(df):\n",
        "    \"\"\"\n",
        "    Separate the processed DataFrame into different taxa groups based on Source column\n",
        "\n",
        "    Parameters:\n",
        "    df (pandas.DataFrame): Processed DataFrame from process_integrated_data()\n",
        "\n",
        "    Returns:\n",
        "    dict: Dictionary containing DataFrames for different taxa groups\n",
        "    \"\"\"\n",
        "    # Split the data into groups based on 'Source' column patterns\n",
        "\n",
        "    # Known corrosion bacteria (any pattern with 'us')\n",
        "    known_bacteria = df[df['Source'].str.contains('us', case=False, na=False)]\n",
        "\n",
        "    # Pure checked bacteria (only 'chk' without 'core' or 'us')\n",
        "    pure_checked = df[\n",
        "        df['Source'].str.contains('chk', case=False, na=False) &\n",
        "        ~df['Source'].str.contains('core|us', case=False, na=False)\n",
        "    ]\n",
        "\n",
        "    # Pure core bacteria (only 'core' without 'chk' or 'us')\n",
        "    pure_core = df[\n",
        "        df['Source'].str.contains('core', case=False, na=False) &\n",
        "        ~df['Source'].str.contains('chk|us', case=False, na=False)\n",
        "    ]\n",
        "\n",
        "    # Checked-core bacteria (contains both 'core' and 'chk' but no 'us')\n",
        "    checked_core = df[\n",
        "        df['Source'].str.contains('chk.*core|core.*chk', case=False, na=False) &\n",
        "        ~df['Source'].str.contains('us', case=False, na=False)\n",
        "    ]\n",
        "\n",
        "    # Create groups dictionary\n",
        "    taxa_groups = {\n",
        "        'known_bacteria': known_bacteria,\n",
        "        'pure_checked': pure_checked,\n",
        "        'pure_core': pure_core,\n",
        "        'checked_core': checked_core\n",
        "    }\n",
        "\n",
        "    # Print summary statistics\n",
        "    print(\"\\nDetailed Classification Results:\")\n",
        "    print(f\"Known corrosion bacteria: {len(known_bacteria)}\")\n",
        "    print(f\"Pure checked bacteria: {len(pure_checked)}\")\n",
        "    print(f\"Pure core bacteria: {len(pure_core)}\")\n",
        "    print(f\"Checked-core bacteria: {len(checked_core)}\")\n",
        "\n",
        "    # Verify total matches expected\n",
        "    total_classified = len(known_bacteria) + len(pure_checked) + len(pure_core) + len(checked_core)\n",
        "    print(f\"\\nTotal classified taxa: {total_classified}\")\n",
        "    print(f\"Total in dataset: {len(df)}\")\n",
        "\n",
        "    return taxa_groups\n",
        "\n",
        "# Usage example:\n",
        "Integrated = process_integrated_data(pre_Integrated)\n",
        "\n",
        "# Get the groups\n",
        "taxa_groups = get_taxa_groups(Integrated)\n",
        "\n",
        "# Access individual groups -\n",
        "known_bacteria = taxa_groups['known_bacteria']\n",
        "pure_core = taxa_groups['pure_core']\n",
        "pure_checked = taxa_groups['pure_checked']\n",
        "checked_core = taxa_groups['checked_core']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g56zbUtiuvgJ"
      },
      "source": [
        "## 2.5. Making the Abundanc Biom dataframe for Picrust\n",
        "\n",
        "The final biom should have as index the Otus numbers no the genera names and a clean formate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qn6xPmvfuvgJ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# droping source and genus and putting GID as index\n",
        "pre_biom= Integrated.drop(columns=[\"Source\", \"GID\"])\n",
        "pre_biom= pre_biom.set_index(\"Genus\").astype(str)\n",
        "# Ensure all data values are float\n",
        "pre_biom = pre_biom.astype(float)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mI5nSqwXuvgJ"
      },
      "source": [
        "__changing genera to otus__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amKUleGLuvgJ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Create genus to OTU mapping from FASTA headers\n",
        "genus_to_otu = {}\n",
        "for record in SeqIO.parse(fasta_file_final, \"fasta\"):\n",
        "    parts = record.description.split()\n",
        "    if len(parts) >= 3:\n",
        "        genus = parts[0]\n",
        "        otu = parts[1]  # We'll use the first OTU number\n",
        "        genus_to_otu[genus] = otu\n",
        "\n",
        "# Print a few mappings to verify\n",
        "print(\"Sample genus to OTU mappings:\")\n",
        "for i, (genus, otu) in enumerate(list(genus_to_otu.items())[:5]):\n",
        "    print(f\"{genus} -> {otu}\")\n",
        "\n",
        "# Replace genus with OTU in the index\n",
        "pre_biom.index = pre_biom.index.map(lambda x: genus_to_otu.get(x, x))\n",
        "\n",
        "# Remove the 'Genus' name from the index\n",
        "pre_biom.index.name = \"OTU\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFYJS2KuuvgJ"
      },
      "source": [
        "__Calculation counts for picrust2__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwLLzgWLuvgJ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "scaling_factor = 10000\n",
        "# Multiply by scaling factor and round to nearest integer\n",
        "count_pre_biom = np.round(pre_biom * scaling_factor).astype(int)\n",
        "count_pre_biom"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gu2uuKLHuvgJ"
      },
      "source": [
        "__Creating the biom table formate__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PsCMsci7w8R7",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Create BIOM table with type specification\n",
        "biom_table = Table(data=count_pre_biom.values,\n",
        "                  observation_ids=count_pre_biom.index.astype(str),\n",
        "                  sample_ids=count_pre_biom.columns.astype(str),\n",
        "                  type=\"OTU table\",\n",
        "                  create_date=datetime.now().isoformat(),\n",
        "                  generated_by=\"BIOM-Format\",\n",
        "                  matrix_type=\"sparse\",\n",
        "                  matrix_element_type=\"float\")\n",
        "\n",
        "# Save with explicit format\n",
        "output_path = output_base / \"count_abundance_85.biom\"\n",
        "\n",
        "with biom_open(output_path, 'w') as f:\n",
        "    biom_table.to_hdf5(f, generated_by=\"BIOM-Format\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gV5uEFS_uvgJ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Validate the table structure\n",
        "print(\"\\nValidating table...\")\n",
        "!biom validate-table -i {output_path}\n",
        "#/home/beatriz/MIC/2_Micro/data_picrust/count_abundance_85.biom\n",
        "\n",
        "# Show table info\n",
        "!biom summarize-table -i {output_path}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzS6DEGb-PJG"
      },
      "source": [
        "Validating table...\n",
        "\n",
        "The input file is a valid BIOM-formatted file.\n",
        "Num samples: 70\n",
        "Num observations: 85\n",
        "Total count: 56747993\n",
        "Table density (fraction of non-zero values): 0.405\n",
        "\n",
        "Counts/sample summary:\n",
        " Min: 181800.000\n",
        " Max: 990578.000\n",
        " Median: 851078.500\n",
        " Mean: 810685.614\n",
        " Std. dev.: 157876.192\n",
        " Sample Metadata Categories: None provided\n",
        " Observation Metadata Categories: None provided\n",
        "\n",
        "Counts/sample detail:\n",
        "site_69: 181800.000\n",
        "site_67: 217903.000\n",
        "site_70: 270600.000\n",
        "site_26: 582999.000\n",
        "site_21: 589725.000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmMvQVofuvgK"
      },
      "source": [
        "# 3. Making the representative sequences\n",
        "\n",
        "__Convert Abundance Biom table and the Sequences into a QIIME2 artifact__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXcOOPN7uvgK",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def create_rep_seqs_with_freq(sequence_file, pre_biom_df, output_fasta):\n",
        "    \"\"\"\n",
        "    Create representative sequences with frequencies written to output\n",
        "\n",
        "    Args:\n",
        "        sequence_file: Path to FASTA file with OTU sequences\n",
        "        pre_biom_df: DataFrame with abundance data\n",
        "        output_fasta: Path to save sequences with frequencies\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Calculate total frequency for each OTU\n",
        "        total_frequencies = round(pre_biom_df.sum(axis=1), 2)\n",
        "\n",
        "        with open(output_fasta, 'w') as out:\n",
        "            for record in SeqIO.parse(sequence_file, \"fasta\"):\n",
        "                otu_id = record.id\n",
        "\n",
        "                if otu_id in total_frequencies.index:\n",
        "                    freq = total_frequencies[otu_id]\n",
        "                    sequence = str(record.seq)\n",
        "\n",
        "                    # Write sequence with frequency to FASTA\n",
        "                    out.write(f\">{otu_id} {sequence} {freq}\\n\")\n",
        "\n",
        "        # First lines of the file\n",
        "        print(\"Representative Sequences head:\")\n",
        "        with open(output_fasta, 'r') as f:\n",
        "            for i, line in enumerate(f):\n",
        "                if i < 1:  # Show first 3 sequences (header + sequence lines)\n",
        "                    print(line.strip())\n",
        "        return output_fasta\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred: {e}\")\n",
        "    return None\n",
        "\n",
        "\n",
        "# Representative sequences\n",
        "sequences_for_picrust = output_base / \"sequences_for_picrust.fasta\"\n",
        "\n",
        "output_fasta = output_base / \"representative_sequences\"\n",
        "\n",
        "repres_sequ = create_rep_seqs_with_freq(sequences_for_picrust, pre_biom, output_fasta)\n",
        "repres_sequ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ku8lUve9uvgK"
      },
      "source": [
        "__Disclamer:__ These notebook was mean to do the analysis of the functional mechanisms of bacteria using picrust2, however the capacity of the laptop was no sufficient to run it, nor colab on public library, nor a virtual machine, that is the reason why the analysis was undertaken in the galaxy website, where the data resides.\n",
        "https://usegalaxy.eu/  \n",
        "username= magicalex238"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zv-yUkfmCqSN"
      },
      "source": [
        "## 3.1. Classifying Bacteria by their Source DataFrame\n",
        "Two distinct classification approaches are implemented to categorize bacteria. The simple approach (get_bacteria_sources_simple) divides bacteria into known corrosion-causers (usual_taxa) and candidates (all others). The detailed approach (get_bacteria_sources_detailed) provides finer categorization by separating bacteria into known corrosion-causers, pure checked taxa, pure core taxa, and those present in both checked and core datasets. Please notice that this function uses df Integrated for source clasification and no abundance.biom which will be used for the picrust2 pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bDVrPwWCqSR",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def get_bacteria_sources_simple(Integrated_df):\n",
        "    \"\"\"\n",
        "    Simple classification:\n",
        "    1. Known (anything with 'us')\n",
        "    2. All others (combined chk, core, chk-core)\n",
        "    \"\"\"\n",
        "    # Get genera and gids from column levels 6 and 7\n",
        "    genera = Integrated_df[\"Genus\"]\n",
        "    gids = Integrated_df[\"GID\"]\n",
        "\n",
        "    # Look for Source in the data, not index\n",
        "    sources = Integrated_df['Source'] if 'Source' in Integrated_df.columns else None\n",
        "\n",
        "    known_bacteria = {}     # usual_taxa\n",
        "    other_bacteria = {}     # everything else\n",
        "\n",
        "    sources_found = set()\n",
        "    source ={}\n",
        "    patterns = ['us', 'core-us', 'chk-us', 'chk-core-us']\n",
        "\n",
        "    for i, (genus, gid) in enumerate (zip(genera, gids)):\n",
        "        if source is not None:  # Check if source exists for this genus\n",
        "            source = str(sources.iloc[i]).strip().lower()\n",
        "            sources_found.add(source)\n",
        "\n",
        "            if source in patterns:\n",
        "                known_bacteria[genus] = int(gid) if str(gid).isdigit() else gid\n",
        "            else:\n",
        "                other_bacteria[genus] = int(gid) if str(gid).isdigit() else gid\n",
        "\n",
        "    print(\"\\nSimple Classification Results:\")\n",
        "    print(f\"Known corrosion bacteria: {len(known_bacteria)}\")\n",
        "    print(f\"Other bacteria: {len(other_bacteria)}\")\n",
        "    print(\"\\nSources found:\", sources_found)\n",
        "\n",
        "    return {\n",
        "        'known_bacteria': known_bacteria,\n",
        "        'other_bacteria': other_bacteria\n",
        "    }\n",
        "\n",
        "def get_bacteria_sources_detailed(Integrated_df):\n",
        "    \"\"\"\n",
        "    Detailed classification with all possible combinations:\n",
        "    1. Known (usual_taxa)\n",
        "    2. Pure checked (only 'chk')\n",
        "    3. Pure core (only 'core')\n",
        "    4. Checked-core (overlap 'chk-core')\n",
        "    \"\"\"\n",
        "\n",
        "    genera = Integrated_df[\"Genus\"]\n",
        "    gids = Integrated_df[\"GID\"]\n",
        "\n",
        "    sources = Integrated_df['Source'] if 'Source' in Integrated_df.columns else None\n",
        "\n",
        "    known_bacteria = {}      # usual_taxa\n",
        "    pure_checked = {}        # only 'chk' checked_taxa\n",
        "    pure_core = {}          # only 'core' core_taxa\n",
        "    checked_core = {}       # 'chk-core' checked and core taxa\n",
        "    source ={}\n",
        "    sources_found = set()\n",
        "    patterns = ['us', 'core-us', 'chk-us', 'chk-core-us']\n",
        "\n",
        "    for i, (genus, gid) in enumerate (zip(genera, gids)):\n",
        "        if source is not None:  # Check if source exists for this genus\n",
        "            source = str(sources.iloc[i]).strip().lower()\n",
        "            sources_found.add(source)\n",
        "\n",
        "            if source in patterns:\n",
        "                known_bacteria[genus] = int(gid) if str(gid).isdigit() else gid\n",
        "                continue\n",
        "\n",
        "            # Then handle other combinations\n",
        "            if source == 'chk':\n",
        "                pure_checked[genus] = gid\n",
        "            elif source == 'core':\n",
        "                pure_core[genus] = gid\n",
        "            elif 'chk-core' in source:\n",
        "                checked_core[genus] = gid\n",
        "\n",
        "    print(\"\\nDetailed Classification Results:\")\n",
        "    print(f\"Known corrosion bacteria: {len(known_bacteria)}\")\n",
        "    print(f\"Pure checked bacteria: {len(pure_checked)}\")\n",
        "    print(f\"Pure core bacteria: {len(pure_core)}\")\n",
        "    print(f\"Checked-core bacteria: {len(checked_core)}\")\n",
        "    print(\"\\nSources found:\", sources_found)\n",
        "\n",
        "    return {\n",
        "        'known_bacteria': known_bacteria,\n",
        "        'pure_checked': pure_checked,\n",
        "        'pure_core': pure_core,\n",
        "        'checked_core': checked_core\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LSxCpNd8Yelz",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "sources_simple = get_bacteria_sources_simple(Integrated)\n",
        "\n",
        "sources_detail = get_bacteria_sources_detailed(Integrated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtD3K9OIYelz",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Extracting the genus lists for each group:\n",
        "known_bacteria_list = list(sources_detail['known_bacteria'].keys())\n",
        "pure_checked_list = list(sources_detail['pure_checked'].keys())\n",
        "pure_core_list = list(sources_detail['pure_core'].keys())\n",
        "checked_core_list = list(sources_detail['checked_core'].keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCA_PPxRYel0"
      },
      "source": [
        "The lists will be utilised later in order to groupby this list int he analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23P2k1QwCqSR"
      },
      "source": [
        "## 3.2. Prepare picrust data and Creating Directories for PICRUSt2 Input\n",
        "The check_missing_genera function processes the integrated data and handles data quality control. Known problematic genera (e.g., 'Clostridium_sensu_stricto_12', 'Oxalobacteraceae_unclassified') are flagged for exclusion to prevent analysis errors. The function also creates an organized directory structure as outlined in the introduction, with separate paths for different bacterial classifications (known_mic, candidate_mic, etc.) and their respective analysis outputs (EC_predictions, pathway_predictions, KO_predictions). Following function prepares the data for picrust analysis but both dataframes the abundance.biom and Integrated have some bacteria that were no sequenciated mostly cause are no known specimens. So it is necesary to do same procedure to both dfs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNfnbXfKCqSS",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def prepare_picrust_data(Integrated_df, aligned_file, function_type='simple'):\n",
        "    \"\"\"\n",
        "    Prepare data for PICRUSt analysis with choice of  function_type method\n",
        "\n",
        "    Args:\n",
        "        Integrated_df: Input DataFrame\n",
        "        aligned_file: Path to aligned sequences\n",
        "        function_type: 'simple' or 'detailed'\n",
        "    \"\"\"\n",
        "    # Get bacteria source_groups based on chosen  function_type\n",
        "    if  function_type == 'simple':\n",
        "        source_groups = get_bacteria_sources_simple(Integrated_df)\n",
        "    else:\n",
        "        source_groups= get_bacteria_sources_detailed(Integrated_df)\n",
        "\n",
        "    # Create appropriate directory structure\n",
        "    create_directory_structure(function_type)\n",
        "\n",
        "    return source_groups\n",
        "\n",
        "def create_directory_structure(function_type='simple'):\n",
        "    \"\"\"Create directory structure for PICRUSt analysis\"\"\"\n",
        "    base_dir = Path(\"/home/beatriz/MIC/2_Micro/data_picrust\")\n",
        "    base_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    if function_type == 'simple':\n",
        "        directories = SIMPLE_BASE\n",
        "    else:\n",
        "        directories = DETAILED_BASE\n",
        "\n",
        "    # Create all required directories\n",
        "    for dir_name in directories.values():\n",
        "        for subdir in SUBDIRS:\n",
        "            (base_dir / dir_name / subdir).mkdir(parents=True, exist_ok=True)\n",
        "    logging.info(\"Directory structure created successfully\")\n",
        "\n",
        "    return True\n",
        "\n",
        "'''  except Exception as e:\n",
        "    logging.error(f\"Error creating directory structure: {str(e)}\")\n",
        "    return False'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4v7JVU8uvgK",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def verify_input_files():\n",
        "    \"\"\"Verify that input files exist and are readable\"\"\"\n",
        "    missing_files = []\n",
        "\n",
        "    if not fasta_file.exists():\n",
        "        missing_files.append(str(fasta_file))\n",
        "    if not biom_table.exists():\n",
        "        missing_files.append(str(biom_table))\n",
        "\n",
        "    if missing_files:\n",
        "        logging.error(f\"Missing input files: {', '.join(missing_files)}\")\n",
        "        return False\n",
        "\n",
        "    logging.info(\"All input files found\")\n",
        "    return True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUwADMKZCqSS"
      },
      "source": [
        "# 4. PICRUSt Pipeline Definition\n",
        "The pipeline processes the aligned sequence data from notebook 5 that has or not undergo cleaning of the sequences as previously done on section 2. Also processes the biom_table in order to account on this anylsis on abundance. It queries the PICRUSt database to predict potential metabolic pathways for each genus. This prediction is based on evolutionary relationships and known genomic capabilities of related organisms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "srMpS5DkCqSS",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def run_picrust2_pipeline(fasta_file, biom_file, output_dir):\n",
        "    \"\"\"\n",
        "    Run the main PICRUSt2 pipeline on input sequences and BIOM table.\n",
        "\n",
        "    Args:\n",
        "        fasta_file: Path to the aligned sequences FASTA file.\n",
        "        biom_file: Path to the BIOM table (without extra columns).\n",
        "        output_dir: Directory for PICRUSt2 output.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Run main PICRUSt2 pipeline\n",
        "        cmd = [\n",
        "            'picrust2_pipeline.py',\n",
        "            '-s', fasta_file,        # Input FASTA file with aligned sequences\n",
        "            '-i', biom_file,         # BIOM table with abundance data\n",
        "            '-o', output_dir,        # Output directory\n",
        "            '--processes', '4',      # Parallel processes\n",
        "            '--verbose',\n",
        "            '--min_align', '0.25'    # Note the split here\n",
        "        ]\n",
        "        subprocess.run(cmd, check=True)\n",
        "\n",
        "        # Add pathway descriptions if the pathway file exists\n",
        "        pathway_file = os.path.join(output_dir, 'pathways_out/path_abun_unstrat.tsv.gz')\n",
        "        if os.path.exists(pathway_file):\n",
        "            cmd_desc = [\n",
        "                'add_descriptions.py',\n",
        "                '-i', pathway_file,\n",
        "                '-m', 'PATHWAY',\n",
        "                '-o', os.path.join(output_dir, 'pathways_with_descriptions.tsv')\n",
        "            ]\n",
        "            subprocess.run(cmd_desc, check=True)\n",
        "\n",
        "        print(f\"PICRUSt2 pipeline completed successfully for {output_dir}\")\n",
        "        return True\n",
        "\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Error running PICRUSt2: {e}\")\n",
        "        return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rx_DyzHbCqSS"
      },
      "source": [
        "# 5. Analysis of Pathways\n",
        "The analysis focuses on metabolic pathways known to be involved in microbially influenced corrosion, including sulfur metabolism, organic acid production, iron metabolism, and biofilm formation. These pathways were selected based on documented mechanisms of known corrosion-inducing bacteria. Separate pipeline runs for simple and detailed classifications ensure proper pathway analysis for each bacterial group."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8eP8MAidCqSS",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def analyze_functional_profiles(picrust_output_dir, bacteria_list):\n",
        "    \"\"\"\n",
        "    Analyze functional profiles with focus on corrosion-relevant pathways\n",
        "\n",
        "    Parameters:\n",
        "    picrust_output_dir: directory containing PICRUSt2 output\n",
        "    bacteria_list: list of bacteria names to analyze\n",
        "    \"\"\"\n",
        "    # Define corrosion-relevant pathways\n",
        "    relevant_pathways = [\n",
        "        'Sulfur metabolism',\n",
        "        'Iron metabolism',\n",
        "        'Energy metabolism',\n",
        "        'Biofilm formation',\n",
        "        'Metal transport',\n",
        "        'ochre formation',\n",
        "        'iron oxide deposits',\n",
        "        'iron precipitation',\n",
        "        'rust formation',\n",
        "        'organic acid production',\n",
        "        'acetate production',\n",
        "        'lactate metabolism',\n",
        "        'formate production',\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        # Read PICRUSt2 output\n",
        "        pathway_file = os.path.join(picrust_output_dir, 'pathways_with_descriptions.tsv')\n",
        "        pathways_df = pd.read_csv(pathway_file, sep='\\t')\n",
        "\n",
        "        # Filter for relevant pathways\n",
        "        filtered_pathways = pathways_df[\n",
        "            pathways_df['description'].str.contains('|'.join(relevant_pathways),\n",
        "                                                  case=False,\n",
        "                                                  na=False)]\n",
        "\n",
        "        # Calculate pathway abundances per bacteria\n",
        "        pathway_abundances = filtered_pathways.groupby('description').sum()\n",
        "\n",
        "        # Calculate pathway similarities between bacteria\n",
        "        pathway_similarities = {}\n",
        "        for bacteria in bacteria_list:\n",
        "            if bacteria in pathways_df.columns:\n",
        "                similarities = pathways_df[bacteria].corr(pathways_df[list(bacteria_list)])\n",
        "                pathway_similarities[bacteria] = similarities\n",
        "\n",
        "        # Predict functional potential\n",
        "        functional_predictions = {}\n",
        "        for pathway in relevant_pathways:\n",
        "            pathway_presence = filtered_pathways[\n",
        "                filtered_pathways['description'].str.contains(pathway, case=False)\n",
        "            ]\n",
        "            if not pathway_presence.empty:\n",
        "                functional_predictions[pathway] = {\n",
        "                    'presence': len(pathway_presence),\n",
        "                    'mean_abundance': pathway_presence.mean().mean(),\n",
        "                    'max_abundance': pathway_presence.max().max()\n",
        "                }\n",
        "\n",
        "        # Calculate correlation scores\n",
        "        correlation_scores = {}\n",
        "        for bacteria in bacteria_list:\n",
        "            if bacteria in pathways_df.columns:\n",
        "                correlations = pathways_df[bacteria].corr(\n",
        "                    pathways_df[filtered_pathways.index]\n",
        "                )\n",
        "                correlation_scores[bacteria] = {\n",
        "                    'mean_correlation': correlations.mean(),\n",
        "                    'max_correlation': correlations.max(),\n",
        "                    'key_pathways': correlations.nlargest(5).index.tolist()\n",
        "                }\n",
        "\n",
        "        comparison_results = {\n",
        "            'pathway_similarities': pathway_similarities,\n",
        "            'functional_predictions': functional_predictions,\n",
        "            'correlation_scores': correlation_scores,\n",
        "            'pathway_abundances': pathway_abundances.to_dict()\n",
        "        }\n",
        "\n",
        "        return filtered_pathways, comparison_results\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in pathway analysis: {str(e)}\")\n",
        "        return None, None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-EEla9jCqSS"
      },
      "source": [
        "## 5.2. Testing the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mnwckS6sCqST",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "'''# ---- RUNNING THE PIPELINE ----\n",
        "\n",
        "# Set paths\n",
        "fasta_file = Path('/home/beatriz/MIC/2_Micro/data_tree/accession_sequences.fasta')\n",
        "abundance_biom_file =  Path('/home/beatriz/MIC/2_Micro/data_picrust/abundance_accession.biom')\n",
        "output_dir = 'picrust_output'\n",
        "\n",
        "# List of bacteria to analyze\n",
        "bacteria_of_interest = ['Azospira', 'Brachybacterium', 'Bulleidia']\n",
        "\n",
        "# Run PICRUSt2\n",
        "if run_picrust2_pipeline(aligned_fasta_file,\n",
        "                         abundance_biom_file,\n",
        "                         output_dir\n",
        "                        ):\n",
        "    # Analyze functional profiles if the pipeline completes successfully\n",
        "    filtered_pathways, abundances = analyze_functional_profiles(output_dir, bacteria_of_interest)'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03KIf3UaCqST"
      },
      "source": [
        "# 6. Functional Analysis\n",
        "## 6.1 Running picrust full pipeline 1\n",
        "The analysis workflow begins by categorizing bacteria into source groups using the classification functions. These categorized data are then processed through the PICRUSt pipeline to predict metabolic capabilities. The functional analysis examines pathway presence, abundance, and correlations between different bacterial groups to identify potential corrosion-related metabolic patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uHpbek-BCqST",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def run_functional_analysis(df, Integrated_df, aligned_file, analysis_type='simple'):\n",
        "    \"\"\"\n",
        "    Run complete functional analysis pipeline for either simple or detailed classification\n",
        "\n",
        "    Parameters:\n",
        "    df: Input DataFrame\n",
        "    aligned_file: Path to aligned sequences file\n",
        "    analysis_type: 'simple' or 'detailed'\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"Starting {analysis_type} classification analysis\")\n",
        "        print(f\"{'='*50}\")\n",
        "\n",
        "        # Prepare data and get source groups\n",
        "        print(\"\\nStep 1: Preparing data...\")\n",
        "\n",
        "        source_groups = prepare_picrust_data(Integrated_df, aligned_file, function_type=analysis_type)\n",
        "\n",
        "        if not source_groups:\n",
        "            raise ValueError(\"Failed to prepare data: No source groups returned\")\n",
        "\n",
        "        # Base directory for PICRUSt output\n",
        "        base_dir = Path(\"~MIC/2_Micro/data_picrust\")\n",
        "\n",
        "        results = {}\n",
        "\n",
        "        if analysis_type == 'simple':\n",
        "            # Run analysis for simple classification\n",
        "            # Known bacteria\n",
        "            known_output_dir = base_dir /SIMPLE_BASE['known']\n",
        "            success_known = run_picrust2_pipeline(aligned_file, df, str(known_output_dir))\n",
        "            if success_known:\n",
        "                results_known = analyze_functional_profiles(str(known_output_dir),\n",
        "                                                        source_groups['known_bacteria'].keys())\n",
        "\n",
        "            # Other bacteria\n",
        "            other_output_dir = base_dir / SIMPLE_BASE['other']\n",
        "            success_other = run_picrust2_pipeline(aligned_file, str(other_output_dir))\n",
        "            if success_other:\n",
        "                results_other = analyze_functional_profiles(str(other_output_dir),\n",
        "                                                        source_groups['other_bacteria'].keys())\n",
        "\n",
        "        else:\n",
        "            # Run analysis for detailed classification\n",
        "            for group, dir_name in DETAILED_BASE.items():\n",
        "\n",
        "                # Known bacteria\n",
        "                known_output_dir = base_dir / DETAILED_BASE['known']\n",
        "                success_known = run_picrust2_pipeline(aligned_file, str(known_output_dir))\n",
        "                if success_known:\n",
        "                    results_known = analyze_functional_profiles(str(known_output_dir),\n",
        "                                                            source_groups['known_bacteria'].keys())\n",
        "\n",
        "                # Pure checked bacteria\n",
        "                checked_output_dir = base_dir /  DETAILED_BASE['pure_checked']\n",
        "                success_checked = run_picrust2_pipeline(aligned_file, str(checked_output_dir))\n",
        "                if success_checked:\n",
        "                    results_checked = analyze_functional_profiles(str(checked_output_dir),\n",
        "                                                            source_groups['pure_checked'].keys())\n",
        "\n",
        "                # Pure core bacteria\n",
        "                core_output_dir = base_dir /DETAILED_BASE['pure_core']\n",
        "                success_core = run_picrust2_pipeline(aligned_file, str(core_output_dir))\n",
        "                if success_core:\n",
        "                    results_core = analyze_functional_profiles(str(core_output_dir),\n",
        "                                                            source_groups['pure_core'].keys())\n",
        "\n",
        "                # Checked-core bacteria\n",
        "                checked_core_output_dir = base_dir /DETAILED_BASE['checked_core']\n",
        "                success_checked_core = run_picrust2_pipeline(aligned_file, str(checked_core_output_dir))\n",
        "                if success_checked_core:\n",
        "                    results_checked_core = analyze_functional_profiles(str(checked_core_output_dir),\n",
        "                                                                    source_groups['checked_core'].keys())\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Error running PICRUSt2: {e}\")\n",
        "\n",
        "        return \"Analysis completed successfully\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3NyEekbBCqST",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "'''# Run the analysis for both types\n",
        "# Simple source classification\n",
        "simple_results = run_functional_analysis(biom_table, aligned_file, analysis_type='simple') # output_biom\n",
        "\n",
        "# Detailed source classification\n",
        "detailed_results = run_functional_analysis(biom_table, aligned_file, analysis_type='detailed')'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4rP1kdUCqST"
      },
      "source": [
        "## 6.2 Running picrust full pipeline 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5A2a1CNLCqSW",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def run_picrust2_pipeline(fasta_file, output_dir, min_align =0.5):\n",
        "    \"\"\"\n",
        "    Run PICRUSt2 pipeline with improved error handling and path management\n",
        "\n",
        "    Args:\n",
        "        fasta_file: Path to aligned sequences fasta file (str or Path)\n",
        "        output_dir: Directory for PICRUSt2 output (str or Path)\n",
        "    \"\"\"\n",
        "    import subprocess\n",
        "    import os\n",
        "    from pathlib import Path\n",
        "\n",
        "    # Convert paths to strings\n",
        "    fasta_file = str(fasta_file)\n",
        "    output_dir = str(output_dir)\n",
        "\n",
        "    try:\n",
        "        # Verify picrust2 is available\n",
        "        picrust_check = subprocess.run(['which', 'picrust2_pipeline.py'],\n",
        "                                     capture_output=True,\n",
        "                                     text=True)\n",
        "        if picrust_check.returncode != 0:\n",
        "            raise RuntimeError(\"picrust2_pipeline.py not found. Please ensure PICRUSt2 is properly installed.\")\n",
        "\n",
        "        # Create output directory\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # Construct command as a single string\n",
        "        cmd = f\"picrust2_pipeline.py -s {fasta_file} -i {fasta_file} -o {output_dir} --processes 1 --verbose\"\n",
        "\n",
        "        # Run pipeline\n",
        "        print(f\"Running command: {cmd}\")\n",
        "        process = subprocess.run(cmd,\n",
        "                               shell=True,  # Use shell to handle command string\n",
        "                               check=True,\n",
        "                               capture_output=True,\n",
        "                               text=True)\n",
        "\n",
        "        print(\"PICRUSt2 Output:\")\n",
        "        print(process.stdout)\n",
        "\n",
        "        if process.stderr:\n",
        "            print(\"Warnings/Errors:\")\n",
        "            print(process.stderr)\n",
        "\n",
        "        # Add descriptions if pathway file exists\n",
        "        pathway_file = os.path.join(output_dir, 'pathways_out/path_abun_unstrat.tsv.gz')\n",
        "        if os.path.exists(pathway_file):\n",
        "            desc_cmd = f\"add_descriptions.py -i {pathway_file} -m PATHWAY -o {os.path.join(output_dir, 'pathways_with_descriptions.tsv')}\"\n",
        "            subprocess.run(desc_cmd, shell=True, check=True)\n",
        "\n",
        "        print(f\"PICRUSt2 pipeline completed successfully for {output_dir}\")\n",
        "        return True\n",
        "\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Error running PICRUSt2 command: {e}\")\n",
        "        print(f\"Command output: {e.output}\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"Error in pipeline: {str(e)}\")\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hbhd5NNkCqSX",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "'''# For original sequences\n",
        "aligned_file = aligned_fasta\n",
        "success = run_picrust2_pipeline(aligned_file, output_large)\n",
        "\n",
        "# For improved sequences\n",
        "optimized_file = output_large / \"picrust_optimized_sequences.fasta\")\n",
        "optimized_output = Path(\"~/MIC/2_Micro/data_picrust/optimized_results\")\n",
        "success_opt = run_picrust2_pipeline(optimized_file, optimized_output)'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EDOctt8uvgP"
      },
      "source": [
        "# 7. Findings and Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXu7ACOAuvgP"
      },
      "source": [
        "The PICRUSt2 pipeline generated a series of interconnected files revealing the functional potential of the microbial community. These files collectively map metabolic pathways, enzymatic functions, and taxonomic relationships, providing a multi-layered view of microbial functional capabilities across samples. Detailed view of the files found in the folder ~data_picrust are located in the manuscript."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Se5YK4UfuvgP"
      },
      "source": [
        "Picrust_Result_SEPP and Picrust_Result_EPA contain the descriptions, pathways and abundance of the full pipeline of picrust."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZ8njtbRuvgP",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "MetaCyc_EPA_path = input_galaxy / \"Galaxy19_PICRUSt2_Add_descriptions_on_data_8.tabular\"\n",
        "Picrust_Result= pd.read_csv(MetaCyc_EPA_path, sep = \"\\t\")\n",
        "Picrust_Result_EPA= pd.read_csv(MetaCyc_EPA_path, sep = \"\\t\")\n",
        "Picrust_Result_EPA.set_index(\"description\", inplace=True)\n",
        "Picrust_Result_EPA = Picrust_Result_EPA.drop(\"pathway\", axis=1)\n",
        "Picrust_Result_EPA.index.name = \"pathway\"\n",
        "MetaCyc_SEPP_path = input_galaxy / \"Galaxy35_Add_descriptions_SEPP.tabular\"\n",
        "Picrust_Result_SEPP= pd.read_csv(MetaCyc_SEPP_path, sep = \"\\t\")\n",
        "Picrust_Result_SEPP.set_index(\"description\", inplace=True)\n",
        "Picrust_Result_SEPP = Picrust_Result_SEPP.drop(\"pathway\", axis=1)\n",
        "Picrust_Result_SEPP.index.name = \"pathway\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H39IXM7-uvgP"
      },
      "source": [
        "## 7.1. Placement Algorithm EPA vs SEPP\n",
        "nsti_SEPP and nsti_EPA Corresponds to a sample-wide measure of how closely related the microbial taxa in that sample are to known reference genomes with two different placement algoritms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TW5Je0oouvgP",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "nsti_path_EPA = Path(input_galaxy  / \"Galaxy13_EC_weighted_nsti.tabular\")\n",
        "nsti_EPA= pd.read_csv(nsti_path_EPA, sep = \"\\t\")\n",
        "nsti_path_SEPP = Path(input_galaxy  / \"Galaxy20_EC_weighted_nsti_SEPP.tabular\")\n",
        "nsti_SEPP= pd.read_csv(nsti_path_SEPP, sep = \"\\t\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESTXja6nuvgP",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Create the scatter plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(nsti_EPA['sample'], nsti_EPA['weighted_NSTI'], alpha=0.5, label= \"EPA\", color=\"blue\")\n",
        "plt.scatter(nsti_SEPP['sample'], nsti_SEPP['weighted_NSTI'], alpha=0.5, label= \"SEPP\", color=\"gray\")\n",
        "\n",
        "# Add the threshold line\n",
        "plt.axhline(y=0.15, color='black', linestyle='--', label='Threshold (0.15)')\n",
        "\n",
        "# Customize the plot\n",
        "plt.title('NSTI Values by Site')\n",
        "plt.xlabel('Site')\n",
        "plt.ylabel('NSTI Value')\n",
        "plt.legend()\n",
        "\n",
        "# Rotate x-axis labels if there are many samples\n",
        "plt.xticks(rotation=90)\n",
        "\n",
        "# Adjust layout and display the plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_fb468duvgP"
      },
      "source": [
        "Interestingly, the results are no as expected, it was though that the algorithm for placing the sequences more convenient for the present samples was SEPP because it is design specially for 16sRNA samples and diverse microbios communities, however the samples show another story. I fail to realise that the present data has been validated with the greenes genes database with the purpose of finding more compatibility with the picrust2 database, and therefore the EPA algoritm is performing much better on the all of samples using EPA placement algoritm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUPl-r2huvgQ"
      },
      "source": [
        "## 7.2. Explore Pathway Patterns\n",
        "The pathway analysis strategy is to do a preliminar exploration before diving into specific hypotheses about organic matter metabolism and corrosion. It was chosen to start with unbiased exploratory data analysis of the PICRUSt pathways. The aim is to let the data reveal natural patterns without preconceptions. That helps to identify unexpected relationships between pathways, providing a baseline understanding of pathway distributions and relationships. This will guide subsequent targeted analyses of corrosion-relevant pathways.\n",
        "The following script takes multiple perspectives in order to visualise the data without bias and let it reveal itself. We do PCA for linear patterns, NMF for modular organization, UMAP for non-linear relationships and take different clustering approaches. The aim being to look for natural Patterns without predefined categories, so that strong strong correlations can be identified regardless of pathway type. It is visualised the distribution of pathway abundances, correlation structure, hierarchical relationships and non-linear patterns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfLGAWTXuvgQ"
      },
      "source": [
        "__Category Dict__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSQwysg7uvgQ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Define category dict outside so that all charts can use same dict\n",
        "\n",
        "category_dict = Integrated_T.T.iloc[0, 0:-1].to_dict()\n",
        "\n",
        "# Define colors and categories\n",
        "category_colors = {1: '#008800',  # Dark green\n",
        "                   2: '#FF8C00',  # Dark orange\n",
        "                   3: '#FF0000'}   # Red\n",
        "\n",
        "categories_labels = {1: 'Normal Operation',\n",
        "              2: 'Early Warning',\n",
        "              3: 'System Failure'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvoGFdS5uvgQ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def explore_pathway_patterns(df):\n",
        "    \"\"\"\n",
        "    Explore pathway patterns using multiple analytical approaches\n",
        "    \"\"\"\n",
        "    # Standardize data\n",
        "    scaler = StandardScaler()\n",
        "    scaled_data = scaler.fit_transform(df)\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    def plot_exploration_results(df, results, category_dict, category_colors, categories_labels):\n",
        "        \"\"\"\n",
        "        Create visualizations for the exploratory analysis with consistent category colors\n",
        "        \"\"\"\n",
        "        # 1. Distribution of pathway abundances with category colors - side by side\n",
        "        plt.figure(figsize=(12, 6))\n",
        "\n",
        "        # Create dictionary to store abundances by category\n",
        "        category_abundances = {cat_id: [] for cat_id in categories_labels.keys()}\n",
        "\n",
        "        # Group abundances by category\n",
        "        for site_col in df.columns:\n",
        "            if site_col.startswith('site_'):\n",
        "                site_num = int(site_col.split('_')[1])\n",
        "                category = category_dict.get(f'site_{site_num}', 0)\n",
        "                if category in category_abundances:\n",
        "                    category_abundances[category].extend(df[site_col].values)\n",
        "\n",
        "        # Plot distribution for each category side by side\n",
        "        for category_id in categories_labels.keys():\n",
        "            sns.histplot(data=category_abundances[category_id],\n",
        "                        bins=50,\n",
        "                        color=category_colors[category_id],\n",
        "                        label=categories_labels[category_id],\n",
        "                        alpha=0.6,\n",
        "                        multiple=\"layer\")\n",
        "\n",
        "        plt.title('Distribution of Pathway Abundances by Category')\n",
        "        plt.xlabel('Abundance')\n",
        "        plt.yscale('log')\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    # 2. PCA Dimensionality Reduction\n",
        "    pca = PCA(n_components=5)\n",
        "    X_pca = pca.fit_transform(scaled_data)\n",
        "    results['pca'] = {\n",
        "        'components': X_pca,\n",
        "        'explained_variance': pca.explained_variance_ratio_,\n",
        "        'loadings': pd.DataFrame(\n",
        "            pca.components_.T,\n",
        "            index=df.columns,\n",
        "            columns=[f'PC{i+1}' for i in range(5)])}\n",
        "\n",
        "    # 3 NMF for pathway modules\n",
        "    nmf = NMF(n_components=5, init='random', random_state=0, max_iter=400)\n",
        "    W = nmf.fit_transform(df.clip(lower=0))\n",
        "    H = nmf.components_\n",
        "    results['nmf'] = {\n",
        "        'W': pd.DataFrame(W, index=df.index, columns=[f'NMF{i+1}' for i in range(5)]), # Pathway contributions\n",
        "        'H': pd.DataFrame(H, columns=df.columns, index=[f'NMF{i+1}' for i in range(5)]), # Sample patterns\n",
        "        'reconstruction_err': nmf.reconstruction_err_ }\n",
        "\n",
        "    # 4 UMAP for non-linear patterns\n",
        "    umap_reducer = umap.UMAP(random_state=0)\n",
        "    umap_result = umap_reducer.fit_transform(scaled_data)\n",
        "    results['umap'] = pd.DataFrame(umap_result, index=df.index, columns=['UMAP1', 'UMAP2'])\n",
        "\n",
        "    # 5. Multiple Clustering Approaches / Hierarchical clustering\n",
        "    linkage_matrix = hierarchy.linkage(scaled_data, method='ward')\n",
        "\n",
        "    # Try different numbers of clusters\n",
        "    cluster_results = {}\n",
        "    for n_clusters in [5, 10, 15]:\n",
        "        # Hierarchical\n",
        "        hc = AgglomerativeClustering(n_clusters=n_clusters)\n",
        "        hc_labels = hc.fit_predict(scaled_data)\n",
        "\n",
        "        # K-means\n",
        "        km = KMeans(n_clusters=n_clusters, random_state=0)\n",
        "        km_labels = km.fit_predict(scaled_data)\n",
        "\n",
        "        cluster_results[n_clusters] = {'hierarchical': pd.Series(hc_labels, index=df.index, name='cluster'),\n",
        "            'kmeans': pd.Series(km_labels, index=df.index, name='cluster')}\n",
        "\n",
        "    results['clustering'] = cluster_results\n",
        "    results['linkage'] = linkage_matrix\n",
        "\n",
        "    # 4. Correlation Analysis/Spearman correlation for non-linear relationships\n",
        "    corr_matrix = spearmanr(df.T)[0]\n",
        "    results['correlation'] = pd.DataFrame(corr_matrix, index=df.index, columns=df.index)\n",
        "\n",
        "    return results, X_pca\n",
        "\n",
        "def plot_exploration_results(df, results, category_dict, category_colors, categories_labels):\n",
        "    \"\"\"\n",
        "    Create visualizations for the exploratory analysis with colored categories_labels\n",
        "    \"\"\"\n",
        "    # Modified PCA visualization with categories\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # Create subplots\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(range(1, 6), results['pca']['explained_variance'], 'bo-')\n",
        "    plt.title('PCA Explained Variance')\n",
        "    plt.xlabel('Component')\n",
        "    plt.ylabel('Explained Variance Ratio')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "\n",
        "    # Get PCA components\n",
        "    pca_data = results['pca']['components']\n",
        "\n",
        "    # Plot each category separately to create the legend\n",
        "    for category_id in categories_labels.keys():\n",
        "        # Get indices for current category\n",
        "        category_mask = [category_dict.get(f'site_{i+1}', 0) == category_id\n",
        "                        for i in range(len(pca_data))]\n",
        "\n",
        "        # Plot points for current category\n",
        "        plt.scatter(pca_data[category_mask, 0],\n",
        "                   pca_data[category_mask, 1],\n",
        "                   c=category_colors[category_id],\n",
        "                   label=categories_labels[category_id],\n",
        "                   alpha=0.6)\n",
        "\n",
        "    plt.title('PCA First Two Components')\n",
        "    plt.xlabel('PC1')\n",
        "    plt.ylabel('PC2')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    # 3. UMAP visualization with categories\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    umap_df = results['umap']\n",
        "\n",
        "    for category_id in categories_labels.keys():\n",
        "        category_mask = [category_dict.get(f'site_{i+1}', 0) == category_id\n",
        "                        for i in range(len(umap_df))]\n",
        "        category_data = umap_df[category_mask]\n",
        "\n",
        "        plt.scatter(category_data['UMAP1'],\n",
        "                   category_data['UMAP2'],\n",
        "                   c=category_colors[category_id],\n",
        "                   label=categories_labels[category_id],\n",
        "                   alpha=0.6)\n",
        "\n",
        "    plt.title('UMAP Projection of Pathways by Category')\n",
        "    plt.xlabel('UMAP1')\n",
        "    plt.ylabel('UMAP2')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # 4. Hierarchical clustering dendrogram - simplified version\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    dendrogram = hierarchy.dendrogram(\n",
        "        results['linkage'],\n",
        "        labels=df.index,  # Use index , columns instead of index\n",
        "        leaf_rotation=90,\n",
        "        leaf_font_size=8\n",
        "    )\n",
        "    plt.title('Pathway Clustering Dendrogram')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # 5. Correlation heatmap\n",
        "    plt.figure(figsize=(12, 12))\n",
        "    mask = np.triu(np.ones_like(results['correlation']))\n",
        "\n",
        "    # Create a custom colormap that uses our category colors\n",
        "    custom_cmap = sns.diverging_palette(220, 20, as_cmap=True)\n",
        "\n",
        "    sns.heatmap(results['correlation'],\n",
        "                mask=mask,\n",
        "                cmap=custom_cmap,\n",
        "                center=0,\n",
        "                vmin=-1,\n",
        "                vmax=1)\n",
        "    plt.title('Pathway Correlation Heatmap')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def identify_key_patterns(df, results):\n",
        "    \"\"\"\n",
        "    Identify and summarize key patterns in the data\n",
        "    \"\"\"\n",
        "    patterns = {}\n",
        "\n",
        "    # Find highly correlated pathway groups\n",
        "    corr = results['correlation']\n",
        "    high_corr = pd.DataFrame(\n",
        "        [(i, j, corr.loc[i,j])\n",
        "         for i in corr.index\n",
        "         for j in corr.index\n",
        "         if i < j and abs(corr.loc[i,j]) > 0.8],\n",
        "        columns=['pathway1', 'pathway2', 'correlation']\n",
        "    ).sort_values('correlation', ascending=False)\n",
        "\n",
        "    # Find pathways with strong PCA loadings\n",
        "    loadings = results['pca']['loadings']\n",
        "    strong_loadings = pd.DataFrame({\n",
        "        'PC1_contribution': abs(loadings['PC1']),\n",
        "        'PC2_contribution': abs(loadings['PC2'])\n",
        "    }).sort_values('PC1_contribution', ascending=False)\n",
        "\n",
        "    patterns['high_correlations'] = high_corr\n",
        "    patterns['strong_loadings'] = strong_loadings\n",
        "\n",
        "    return patterns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TrS9Ii_GuvgQ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Calling the function for the pipeline using EPA algoritm\n",
        "results_SEPP, X_pca_SEPP = explore_pathway_patterns(Picrust_Result_SEPP)\n",
        "plot_exploration_results(Picrust_Result_SEPP, results_SEPP, category_dict, category_colors, categories_labels)\n",
        "patterns_SEPP = identify_key_patterns(Picrust_Result_SEPP, results_SEPP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EDmJPqM7uvgQ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Calling the function for the pipeline using EPA algoritm\n",
        "results_EPA, X_pca_EPA = explore_pathway_patterns(Picrust_Result_EPA)\n",
        "plot_exploration_results(Picrust_Result_EPA, results_EPA, category_dict, category_colors, categories_labels)\n",
        "patterns_EPA = identify_key_patterns(Picrust_Result_EPA, results_EPA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R59lKZ60uvgQ"
      },
      "source": [
        "__Discussing first results__\n",
        "\n",
        "The distribution of pathway abundances shows a typical microbial community pattern with few dominant pathways, suggesting key metabolic processes are essential across samples. PCA analysis reveals that only two components explain over 80% of the variance, indicating that metabolism in these systems might be driven by two major functional groups. The UMAP visualization confirms this binary pattern through two distinct clusters, demonstrating the robustness of this separation across different dimensional reduction techniques. The hierarchical clustering dendrogram further validates this division by showing two major branches, which notably align with previously observed physicochemical patterns in our Pourbaix plot analysis. The correlation heatmap exhibits strong relationships between specific pathway groups, suggesting coordinated metabolic activities that require detailed pathway mapping for full biological interpretation. EPA sequence placement shows much better differenciation on the pc plot."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RI1NL6_uuvgQ"
      },
      "source": [
        "## 7.3. Distribution of pathway abundances and Heatmap Hierarchies\n",
        "In the following script we map the column pathway on the dataframe Picrust_Result_raw to the actual names provided by the Galaxy website that corresponds to the MetaCyc pathways. We will end up with the original Picrust_Results df with disernible names.After the 20 most abundant pathways will be plotted and the heatmap with the hierarchichal pathways drawn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zczh30NruvgQ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Define category dict outside so that all charts can use same dict\n",
        "\n",
        "category_dict = Integrated_T.T.iloc[0, 0:-1].to_dict()\n",
        "\n",
        "# Define colors and categories\n",
        "category_colors = {1: '#008800',  # Dark green\n",
        "                   2: '#FF8C00',  # Dark orange\n",
        "                   3: '#FF0000'}   # Red\n",
        "\n",
        "categories_labels = {1: 'Normal Operation',\n",
        "              2: 'Early Warning',\n",
        "              3: 'System Failure'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "praqb_f6uvgQ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def analyze_pathway_patterns(df, mean_abundances, category_dict, top_n=20):\n",
        "    \"\"\"\n",
        "    Create two separate visualizations for pathway analysis:\n",
        "    1. Stacked bar chart of top pathways by system state\n",
        "    2. Correlation heatmap of top pathways\n",
        "\n",
        "    Parameters:\n",
        "    df: DataFrame with pathway data\n",
        "    mean_abundances: Series with pre-calculated mean abundances\n",
        "    category_dict: Dictionary mapping sites to risk categories\n",
        "    top_n: Number of top pathways to display\n",
        "    \"\"\"\n",
        "    # Get top pathways\n",
        "    top_pathways = mean_abundances.nlargest(top_n)\n",
        "\n",
        "    # 1. Stacked Bar Chart\n",
        "    plt.figure(figsize=(15, 8))\n",
        "\n",
        "    # Prepare data for stacking\n",
        "    pathway_data = []\n",
        "    for pathway in top_pathways.index:\n",
        "        cat_means = {}\n",
        "        for cat in [1, 2, 3]:\n",
        "            cat_sites = [site for site, c in category_dict.items() if c == cat]\n",
        "            if cat_sites:\n",
        "                cat_means[cat] = df.loc[pathway, cat_sites].mean()\n",
        "            else:\n",
        "                cat_means[cat] = 0\n",
        "        pathway_data.append((pathway, cat_means))\n",
        "\n",
        "    # Create stacked bars\n",
        "    bottoms = np.zeros(len(top_pathways))\n",
        "    for cat in [1, 2, 3]:\n",
        "        values = [d[1][cat] for d in pathway_data]\n",
        "        plt.bar(range(len(top_pathways)), values, bottom=bottoms,\n",
        "                label=categories_labels[cat], color=category_colors[cat], alpha=0.7)\n",
        "        bottoms += values\n",
        "\n",
        "    plt.title('Top 20 Most Abundant Pathways by System State', fontsize=14, pad=20)\n",
        "    plt.xlabel('Pathway', fontsize=12)\n",
        "    plt.ylabel('Mean Abundance', fontsize=12)\n",
        "    plt.xticks(range(len(top_pathways)), top_pathways.index,\n",
        "               rotation=45, ha='right', fontsize=10)\n",
        "    plt.legend(title='System State', title_fontsize=12, fontsize=10)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # 2. Correlation Heatmap (separate figure)\n",
        "    plt.figure(figsize=(15, 12))\n",
        "    top_data = df.loc[top_pathways.index]\n",
        "    corr = top_data.T.corr()\n",
        "\n",
        "    # Create mask for upper triangle\n",
        "    mask = np.triu(np.ones_like(corr), k=1)\n",
        "\n",
        "    # Create heatmap with improved readability\n",
        "    sns.heatmap(corr,\n",
        "                mask=mask,\n",
        "                cmap='coolwarm',\n",
        "                center=0,\n",
        "                annot=True,\n",
        "                fmt='.2f',\n",
        "                square=True,\n",
        "                cbar_kws={'label': 'Correlation Coefficient'},\n",
        "                annot_kws={'size': 8})\n",
        "\n",
        "    plt.title('Pathway Correlation Heatmap\\n(Top 20 Most Abundant)',\n",
        "              fontsize=14,\n",
        "              pad=20)\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return corr, top_data\n",
        "\n",
        "#Calculate mean abundances and run analysis\n",
        "mean_abundances_epa = Picrust_Result_EPA.mean(axis=1)\n",
        "corr_epa, top_data = analyze_pathway_patterns(Picrust_Result_EPA, mean_abundances_epa, category_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meleGILNuvgR"
      },
      "source": [
        "__Discussing the 20 biggest metabolisms and their Hierarchical Heatmap__\n",
        "The metabolic pathway analysis reveals aerobic respiration as the dominant metabolism, showing approximately 75% higher abundance than other pathways across all systems. Correlation analysis highlights strong relationships between aerobic respiration and key metabolic processes, including TCA cycles and amino acid biosynthesis pathways, particularly those involved in biofilm formation. While these patterns provide insights into the overall metabolic landscape, a more detailed analysis separating corroded and non-corroded systems, along with integration of physicochemical variables and risk labels, would be necessary for actionable conclusions about corrosion processes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LmHp51DYel2"
      },
      "source": [
        "## 7.4. Distribution of Reactions abundances and Heatmap Hierarchies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISSI1EOxyxkB",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#parsing pathways (PWY) to the reactions (RXN), parce has a single column with 575 rows, that will mean that the patways can be more than once with different reactions\n",
        "parce_path = input_galaxy / \"Galaxy17_parsed_mapfile.tabular\"\n",
        "parce= pd.read_csv(parce_path, sep = \"\\t\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSBe_WevhaZy"
      },
      "source": [
        "However attemps to parse the parce df were no suscessful, therefore it was used elsewhere."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65XT_sm0R0lC",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "print(parce.head(20))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPFMKGrgYel2",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# reaction is a regroup file comprises the list of reactions in the index and the sites with abundances, similar to the pathways with abundances master file\n",
        "# whiles pathways has 366 rows (pathway), react has 2956 rows(reactions)\n",
        "react_path = input_galaxy / \"Galaxy18_regrouped_infile.tabular\"\n",
        "react= pd.read_csv(react_path, sep = \"\\t\")\n",
        "react = react.set_index(\"function\")\n",
        "react.index = react.index.astype(str)\n",
        "# Sort columns numerically\n",
        "def sort_sites_numerically(df):\n",
        "    # Extract site numbers\n",
        "    site_numbers = [int(col.replace('site_', '')) for col in df.columns if col.startswith('site_')]\n",
        "\n",
        "    # Create sorted column list\n",
        "    sorted_cols = ['site_' + str(num) for num in sorted(site_numbers)]\n",
        "\n",
        "    # Return reordered dataframe\n",
        "    return df[sorted_cols]\n",
        "react = sort_sites_numerically(react)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYplXnPPv-j-"
      },
      "source": [
        "The parce dataframe was used for parsing the names of the function on the df reaction but the results were no human readable, so an api call was done to the rea, kegg dbs to get the names, however none of them gave results, a manual retrieval of the top 20 was done through https://gem-aureme.genouest.org/."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDPL2I7v65sa",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "react['mean_abundances'] = react.mean(axis=1)\n",
        "\n",
        "# Get the top 20 most abundant functions\n",
        "top_functions = react['mean_abundances'].nlargest(20)\n",
        "top_functions.index = top_functions.index.astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctzF9Q1CPW6o",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "top_functions.index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_tOz3yil85rb",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "rxn_dict = {'DNA-DIRECTED-DNA-POLYMERASE-RXN': 'DNA-directed DNA Polymerase',\n",
        " 'RXN-11135' : 'DNA helicase (DNA repair), Rad3 type' ,\n",
        " 'NADH-DEHYDROG-A-RXN': 'NADH dehydrogenase' ,\n",
        " '2.7.13.3-RXN': 'histidine kinase' ,\n",
        " 'PEPTIDYLPROLYL-ISOMERASE-RXN': 'peptidylprolyl isomerase ',\n",
        " '3-OXOACYL-ACP-REDUCT-RXN': '3-oxoacyl-(acyl-carrier-protein)',\n",
        " 'RXN-10060': '3-oxocerotoyl-[acp] reductase',\n",
        " 'RXN-10655': '3-oxo-cis-Δ7-tetradecenoyl-[acp] reductase',\n",
        " 'RXN-10659': '3-oxo-cis-Δ9-hexadecenoyl-[acp] reductase',\n",
        " 'RXN-11476': '3-oxo-glutaryl-[acp] methyl ester reductase',\n",
        " 'RXN-11480': '3-oxo-pimeloyl-[acp] methyl ester reductase',\n",
        " 'RXN-13008': '3-oxo-docosapentaenoyl [acp][c] ',\n",
        " 'RXN-16616': '(5Z)-3-oxo-tetradec-5-enoyl-[acyl-carrier-protein] reductase',\n",
        " 'RXN-16622': '(7Z)-3-oxo-hexadec-7-enoyl-[acp] reductase',\n",
        " 'RXN-16626': '(9Z)-3-oxo-octadec-9-enoyl-[acp] reductase',\n",
        " 'RXN-16630': '(11Z)-3-oxo-icos-11-enoyl-[acp] reductase',\n",
        " 'RXN-9514': 'acetoacetyl-[acyl-carrier protein] reductase',\n",
        " 'RXN-9518': '3-hydroxyhexanoyl-[acyl-carrier protein] reductase',\n",
        " 'RXN-9524': '3-oxo-octanoyl-[acyl-carrier protein] reductase',\n",
        " 'RXN-9528': '3-oxo-decanoyl-[acyl-carrier protein] reductase'}\n",
        "\n",
        "clean_rxn_dict = {k.strip(): v for k, v in rxn_dict.items()}\n",
        "\n",
        "react= react.rename(index =clean_rxn_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7kaHJFkYel2",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def analyze_reaction_patterns(top_functions, mean_abundances, category_dict):\n",
        "    \"\"\"\n",
        "    Analyzes pathway patterns for a DataFrame with 'function' as index and 'Sites' as columns.\n",
        "    This is the FINAL, CORRECTED implementation.\n",
        "    \"\"\"\n",
        "    # 1. Stacked Bar Chart\n",
        "    plt.figure(figsize=(15, 8))\n",
        "\n",
        "    function_data = []\n",
        "    for function in top_functions.index:\n",
        "        cat_means = {}\n",
        "        for cat in [1, 2, 3]:\n",
        "            cat_sites = [site for site, c in category_dict.items() if c == cat]\n",
        "            # Optimized site selection:\n",
        "            relevant_sites = list(df.columns.intersection(cat_sites)) # More efficient intersection\n",
        "            if relevant_sites:\n",
        "                cat_means[cat] = df.loc[function, relevant_sites].mean()\n",
        "            else:\n",
        "                cat_means[cat] = 0\n",
        "        function_data.append((function, cat_means))\n",
        "\n",
        "    bottoms = np.zeros(len(top_functions))\n",
        "    for cat in [1, 2, 3]:\n",
        "        values = [d[1][cat] for d in function_data]\n",
        "        plt.bar(range(len(top_functions)), values, bottom=bottoms,\n",
        "                label=categories_labels[cat], color=category_colors[cat], alpha=0.7)\n",
        "        bottoms += values\n",
        "\n",
        "    plt.title('Top 20 Most Abundant Functions by System State', fontsize=14, pad=20)\n",
        "    plt.xlabel('Function', fontsize=12)\n",
        "    plt.ylabel('Mean Abundance', fontsize=12)\n",
        "    plt.xticks(range(len(top_functions)), top_functions.index, rotation=45, ha='right', fontsize=10)\n",
        "    plt.legend(title='System State', title_fontsize=12, fontsize=10)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # 2. Correlation Heatmap\n",
        "    plt.figure(figsize=(15, 12))\n",
        "    top_data = df.loc[top_functions.index]\n",
        "    # Convert all columns of top_data to numeric, coercing errors to NaN\n",
        "    top_data = top_data.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "    # Drop rows with any NaN values to ensure only numeric data is used for correlation\n",
        "    top_data = top_data.dropna(axis=1, how='all')\n",
        "    # Check if there are any columns left after dropping NaNs\n",
        "    if top_data.empty:\n",
        "        print(\"Warning: DataFrame is empty after dropping NaN columns. Skipping correlation heatmap.\")\n",
        "        return None  # Or return an empty DataFrame or a placeholder\n",
        "\n",
        "    corr = top_data.T.corr()  # Transpose for function correlation\n",
        "\n",
        "    mask = np.triu(np.ones_like(corr), k=1)  # Mask for upper triangle\n",
        "    sns.heatmap(corr, mask=mask, cmap='coolwarm', center=0, annot=True, fmt='.2f',\n",
        "                square=True, cbar_kws={'label': 'Correlation Coefficient'}, annot_kws={'size': 8})\n",
        "\n",
        "    plt.title('Function Correlation Heatmap\\n(Top 20 Most Abundant)', fontsize=14, pad=20)\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return corr, top_data\n",
        "\n",
        "# Convert numeric columns to the correct data type\n",
        "for col in react.columns[1:]:  # Exclude 'function' column\n",
        "    try:\n",
        "        react[col] = pd.to_numeric(react[col], errors='coerce') # Skip errors but convert rest\n",
        "    except ValueError:\n",
        "        print(f\"Could not convert column '{col}' to numeric. Check its contents.\")\n",
        "        # Handle the error or investigate the column for non-numeric values\n",
        "\n",
        "# Calculate the mean after type conversion\n",
        "mean_abundances_react = react.mean(axis=1, numeric_only=True) # Specify only numeric in case strings remain\n",
        "corr_react, top_data = analyze_pathway_patterns(react, mean_abundances_react, category_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYrg4kbRUSll"
      },
      "source": [
        "Reactions chart confirm our label system, the clear progression of abundance across our system states (normal operation → early warning → system failure) is particularly compelling evidence that these reactions are directly involved in the corrosion process rather than just coincidental. Also the top reactions corresponds to core taxa that however are seem to be implicated on the corrosion failure. Additionally the fact that most of the reactions deal with oxo groups on an aromatic ring, further reinfor the study intuition that oxalic and acetic acid can be a good dummy compounds to represent organic matter on the TOC. These small organic acids are likely end products or intermediates of the metabolic pathways involving the 3-oxoacyl compounds so abundant in the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-Xa6aVgUSGf"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0Sbd6A7uvgR"
      },
      "source": [
        "# 8. Mapping the Pathways back to the Genera\n",
        "\n",
        "The result we obtained from the picrust pipeline contain the following dataframes, here described so it would be possible to parse. Following are the files description with the shape\n",
        "| Picrust_Result | Picrust_Result | Picrust_Result | parce | parce | parce | ECcontri | ECcontri | ECcontri | ECcontri | React | React |\n",
        "|---|---|---|---|---|---|---|---|---|---|---|---|\n",
        "| pathway | description | Sites/abund | pathway | RXN | EC number | EC number | varios abundances | Sites | OTU | Sites/abund | Reactions |\n",
        "|366,72|366,72|366,72|574,1|574,1|574,1|1491288, 9|1491288, 9|1491288, 9|1491288, 9| (2955, 71)|(2955, 71)|\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGMCpOrRuvgR",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ECcontri and KOcontri files contain sample, function (EC/KO number), taxon (genus/OTU ID), and abundance metrics.\n",
        "ECcontri_path = input_galaxy / \"Galaxy26_contrib.tabular\"\n",
        "\n",
        "#ECcontri_path =  Path(base_dir / \"Galaxy26_contrib.tabular\") # for Kaggle\n",
        "ECcontri= pd.read_csv(ECcontri_path, sep = \"\\t\")\n",
        "#KOcontri_path = Path(large_dir / \"Galaxy30-[KO_pred_metagenome_contrib].tabular\")\n",
        "#KOcontri= pd.read_csv(KOcontri_path, sep = \"\\t\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HB57hdTiuvgR"
      },
      "source": [
        "## 8.1. Mapping Genera to Otu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_7z90_iuvgR",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Mapping the Genera to Otu for the Taxonomy assigment requeriment\n",
        "def create_otu_mapping(fasta_file_final):\n",
        "    \"\"\"Creates a DataFrame mapping OTUs to genera from a FASTA file\n",
        "    Args: fasta_file (str): Path to FASTA file\n",
        "    Returns: pd.DataFrame: DataFrame with columns ['Genus', 'OTU']\n",
        "    \"\"\"\n",
        "    mapping_data = []\n",
        "\n",
        "    for record in SeqIO.parse(fasta_file_final, \"fasta\"):\n",
        "        # Split description to get genus and OTU\n",
        "        parts = record.description.split()\n",
        "        genus = parts[0]\n",
        "        otu = parts[1]  # Take first OTU number\n",
        "\n",
        "        mapping_data.append({'Genus': genus,'OTU': otu})\n",
        "\n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame(mapping_data).sort_values('Genus')\n",
        "\n",
        "    return df\n",
        "\n",
        "otu_mapping = create_otu_mapping(fasta_file_final)\n",
        "# Change the name of the Otus since they using taxon\n",
        "otu_mapping = otu_mapping.rename(columns={\"OTU\" : \"taxon\"})\n",
        "\n",
        "otu_path = output_base / \"otu_mapping.tsv\"\n",
        "otu_mapping.to_csv(otu_path, sep='\\t', index= False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SWsDEMsuvgR"
      },
      "source": [
        "Data Extraction from the Parce File: The parce file, containing pathway, reaction, and EC number information, is used to extract EC numbers. These EC numbers are intended to link to the corresponding pathways.\n",
        "\n",
        "Initial Mapping Approach: Pathways (from Picrust_Result) were mapped to the parce file to ensure accurate EC–pathway links.\n",
        "Reactions were similarly mapped to maintain correct RXN–pathway links.\n",
        "These mappings were then used to update the ECcontri dataframe, which represents stratified pathway abundance contributions (including KO/EC, taxon, taxon abundance, etc.).\n",
        "Unmapped EC numbers were kept separate for review.\n",
        "Revised Mapping Strategy: Due to incomplete overlap between the EC numbers and pathways in the parce file and ECcontri, the strategy was adjusted. Instead of relying solely on the parce file, the mapping now integrates the 'description' and 'pathways' columns directly from the Picrust_Result file into ECcontri. This integration is performed by matching on the 'Site' column, rather than using the EC number from the function column.\n",
        "\n",
        "Final Integration: The taxonomy assignment file (linking OTUs to genera) is joined with ECcontri via the taxon column. This final combined dataset (Picrust_Result joined with ECcontri) provides complete pathway descriptions and abundance data for subsequent visualizations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgN_STGJj9Qz"
      },
      "source": [
        "## 8.3. Pathway Mapping Analysis\n",
        "\n",
        "There were identified a discrepancy between EC predictions and pathway abundances. Found 61 pathways with EC number evidence that were not included in final predictions. Total number of reference pathways: 574 (from MetaCyc), total pathways in final predictions: 366, example missing pathway: PWY-6486 supported by EC:4.2.1.41\n",
        "\n",
        "Implications\n",
        "This finding suggests that the pathway prediction pipeline might be filtering out potentially relevant pathways despite having supporting EC evidence. This could impact the biological interpretation of the functional profiles and warrants further investigation.\n",
        "So in this study we mapped the pathways dataframe directly to the parce file and in doing so, we have also the reaction information, avoiding the discrepancy with the Picrust_Result missing pathways."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCs2cimyuvgS"
      },
      "source": [
        "## 8.4. Map Econtri to pathways"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEkDJg5zuvgS"
      },
      "source": [
        "It is possible now directly map the description and the pathway from Picrust_Result into ECcontri because each site can have several pathways, so we reshaping the Picrust_Result to long format and so that each row corresponds to a pathway for a given site. It is no possible to do this on a go using the whole 1491288 rows on ECcontri, so it would have to be done on agreggated data, as suggested by McKinney, 2010.\n",
        "Source: McKinney, W. (2010). Data Structures for Statistical Computing in Python. Retrieved from https://pandas.pydata.org/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npxOZlx8uvgS",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Reshape Picrust_Result to long format: each row now corresponds to a pathway for a given site\n",
        "picrust_long = Picrust_Result.melt(id_vars=['pathway', 'description'],\n",
        "                                   var_name='sample',\n",
        "                                   value_name='abundance')\n",
        "\n",
        "# Filter out rows where the abundance is 0\n",
        "picrust_long = picrust_long[picrust_long['abundance'] > 0]\n",
        "\n",
        "# Aggregate pathway info per site\n",
        "mapping = picrust_long.groupby('sample').agg({\n",
        "    'pathway': lambda x: list(x),\n",
        "    'description': lambda x: list(x)\n",
        "}).reset_index()\n",
        "\n",
        "# Merge the aggregated mapping with ECcontri\n",
        "ECcontri_agg_site = pd.merge(ECcontri, mapping, on='sample', how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5EXHu-QeuvgS",
        "trusted": true
      },
      "outputs": [],
      "source": [
        " # Add genus information from otu_mapping\n",
        "ECcontri_agg_site['taxon'] = ECcontri_agg_site['taxon'].astype(str)\n",
        "otu_mapping['taxon'] = otu_mapping['taxon'].astype(str)\n",
        "\n",
        "ECcontri_otu= pd.merge(ECcontri_agg_site, otu_mapping, on='taxon', how='left', validate='m:1')\n",
        "\n",
        "unmapped = ECcontri_otu['Genus'].isna().sum()\n",
        "if unmapped > 0:\n",
        "    print(f\"Warning: {unmapped} rows could not be mapped to genera\")\n",
        "# Rename columns: here \"description\" becomes \"pathway\" and \"pathway\" becomes \"npath\"\n",
        "ECcontri_otu  = ECcontri_otu.rename(columns={\"sample\":\"Sites\", \"function\": \"EC\", \"taxon\": \"OTU\", \"description\":\"pathway\", \"pathway\":\"npath\",\n",
        "                                     \"taxon_abun\": \"abund_raw\", \"taxon_function_abun\": \"abund_contri\", \"taxon_rel_abun\": \"rel_abund_raw\",\n",
        "                                       \"taxon_rel_function_abun\": \"rel_abund_contri\", \"norm_taxon_function_contrib\" :\"norm_abund_contri\", \"genome_function_count\":\"genome_EC_count\"})\n",
        "# Organize columns in logical groups\n",
        "cols_order = ['Sites', 'Genus', 'OTU', 'EC', # Identification columns\n",
        "              'npath', 'pathway', # Pathway information\n",
        "              'abund_raw', 'rel_abund_raw', # Raw abundance metrics\n",
        "              'genome_EC_count', 'abund_contri', 'rel_abund_contri', 'norm_abund_contri'] # Contribution metrics\n",
        "# Reorder columns, takes like 4 minutes on this slow laptop\n",
        "ECcontri_otu = ECcontri_otu[cols_order]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RaauSfmoP0tb",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "ECcontri_otu.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPaVL4T0uvgS"
      },
      "source": [
        "ECcontri_otu is a comprehensive dataframe that combines site locations, taxonomic information (genera and OTUs), enzyme classifications (ECs), and pathways (code for pathway (npath) and description (pathway)). The associated abundance metrics belong to the original ECcontri. The abundance metrics include:\n",
        "abund_raw: The original count of each organism (OTU) at each site\n",
        "rel_abund_raw: The relative abundance of each organism at each site, expressed as a proportion of total counts\n",
        "genome_function_count represents the predicted number of copies of a particular EC number (enzyme) in an organism's genome. This prediction comes from PICRUSt's hidden-state prediction process, which infers gene family abundances for each organism based on its phylogenetic placement relative to reference genomes\n",
        "abund_contri: The contribution of each organism to a specific enzyme function, calculated by multiplying the raw abundance by the number of copies of that enzyme in the organism's genome\n",
        "rel_abund_contri: The relative contribution of each organism to the enzyme function, accounting for both abundance and genome copy number\n",
        "norm_abund_contri: The normalized contribution metric that allows comparison across different sites and functions\n",
        "\n",
        "## 8.5. Statistical Analysis of the Genome Function Count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xeKQezwyuvgS",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Analyze genome_function_count\n",
        "print(\"Genome function count statistics:\")\n",
        "print(\"\\nOverall statistics:\")\n",
        "print(ECcontri_otu['genome_EC_count'].describe())\n",
        "\n",
        "# Look at distribution by EC number\n",
        "print(\"\\nExample EC numbers and their genome counts:\")\n",
        "ec_counts = ECcontri_otu.groupby('EC')['genome_EC_count'].agg(['unique', 'mean', 'max']).head()\n",
        "print(ec_counts)\n",
        "\n",
        "# Check if genome_function_count is consistent for each OTU-EC pair\n",
        "print(\"\\nCheck if genome_EC_count is consistent for OTU-EC combinations:\")\n",
        "consistency_check = ECcontri_otu.groupby(['OTU', 'EC'])['genome_EC_count'].nunique()\n",
        "inconsistent = consistency_check[consistency_check > 1]\n",
        "if len(inconsistent) > 0:\n",
        "    print(f\"Found {len(inconsistent)} OTU-EC pairs with inconsistent genome counts\")\n",
        "else:\n",
        "    print(\"Genome counts are consistent for all OTU-EC pairs\")\n",
        "\n",
        "# Explain the metrics in the dataframe\n",
        "print(\"\\nDataframe Components:\")\n",
        "print(\"1. Abundance Metrics:\")\n",
        "print(\"   - abund_raw: Raw abundance of each organism in each site\")\n",
        "print(\"   - abund_contri: Organism's abundance contribution to function/pathway\")\n",
        "print(\"   - rel_abund_raw: Original relative abundance\")\n",
        "print(\"   - rel_abund_contri: Relative abundance contribution to pathway\")\n",
        "print(\"   - norm_abund_contri: Normalized abundance contribution\")\n",
        "print(\"\\n2. Genome Function Count:\")\n",
        "print(\"   Number of copies of each EC (enzyme) in organism's genome\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpjxR3vk-PJK"
      },
      "source": [
        "Genome function count statistics:\n",
        "\n",
        "Overall statistics:\n",
        "count    1.491288e+06\n",
        "mean     1.390277e+00\n",
        "std      1.071974e+00\n",
        "min      1.000000e+00\n",
        "25%      1.000000e+00\n",
        "50%      1.000000e+00\n",
        "75%      1.000000e+00\n",
        "max      1.000000e+01\n",
        "Name: genome_EC_count, dtype: float64\n",
        "\n",
        "Example EC numbers and their genome counts:\n",
        "                                    unique      mean  max\n",
        "EC                                                       \n",
        "EC:1.1.1.1              [3, 2, 1, 5, 4, 8]  2.310375    8\n",
        "EC:1.1.1.100  [8, 5, 2, 3, 4, 9, 10, 6, 1]  4.237317   10\n",
        "EC:1.1.1.102                           [1]  1.000000    1\n",
        "EC:1.1.1.103                           [1]  1.000000    1\n",
        "EC:1.1.1.105                           [1]  1.000000    1\n",
        "\n",
        "Check if genome_EC_count is consistent for OTU-EC combinations:\n",
        "Genome counts are consistent for all OTU-EC pairs\n",
        "\n",
        "Dataframe Components:\n",
        "1. Abundance Metrics:\n",
        "   - abund_raw: Raw abundance of each organism in each site\n",
        "   - abund_contri: Organism's abundance contribution to function/pathway\n",
        "   - rel_abund_raw: Original relative abundance\n",
        "   - rel_abund_contri: Relative abundance contribution to pathway\n",
        "   - norm_abund_contri: Normalized abundance contribution\n",
        "\n",
        "2. Genome Function Count:\n",
        "   Number of copies of each EC (enzyme) in organism's genome"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MA-sQJpQuvgS"
      },
      "source": [
        "Analysis of genome_function_count(genome_EC_count) shows that most organisms typically have just one copy of any given enzyme (EC number) in their genome, with 75% of all cases showing a single copy. However, there is notable variation, with some organisms having up to 10 copies of certain enzymes. The average across all cases is 1.4 copies per enzyme per organism.\n",
        "Some enzymes show more variation than others. For example:\n",
        "\n",
        "EC:1.1.1.1 varies from 1 to 8 copies across different organisms\n",
        "EC:1.1.1.100 shows the widest range, from 1 to 10 copies\n",
        "Many enzymes (like EC:1.1.1.102, 103, 105) consistently appear as single copies\n",
        "\n",
        "Importantly, the copy number is consistent for each organism-enzyme combination across all sites, indicating this is a stable genomic characteristic.\n",
        "____________________________________________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voefPo-VuvgS"
      },
      "source": [
        "\n",
        "\n",
        "Now ECcontri_otu has several rows and columns providing information of the EC contribution to the metrics to each enzime aka EC number to the sites, genera combination, however the pathways are from origin link to most of the sites. This is perhaps because the methos infwee dunxriona bAWS ON XOMON sets of reference genomes.  Then, same environment in this case heating and cooling water systems poses similar organisms with similar pathways, the difference being on the abundance. So in order for this data to be usable, it is necesary to parse the EC into human readable information from a external enzyme databases to retrieve functional information about an EC number. Common resources include:\n",
        "\n",
        "UniProt: query UniProt’s REST API to get enzyme details by searching with the EC number.\n",
        "ExPASy Enzyme Database: Provides enzyme information based on EC numbers.\n",
        "BRENDA: A comprehensive enzyme database that can be queried either via its web interface or programmatically (e.g., using the bioservices Python package). Following script creates an EnzymeRetriever class that handles API requests to UniProt, processes unique EC numbers to avoid duplicate requests\n",
        "Adds protein names, functions, and UniProt IDs to ECcontri_otu df and includes rate limiting to avoid API restrictions.\n",
        "The retrieval was done localy using vscode and via colab because the retrieval require a superior ram and cpu, it took batches spread on several days.\n",
        "\n",
        "## 8.6. Retrieval of protein names from Uniprot though Api call"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTO90O2duvgT",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class ColabEnzymeRetriever:\n",
        "    def __init__(self, batch_size=100, save_every=5):\n",
        "        self.uniprot_api = \"https://rest.uniprot.org/uniprotkb/search\"\n",
        "        self.batch_size = batch_size\n",
        "        self.save_every = save_every\n",
        "        self.results_file = Path('uniprot_results.tsv')\n",
        "        self.state_file = Path('retrieval_state.json')\n",
        "        self.processed_pairs: Set[Tuple[str, str]] = set()\n",
        "        self.existing_results = None\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "        self.logger.setLevel(logging.INFO)\n",
        "\n",
        "        if not self.logger.handlers:\n",
        "            handler = logging.StreamHandler()\n",
        "            formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
        "            handler.setFormatter(formatter)\n",
        "            self.logger.addHandler(handler)\n",
        "\n",
        "    def load_existing_results(self, file_path: Path) -> pd.DataFrame:\n",
        "        \"\"\"Load and validate existing results\"\"\"\n",
        "        if file_path.exists():\n",
        "            try:\n",
        "                self.existing_results = pd.read_csv(file_path, sep='\\t')\n",
        "                self.logger.info(f\"Loaded {len(self.existing_results)} existing results\")\n",
        "\n",
        "                # Build set of processed pairs\n",
        "                self.processed_pairs = set()\n",
        "                for _, row in self.existing_results.iterrows():\n",
        "                    if pd.notna(row['ec_number']) and pd.notna(row['organism']):\n",
        "                        ec_num = str(row['ec_number']).strip()\n",
        "                        org = str(row['organism']).split()[0].strip()\n",
        "                        self.processed_pairs.add((ec_num, org))\n",
        "\n",
        "                return self.existing_results\n",
        "            except Exception as e:\n",
        "                self.logger.error(f\"Error loading results file: {e}\")\n",
        "                self.existing_results = pd.DataFrame(\n",
        "                    columns=['uniprot_id', 'ec_number', 'protein_name', 'organism', 'score']\n",
        "                )\n",
        "                return self.existing_results\n",
        "\n",
        "        self.existing_results = pd.DataFrame(\n",
        "            columns=['uniprot_id', 'ec_number', 'protein_name', 'organism', 'score']\n",
        "        )\n",
        "        return self.existing_results\n",
        "\n",
        "    def get_uniprot_info(self, ec: str, organism: str) -> Optional[dict]:\n",
        "        \"\"\"Get UniProt information for a specific EC-organism pair\"\"\"\n",
        "        if (ec, organism) in self.processed_pairs:\n",
        "            return None\n",
        "\n",
        "        query = f'({ec}) AND (organism_name:\"{organism}*\")'\n",
        "        params = {\n",
        "            'query': query,\n",
        "            'format': 'tsv',\n",
        "            'fields': 'id,ec,protein_name,organism_name',\n",
        "            'size': 10\n",
        "        }\n",
        "\n",
        "        max_retries = 3\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                response = requests.get(self.uniprot_api, params=params)\n",
        "                response.raise_for_status()\n",
        "                time.sleep(0.5)\n",
        "\n",
        "                lines = response.text.strip().split('\\n')\n",
        "                if len(lines) < 2:\n",
        "                    return None\n",
        "\n",
        "                best_match = None\n",
        "                best_score = -float('inf')\n",
        "\n",
        "                for line in lines[1:]:\n",
        "                    parts = line.split('\\t')\n",
        "                    if len(parts) < 4:\n",
        "                        continue\n",
        "\n",
        "                    uniprot_id, ec_numbers, protein_name, organism_name = parts\n",
        "\n",
        "                score = 0\n",
        "                if organism_name and isinstance(organism_name, str):\n",
        "                    name_parts = organism_name.split()\n",
        "                    genus = name_parts[0] if name_parts else \"\"\n",
        "\n",
        "                    # Exact genus match gets highest score\n",
        "                    if genus.lower() == organism.lower():\n",
        "                        score += 500\n",
        "                        # Prefer entries with just the genus name\n",
        "                        if len(name_parts) == 1:\n",
        "                            score += 300\n",
        "                        # Heavily penalize strain designations or subspecies\n",
        "                        elif len(name_parts) > 2 or any(char.isdigit() for char in organism_name):\n",
        "                            score -= 400\n",
        "\n",
        "                    if score > -float('inf'):\n",
        "                        if ec.replace('EC:', '') in ec_numbers.split('; '):\n",
        "                            score += 150\n",
        "\n",
        "                            if score > best_score:\n",
        "                                best_score = score\n",
        "                                best_match = {\n",
        "                                    'uniprot_id': uniprot_id,\n",
        "                                    'ec_number': ec,\n",
        "                                    'protein_name': protein_name,\n",
        "                                    'organism': organism_name,\n",
        "                                    'score': score\n",
        "                                }\n",
        "\n",
        "                return best_match if best_match else None\n",
        "\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                if attempt < max_retries - 1:\n",
        "                    time.sleep(2 ** attempt)\n",
        "                    continue\n",
        "                self.logger.error(f\"Error fetching data from UniProt: {e}\")\n",
        "                return None\n",
        "\n",
        "    def process_remaining_pairs(self, unique_pairs: pd.DataFrame, start_ec: str) -> pd.DataFrame:\n",
        "        \"\"\"Process remaining pairs with enforced starting point\"\"\"\n",
        "        # Ensure EC format consistency\n",
        "        if not start_ec.startswith('EC:'):\n",
        "            start_ec = f\"EC:{start_ec.replace('EC:', '')}\"\n",
        "\n",
        "        # Sort and filter pairs\n",
        "        unique_pairs = unique_pairs.sort_values(['EC', 'Genus']).reset_index(drop=True)\n",
        "        unique_pairs = unique_pairs[unique_pairs['EC'] >= start_ec].reset_index(drop=True)\n",
        "\n",
        "        if len(unique_pairs) == 0:\n",
        "            self.logger.warning(f\"No EC numbers found after {start_ec}\")\n",
        "            return self.existing_results\n",
        "\n",
        "        self.logger.info(f\"Starting processing from {unique_pairs.iloc[0]['EC']}\")\n",
        "        total_pairs = len(unique_pairs)\n",
        "\n",
        "        results = []\n",
        "        for idx in range(0, total_pairs, self.batch_size):\n",
        "            batch = unique_pairs.iloc[idx:idx + self.batch_size]\n",
        "            batch_results = []\n",
        "\n",
        "            self.logger.info(f\"\\nProcessing batch {idx//self.batch_size + 1} of {total_pairs//self.batch_size + 1}\")\n",
        "            self.logger.info(f\"Progress: {idx}/{total_pairs} pairs ({(idx/total_pairs)*100:.1f}%)\")\n",
        "\n",
        "            current_ec = None\n",
        "            for _, row in batch.iterrows():\n",
        "                if current_ec != row['EC']:\n",
        "                    current_ec = row['EC']\n",
        "                    self.logger.info(f\"\\nProcessing EC number: {current_ec}\")\n",
        "\n",
        "                if (row['EC'], row['Genus']) not in self.processed_pairs:\n",
        "                    result = self.get_uniprot_info(row['EC'], row['Genus'])\n",
        "                    if result:\n",
        "                        batch_results.append(result)\n",
        "                        self.processed_pairs.add((row['EC'], row['Genus']))\n",
        "\n",
        "            if batch_results:\n",
        "                results.extend(batch_results)\n",
        "\n",
        "                # Save progress periodically\n",
        "                if (idx//self.batch_size) % self.save_every == 0:\n",
        "                    combined_results = pd.concat(\n",
        "                        [self.existing_results, pd.DataFrame(results)],\n",
        "                        ignore_index=True\n",
        "                    )\n",
        "                    combined_results.to_csv(self.results_file, sep='\\t', index=False)\n",
        "                    self.logger.info(f\"Saved {len(combined_results)} total results to file\")\n",
        "\n",
        "        # Final save\n",
        "        final_results = pd.concat(\n",
        "            [self.existing_results, pd.DataFrame(results)],\n",
        "            ignore_index=True\n",
        "        )\n",
        "        final_results.to_csv(self.results_file, sep='\\t', index=False)\n",
        "\n",
        "        return final_results\n",
        "\n",
        "def continue_enzyme_retrieval(unique_pairs: pd.DataFrame, existing_results_file: Path, start_ec: str):\n",
        "    \"\"\"Main function to continue enzyme data retrieval\"\"\"\n",
        "    retriever = ColabEnzymeRetriever(batch_size=100)\n",
        "\n",
        "    # Load existing results\n",
        "    retriever.load_existing_results(existing_results_file)\n",
        "\n",
        "    # Ensure input data is properly formatted\n",
        "    if isinstance(unique_pairs, str):\n",
        "        unique_pairs = pd.read_csv(unique_pairs, sep='\\t')\n",
        "    elif isinstance(unique_pairs, pd.DataFrame):\n",
        "        unique_pairs = unique_pairs.copy(deep=False)\n",
        "    else:\n",
        "        raise ValueError(\"Input must be either a file path or a pandas DataFrame\")\n",
        "\n",
        "    # Validate and prepare input data\n",
        "    required_columns = ['EC', 'Genus']\n",
        "    if not all(col in unique_pairs.columns for col in required_columns):\n",
        "        raise ValueError(f\"Input data must contain columns: {required_columns}\")\n",
        "\n",
        "    unique_pairs['EC'] = unique_pairs['EC'].astype(str).apply(lambda x: f\"EC:{x.replace('EC:', '')}\")\n",
        "    unique_pairs['Genus'] = unique_pairs['Genus'].astype(str).str.strip()\n",
        "    unique_pairs = unique_pairs[['EC', 'Genus']].drop_duplicates()\n",
        "\n",
        "    # Process remaining pairs\n",
        "    results_df = retriever.process_remaining_pairs(unique_pairs, start_ec)\n",
        "\n",
        "    # Save final results\n",
        "    final_path = Path('uniprot_results_final.tsv')\n",
        "    results_df.to_csv(final_path, sep='\\t', index=False)\n",
        "\n",
        "    return results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mowtkQwJuvgT",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "'''uniprot_results_path = Path(base_dir '/uniprot_results.tsv')\n",
        "# Usage (after uploading files to Colab), ECcontri_otu was made in colab because it was too big to upload after transformed\n",
        "results = continue_enzyme_retrieval(ECcontri_otu, uniprot_results_path, start_ec=\"x.3.1.12\" )'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nARoxMBKuvgT"
      },
      "source": [
        "## 8.6. Cleaning and Preparing Retrieved Data to integrate to ECContri"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9r_F4kMtuvgU",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "df_1_path = Path(base_dir / \"uniprot_1_4_sorted.tsv\") # First file retrieved on first run 4 am\n",
        "df_2_path = Path(base_dir / \"uniprot_2_1.38_sorted.tsv\") # Same file retrieven when corrupted around 1:38 following day\n",
        "df_3_path = Path(base_dir / \"uniprot_3_sorted.tsv\") # Rerun done trying to get following EC numbers\n",
        "df_4_path = Path(base_dir / \"uniprot_4_missing_sorted.tsv\") # Final run in missing data\n",
        "\n",
        "df_1 = pd.read_csv(df_1_path, sep='\\t')\n",
        "df_2 = pd.read_csv(df_2_path, sep='\\t')\n",
        "df_3 = pd.read_csv(df_3_path, sep='\\t')\n",
        "df_4 = pd.read_csv(df_4_path, sep='\\t')\n",
        "print(len(df_1), len(df_2), len(df_3), len(df_4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9a2RbXR1uvgU",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# extract EC and Genus from the Retrieved files, so I need to join them first\n",
        "retrieved = pd.concat([df_1, df_2, df_3, df_4], axis = 0)\n",
        "# unique pairs on our data\n",
        "unique_pairs = ECcontri_otu[['EC', 'Genus']].drop_duplicates()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwbF_HFmuvgU"
      },
      "source": [
        "## 8.7 Extracting the Genus from the retrieved_pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jfKMErDWuvgU",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Function to extract the Genus from the organism str\n",
        "def extract_genus(organism_str):\n",
        "    # Assumes Genus is the first word that starts with an uppercase letter.\n",
        "    match = re.search(r'([A-Z][a-z]+)', organism_str)\n",
        "    return match.group(1) if match else None\n",
        "# Creating a Genus column in the retrieved dataframe.\n",
        "retrieved['Genus'] = retrieved['organism'].astype(str).apply(extract_genus)\n",
        "\n",
        "# if there are duplicates, we want the best entry based on score:\n",
        "retrieved_unique = retrieved.sort_values('score', ascending=False)\\\n",
        "                            .drop_duplicates(subset=['ec_number', 'Genus'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZiDFgoGgmmJ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "print(f\"unique_pairs Galaxy data:{len(unique_pairs)}, Uniprot retrieved data:{len(retrieved_unique)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yoQasxuauvgU",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Merging using a left join on the two keys (EC_number and Genus). Plus an indicator of missing data.\n",
        "ECcontri_Uniprot  = pd.merge(\n",
        "    ECcontri_otu,\n",
        "    retrieved_unique[['ec_number', 'Genus', 'protein_name', 'score', 'uniprot_id']],\n",
        "    left_on=['EC', 'Genus'],\n",
        "    right_on=['ec_number', 'Genus'],\n",
        "    how='left',\n",
        "    suffixes=('', '_retr')\n",
        ")\n",
        "print(ECcontri_Uniprot.shape) # Very slow 1 minute, can kill the kernel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elCz1V27O-E3",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "ECcontri_Uniprot = ECcontri_Uniprot.drop(columns = [\"OTU\",\t\"ec_number\",\t\"npath\", \"pathway\",\t\"score\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qA_nz2qFuvgU"
      },
      "source": [
        "## 8.8. Missing Values\n",
        "ECcontri_uniprot_info is the final df mixed and is keep for reference only purposes. With the missing unique df I will retrive again the rest of the missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6iKlSuHuvgU",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#Rows with no match from retrieved_unique will have '_merge' value of 'left_only'\n",
        "merged_unique = pd.merge(\n",
        "    unique_pairs,\n",
        "    retrieved_unique,\n",
        "    left_on=['EC', 'Genus'],\n",
        "    right_on=['ec_number', 'Genus'],\n",
        "    how='left',\n",
        "    indicator=True\n",
        ")\n",
        "\n",
        "# Filter unique pairs missing from retrieved data\n",
        "ECcontri_missing = merged_unique[merged_unique['_merge'] == 'left_only']\n",
        "print(\"Missing unique pairs count:\", ECcontri_missing.shape[0])\n",
        "ECcontri_missing = ECcontri_missing[[\"EC\", \"Genus\"]]\n",
        "file_path = os.path.join(output_base, \"ECcontri_missing.tsv\")\n",
        "ECcontri_missing.to_csv(file_path, sep='\\t', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lMQJ2mM8ZGp"
      },
      "source": [
        "## 8.9 Cleaning Protein Names on ECcontri_Uniprot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-16T20:56:20.782994Z",
          "iopub.status.busy": "2025-03-16T20:56:20.782576Z",
          "iopub.status.idle": "2025-03-16T20:56:20.791336Z",
          "shell.execute_reply": "2025-03-16T20:56:20.790135Z",
          "shell.execute_reply.started": "2025-03-16T20:56:20.782958Z"
        },
        "id": "8iNh9JG58gI9",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def clean_protein_name(name):\n",
        "    \"\"\"\n",
        "    Enhanced protein name cleaning:\n",
        "    1. Remove EC numbers unless it's the only information\n",
        "    2. Remove redundant information in parentheses\n",
        "    3. Remove duplicated terms\n",
        "    4. Handle special cases\n",
        "    \"\"\"\n",
        "    if pd.isna(name):\n",
        "        return \"Uncharacterized protein\"\n",
        "\n",
        "    # If the name is just an EC number in any format, return it\n",
        "    if re.match(r'^[\\s\\(\\)]*EC\\s*[\\d\\.]+[\\s\\(\\)]*$', name):\n",
        "        return name.strip()\n",
        "\n",
        "    # Remove EC numbers and content in parentheses\n",
        "    name = re.sub(r'\\(EC\\s*[\\d\\.]+\\)', '', name)\n",
        "    name = re.sub(r'\\([^)]*\\)', '', name)\n",
        "\n",
        "    # Split into words and remove duplicates while preserving order\n",
        "    words = name.split()\n",
        "    seen = set()\n",
        "    unique_words = []\n",
        "    for word in words:\n",
        "        # Convert to lowercase for comparison but keep original case in result\n",
        "        lower_word = word.lower()\n",
        "        if lower_word not in seen:\n",
        "            seen.add(lower_word)\n",
        "            unique_words.append(word)\n",
        "\n",
        "    # Rejoin words\n",
        "    name = ' '.join(unique_words)\n",
        "\n",
        "    # Remove specific redundant patterns\n",
        "    redundant_patterns = [\n",
        "        (r'enzyme\\s+enzyme', 'enzyme'),\n",
        "        (r'synthase\\s+synthase', 'synthase'),\n",
        "        (r'transferase\\s+transferase', 'transferase'),\n",
        "        (r'-glucan\\s+glucan', 'glucan'),\n",
        "        (r'protein\\s+protein', 'protein')\n",
        "    ]\n",
        "\n",
        "    for pattern, replacement in redundant_patterns:\n",
        "        name = re.sub(pattern, replacement, name, flags=re.IGNORECASE)\n",
        "\n",
        "    return name.strip()\n",
        "\n",
        "def check_cleaning(df, n_samples=10):\n",
        "    \"\"\"\n",
        "    Check the cleaning results with before/after comparison\n",
        "    \"\"\"\n",
        "    sample_names = df['protein_name'].dropna().sample(n=n_samples)\n",
        "    cleaned_names = sample_names.apply(clean_protein_name)\n",
        "\n",
        "    print(\"Sample of name cleaning results:\")\n",
        "    for orig, cleaned in zip(sample_names, cleaned_names):\n",
        "        print(f\"\\nOriginal:  {orig}\")\n",
        "        print(f\"Cleaned:   {cleaned}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOok4gH71k5L",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Standardize EC format: Extract numbers without the 'EC:' prefix\n",
        "ECcontri_Uniprot['EC_clean'] = ECcontri_Uniprot['EC'].str.replace('EC:', '', regex=False)\n",
        "ECcontri_Uniprot = ECcontri_Uniprot.drop(columns = [\"EC\"])\n",
        "ECcontri_Uniprot = ECcontri_Uniprot.rename(columns={\"EC_clean\": \"EC\"})\n",
        "\n",
        "# Applaying the protein name cleaning\n",
        "check_cleaning(ECcontri_Uniprot)\n",
        "\n",
        "ECcontri_Uniprot['protein_name'] = ECcontri_Uniprot['protein_name'].apply(clean_protein_name)\n",
        "# Saving the data\n",
        "ECcontri_Uniprot_path = output_large / 'ECcontri_Uniprot.tsv'\n",
        "ECcontri_Uniprot.to_csv(ECcontri_Uniprot_path, sep='\\t', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVnGj7D_-PJL"
      },
      "source": [
        "### Cleaning anc collecting garbage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2rdKHvKj-PJM",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "del picrust_long\n",
        "del retrieved\n",
        "del retrieved_unique\n",
        "del unique_pairs\n",
        "del df_1\n",
        "del df_2\n",
        "del df_3\n",
        "del df_4\n",
        "del ECcontri\n",
        "del ECcontri_agg_site\n",
        "#del ECcontri_otu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwyO_mme-PJM",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d2KfaHxuvgU"
      },
      "source": [
        "### Data Retrieval Completion Note\n",
        "After multiple retrieval attempts, 12,656 pairs remain unmapped out of approximately 1,500,000 total rows (0.84%). Given this small percentage and the diminishing returns from further retrieval attempts, we concluded that this level of completeness is acceptable for analysis.\n",
        "______________________________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lnzHgflRGFV"
      },
      "source": [
        "#9. Building a Dictionary from Databases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNr6ECYly6LJ"
      },
      "source": [
        "Data Normalization and Mapping\n",
        "It was ensured that all protein/EC data (including EC numbers, KO numbers, and reaction IDs) were parsed and cleaned. This identifiers were mapped to their corresponding metabolic pathways using databases such as KEGG, MetaCyc, and BioCyc.\n",
        "\n",
        "Identifying Metal-Related Proteins:\n",
        "cross-reference proteins with metal-related databases (BRENDA, MetalPDB, TransportDB) where crossreferenced to flag those with direct metal-binding or metal-transport roles. Then consolidate similar metal terms (e.g., “iron”, “Fe”, “ferric”) into a unified field to improve consistency in later analyses.\n",
        "\n",
        "\n",
        "\n",
        "Final Assembly:\n",
        "I compile the data into a final dictionary/table that includes all relevant columns (Protein, EC/KO, Metabolism, Pathway, Metal Interaction, MIC Function). This allows me to search programmatically for the functional roles of proteins that are influential in corrosion studies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FEargpNkucP"
      },
      "source": [
        "## 9.1 Setting up Paths and Parsing the Dataframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-16T20:57:03.991392Z",
          "iopub.status.busy": "2025-03-16T20:57:03.991055Z",
          "iopub.status.idle": "2025-03-16T20:57:04.051860Z",
          "shell.execute_reply": "2025-03-16T20:57:04.050768Z",
          "shell.execute_reply.started": "2025-03-16T20:57:03.991368Z"
        },
        "id": "0efKvfwuHc0f",
        "outputId": "56d894d1-a87e-47c4-f90a-be846fad32c5",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "enzyme: /kaggle/input/databases/Databases/enzyme\n",
            "Exists: True\n",
            "enzyme_class: /kaggle/input/databases/Databases/enzclass.txt\n",
            "Exists: True\n",
            "enzyme_brenda: /kaggle/input/databases/Databases/brenda_2024.txt\n",
            "Exists: True\n",
            "ko: /kaggle/input/databases/Databases/ko\n",
            "Exists: True\n",
            "ko_hierarchy: /kaggle/input/databases/Databases/ko_hierarchy.txt\n",
            "Exists: True\n",
            "pathway: /kaggle/input/databases/Databases/pathway\n",
            "Exists: True\n",
            "module: /kaggle/input/databases/Databases/module\n",
            "Exists: True\n",
            "reaction: /kaggle/input/databases/Databases/reaction\n",
            "Exists: True\n",
            "compound: /kaggle/input/databases/Databases/compound\n",
            "Exists: True\n",
            "metalpdb: /kaggle/input/databases/Databases/flat_db_file.xml\n",
            "Exists: True\n",
            "ko_pathway: /kaggle/input/databases/Databases/ec_pathway.list\n",
            "Exists: True\n"
          ]
        }
      ],
      "source": [
        "db_dir = Path(\"/kaggle/input/databases/Databases\") #kaggle\n",
        "def setup_paths():\n",
        "    \"\"\"Set up paths for database access\"\"\"\n",
        "\n",
        "    # Database paths\n",
        "    db_paths = {\n",
        "        'enzyme': db_dir / 'enzyme',\n",
        "        'enzyme_class': db_dir / 'enzclass.txt',\n",
        "        'enzyme_brenda' : db_dir/ 'brenda_2024.txt',\n",
        "        'ko': db_dir / 'ko',\n",
        "        'ko_hierarchy': db_dir / 'ko_hierarchy.txt',\n",
        "        'pathway': db_dir / 'pathway',\n",
        "        'module': db_dir / 'module',\n",
        "        'reaction': db_dir / 'reaction',\n",
        "        'compound': db_dir / 'compound',\n",
        "        'metalpdb': db_dir / 'flat_db_file.xml',\n",
        "        'ko_pathway': db_dir / 'ec_pathway.list'\n",
        "    }\n",
        "\n",
        "    return db_paths\n",
        "\n",
        "#  Calling the paths\n",
        "if __name__ == \"__main__\":\n",
        "    paths = setup_paths()\n",
        "    # Print paths to verify\n",
        "    for db_name, path in paths.items():\n",
        "        print(f\"{db_name}: {path}\")\n",
        "        print(f\"Exists: {path.exists()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7P2TUJLNWX3B"
      },
      "source": [
        "### Brenda Enzyme Parse Brenda\n",
        "\n",
        "https://www.brenda-enzymes.org/download.php\n",
        "\n",
        "Chang A., Jeske L., Ulbrich S., Hofmann J., Koblitz J., Schomburg I., Neumann-Schaal M., Jahn D., Schomburg D.\n",
        "BRENDA, the ELIXIR core data resource in 2021: new developments and updates. (2021), Nucleic Acids Res., 49:D498-D508.\n",
        "DOI: 10.1093/nar/gkaa1025 PubMed: 33211880"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-16T20:57:04.053963Z",
          "iopub.status.busy": "2025-03-16T20:57:04.053530Z",
          "iopub.status.idle": "2025-03-16T20:57:12.908329Z",
          "shell.execute_reply": "2025-03-16T20:57:12.907248Z",
          "shell.execute_reply.started": "2025-03-16T20:57:04.053901Z"
        },
        "id": "mkW7GPMhvlL3",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "def parse_brenda_file():\n",
        "    \"\"\"Parse BRENDA database file for detailed enzyme information\"\"\"\n",
        "    paths = setup_paths()\n",
        "    enzyme_brenda_path = paths['enzyme_brenda']\n",
        "\n",
        "    ec_detailed_info = {}\n",
        "    current_ec = None\n",
        "    in_enzyme_entry = False\n",
        "\n",
        "    try:\n",
        "        with open(enzyme_brenda_path, 'r') as f:\n",
        "          for line in f:\n",
        "              line = line.strip()\n",
        "\n",
        "              # Skip empty lines\n",
        "              if not line:\n",
        "                  continue\n",
        "\n",
        "              # Check for the end of an entry\n",
        "              if line == \"///\":\n",
        "                  current_ec = None\n",
        "                  in_enzyme_entry = False\n",
        "                  continue\n",
        "\n",
        "              # Process ID line - identify enzyme entries\n",
        "              if line.startswith('ID\\t'):\n",
        "                  current_ec = line.split('\\t')[1]\n",
        "\n",
        "                  # Skip \"spontaneous\" and other non-EC entries\n",
        "                  if not any(c.isdigit() for c in current_ec):\n",
        "                      current_ec = None\n",
        "                      in_enzyme_entry = False\n",
        "                      continue\n",
        "\n",
        "                  # Initialize proper EC entry\n",
        "                  ec_detailed_info[current_ec] = {\n",
        "                      'metals': [],\n",
        "                      'cofactors': [],\n",
        "                      'reactions': [],\n",
        "                      'substrates': [],\n",
        "                      'inhibitors': []\n",
        "                  }\n",
        "                  in_enzyme_entry = True\n",
        "\n",
        "              # Only process other lines if we're in a valid enzyme entry\n",
        "              elif in_enzyme_entry and current_ec:\n",
        "                  if line.startswith('ME\\t'):\n",
        "                      # Extract metal information\n",
        "                      metal_info = line.split('\\t')[1]\n",
        "                      ec_detailed_info[current_ec]['metals'].append(metal_info)\n",
        "\n",
        "                  elif line.startswith('CF\\t'):\n",
        "                      # Extract cofactor information\n",
        "                      cofactor_info = line.split('\\t')[1]\n",
        "                      ec_detailed_info[current_ec]['cofactors'].append(cofactor_info)\n",
        "\n",
        "                  elif line.startswith('RE\\t'):\n",
        "                      # Extract detailed reaction information\n",
        "                      reaction_info = line.split('\\t')[1]\n",
        "                      ec_detailed_info[current_ec]['reactions'].append(reaction_info)\n",
        "\n",
        "                  elif line.startswith('SP\\t') or line.startswith('NSP\\t'):\n",
        "                      # Extract substrate information\n",
        "                      substrate_info = line.split('\\t')[1]\n",
        "                      ec_detailed_info[current_ec]['substrates'].append(substrate_info)\n",
        "\n",
        "                  elif line.startswith('IN\\t'):\n",
        "                      # Extract inhibitor information\n",
        "                      inhibitor_info = line.split('\\t')[1]\n",
        "                      ec_detailed_info[current_ec]['inhibitors'].append(inhibitor_info)\n",
        "\n",
        "        # Verify we have valid EC numbers\n",
        "        ec_detailed_info = {ec: info for ec, info in ec_detailed_info.items()\n",
        "                            if ec.count('.') == 3 and all(part.isdigit() for part in ec.split('.'))}\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(\"Error parsing BRENDA file: %s\", e)\n",
        "        return {}\n",
        "    return ec_detailed_info\n",
        "\n",
        "brenda_data = parse_brenda_file()\n",
        "#brenda_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-16T20:57:12.910627Z",
          "iopub.status.busy": "2025-03-16T20:57:12.910352Z",
          "iopub.status.idle": "2025-03-16T20:57:13.708329Z",
          "shell.execute_reply": "2025-03-16T20:57:13.707219Z",
          "shell.execute_reply.started": "2025-03-16T20:57:12.910603Z"
        },
        "id": "d2S3JREa6rg7",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def process_brenda_data(brenda_data):\n",
        "    \"\"\"Process BRENDA data to extract clean metal information while keeping other data intact\"\"\"\n",
        "    processed_data = {}\n",
        "\n",
        "    # Common metal ions to look for\n",
        "    metal_patterns = {'iron': ['Fe2+', 'Fe3+', 'iron', 'ferrous', 'ferric'],\n",
        "        'manganese': ['Mn2+', 'manganese'],\n",
        "        'copper': ['Cu+', 'Cu2+', 'copper'],\n",
        "        'nickel': ['Ni2+', 'nickel'],\n",
        "        'cobalt': ['Co2+', 'cobalt'],\n",
        "        'magnesium': ['Mg2+', 'magnesium'],\n",
        "        'calcium': ['Ca2+', 'calcium'],\n",
        "        'Mo': ['Mo', 'molybdenum'],\n",
        "        'V5+': ['V5+', 'vanadium'],\n",
        "        'Al3+': ['Al3+', 'aluminum'],\n",
        "        'Cr3+': ['Cr3+', 'chromium'],\n",
        "        'zinc': ['Zn2+', 'zinc'],\n",
        "        'sodium': ['Na+', 'sodium', 'NaCl'],\n",
        "        'potassium': ['K+', 'potassium', 'KCl'],\n",
        "        'selenium': ['selenium', 'Se'],\n",
        "        'barium': ['Ba2+', 'barium'],\n",
        "        'phosphate': ['HPO4-2', 'PO4-3', 'phosphate', 'phosphates'],\n",
        "        'nitrate': ['NO3-', 'nitrate', 'nitrates'],\n",
        "        'nitrite': ['NO2-', 'nitrite', 'nitrites'],\n",
        "        'chloride': ['Cl-', 'chloride', 'chlorine'],\n",
        "        'sulfate': ['SO4-2', 'sulfate', 'sulfates'],\n",
        "        'sulfide': ['S', 'sulfide', 'sulfides'],\n",
        "        'thiosulfate': [ 'S'],\n",
        "        's-s': ['S']\n",
        "    }\n",
        "    # Pathway categories collecting all terms\n",
        "    pathway_categories = {\n",
        "        'organic_acid_metabolism': [\n",
        "            'acetate', 'acetic acid', 'acetyl',\n",
        "            'oxalate', 'oxalic acid',\n",
        "            'organic acid', 'fatty acid',\n",
        "            'carboxylic acid'\n",
        "        ],\n",
        "        'metal_organic_interaction': [\n",
        "            'siderophore', 'metal binding',\n",
        "            'iron complex', 'metal transport',\n",
        "            'metallophore', 'metal organic'\n",
        "        ],\n",
        "        'biofilm_formation': [\n",
        "            'biofilm', 'exopolysaccharide',\n",
        "            'extracellular matrix', 'adhesion'\n",
        "        ],\n",
        "        'halogen_related': [\n",
        "            'halogen', 'chloride', 'bromide',\n",
        "            'halide', 'dehalogenation'\n",
        "        ],\n",
        "        'sulfur': [\n",
        "            'sulfur', 'sulfate', 'sulfide',\n",
        "            'thiosulfate', 'sulfite', 'sulfonate'\n",
        "        ],\n",
        "        'electron_transfer': [\n",
        "            'cytochrome', 'electron transport',\n",
        "            'oxidoreductase', 'redox'\n",
        "        ],\n",
        "        'carbon_metabolism': [\n",
        "            'carbon fixation', 'carbon utilization',\n",
        "            'carbohydrate metabolism'\n",
        "        ],\n",
        "        'ph_modulation': [\n",
        "            'acid', 'alkaline', 'proton pump',\n",
        "            'pH homeostasis'\n",
        "        ],\n",
        "        'temp_response': [\n",
        "            'heat shock', 'cold shock',\n",
        "            'temperature response'\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    # Define organic matter categories\n",
        "    organic_categories = {\n",
        "        'degradation': ['degradation', 'breakdown', 'catabolism'],\n",
        "        'synthesis': ['biosynthesis', 'anabolism', 'synthesis'],\n",
        "        'transport': ['transport', 'uptake', 'export'],\n",
        "        'modification': ['modification', 'conversion', 'transformation']\n",
        "    }\n",
        "\n",
        "    for ec_number, data in brenda_data.items():\n",
        "        processed_data[ec_number] = {\n",
        "            'cofactors': data.get('cofactors', []),\n",
        "            'reactions': data.get('reactions', []),\n",
        "            'substrates': data.get('substrates', []),\n",
        "            'inhibitors': data.get('inhibitors', []),\n",
        "            'raw_metals': data.get('metals', []),\n",
        "            'clean_metals': [],\n",
        "            'pathway_categories': {},\n",
        "            'organic_processes': {}\n",
        "\n",
        "        }\n",
        "\n",
        "        # Extract clean metal names\n",
        "        for entry in data.get('metals', []):\n",
        "            entry_lower = entry.lower()\n",
        "            for metal in metal_patterns:\n",
        "                metal_lower = metal.lower()\n",
        "                if metal_lower in entry_lower:\n",
        "                    if metal not in processed_data[ec_number]['clean_metals']:\n",
        "                        processed_data[ec_number]['clean_metals'].append(metal)\n",
        "\n",
        "        # Create a single text string for pathway searching\n",
        "        all_text = ' '.join([\n",
        "            ' '.join(data.get('reactions', [])),\n",
        "            ' '.join(data.get('substrates', [])),\n",
        "            ' '.join(data.get('cofactors', []))\n",
        "        ]).lower()\n",
        "\n",
        "        # Add corrosion relevance information\n",
        "        processed_data[ec_number]['corrosion_metals_from_brenda'] = [\n",
        "            metal for metal in processed_data[ec_number]['clean_metals']\n",
        "            if metal in ['Fe2+', 'Fe3+', 'iron', 'Mn2+', 'manganese', 'Cu+', 'Cu2+',\n",
        "                         'copper', 'Ni2+', 'nickel', 'Co2+', 'cobalt']\n",
        "        ]\n",
        "        ## Add pathway relevance information, would it no be better to search for this relevance on the pathway database?\n",
        "        for category, terms in pathway_categories.items():\n",
        "            if any(term.lower() in all_text for term in terms):\n",
        "                processed_data[ec_number]['pathway_categories'][category] = True\n",
        "\n",
        "        # Check for organic matter processes\n",
        "        for category, terms in organic_categories.items():\n",
        "            if any(term.lower() in all_text for term in terms):\n",
        "                processed_data[ec_number]['organic_processes'][category] = True\n",
        "\n",
        "        # Calculate corrosion relevance score based on metals\n",
        "        if processed_data[ec_number]['corrosion_metals_from_brenda']:\n",
        "            processed_data[ec_number]['corrosion_relevance'] = 'high'\n",
        "        elif processed_data[ec_number]['clean_metals']:\n",
        "            processed_data[ec_number]['corrosion_relevance'] = 'medium'\n",
        "        else:\n",
        "            processed_data[ec_number]['corrosion_relevance'] = 'low'\n",
        "\n",
        "    return processed_data\n",
        "\n",
        "brenda_en= process_brenda_data(brenda_data)\n",
        "#brenda_en.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EA-pSl0dvd0W"
      },
      "source": [
        "### Enzyme names\n",
        "The database containing enzyme names and EC numbers\n",
        "\n",
        "wget https://www.enzyme-database.org/downloads/enzyme-database.sql.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6FSLRy-E1_a",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def read_enzyme_names():\n",
        "    \"\"\"Read and parse enzyme file to get EC numbers and their names\"\"\"\n",
        "    paths = setup_paths()\n",
        "    enzyme_path = paths['enzyme']\n",
        "\n",
        "    ec_to_names = {}  # More descriptive name\n",
        "    with open(enzyme_path, 'r') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split('\\t')\n",
        "            if len(parts) >= 2:\n",
        "                ec_number = parts[0]\n",
        "                names = parts[1].split('; ')\n",
        "                ec_to_names[ec_number] = names\n",
        "\n",
        "    for ec_number, names_list in ec_to_names.items():\n",
        "\n",
        "        cleaned_names = [clean_protein_name(name) if isinstance(name, str) else str(name) for name in names_list]\n",
        "        ec_to_names[ec_number] = cleaned_names\n",
        "\n",
        "    return ec_to_names\n",
        "ec_to_names = read_enzyme_names()\n",
        "#list(ec_to_names.items())[:100]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUUP7OAF9iaM"
      },
      "source": [
        "### Enzyme Class\n",
        "The enzyme classification system (text-based hierarchy)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-16T20:57:14.109680Z",
          "iopub.status.busy": "2025-03-16T20:57:14.109394Z",
          "iopub.status.idle": "2025-03-16T20:57:14.119242Z",
          "shell.execute_reply": "2025-03-16T20:57:14.118240Z",
          "shell.execute_reply.started": "2025-03-16T20:57:14.109656Z"
        },
        "id": "6FTH4E8gZfdu",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def read_enzyme_class():\n",
        "    paths = setup_paths()\n",
        "    ec_file_path = paths['enzyme_class']\n",
        "\n",
        "    enzyme_class = {}\n",
        "\n",
        "    with open(ec_file_path, 'r') as f:\n",
        "        for line in f:\n",
        "            # Format is like \"1. 1. 1.-    With NAD(+) or NADP(+) as acceptor.\"\n",
        "            if line.strip() and any(line.startswith(str(i)) for i in range(1, 7)):\n",
        "                parts = line.strip().split('  ')\n",
        "                if len(parts) >= 2:\n",
        "                    ec_id = parts[0].replace(' ', '')\n",
        "                    desc = parts[1].strip()\n",
        "                    enzyme_class[ec_id] = desc\n",
        "    return enzyme_class\n",
        "enzyme_class = read_enzyme_class()\n",
        "#list(enzyme_class.items())[:100]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBIfRLP65Jzc"
      },
      "source": [
        "Ko list\n",
        "\n",
        "wget -O ec_pathway.list \"https://rest.kegg.jp/link/pathway/ec\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-16T20:57:14.120636Z",
          "iopub.status.busy": "2025-03-16T20:57:14.120354Z",
          "iopub.status.idle": "2025-03-16T20:57:14.162197Z",
          "shell.execute_reply": "2025-03-16T20:57:14.161119Z",
          "shell.execute_reply.started": "2025-03-16T20:57:14.120610Z"
        },
        "id": "EEH4ACcm558K",
        "outputId": "78fab2d8-f69d-4af6-f02c-ecdff876be57",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded pathway mappings for 3889 EC numbers\n"
          ]
        }
      ],
      "source": [
        "def read_ec_pathway_mapping():\n",
        "    \"\"\"Read EC to pathway mapping file downloaded from KEGG\"\"\"\n",
        "\n",
        "    paths = setup_paths()\n",
        "    ko_pathway_path = paths['ko_pathway']\n",
        "\n",
        "    ec_to_pathway = {}\n",
        "\n",
        "    try:\n",
        "        with open(ko_pathway_path , 'r') as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split('\\t')\n",
        "                if len(parts) == 2:\n",
        "                    ec_id = parts[0].replace('ec:', '')\n",
        "                    pathway_id = parts[1].replace('path:', '')\n",
        "\n",
        "                    if ec_id not in ec_to_pathway:\n",
        "                        ec_to_pathway[ec_id] = []\n",
        "                    ec_to_pathway[ec_id].append(pathway_id)\n",
        "\n",
        "        print(f\"Loaded pathway mappings for {len(ec_to_pathway)} EC numbers\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading EC-pathway mapping: {e}\")\n",
        "        return {}\n",
        "\n",
        "    return ec_to_pathway\n",
        "\n",
        "ec_to_pathway = read_ec_pathway_mapping()\n",
        "#ec_to_pathway"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBtayQh4eQ82"
      },
      "source": [
        "### Ko Database\n",
        " A new variable mapping  KO numbers to EC numbers from KEGG KO file.\n",
        "rsync -avz rsync://rest.kegg.jp/kegg/pathway/ ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-16T20:57:14.163405Z",
          "iopub.status.busy": "2025-03-16T20:57:14.163116Z",
          "iopub.status.idle": "2025-03-16T20:57:14.208344Z",
          "shell.execute_reply": "2025-03-16T20:57:14.207262Z",
          "shell.execute_reply.started": "2025-03-16T20:57:14.163383Z"
        },
        "id": "Nz9K72kaKM52",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def read_ko_data():\n",
        "    \"\"\"Read and parse KEGG KO file\"\"\"\n",
        "    paths = setup_paths()\n",
        "    ko_file_path = paths['ko']\n",
        "\n",
        "    ko_info = {}\n",
        "    with open(ko_file_path, 'r') as f:\n",
        "        for line in f:\n",
        "            if line.startswith('K'):\n",
        "                parts = line.strip().split('\\t')\n",
        "                if len(parts) > 1:\n",
        "                    ko_info[parts[0]] = {\n",
        "                        'definition': parts[1],\n",
        "                        'pathway': parts[2] if len(parts) > 2 else ''\n",
        "                    }\n",
        "    return ko_info\n",
        "\n",
        "ko_ec =read_ko_data()\n",
        "#list(ko_ec.items())[:100]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBwbp_UkeTI6"
      },
      "source": [
        "Ko Hierarchi Database\n",
        "Hierarchy of KO numbers (helps in pathway mapping).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-16T20:57:14.211557Z",
          "iopub.status.busy": "2025-03-16T20:57:14.211289Z",
          "iopub.status.idle": "2025-03-16T20:57:14.425509Z",
          "shell.execute_reply": "2025-03-16T20:57:14.424511Z",
          "shell.execute_reply.started": "2025-03-16T20:57:14.211534Z"
        },
        "id": "l-PjltuxE1_b",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def read_ko_hierarchy():\n",
        "    paths = setup_paths()\n",
        "    ko_path = paths['ko_hierarchy']\n",
        "\n",
        "    hierarchy = {\n",
        "        'A': {},  # Top level\n",
        "        'B': {},  # ko Category\n",
        "        'C': {},  # Pathway\n",
        "        'D': {}   # KO/Enzyme\n",
        "    }\n",
        "\n",
        "    current = {'A': None, 'B': None, 'C': None}\n",
        "\n",
        "    with open(ko_path, 'r') as f:\n",
        "        for line in f:\n",
        "            if line.startswith('A'):\n",
        "                parts = line.strip().split()\n",
        "                id = parts[1]\n",
        "                name = ' '.join(parts[2:])\n",
        "                hierarchy['A'][id] = name\n",
        "                current['A'] = id\n",
        "\n",
        "            elif line.startswith('B'):\n",
        "                parts = line.strip().split()\n",
        "                id = parts[1]\n",
        "                name = ' '.join(parts[2:])\n",
        "                hierarchy['B'][id] = {'name': name, 'parent': current['A']}\n",
        "                current['B'] = id\n",
        "\n",
        "            elif line.startswith('C'):\n",
        "                parts = line.strip().split()\n",
        "                id = parts[1]\n",
        "                name = ' '.join(parts[2:])\n",
        "                if '[PATH:' in name:\n",
        "                    path_parts = name.split('[PATH:')\n",
        "                    name = path_parts[0].strip()\n",
        "                    path_id = path_parts[1].split(']')[0]\n",
        "                else:\n",
        "                    path_id = None\n",
        "\n",
        "                hierarchy['C'][id] = {\n",
        "                    'name': name,\n",
        "                    'parent': current['B'],\n",
        "                    'path_id': path_id\n",
        "                }\n",
        "                current['C'] = id\n",
        "\n",
        "            elif line.startswith('D'):\n",
        "                parts = line.strip().split()\n",
        "                ko_id = parts[1]\n",
        "                name = ' '.join(parts[2:])\n",
        "\n",
        "                # Extract EC numbers if present\n",
        "                ec_numbers = []\n",
        "                if '[EC:' in name:\n",
        "                    ec_part = name.split('[EC:')[1].split(']')[0]\n",
        "                    ec_numbers = ec_part.split()\n",
        "                    name = name.split('[EC:')[0].strip()\n",
        "\n",
        "                hierarchy['D'][ko_id] = {\n",
        "                    'name': name,\n",
        "                    'parent': current['C'],\n",
        "                    'ec_numbers': ec_numbers\n",
        "                }\n",
        "\n",
        "    return hierarchy\n",
        "\n",
        "ko_hierarchy = read_ko_hierarchy()\n",
        "#list(ko_hierarchy.items())[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-kOhcGCcn-i"
      },
      "source": [
        "Reaction Data\n",
        " Reaction-level information.\n",
        "\n",
        "!wget -c \"ftp://ftp.genome.jp/pub/kegg/reaction/reaction.tar.gz\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-16T20:57:14.427532Z",
          "iopub.status.busy": "2025-03-16T20:57:14.427225Z",
          "iopub.status.idle": "2025-03-16T20:57:14.473348Z",
          "shell.execute_reply": "2025-03-16T20:57:14.472252Z",
          "shell.execute_reply.started": "2025-03-16T20:57:14.427504Z"
        },
        "id": "s5Pd6vUTLR4M",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def read_reaction_data():\n",
        "    paths = setup_paths()\n",
        "    reaction_file_path = paths['reaction']\n",
        "\n",
        "    reaction_info = {}\n",
        "\n",
        "    with open(reaction_file_path, 'r') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "\n",
        "            parts = line.split(None, 1)  # Split on first whitespace\n",
        "            if len(parts) >= 2:\n",
        "                rxn_id = parts[0]\n",
        "                desc_parts = parts[1].split(';')\n",
        "\n",
        "                # First part is reaction name\n",
        "                name = desc_parts[0].strip()\n",
        "\n",
        "                # Rest might contain equation\n",
        "                equation = desc_parts[1].strip() if len(desc_parts) > 1 else \"\"\n",
        "\n",
        "                reaction_info[rxn_id] = {\n",
        "                    'name': name,\n",
        "                    'equation': equation\n",
        "                }\n",
        "\n",
        "    return reaction_info\n",
        "\n",
        "reaction_equation = read_reaction_data()\n",
        "#reaction_equation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIyE7Tx5chQ6"
      },
      "source": [
        "### Pathway Database\n",
        "hemical compounds database.\n",
        "\n",
        "wget https://biocyc.org/download.shtml\n",
        "\n",
        "wget https://www.brenda-enzymes.org/download.php\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-16T20:57:14.474773Z",
          "iopub.status.busy": "2025-03-16T20:57:14.474437Z",
          "iopub.status.idle": "2025-03-16T20:57:14.484183Z",
          "shell.execute_reply": "2025-03-16T20:57:14.483238Z",
          "shell.execute_reply.started": "2025-03-16T20:57:14.474694Z"
        },
        "id": "xu5sqGIPc0op",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def read_pathway_data():\n",
        "    paths = setup_paths()\n",
        "    pathway_path = paths['pathway']\n",
        "\n",
        "    pathway_info = {}\n",
        "    with open(pathway_path, 'r') as f:\n",
        "          for line in f:\n",
        "              parts = line.strip().split('\\t')\n",
        "              if len(parts) >= 2:\n",
        "                  pathway_id = parts[0]\n",
        "                  pathway_name = parts[1]\n",
        "                  pathway_info[pathway_id] = pathway_name\n",
        "    return pathway_info\n",
        "\n",
        "pathway_data = read_pathway_data()\n",
        "#pathway_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxYkP8l0dCXo"
      },
      "source": [
        "### Module Database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-16T20:57:14.485530Z",
          "iopub.status.busy": "2025-03-16T20:57:14.485278Z",
          "iopub.status.idle": "2025-03-16T20:57:14.504040Z",
          "shell.execute_reply": "2025-03-16T20:57:14.502838Z",
          "shell.execute_reply.started": "2025-03-16T20:57:14.485508Z"
        },
        "id": "0Ue-7KZpdEfe",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def read_module_data():\n",
        "    paths = setup_paths()\n",
        "    module_path = paths['module']\n",
        "\n",
        "    module_info = {}\n",
        "    with open(module_path, 'r') as f:\n",
        "      for line in f:\n",
        "          parts = line.strip().split('\\t')\n",
        "          if len(parts) >= 2:\n",
        "              module_id = parts[0]\n",
        "              module_desc = parts[1]\n",
        "              module_info[module_id] = module_desc\n",
        "    return module_info\n",
        "\n",
        "module_info = read_module_data()\n",
        "#module_info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2GClcq8dmrP"
      },
      "source": [
        "### Compound Database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-16T20:57:14.505525Z",
          "iopub.status.busy": "2025-03-16T20:57:14.505147Z",
          "iopub.status.idle": "2025-03-16T20:57:14.554277Z",
          "shell.execute_reply": "2025-03-16T20:57:14.553275Z",
          "shell.execute_reply.started": "2025-03-16T20:57:14.505490Z"
        },
        "id": "K9riHOnadqSm",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def read_compound_data():\n",
        "    paths = setup_paths()\n",
        "    compound_path = paths['compound']\n",
        "\n",
        "    compound_info = {}\n",
        "    with open(compound_path, 'r') as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split('\\t')\n",
        "                if len(parts) >= 2:\n",
        "                    compound_id = parts[0]\n",
        "                    compound_names = parts[1].split('; ')\n",
        "                    compound_info[compound_id] = {\n",
        "                        'name': compound_names[0],\n",
        "                        'synonyms': compound_names[1:] if len(compound_names) > 1 else []\n",
        "                    }\n",
        "    return compound_info\n",
        "\n",
        "compound_info = read_compound_data()\n",
        "#compound_info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hi9MNHn55BT3"
      },
      "source": [
        "### Metal pdb\n",
        "MetalPDB in 2018: a database of metal sites in biological macromolecular structures.\n",
        "Putignano V., Rosato A., Banci L., Andreini C.\n",
        "Nucleic Acids Res. 2018 Jan;46(D1):D459-D464. [PMID: 29077942]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-16T20:57:14.555673Z",
          "iopub.status.busy": "2025-03-16T20:57:14.555300Z",
          "iopub.status.idle": "2025-03-16T20:59:02.086252Z",
          "shell.execute_reply": "2025-03-16T20:59:02.085052Z",
          "shell.execute_reply.started": "2025-03-16T20:57:14.555635Z"
        },
        "id": "MhoKLqYQihT8",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def parse_metalpdb_xml():\n",
        "    \"\"\"Parse MetalPDB XML file to extract metal-binding information\"\"\"\n",
        "    paths = setup_paths()\n",
        "    metalpdb_path = paths['metalpdb']\n",
        "\n",
        "    metal_binding_data = {}\n",
        "\n",
        "    try:\n",
        "        # Use a more tolerant parser\n",
        "        parser = etree.XMLParser(recover=True)\n",
        "        tree = etree.parse(metalpdb_path, parser)\n",
        "        root = tree.getroot()\n",
        "\n",
        "        # Process each site\n",
        "        for site in root.findall('.//site'):\n",
        "            # Extract site information\n",
        "            site_name = site.findtext('site_name')\n",
        "            pdb_code = site.findtext('pdb_code')\n",
        "            site_nuclearity = site.findtext('site_nuclearity')\n",
        "\n",
        "            # Process each metal in the site\n",
        "            for metal in site.findall('.//metal'):\n",
        "                metal_symbol = metal.findtext('periodic_symbol')\n",
        "                metal_name = metal.findtext('periodic_name')\n",
        "                coordination_number = metal.findtext('coordination_number')\n",
        "                geometry = metal.findtext('geometry')\n",
        "\n",
        "                # Process ligands\n",
        "                ligands = []\n",
        "                for ligand in metal.findall('.//ligand'):\n",
        "                    residue_name = ligand.findtext('residue_name')\n",
        "                    residue_num = ligand.findtext('residue_pdb_number')\n",
        "                    chain = ligand.findtext('chain_letter')\n",
        "                    binding_type = ligand.findtext('endo_exo')\n",
        "\n",
        "                    # Process donor atoms\n",
        "                    donors = []\n",
        "                    for donor in ligand.findall('.//donor'):\n",
        "                        distance = donor.findtext('distance')\n",
        "                        atom_name = donor.findtext('atom_pdb_name')\n",
        "                        atom_symbol = donor.findtext('atom_symbol')\n",
        "                        interaction_type = donor.findtext('interaction_type')\n",
        "\n",
        "                        donors.append({\n",
        "                            'distance': distance,\n",
        "                            'atom_name': atom_name,\n",
        "                            'atom_symbol': atom_symbol,\n",
        "                            'interaction_type': interaction_type\n",
        "                        })\n",
        "\n",
        "                    ligands.append({\n",
        "                        'residue_name': residue_name,\n",
        "                        'residue_number': residue_num,\n",
        "                        'chain': chain,\n",
        "                        'binding_type': binding_type,\n",
        "                        'donors': donors\n",
        "                    })\n",
        "\n",
        "                # Get the protein/molecule information\n",
        "                site_chains = []\n",
        "                for chain in site.findall('.//site_chain'):\n",
        "                    molecule_name = chain.findtext('molecule_name')\n",
        "                    molecule_type = chain.findtext('molecule_type')\n",
        "                    chain_letter = chain.findtext('letter')\n",
        "\n",
        "                    site_chains.append({\n",
        "                        'molecule_name': molecule_name,\n",
        "                        'molecule_type': molecule_type,\n",
        "                        'chain_letter': chain_letter\n",
        "                    })\n",
        "\n",
        "                # Create a unique key for this metal site\n",
        "                metal_site_key = f\"{pdb_code}_{site_name}_{metal_symbol}\"\n",
        "\n",
        "                # Store the data\n",
        "                metal_binding_data[metal_site_key] = {\n",
        "                    'pdb_code': pdb_code,\n",
        "                    'site_name': site_name,\n",
        "                    'site_nuclearity': site_nuclearity,\n",
        "                    'metal': {\n",
        "                        'symbol': metal_symbol,\n",
        "                        'name': metal_name,\n",
        "                        'coordination_number': coordination_number,\n",
        "                        'geometry': geometry\n",
        "                    },\n",
        "                    'ligands': ligands,\n",
        "                    'site_chains': site_chains\n",
        "                }\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(\"Error parsing MetalPDB XML: %s\", e)\n",
        "        return {}\n",
        "\n",
        "    return metal_binding_data\n",
        "metal_binding_data = parse_metalpdb_xml()\n",
        "#metal_binding_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ScqMYjZ3dIM"
      },
      "source": [
        "### Extracting metal binding patterns from metal binding data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-16T20:59:05.567087Z",
          "iopub.status.busy": "2025-03-16T20:59:05.566612Z",
          "iopub.status.idle": "2025-03-16T20:59:06.431674Z",
          "shell.execute_reply": "2025-03-16T20:59:06.430361Z",
          "shell.execute_reply.started": "2025-03-16T20:59:05.567050Z"
        },
        "id": "2SwP7fDN3so5",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def extract_metal_coordination_patterns(metal_binding_data):\n",
        "    \"\"\"Extract metal coordination patterns from MetalPDB data\"\"\"\n",
        "\n",
        "    # Track metal coordination patterns\n",
        "    metal_coordination = {}\n",
        "    metal_residue_binding = {}\n",
        "\n",
        "    for site_key, site_data in metal_binding_data.items():\n",
        "        metal_symbol = site_data['metal']['symbol']\n",
        "\n",
        "        # Track coordination environments\n",
        "        coord_num = site_data['metal']['coordination_number']\n",
        "        geometry = site_data['metal']['geometry']\n",
        "        coord_key = f\"{metal_symbol}_{coord_num}_{geometry}\"\n",
        "\n",
        "        if coord_key not in metal_coordination:\n",
        "            metal_coordination[coord_key] = 0\n",
        "        metal_coordination[coord_key] += 1\n",
        "\n",
        "        # Track metal-residue binding\n",
        "        if metal_symbol not in metal_residue_binding:\n",
        "            metal_residue_binding[metal_symbol] = {}\n",
        "\n",
        "        for ligand in site_data['ligands']:\n",
        "            residue = ligand['residue_name']\n",
        "            if residue not in metal_residue_binding[metal_symbol]:\n",
        "                metal_residue_binding[metal_symbol][residue] = 0\n",
        "            metal_residue_binding[metal_symbol][residue] += 1\n",
        "\n",
        "    return {\n",
        "        'coordination': metal_coordination, # metal_coordination,\n",
        "        'residue_binding': metal_residue_binding# metal_residue_binding\n",
        "    }\n",
        "\n",
        "metal_patterns = extract_metal_coordination_patterns(metal_binding_data)\n",
        "#metal_patterns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdl9jF7TikIR"
      },
      "source": [
        "### EC to reaction Mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-16T20:59:06.433336Z",
          "iopub.status.busy": "2025-03-16T20:59:06.433022Z",
          "iopub.status.idle": "2025-03-16T21:05:34.887182Z",
          "shell.execute_reply": "2025-03-16T21:05:34.885947Z",
          "shell.execute_reply.started": "2025-03-16T20:59:06.433309Z"
        },
        "id": "FH9jMXKKSJM7",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def create_ec_to_reaction_mapping():\n",
        "    # Get EC to enzyme names mapping\n",
        "    ec_to_names = read_enzyme_names()\n",
        "\n",
        "    # Get reaction data\n",
        "    reaction_info = read_reaction_data()\n",
        "\n",
        "    # Create a mapping from EC to reactions\n",
        "    ec_to_reaction = {}\n",
        "\n",
        "    # Use string pattern matching to find EC numbers in reaction names\n",
        "    for rxn_id, rxn_info in reaction_info.items():\n",
        "        rxn_name = rxn_info['name'].lower()\n",
        "\n",
        "        # Look through all EC numbers and their names\n",
        "        for ec, names in ec_to_names.items():\n",
        "            enzyme_text = ' '.join(names).lower()\n",
        "\n",
        "            # Check for common significant words\n",
        "            if any(word in rxn_name for word in enzyme_text.split() if len(word) > 4):\n",
        "                if ec not in ec_to_reaction:\n",
        "                    ec_to_reaction[ec] = []\n",
        "                if rxn_id not in ec_to_reaction[ec]:\n",
        "                    ec_to_reaction[ec].append(rxn_id)\n",
        "\n",
        "    return ec_to_reaction\n",
        "\n",
        "ec_to_rxn = create_ec_to_reaction_mapping()\n",
        "#ec_to_rxn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQ3q5YlPYZWJ"
      },
      "source": [
        "## 9.2 Creating an Integrated Database"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBwBw-ufrWsh"
      },
      "source": [
        "### Consolidation Metals from Different Origins Helper Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-16T21:05:34.888791Z",
          "iopub.status.busy": "2025-03-16T21:05:34.888462Z",
          "iopub.status.idle": "2025-03-16T21:05:34.925356Z",
          "shell.execute_reply": "2025-03-16T21:05:34.924078Z",
          "shell.execute_reply.started": "2025-03-16T21:05:34.888762Z"
        },
        "id": "HXsb57_SraU0",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Standard mapping: lower-case keys for matching, with standard symbols as values\n",
        "metal_mapping = {\n",
        "    'iron': 'Fe',\n",
        "    'fe': 'Fe',\n",
        "    'ferrous': 'Fe',\n",
        "    'ferric': 'Fe',\n",
        "    'heme': 'Fe',\n",
        "    'iron-sulfur': 'Fe',\n",
        "    'fe2+': 'Fe',\n",
        "    'fe3+': 'Fe',\n",
        "    'manganese': 'Mn',\n",
        "    'mn': 'Mn',\n",
        "    'manganous': 'Mn',\n",
        "    'manganic': 'Mn',\n",
        "    'manganese oxidation': 'Mn',\n",
        "    'metal oxide': 'Mn',\n",
        "    'copper': 'Cu',\n",
        "    'cu+': 'Cu',\n",
        "    'cu2+': 'Cu',\n",
        "    'nickel': 'Ni',\n",
        "    'ni2+': 'Ni',\n",
        "    'cobalt': 'Co',\n",
        "    'co2+': 'Co',\n",
        "    'zinc': 'Zn',\n",
        "    'zn2+': 'Zn',\n",
        "    'calcium': 'Ca',\n",
        "    'ca2+': 'Ca',\n",
        "    'molybdenum': 'Mo',\n",
        "    'mo': 'Mo',\n",
        "    'vanadium': 'V5+',\n",
        "    'v5+': 'V5+',\n",
        "    'aluminum': 'Al3+',\n",
        "    'al3+': 'Al3+',\n",
        "    'chromium': 'Cr3+',\n",
        "    'cr3+': 'Cr3+',\n",
        "    'sodium': 'Na',\n",
        "    'na+': 'Na',\n",
        "    'nacl': 'Na',\n",
        "    'potassium': 'K',\n",
        "    'k+': 'K',\n",
        "    'kcl': 'K',\n",
        "    'selenium': 'Se',\n",
        "    'se': 'Se',\n",
        "    'barium': 'Ba2+',\n",
        "    'ba2+': 'Ba2+',\n",
        "    'sulfate': 'S',\n",
        "    'sulfide': 'S',\n",
        "    'thiosulfate': 'S',\n",
        "    's-s': 'S',\n",
        "    'sulfur': 'S',\n",
        "    'sulfur oxidation': 'S',\n",
        "    'srb': 'S',\n",
        "    'hydrogen': 'H',\n",
        "    'h2': 'H',\n",
        "    'h2o': 'H',\n",
        "    'h2s': 'H',\n",
        "    'phosfate': 'po4-3',\n",
        "    'nitrate': 'NO3-',\n",
        "    'nitrite': 'NO2',\n",
        "    'chloride': 'Cl-'\n",
        " }\n",
        "\n",
        "def consolidate_metal_terms(brenda_metals, text_detected_metals):\n",
        "    \"\"\"\n",
        "    Consolidates metal names from BRENDA and text mining into standardized symbols.\n",
        "\n",
        "    Parameters:\n",
        "        brenda_metals (list of str): Metals obtained from BRENDA data.\n",
        "        text_detected_metals (list of str): Metals detected from text mining.\n",
        "\n",
        "    Returns:\n",
        "        list: Consolidated list of unique, standardized metal symbols.\n",
        "    \"\"\"\n",
        "    consolidated = set()\n",
        "    all_metals = (brenda_metals or []) + (text_detected_metals or [])\n",
        "\n",
        "    for metal in all_metals:\n",
        "        metal_norm = metal.strip().lower()\n",
        "        # Check if the normalized term matches any key in the standard mapping\n",
        "        for key, symbol in metal_mapping.items():\n",
        "            if key in metal_norm:\n",
        "                consolidated.add(symbol)\n",
        "                break\n",
        "        else:\n",
        "            # If no mapping is found, add the original\n",
        "            consolidated.add(metal.strip())\n",
        "    return list(consolidated)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BNjB4FHE1_e"
      },
      "source": [
        "## Creating a consolidated DataBase"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZMB95fLE1_e"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHVkndSJpuS4"
      },
      "source": [
        "Brenda Enzyme Database. (n.d.). In BRENDA. Retrieved from https://www.brenda-enzymes.org (APA citation format)\n",
        "MetalPDB: a database of metal sites in biological macromolecular structures. (n.d.). Retrieved from http://metalpdb.cerm.unifi.it (APA citation format)\n",
        "\n",
        "### Main Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-16T21:05:34.927096Z",
          "iopub.status.busy": "2025-03-16T21:05:34.926727Z",
          "iopub.status.idle": "2025-03-16T21:05:34.977979Z",
          "shell.execute_reply": "2025-03-16T21:05:34.976717Z",
          "shell.execute_reply.started": "2025-03-16T21:05:34.927053Z"
        },
        "id": "Smqh2fy8-PJP",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def create_metabolism_database():\n",
        "    \"\"\"\n",
        "    Build a list of dictionaries, each representing a single EC record.\n",
        "    This approach is for conversion to a DataFrame, with proper error handling\n",
        "    and data validation.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Read all necessary files\n",
        "        ec_to_names = read_enzyme_names() or {}\n",
        "        enzyme_class = read_enzyme_class() or {}\n",
        "        reaction_equation = read_reaction_data() or {}\n",
        "        ko_ec = read_ko_data() or {}\n",
        "        ko_hierarchy = read_ko_hierarchy() or {}\n",
        "        pathway_data = read_pathway_data() or {}\n",
        "        module_info = read_module_data() or {}\n",
        "        compound_info = read_compound_data() or {}\n",
        "        brenda_en = process_brenda_data(brenda_data) or {}\n",
        "        metal_binding_data = parse_metalpdb_xml() or {}\n",
        "        metal_patterns = extract_metal_coordination_patterns(metal_binding_data)\n",
        "        ec_pathway_mapping = read_ec_pathway_mapping() or {}\n",
        "\n",
        "        print(f\"Loaded: {len(ec_to_names)} enzymes, {len(pathway_data)} pathways, {len(brenda_en)} BRENDA entries\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data sources: {e}\")\n",
        "        return []\n",
        "\n",
        "    # metal and corrosion keywords\n",
        "    metal_terms = {\n",
        "        'iron': ['iron', 'iron reduction','fe', 'ferrous', 'ferric', 'heme', 'iron-sulfur', 'Fe2+', 'Fe3+'],\n",
        "        'sulfur': ['sulfate', 'sulfide', 'thiosulfate', 'S-S', 'sulfur', 'srb', 'sulfur oxidation', 'sulfur reduction'],\n",
        "        'hydrogen': ['hydrogen', 'hydrogenase', 'h2'],\n",
        "        'manganese': ['Mn2+', 'manganese', 'mn', 'manganous', 'manganic', 'manganese oxidation', 'metal oxide'],\n",
        "        'biofilm': ['exopolysaccharide', 'biofilm', 'adhesin', 'eps', 'polysaccharide'],\n",
        "        'copper': ['Cu+', 'Cu2+', 'copper'],\n",
        "        'nickel': ['Ni2+', 'nickel'],\n",
        "        'cobalt': ['Co2+', 'cobalt'],\n",
        "        'calcium': ['Ca2+', 'calcium'],\n",
        "        'Mo': ['Mo', 'molybdenum'],\n",
        "        'V5+': ['V5+', 'vanadium'],\n",
        "        'Al3+': ['Al3+', 'aluminum'],\n",
        "        'Cr3+': ['Cr3+', 'chromium'],\n",
        "        'zinc': ['Zn2+', 'zinc'],\n",
        "        'sodium': ['Na+', 'sodium', 'NaCl'],\n",
        "        'potassium': ['K+', 'potassium', 'KCl'],\n",
        "        'selenium': ['selenium', 'Se'],\n",
        "        'barium': ['Ba2+', 'barium'],\n",
        "        'chloride': ['chloride', 'cl-'],\n",
        "        'nitrate': ['nitrate', 'NO3-'],\n",
        "        'nitrite': ['nitrite', 'NO2-'],\n",
        "        'phosphate': ['phosphate', 'po4-3']\n",
        "    }\n",
        "\n",
        "    # Corrosion mechanism classification\n",
        "    corrosion_mechanisms = {\n",
        "          'direct_eet': ['cytochrome', 'electron transfer', 'conductive pili', 'nanowire', 'mtrABC', 'omcS','oxidoreductase', 'redox', 'reductase', 'oxidase'],\n",
        "          'indirect_eet': ['shuttle', 'mediator', 'redox mediator'],\n",
        "          'acid_production': ['acid', 'acidification', 'fermentation', 'lactic acid', 'formic acid', 'acetic acid', 'oxalic acid', 'organic acid', 'acetate production', 'lactate metabolism', 'formate production'],\n",
        "          'h2_consumption': ['hydrogenase', 'hydrogen uptake', 'hydrogen consumption', 'h2', 'H2 oxidation', 'H2ase'],\n",
        "          'o2_consumption': ['oxidase', 'oxygen reduction', 'aerobic respiration','oxygen reduc', 'aerobic respiration', 'oxygen consum'],\n",
        "          'biofilm_formation': ['polysaccharide', 'adhesin', 'biofilm', 'EPS', 'extracellular polymeric substance', 'curli', 'exopolymer', 'extracellular matrix', 'adhesion', 'colonization', 'attachment'],\n",
        "          'sulfur_metabolism': ['sulfate reduc', 'sulfide', 'sulfite', 'thiosulfate', 'sulfur oxidation', 'SRB'],\n",
        "          'metal_transformation': ['iron reduction', 'manganese oxidation', 'metal oxide', 'ochre formation', 'iron oxide deposits', 'iron precipitation', 'rust formation',],\n",
        "          'iron_metabolism': ['iron reduc', 'ferric reduc', 'iron oxid', 'ferrous oxid'],\n",
        "          'metal_chelation': ['siderophore', 'metal binding', 'chelator', 'metallophore', 'iron complex', 'metal transport'],\n",
        "          'carbon_metabolism': ['carbon fixation', 'carbon utilization', 'carbohydrate metabolism', 'glycolysis', 'TCA cycle'],\n",
        "          'ph_modulation': ['acid tolerance', 'alkaline tolerance', 'proton pump', 'pH homeostasis', 'pH stress']\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "    # Get EC to reaction mapping\n",
        "    try:\n",
        "        ec_to_rxn = create_ec_to_reaction_mapping()\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating EC to reaction mapping: {e}\")\n",
        "        ec_to_rxn =  {}\n",
        "\n",
        "    # Prepare a list to store all records\n",
        "    ec_records = []\n",
        "\n",
        "    # Track statistics for validation\n",
        "    stats = {\n",
        "        'total_enzymes': 0,\n",
        "        'with_brenda_data': 0,\n",
        "        'with_reactions': 0,\n",
        "        'with_pathways': 0,\n",
        "        'with_ko': 0,\n",
        "        'with_metal_involvement': 0,\n",
        "        'with_corrosion_mechanisms': 0\n",
        "    }\n",
        "\n",
        "    # populate from ec_to_names for basic enzyme names\n",
        "    for ec_number, names in ec_to_names.items():\n",
        "        stats['total_enzymes'] += 1\n",
        "\n",
        "        # Data validation for EC number format\n",
        "        if not (ec_number.count('.') == 3 and all(part.isdigit() for part in ec_number.split('.'))):\n",
        "            print(f\"Warning: Invalid EC number format: {ec_number}\")\n",
        "            continue\n",
        "\n",
        "        record ={\n",
        "            'ec_number': ec_number,\n",
        "            'enzyme_names':  names if isinstance(names, list) else [str(names)],\n",
        "            'enzyme_class': None,\n",
        "            'pathways': [],\n",
        "            'hierarchy': [],\n",
        "            'ko': [],\n",
        "            'reactions': [],\n",
        "            'compounds': [],\n",
        "            'modules': []\n",
        "        }\n",
        "\n",
        "        # Add pathways from EC-pathway mapping\n",
        "        if ec_number in ec_pathway_mapping:\n",
        "            for pathway_id in ec_pathway_mapping[ec_number]:\n",
        "                # Standardize to map prefix\n",
        "                std_id = pathway_id\n",
        "                if pathway_id.startswith('ec'):\n",
        "                    std_id = 'map' + pathway_id[2:]\n",
        "\n",
        "                # Look up the pathway name\n",
        "                if std_id in pathway_data:\n",
        "                    pathway_name = pathway_data[std_id]\n",
        "                    if pathway_name not in record['pathways']:\n",
        "                        record['pathways'].append(pathway_name)\n",
        "\n",
        "        # Add pathways from KO data\n",
        "        if ec_number in ko_ec and isinstance(ko_ec[ec_number], list):\n",
        "            for path in ko_ec[ec_number]:\n",
        "                if path not in record['pathways']:\n",
        "                    record['pathways'].append(path)\n",
        "        elif ec_number in ko_ec and isinstance(ko_ec[ec_number], dict) and 'pathway' in ko_ec[ec_number]:\n",
        "            path = ko_ec[ec_number]['pathway']\n",
        "            if path not in record['pathways']:\n",
        "                record['pathways'].append(path)\n",
        "\n",
        "        # Add KO IDs\n",
        "        ko_ids = []\n",
        "        for ko, data in ko_ec.items():\n",
        "            if isinstance(data, dict) and 'definition' in data and f\"[EC:{ec_number}]\" in data['definition']:\n",
        "                ko_ids.append(ko)\n",
        "        record['ko'] = ko_ids\n",
        "\n",
        "        if ko_ids:\n",
        "            stats['with_ko'] += 1\n",
        "\n",
        "        # Build reaction list\n",
        "        rxns = ec_to_rxn.get(ec_number, [])\n",
        "        for rxn_id in rxns:\n",
        "            if rxn_id in reaction_equation:\n",
        "                eqn = reaction_equation.get(rxn_id, {}).get('equation', 'Unknown')\n",
        "                record['reactions'].append({'id': rxn_id, 'equation': eqn})\n",
        "\n",
        "                # Add compounds involved in this reaction\n",
        "                for compound_id in reaction_equation.get(rxn_id, {}).get('compounds', []):\n",
        "                    if compound_id in compound_info:\n",
        "                        compound_data = compound_info[compound_id]\n",
        "                        if compound_data not in record['compounds']:\n",
        "                            record['compounds'].append(compound_data)\n",
        "\n",
        "        if rxns:\n",
        "            stats['with_reactions'] += 1\n",
        "\n",
        "        # Add module information\n",
        "        for module_id, module_desc in module_info.items():\n",
        "            if f\"[EC:{ec_number}]\" in module_desc:\n",
        "                record['modules'].append({'id': module_id, 'description': module_desc})\n",
        "\n",
        "        # Reconcile metals from BRENDA with text mining\n",
        "        record['metals_from_brenda'] = []\n",
        "        record['corrosion_metals_from_brenda'] = []\n",
        "\n",
        "        # Add BRENDA metal information\n",
        "        if brenda_en and ec_number in brenda_en:\n",
        "            record['metals_from_brenda'] = brenda_en[ec_number].get('clean_metals', [])\n",
        "            record['corrosion_metals_from_brenda'] = brenda_en[ec_number].get('corrosion_metals_from_brenda', [])\n",
        "            stats['with_brenda_data'] += 1\n",
        "\n",
        "        ec_records.append(record)\n",
        "\n",
        "    # Count records with pathways\n",
        "    pathway_count = 0\n",
        "\n",
        "    # Add pathway information from pathway_data\n",
        "    for rec in ec_records:\n",
        "        # Get KO terms for this EC number\n",
        "        ko_ids = rec.get('ko', [])\n",
        "\n",
        "        # For each KO, check if it has a pathway\n",
        "        for ko_id in ko_ids:\n",
        "            if ko_id in ko_ec and 'pathway' in ko_ec[ko_id]:\n",
        "                pathway_id = ko_ec[ko_id]['pathway']\n",
        "\n",
        "                # If the pathway ID exists in pathway_data, add it\n",
        "                if pathway_id and pathway_id in pathway_data:\n",
        "                    pathway_name = pathway_data[pathway_id]\n",
        "                    if pathway_name not in rec['pathways']:\n",
        "                        rec['pathways'].append(pathway_name)\n",
        "\n",
        "        # If records has pathways it would update them\n",
        "        if rec['pathways']:\n",
        "            stats['with_pathways'] += 1\n",
        "            pathway_count += 1\n",
        "\n",
        "    # After processing all records, print count\n",
        "    pathway_count = sum(1 for rec in ec_records if rec['pathways'])\n",
        "    print(f\"Added pathway information to {pathway_count} records\")\n",
        "\n",
        "    # Add enzyme class info\n",
        "    for rec in ec_records:\n",
        "        try:\n",
        "            ec_number = rec['ec_number']\n",
        "            ec_prefix = '.'.join(ec_number.split('.')[:2])\n",
        "\n",
        "            # Try exact match first\n",
        "            if ec_prefix in enzyme_class:\n",
        "                rec['enzyme_class'] = enzyme_class[ec_prefix]\n",
        "            # Then try pattern match\n",
        "            else:\n",
        "                pattern_key = f\"{ec_prefix}.-.-\"\n",
        "                if pattern_key in enzyme_class:\n",
        "                    rec['enzyme_class'] = enzyme_class[pattern_key]\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing enzyme class for {rec.get('ec_number')}: {e}\")\n",
        "\n",
        "    # Add metal_involved & corrosion_mechanisms\n",
        "    for rec in ec_records:\n",
        "        try:\n",
        "            # Combine name and class text properly\n",
        "            names_text = ' '.join(rec.get('enzyme_names', []))\n",
        "            class_text = rec.get('enzyme_class', '')\n",
        "            all_text = f\"{names_text} {class_text}\".lower()\n",
        "\n",
        "            # Add reaction text for more context\n",
        "            reaction_text = ' '.join([r.get('equation', '') for r in rec['reactions']])\n",
        "            all_text += f\" {reaction_text.lower()}\"\n",
        "\n",
        "            # Reconcile BRENDA metals with text mining\n",
        "            detected_metals = {}\n",
        "\n",
        "            # First add metals from BRENDA\n",
        "            for metal in rec.get('metals_from_brenda', []):\n",
        "                detected_metals[metal] = True\n",
        "\n",
        "            # Then add metals from text mining\n",
        "            for metal, terms in metal_terms.items():\n",
        "                if any(term.lower() in all_text for term in terms):\n",
        "                    detected_metals[metal] = True\n",
        "\n",
        "            # Store consolidated metals\n",
        "            rec['metals_involved'] = list(detected_metals.keys())\n",
        "\n",
        "            if detected_metals:\n",
        "                stats['with_metal_involvement'] += 1\n",
        "\n",
        "            rec['metals_consolidated']= consolidate_metal_terms(\n",
        "                rec.get('metals_from_brenda', []),\n",
        "                rec.get('metals_involved', [])\n",
        "            )\n",
        "            # corrosion_mechanisms (use a set for efficiency)\n",
        "            corrosion_mechs = set()\n",
        "            for mech, terms in corrosion_mechanisms.items():\n",
        "                if any(term.lower() in all_text for term in terms):\n",
        "                    corrosion_mechs.add(mech)\n",
        "\n",
        "            rec['corrosion_mechanisms'] = list(corrosion_mechs)\n",
        "\n",
        "            if corrosion_mechs:\n",
        "                stats['with_corrosion_mechanisms'] += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing metal/mechanisms data for {rec.get('ec_number')}: {e}\")\n",
        "\n",
        "    # Add metal binding potential from MetalPDB\n",
        "    try:\n",
        "        # Assuming parse_metalpdb_xml and extract_metal_coordination_patterns are defined\n",
        "        metal_binding_data = parse_metalpdb_xml()\n",
        "        metal_patterns = extract_metal_coordination_patterns(metal_binding_data)\n",
        "\n",
        "        print(f\"Extracted coordination patterns for {len(metal_patterns.get('coordination', {}))} metal-coordination environments\")\n",
        "        print(f\"Extracted residue binding patterns for {len(metal_patterns.get('residue_binding', {}))} metals\")\n",
        "\n",
        "        # Add metal binding information to records\n",
        "        for rec in ec_records:\n",
        "            rec['metal_binding_info'] = {}\n",
        "\n",
        "            # Check metals from BRENDA or detected in text\n",
        "            all_metals = set(rec.get('metals_from_brenda', []) + rec.get('metals_involved', []))\n",
        "\n",
        "            for metal in all_metals:\n",
        "                # Try to map to standard symbol\n",
        "                for metal_name, symbol in metal_mapping.items():\n",
        "                    if metal_name in metal.lower() or symbol.lower() in metal.lower():\n",
        "                        # Check if we have binding data for this metal\n",
        "                        if symbol in metal_patterns.get('residue_binding', {}):\n",
        "                            # Get top binding residues\n",
        "                            residue_counts = metal_patterns['residue_binding'][symbol]\n",
        "                            top_residues = sorted(residue_counts.items(), key=lambda x: x[1], reverse=True)[:5]\n",
        "\n",
        "                            rec['metal_binding_info'][symbol] = {\n",
        "                                'common_residues': [res for res, count in top_residues],\n",
        "                                'binding_count': sum(residue_counts.values())\n",
        "                            }\n",
        "\n",
        "                            # Check for common coordination geometries\n",
        "                            geometries = [\n",
        "                                key.split('_')[2:] for key, count in metal_patterns['coordination'].items()\n",
        "                                if key.startswith(f\"{symbol}_\")\n",
        "                            ]\n",
        "                            if geometries:\n",
        "                                rec['metal_binding_info'][symbol]['common_geometries'] = geometries[:3]\n",
        "\n",
        "        # Add this to corrosion relevance calculation\n",
        "        corrosion_metals = ['Fe', 'Mn', 'Cu', 'Ni', 'Co', 'Zn', 'Al', 'Cr']\n",
        "        for rec in ec_records:\n",
        "            # Add binding score to corrosion relevance\n",
        "            binding_score = sum(\n",
        "                2 for metal in corrosion_metals\n",
        "                if metal in rec.get('metal_binding_info', {})\n",
        "            )\n",
        "\n",
        "            # Update corrosion score\n",
        "            if 'corrosion_relevance_score' in rec:\n",
        "                rec['corrosion_relevance_score'] += binding_score\n",
        "            else:\n",
        "                rec['corrosion_relevance_score'] = binding_score\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing metal binding data: {e}\")\n",
        "\n",
        "    # Integrate KO Hierarchy\n",
        "    if 'D' in ko_hierarchy:\n",
        "        for ko, info in ko_hierarchy.get('D', {}).items():\n",
        "            for ec in info.get('ec_numbers', []):\n",
        "                # find matching records\n",
        "                for rec in ec_records:\n",
        "                    if rec['ec_number'] == ec:\n",
        "                        try:\n",
        "                            parent_c = info.get('parent')\n",
        "                            if parent_c and 'C' in ko_hierarchy and parent_c in ko_hierarchy['C']:\n",
        "                                path_info = ko_hierarchy['C'][parent_c]\n",
        "                                parent_b = path_info.get('parent')\n",
        "                                if parent_b and 'B' in ko_hierarchy and parent_b in ko_hierarchy['B']:\n",
        "                                    hi_category = ko_hierarchy['B'][parent_b].get('name', '')\n",
        "                                    pathway = path_info.get('name', '')\n",
        "\n",
        "                                    # Use sets to efficiently track unique values\n",
        "                                    if pathway and pathway not in rec['pathways']:\n",
        "                                        rec['pathways'].append(pathway)\n",
        "\n",
        "                                    hierarchy = f\"{hi_category} > {pathway}\"\n",
        "                                    if hierarchy and hierarchy not in rec['hierarchy']:\n",
        "                                        rec['hierarchy'].append(hierarchy)\n",
        "                        except Exception as e:\n",
        "                            print(f\"Error processing KO hierarchy for {rec.get('ec_number')}: {e}\")\n",
        "\n",
        "    # Calculate corrosion relevance score\n",
        "    for rec in ec_records:\n",
        "        try:\n",
        "            # Base score on metal involvement and corrosion mechanisms\n",
        "            metal_score = len(rec.get('metals_involved', [])) * 1.5\n",
        "            mech_score = len(rec.get('corrosion_mechanisms', [])) * 2\n",
        "            pathway_score = 0\n",
        "\n",
        "            # Add scores for relevant pathways\n",
        "            corrosion_pathway_terms = corrosion_pathway_terms = ['iron', 'sulfur', 'oxide', 'corrosion', 'metal', 'biofilm',\n",
        "                              'ochre', 'acid', 'rust', 'precipitation', 'electron transfer',\n",
        "                              'redox', 'siderophore', 'chelation', 'acidification',\n",
        "                              'hydrogen', 'oxygen consumption', 'degradation']\n",
        "\n",
        "            for pathway in rec.get('pathways', []):\n",
        "                if any(term in pathway.lower() for term in corrosion_pathway_terms):\n",
        "                    pathway_score += 1\n",
        "\n",
        "            # Calculate final score\n",
        "            rec['corrosion_relevance_score'] = metal_score + mech_score + pathway_score\n",
        "\n",
        "            # Categorize\n",
        "            if rec['corrosion_relevance_score'] >= 5:\n",
        "                rec['corrosion_relevance'] = 'high'\n",
        "            elif rec['corrosion_relevance_score'] >= 2:\n",
        "                rec['corrosion_relevance'] = 'medium'\n",
        "            else:\n",
        "                rec['corrosion_relevance'] = 'low'\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error calculating corrosion score for {rec.get('ec_number')}: {e}\")\n",
        "            rec['corrosion_relevance_score'] = 0\n",
        "            rec['corrosion_relevance'] = 'unknown'\n",
        "\n",
        "    # Filter records without content\n",
        "    filtered_ec_records = []\n",
        "    for record in ec_records:\n",
        "        protein_name = record.get('protein_name', \"\").lower()\n",
        "        enzyme_names = record.get('enzyme_names', [])\n",
        "        ec_number = record.get('ec_number', \"\")\n",
        "\n",
        "        # Condition 1: At least one valid identifier must be present\n",
        "        has_valid_protein = \"uncharacterized\" not in protein_name and len(protein_name) > 2\n",
        "        has_valid_enzyme = any(len(name) > 2 for name in enzyme_names)\n",
        "        has_valid_ec = ec_number.count('.') == 3 and all(part.isdigit() for part in ec_number.split('.'))\n",
        "\n",
        "        # Condition 2: Check for valuable data that should be preserved\n",
        "        has_mechanisms = len(record.get('corrosion_mechanisms', [])) > 0\n",
        "        has_pathways = len(record.get('pathways', [])) > 0\n",
        "        has_metal_involvement = len(record.get('metals_consolidated', [])) > 0\n",
        "\n",
        "        # Include record if it meets either condition\n",
        "        if (has_valid_protein or has_valid_enzyme or has_valid_ec) or \\\n",
        "           (has_mechanisms or has_pathways or has_metal_involvement):\n",
        "            filtered_ec_records.append(record)\n",
        "\n",
        "    # Replace original list with filtered version\n",
        "    ec_records = filtered_ec_records\n",
        "\n",
        "    # Print summary statistics\n",
        "    print(\"\\nMetabolism Database Summary:\")\n",
        "    print(f\"Total enzyme records: {stats['total_enzymes']}\")\n",
        "    print(f\"Records with BRENDA data: {stats['with_brenda_data']} ({stats['with_brenda_data']/stats['total_enzymes']*100:.1f}%)\")\n",
        "    print(f\"Records with reactions: {stats['with_reactions']} ({stats['with_reactions']/stats['total_enzymes']*100:.1f}%)\")\n",
        "    print(f\"Records with pathways: {stats['with_pathways']} ({stats['with_pathways']/stats['total_enzymes']*100:.1f}%)\")\n",
        "    print(f\"Records with KO terms: {stats['with_ko']} ({stats['with_ko']/stats['total_enzymes']*100:.1f}%)\")\n",
        "    print(f\"Records with metal involvement: {stats['with_metal_involvement']} ({stats['with_metal_involvement']/stats['total_enzymes']*100:.1f}%)\")\n",
        "    print(f\"Records with corrosion mechanisms: {stats['with_corrosion_mechanisms']} ({stats['with_corrosion_mechanisms']/stats['total_enzymes']*100:.1f}%)\")\n",
        "\n",
        "    # Validate the data - check for missing essential fields\n",
        "    validation_issues = []\n",
        "    for i, rec in enumerate(ec_records):\n",
        "        if not rec.get('ec_number'):\n",
        "            validation_issues.append(f\"Record {i} missing EC number\")\n",
        "        if not rec.get('enzyme_names'):\n",
        "            validation_issues.append(f\"EC {rec.get('ec_number')} missing enzyme names\")\n",
        "\n",
        "    if validation_issues:\n",
        "        print(\"\\nValidation Issues:\")\n",
        "        for issue in validation_issues[:10]:  # Show first 10 issues\n",
        "            print(f\"- {issue}\")\n",
        "        if len(validation_issues) > 10:\n",
        "            print(f\"...and {len(validation_issues) - 10} more issues\")\n",
        "    else:\n",
        "        print(\"\\nValidation: All records have essential fields.\")\n",
        "\n",
        "    return ec_records"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-16T21:05:34.979756Z",
          "iopub.status.busy": "2025-03-16T21:05:34.979426Z",
          "iopub.status.idle": "2025-03-16T21:17:47.279026Z",
          "shell.execute_reply": "2025-03-16T21:17:47.277736Z",
          "shell.execute_reply.started": "2025-03-16T21:05:34.979727Z"
        },
        "id": "G_pX1Jqj4KNZ",
        "outputId": "9acbf133-b156-473a-a2d4-1bbca6db2dc8",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded pathway mappings for 3889 EC numbers\n",
            "Loaded: 8235 enzymes, 578 pathways, 6710 BRENDA entries\n",
            "Added pathway information to 3888 records\n",
            "Extracted coordination patterns for 1586 metal-coordination environments\n",
            "Extracted residue binding patterns for 65 metals\n",
            "\n",
            "Metabolism Database Summary:\n",
            "Total enzyme records: 8235\n",
            "Records with BRENDA data: 6710 (81.5%)\n",
            "Records with reactions: 6403 (77.8%)\n",
            "Records with pathways: 3888 (47.2%)\n",
            "Records with KO terms: 4873 (59.2%)\n",
            "Records with metal involvement: 8121 (98.6%)\n",
            "Records with corrosion mechanisms: 6284 (76.3%)\n",
            "\n",
            "Validation: All records have essential fields.\n"
          ]
        }
      ],
      "source": [
        "# Takes around 10 min\n",
        "ec_records = create_metabolism_database()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-16T21:17:47.281105Z",
          "iopub.status.busy": "2025-03-16T21:17:47.280719Z",
          "iopub.status.idle": "2025-03-16T21:17:47.329898Z",
          "shell.execute_reply": "2025-03-16T21:17:47.328890Z",
          "shell.execute_reply.started": "2025-03-16T21:17:47.281056Z"
        },
        "id": "HgVjVgyVjib0",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "ec_metadata = pd.DataFrame(ec_records)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TO-o4FC3933"
      },
      "source": [
        "Loaded pathway mappings for 3889 EC numbers   \n",
        "Loaded: 8235 enzymes, 578 pathways, 6710 BRENDA entries   \n",
        "Added pathway information to 3888 records   \n",
        "Extracted coordination patterns for 1586 metal-coordination environments   \n",
        "Extracted residue binding patterns for 65 metals   \n",
        "\n",
        "Metabolism Database Summary:   \n",
        "Total enzyme records: 8235   \n",
        "Records with BRENDA data: 6710 (81.5%)  \n",
        "Records with reactions: 6405 (77.8%)  \n",
        "Records with pathways: 3888 (47.2%)  \n",
        "Records with KO terms: 4873 (59.2%)  \n",
        "Records with metal involvement: 7873 (95.6%)  \n",
        "Records with corrosion mechanisms: 6131 (74.5%)  \n",
        "\n",
        "Validation: All records have essential fields..  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-16T21:33:44.746497Z",
          "iopub.status.busy": "2025-03-16T21:33:44.746097Z",
          "iopub.status.idle": "2025-03-16T21:33:44.885795Z",
          "shell.execute_reply": "2025-03-16T21:33:44.884573Z",
          "shell.execute_reply.started": "2025-03-16T21:33:44.746469Z"
        },
        "id": "RmQeLU0ZE1_g",
        "outputId": "9997f308-1ff9-4735-f5e2-91fd4d3b5be3",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ec_number</th>\n",
              "      <th>enzyme_names</th>\n",
              "      <th>enzyme_class</th>\n",
              "      <th>pathways</th>\n",
              "      <th>hierarchy</th>\n",
              "      <th>ko</th>\n",
              "      <th>reactions</th>\n",
              "      <th>compounds</th>\n",
              "      <th>modules</th>\n",
              "      <th>metals_from_brenda</th>\n",
              "      <th>corrosion_metals_from_brenda</th>\n",
              "      <th>metals_involved</th>\n",
              "      <th>metals_consolidated</th>\n",
              "      <th>corrosion_mechanisms</th>\n",
              "      <th>metal_binding_info</th>\n",
              "      <th>corrosion_relevance_score</th>\n",
              "      <th>corrosion_relevance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.1.1.1</td>\n",
              "      <td>[alcohol dehydrogenase, aldehyde reductase, AD...</td>\n",
              "      <td>Acting on the CH-OH group of donors.</td>\n",
              "      <td>[Glycolysis / Gluconeogenesis, Fatty acid degr...</td>\n",
              "      <td>[Endocrine and metabolic disease &gt; Alcoholic l...</td>\n",
              "      <td>[K00001, K11440, K13951, K13952, K13953, K1395...</td>\n",
              "      <td>[{'id': 'R00002', 'equation': '16 ATP + 16 H2O...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[selenium, Mo, copper, zinc]</td>\n",
              "      <td>[copper]</td>\n",
              "      <td>[selenium, Mo, copper, zinc, iron, sulfur, hyd...</td>\n",
              "      <td>[H, biofilm, S, Mn, NO2, Se, phosphate, Cl-, F...</td>\n",
              "      <td>[direct_eet, o2_consumption, biofilm_formation...</td>\n",
              "      <td>{'Ni': {'common_residues': ['HIS', 'HOH', 'CYS...</td>\n",
              "      <td>38.0</td>\n",
              "      <td>high</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.1.1.2</td>\n",
              "      <td>[alcohol dehydrogenase, aldehyde reductase, NA...</td>\n",
              "      <td>Acting on the CH-OH group of donors.</td>\n",
              "      <td>[Glycolysis / Gluconeogenesis, Pentose and glu...</td>\n",
              "      <td>[Protein families: signaling and cellular proc...</td>\n",
              "      <td>[K00002, K13979]</td>\n",
              "      <td>[{'id': 'R00002', 'equation': '16 ATP + 16 H2O...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[iron, zinc, Mo]</td>\n",
              "      <td>[iron]</td>\n",
              "      <td>[iron, zinc, Mo, sulfur, hydrogen, manganese, ...</td>\n",
              "      <td>[H, biofilm, S, Mn, NO2, Se, phosphate, Cl-, F...</td>\n",
              "      <td>[direct_eet, o2_consumption, biofilm_formation...</td>\n",
              "      <td>{'Ni': {'common_residues': ['HIS', 'HOH', 'CYS...</td>\n",
              "      <td>36.0</td>\n",
              "      <td>high</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.1.1.3</td>\n",
              "      <td>[homoserine dehydrogenase, HSDH, HSD]</td>\n",
              "      <td>Acting on the CH-OH group of donors.</td>\n",
              "      <td>[Glycine, serine and threonine metabolism, Cys...</td>\n",
              "      <td>[Biosynthesis of other secondary metabolites &gt;...</td>\n",
              "      <td>[K00003]</td>\n",
              "      <td>[{'id': 'R00651', 'equation': 'O-acetyl-L-homo...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[iron, sulfur, hydrogen, cobalt, Mo, selenium,...</td>\n",
              "      <td>[H, S, Se, phosphate, Fe, Co, Mo]</td>\n",
              "      <td>[direct_eet, o2_consumption, h2_consumption, a...</td>\n",
              "      <td>{'Co': {'common_residues': ['HOH', 'HIS', 'NCO...</td>\n",
              "      <td>21.5</td>\n",
              "      <td>high</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.1.1.4</td>\n",
              "      <td>[-butanediol dehydrogenase, butyleneglycol deh...</td>\n",
              "      <td>Acting on the CH-OH group of donors.</td>\n",
              "      <td>[Butanoate metabolism, Biosynthesis of seconda...</td>\n",
              "      <td>[Carbohydrate metabolism &gt; Butanoate metabolism]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[{'id': 'R00002', 'equation': '16 ATP + 16 H2O...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[Mo]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[Mo, iron, sulfur, hydrogen, manganese, biofil...</td>\n",
              "      <td>[H, biofilm, S, Mn, NO2, Se, phosphate, Cl-, F...</td>\n",
              "      <td>[direct_eet, o2_consumption, biofilm_formation...</td>\n",
              "      <td>{'Ni': {'common_residues': ['HIS', 'HOH', 'CYS...</td>\n",
              "      <td>31.5</td>\n",
              "      <td>high</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.1.1.5</td>\n",
              "      <td>[Transferred to 1.1.1.303 and 1.1.1.304]</td>\n",
              "      <td>Acting on the CH-OH group of donors.</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[iron]</td>\n",
              "      <td>[Fe]</td>\n",
              "      <td>[]</td>\n",
              "      <td>{'Fe': {'common_residues': ['HIS', 'HEM', 'CYS...</td>\n",
              "      <td>1.5</td>\n",
              "      <td>low</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  ec_number                                       enzyme_names  \\\n",
              "0   1.1.1.1  [alcohol dehydrogenase, aldehyde reductase, AD...   \n",
              "1   1.1.1.2  [alcohol dehydrogenase, aldehyde reductase, NA...   \n",
              "2   1.1.1.3              [homoserine dehydrogenase, HSDH, HSD]   \n",
              "3   1.1.1.4  [-butanediol dehydrogenase, butyleneglycol deh...   \n",
              "4   1.1.1.5           [Transferred to 1.1.1.303 and 1.1.1.304]   \n",
              "\n",
              "                           enzyme_class  \\\n",
              "0  Acting on the CH-OH group of donors.   \n",
              "1  Acting on the CH-OH group of donors.   \n",
              "2  Acting on the CH-OH group of donors.   \n",
              "3  Acting on the CH-OH group of donors.   \n",
              "4  Acting on the CH-OH group of donors.   \n",
              "\n",
              "                                            pathways  \\\n",
              "0  [Glycolysis / Gluconeogenesis, Fatty acid degr...   \n",
              "1  [Glycolysis / Gluconeogenesis, Pentose and glu...   \n",
              "2  [Glycine, serine and threonine metabolism, Cys...   \n",
              "3  [Butanoate metabolism, Biosynthesis of seconda...   \n",
              "4                                                 []   \n",
              "\n",
              "                                           hierarchy  \\\n",
              "0  [Endocrine and metabolic disease > Alcoholic l...   \n",
              "1  [Protein families: signaling and cellular proc...   \n",
              "2  [Biosynthesis of other secondary metabolites >...   \n",
              "3   [Carbohydrate metabolism > Butanoate metabolism]   \n",
              "4                                                 []   \n",
              "\n",
              "                                                  ko  \\\n",
              "0  [K00001, K11440, K13951, K13952, K13953, K1395...   \n",
              "1                                   [K00002, K13979]   \n",
              "2                                           [K00003]   \n",
              "3                                                 []   \n",
              "4                                                 []   \n",
              "\n",
              "                                           reactions compounds modules  \\\n",
              "0  [{'id': 'R00002', 'equation': '16 ATP + 16 H2O...        []      []   \n",
              "1  [{'id': 'R00002', 'equation': '16 ATP + 16 H2O...        []      []   \n",
              "2  [{'id': 'R00651', 'equation': 'O-acetyl-L-homo...        []      []   \n",
              "3  [{'id': 'R00002', 'equation': '16 ATP + 16 H2O...        []      []   \n",
              "4                                                 []        []      []   \n",
              "\n",
              "             metals_from_brenda corrosion_metals_from_brenda  \\\n",
              "0  [selenium, Mo, copper, zinc]                     [copper]   \n",
              "1              [iron, zinc, Mo]                       [iron]   \n",
              "2                            []                           []   \n",
              "3                          [Mo]                           []   \n",
              "4                            []                           []   \n",
              "\n",
              "                                     metals_involved  \\\n",
              "0  [selenium, Mo, copper, zinc, iron, sulfur, hyd...   \n",
              "1  [iron, zinc, Mo, sulfur, hydrogen, manganese, ...   \n",
              "2  [iron, sulfur, hydrogen, cobalt, Mo, selenium,...   \n",
              "3  [Mo, iron, sulfur, hydrogen, manganese, biofil...   \n",
              "4                                             [iron]   \n",
              "\n",
              "                                 metals_consolidated  \\\n",
              "0  [H, biofilm, S, Mn, NO2, Se, phosphate, Cl-, F...   \n",
              "1  [H, biofilm, S, Mn, NO2, Se, phosphate, Cl-, F...   \n",
              "2                  [H, S, Se, phosphate, Fe, Co, Mo]   \n",
              "3  [H, biofilm, S, Mn, NO2, Se, phosphate, Cl-, F...   \n",
              "4                                               [Fe]   \n",
              "\n",
              "                                corrosion_mechanisms  \\\n",
              "0  [direct_eet, o2_consumption, biofilm_formation...   \n",
              "1  [direct_eet, o2_consumption, biofilm_formation...   \n",
              "2  [direct_eet, o2_consumption, h2_consumption, a...   \n",
              "3  [direct_eet, o2_consumption, biofilm_formation...   \n",
              "4                                                 []   \n",
              "\n",
              "                                  metal_binding_info  \\\n",
              "0  {'Ni': {'common_residues': ['HIS', 'HOH', 'CYS...   \n",
              "1  {'Ni': {'common_residues': ['HIS', 'HOH', 'CYS...   \n",
              "2  {'Co': {'common_residues': ['HOH', 'HIS', 'NCO...   \n",
              "3  {'Ni': {'common_residues': ['HIS', 'HOH', 'CYS...   \n",
              "4  {'Fe': {'common_residues': ['HIS', 'HEM', 'CYS...   \n",
              "\n",
              "   corrosion_relevance_score corrosion_relevance  \n",
              "0                       38.0                high  \n",
              "1                       36.0                high  \n",
              "2                       21.5                high  \n",
              "3                       31.5                high  \n",
              "4                        1.5                 low  "
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ec_metadata.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCE1edxRX7Sf"
      },
      "source": [
        "### Saving the database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-16T21:31:36.807499Z",
          "iopub.status.busy": "2025-03-16T21:31:36.807047Z",
          "iopub.status.idle": "2025-03-16T21:32:01.699560Z",
          "shell.execute_reply": "2025-03-16T21:32:01.698194Z",
          "shell.execute_reply.started": "2025-03-16T21:31:36.807468Z"
        },
        "id": "1TknGS_gE1_h",
        "outputId": "60477b44-52b7-402e-fb7a-8f50fd6f641f",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting JSON save to /kaggle/working/output_large/ec_records.json...\n",
            "Successfully saved to /kaggle/working/output_large/ec_records.json in 24.88 seconds (494.54 MB)\n"
          ]
        }
      ],
      "source": [
        "# Making sure the output directory exists\n",
        "output_large = Path(\"/content/drive/MyDrive/MIC/output_large\")\n",
        "\n",
        "# Save to JSON with timing\n",
        "json_path = output_large / \"ec_records.json\"\n",
        "print(f\"Starting JSON save to {json_path}...\")\n",
        "start_time = time.time()\n",
        "\n",
        "try:\n",
        "    with open(json_path, 'w') as f:\n",
        "        json.dump(ec_records, f)\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed = end_time - start_time\n",
        "    size_mb = os.path.getsize(json_path) / 1024 / 1024\n",
        "\n",
        "    print(f\"Successfully saved to {json_path} in {elapsed:.2f} seconds ({size_mb:.2f} MB)\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving to JSON: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pm0Bj8j71rUw"
      },
      "source": [
        "Corrosion-Specific Filtering:\n",
        "I filter proteins that are involved in key corrosion mechanisms (electron transfer, biofilm formation, sulfate reduction, iron oxidation, etc.). I classify proteins into different MIC categories (e.g., redox proteins, sulfate reducers, oxidizers) and identify those that are common across all categories versus those enriched in specific groups.\n",
        "\n",
        "Statistical Testing:\n",
        "I build a table of protein–genus combinations with their classifications (including metabolism, pathway, metal interaction, and MIC function). I then perform statistical tests (Kruskal–Wallis with appropriate post-hoc tests and FDR corrections) to identify which combinations show significant differential abundance between the MIC categories. Only proteins with a significant difference will be carried forward for further analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3n7sW03WDVOP"
      },
      "source": [
        "ec_records: [{'ec_number': '1.1.1.1',\n",
        "  'enzyme_names': ['alcohol dehydrogenase',\n",
        "   'aldehyde reductase',\n",
        "   'ADH',\n",
        "   'alcohol dehydrogenase (NAD)',\n",
        "   'aliphatic alcohol dehydrogenase',\n",
        "   'ethanol dehydrogenase',\n",
        "   'NAD-dependent alcohol dehydrogenase',\n",
        "   'NAD-specific aromatic alcohol dehydrogenase',\n",
        "   'NADH-alcohol dehydrogenase',\n",
        "   'NADH-aldehyde dehydrogenase',\n",
        "   'primary alcohol dehydrogenase',\n",
        "   'yeast alcohol dehydrogenase'],"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-16T21:33:05.826429Z",
          "iopub.status.busy": "2025-03-16T21:33:05.826010Z",
          "iopub.status.idle": "2025-03-16T21:33:10.843238Z",
          "shell.execute_reply": "2025-03-16T21:33:10.842231Z",
          "shell.execute_reply.started": "2025-03-16T21:33:05.826400Z"
        },
        "id": "Sf9ZTMVIE1_i",
        "outputId": "aee4234a-22e3-4001-f036-db3ed2249388",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting JSON load from /kaggle/working/output_large/ec_records.json...\n",
            "Loaded 8235 records from JSON in 5.01 seconds\n"
          ]
        }
      ],
      "source": [
        "# To load ec_records: /kaggle/working/output_large/ec_records.parquet\n",
        "import json\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "# Load from JSON with timing\n",
        "json_path = Path(\"/kaggle/working/output_large/ec_records.json\")\n",
        "print(f\"Starting JSON load from {json_path}...\")\n",
        "start_time = time.time()\n",
        "\n",
        "with open(json_path, 'r') as f:\n",
        "    ec_records = json.load(f)\n",
        "\n",
        "end_time = time.time()\n",
        "elapsed = end_time - start_time\n",
        "print(f\"Loaded {len(ec_records)} records from JSON in {elapsed:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-16T21:33:19.762032Z",
          "iopub.status.busy": "2025-03-16T21:33:19.761605Z",
          "iopub.status.idle": "2025-03-16T21:33:24.030300Z",
          "shell.execute_reply": "2025-03-16T21:33:24.029138Z",
          "shell.execute_reply.started": "2025-03-16T21:33:19.762001Z"
        },
        "id": "spPAz5h286uL",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "ECcontri_Uniprot_path = output_large / 'ECcontri_Uniprot.tsv'\n",
        "ECcontri_Uniprot = pd.read_csv(ECcontri_Uniprot_path, sep='\\t')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zqpU2g8Xpkv"
      },
      "source": [
        "## 9.3.  Building Enriched Dataframe of ECcontri\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-16T22:17:43.416313Z",
          "iopub.status.busy": "2025-03-16T22:17:43.415841Z",
          "iopub.status.idle": "2025-03-16T22:17:43.445476Z",
          "shell.execute_reply": "2025-03-16T22:17:43.444223Z",
          "shell.execute_reply.started": "2025-03-16T22:17:43.416277Z"
        },
        "id": "--jwekk9SHE0",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def enrich_eccontri_data(eccontri_df, ec_records):\n",
        "    \"\"\"\n",
        "    Enrich the ECcontri_Uniprot dataframe with complete information from ec_records dictionary\n",
        "\n",
        "    Parameters:    eccontri_df : pandas DataFrame original ECcontri_Uniprot data with EC numbers in format EC:x.x.x.x\n",
        "                   ec_records : list of Dictionary where keys are EC numbers (without 'EC:' prefix) and values are metadata dictionaries\n",
        "\n",
        "    Returns:       enriched_df : pandas DataFrame with additional metadata columns\n",
        "    \"\"\"\n",
        "    # Make a copy to avoid modifying the original\n",
        "    enriched_df = eccontri_df.copy()\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Create dictionaries for faster lookups\n",
        "    print(\"Creating lookup dictionaries...\")\n",
        "\n",
        "    # EC number dictionary\n",
        "    ec_dict = {record['ec_number']: record for record in ec_records if 'ec_number' in record}\n",
        "    print(f\"Created EC dictionary with {len(ec_dict)} entries\")\n",
        "\n",
        "    # Protein name dictionary\n",
        "    protein_name_dict = {}\n",
        "    for record in ec_records:\n",
        "        enzyme_names = record.get('enzyme_names', [])\n",
        "        if isinstance(enzyme_names, list):\n",
        "            for name in enzyme_names:\n",
        "                if name:  # Skip empty names\n",
        "                    protein_name_dict[name.lower()] = record\n",
        "        elif enzyme_names:  # If it's a string and not empty\n",
        "            protein_name_dict[str(enzyme_names).lower()] = record\n",
        "    print(f\"Created protein name dictionary with {len(protein_name_dict)} entries\")\n",
        "\n",
        "    # Create a mapping dictionary to store all matches\n",
        "    idx_to_metadata = {}\n",
        "\n",
        "    # Add all metadata columns\n",
        "    metadata_columns = ['enzyme_names', 'enzyme_class', 'pathways', 'hierarchy',\n",
        "                        'metals_involved', 'metals_consolidated', 'corrosion_mechanisms',\n",
        "                        'corrosion_relevance_score', 'corrosion_relevance']\n",
        "\n",
        "    for col in metadata_columns:\n",
        "        enriched_df[col] = None\n",
        "\n",
        "    # Define metal and corrosion terms within the function\n",
        "    metal_terms = {\n",
        "        'iron': ['iron', 'iron reduction','fe', 'ferrous', 'ferric', 'heme', 'iron-sulfur', 'Fe2+', 'Fe3+', 'rust', 'ochre', 'iron oxide', 'iron precipitation', 'siderophore', 'ferritin'],\n",
        "        'sulfur': ['sulfate', 'sulfide', 'thiosulfate', 'S-S', 'sulfur', 'sulfite', 'sulfonate', 'cysteine', 'methionine'],\n",
        "        'hydrogen': ['hydrogen', 'hydrogenase', 'h2', 'hydrogen uptake', 'hydrogen evolution', 'proton reduction'],\n",
        "        'manganese': ['Mn2+', 'manganese', 'mn', 'manganous', 'manganic', 'manganese oxidation', 'metal oxide', 'metal oxide', 'manganese oxide', 'MnO2'],\n",
        "        'biofilm': ['exopolysaccharide', 'biofilm', 'adhesin', 'eps', 'polysaccharide', 'extracellular matrix', 'colonization', 'attachment', 'surface adherence'],\n",
        "        'copper': ['Cu+', 'Cu2+', 'copper', 'cupric', 'cuprous', 'copper oxide', 'copper corrosion'],\n",
        "        'nickel': ['Ni2+', 'nickel', 'nickelous', 'nickel oxidation', 'nickel reduction'],\n",
        "        'cobalt': ['Co2+', 'cobalt', 'cobaltous', 'cobalamin', 'vitamin B12'],\n",
        "        'calcium': ['Ca2+', 'calcium', 'calcium carbonate', 'calcite', 'calcium precipitation'],\n",
        "        'Mo': ['Mo', 'molybdenum', 'molybdopterin', 'molybdenum cofactor'],\n",
        "        'V5+': ['V5+', 'vanadium', 'vanadate', 'vanadyl'],\n",
        "        'Al3+': ['Al3+', 'aluminum', 'aluminate', 'aluminum oxide'],\n",
        "        'Cr3+': ['Cr3+', 'chromium', 'chromate', 'dichromate', 'chromium oxide'],\n",
        "        'zinc': ['Zn2+', 'zinc', 'zinc finger', 'zinc oxide'],\n",
        "        'sodium': ['Na+', 'sodium', 'NaCl', 'sodium transport', 'sodium gradient'],\n",
        "        'potassium': ['K+', 'potassium', 'KCl','potassium transport', 'potassium channel'],\n",
        "        'selenium': ['selenium', 'Se', 'selenocysteine', 'selenoprotein', 'selenite'],\n",
        "        'barium': ['Ba2+', 'barium', 'barium sulfate', 'barite'],\n",
        "        'phosphate': ['HPO4-2', 'PO4-3', 'phosphate', 'phosphates'],\n",
        "        'nitrate': ['NO3-', 'nitrate', 'nitrates'],\n",
        "        'nitrite': ['NO2-', 'nitrite', 'nitrites'],\n",
        "        'chloride': ['Cl-', 'chloride', 'chlorine'],\n",
        "        'magnesium': ['Mg2+', 'magnesium', 'magnesium oxide'],\n",
        "        'chlorine': ['Cl-', 'chloride', 'chlorine'],\n",
        "        }\n",
        "\n",
        "    # Corrosion mechanism classification\n",
        "    corrosion_mechanisms = {\n",
        "        'direct_eet': ['cytochrome', 'electron transfer', 'conductive pili', 'nanowire', 'mtrABC', 'omcS','oxidoreductase',  'redox', 'reductase', 'oxidase', 'electron conduit', 'direct electron transfer'],\n",
        "        'indirect_eet': ['shuttle', 'mediator', 'redox mediator', 'electron shuttle', 'flavin', 'quinone', 'humic substance'],\n",
        "        'acid_production': ['acid', 'acidification', 'fermentation', 'lactic acid', 'formic acid', 'acetic acid','oxalic acid', 'organic acid', 'acetate production', 'lactate metabolism', 'formate production', 'proton generation', 'low pH'],\n",
        "        'h2_consumption': ['hydrogenase', 'hydrogen uptake', 'hydrogen consumption', 'h2', 'H2 oxidation', 'H2ase', 'hydrogen metabolism'],\n",
        "        'o2_consumption': ['oxidase', 'oxygen reduction', 'aerobic respiration','oxygen reduc', 'aerobic respiration', 'oxygen consum', 'oxygen scavenging', 'oxygen stress', 'oxidative phosphorylation'],\n",
        "        'biofilm_formation': ['polysaccharide', 'adhesin', 'biofilm', 'EPS', 'extracellular polymeric substance', 'curli''exopolymer', 'extracellular matrix', 'adhesion', 'colonization', 'attachment', 'surface adherence', 'biofilm maturation'],\n",
        "        'sulfur_metabolism': ['sulfate reduc', 'sulfide', 'sulfite', 'thiosulfate', 'sulfur oxidation', 'SRB', 'sulfur disproportionation', 'sulfate-reducing bacteria', 'sulfur respiration'],\n",
        "        'metal_transformation': ['iron reduction', 'manganese oxidation', 'metal oxide', 'ochre formation', 'iron oxide deposits', 'iron precipitation', 'rust formation', 'metal deposition', 'metal solubilization', 'mineral dissolution', 'mineral precipitation'],\n",
        "        'iron_metabolism': ['iron reduc', 'ferric reduc', 'iron oxid', 'ferrous oxid', 'iron uptake', 'iron transport', 'iron storage', 'iron homeostasis', 'siderophore production'],\n",
        "        'metal_chelation': ['siderophore', 'metal binding', 'chelator', 'metallophore', 'iron complex', 'metal transport', 'chelation', 'metal complexation', 'metal sequestration'],\n",
        "        'carbon_metabolism': ['carbon fixation', 'carbon utilization', 'carbohydrate metabolism', 'glycolysis', 'TCA cycle', 'carbon flux', 'carbon assimilation'],\n",
        "        'ph_modulation': ['acid tolerance', 'alkaline tolerance', 'proton pump', 'pH homeostasis', 'pH stress', 'pH regulation', 'acid resistance']\n",
        "    }\n",
        "    # a boolean 'has_metal' column\n",
        "    enriched_df['has_metal'] = False\n",
        "\n",
        "     # Define progress reporting\n",
        "    total_rows = len(enriched_df)\n",
        "    log_interval = max(1, min(10000, total_rows // 20))  # Log at most 20 times, minimum every 10000 rows\n",
        "\n",
        "    print(f\"Processing {total_rows} rows with logging every {log_interval} rows\")\n",
        "\n",
        "    # try protein name matches\n",
        "    print(\"Performing protein name matches...\")\n",
        "    # Get rows without EC+Genus matches\n",
        "    remaining_indices = set(enriched_df.index) - set(idx_to_metadata.keys())\n",
        "    mask_remaining = enriched_df.index.isin(remaining_indices)\n",
        "    mask_valid_protein = enriched_df['protein_name'].notna() & (enriched_df['protein_name'] != \"Uncharacterized protein\")\n",
        "    mask_protein_match = mask_remaining & mask_valid_protein\n",
        "\n",
        "    # This part still needs row-by-row processing for fuzzy matching\n",
        "    protein_matches = 0\n",
        "    for idx in enriched_df.index[mask_protein_match]:\n",
        "        protein_name = enriched_df.loc[idx, 'protein_name'].lower()\n",
        "\n",
        "        # Direct lookup in protein name dictionary\n",
        "        if protein_name in protein_name_dict:\n",
        "            idx_to_metadata[idx] = protein_name_dict[protein_name]\n",
        "            protein_matches += 1\n",
        "        else:\n",
        "            # Try partial matches\n",
        "            for name, record in protein_name_dict.items():\n",
        "                if name in protein_name or protein_name in name:\n",
        "                    idx_to_metadata[idx] = record\n",
        "                    protein_matches += 1\n",
        "                    break\n",
        "\n",
        "    print(f\"Found {protein_matches} protein name matches\")\n",
        "\n",
        "    # For any remaining rows, try EC-only matching\n",
        "    print(\"Performing EC-only matches...\")\n",
        "    # Get rows without matches so far\n",
        "    remaining_indices = set(enriched_df.index) - set(idx_to_metadata.keys())\n",
        "    mask_remaining = enriched_df.index.isin(remaining_indices)\n",
        "    mask_valid_ec = enriched_df['EC'].notna()\n",
        "    mask_ec_match = mask_remaining & mask_valid_ec\n",
        "\n",
        "    ec_only_matches = 0\n",
        "    for idx in enriched_df.index[mask_ec_match]:\n",
        "        ec_num = enriched_df.loc[idx, 'EC']\n",
        "        if ec_num in ec_dict:\n",
        "            idx_to_metadata[idx] = ec_dict[ec_num]\n",
        "            ec_only_matches += 1\n",
        "\n",
        "    print(f\"Found {ec_only_matches} EC-only matches\")\n",
        "\n",
        "    # Apply all metadata in one go based on the matches we found\n",
        "    print(\"Applying metadata to matched rows...\")\n",
        "    for idx, metadata in idx_to_metadata.items():\n",
        "\n",
        "        # Only proceed if we have metadata (either from EC or from protein/enzyme name)\n",
        "        if metadata is not None:\n",
        "\n",
        "            # Add basic metadata\n",
        "            if 'enzyme_names' in metadata and metadata['enzyme_names']:\n",
        "                if isinstance(metadata['enzyme_names'], list):\n",
        "                    enriched_df.at[idx, 'enzyme_names'] = '; '.join(metadata['enzyme_names'])\n",
        "                else:\n",
        "                    enriched_df.at[idx, 'enzyme_names'] = str(metadata['enzyme_names'])\n",
        "\n",
        "            if 'enzyme_class' in metadata and metadata['enzyme_class']:\n",
        "                enriched_df.at[idx, 'enzyme_class'] = metadata['enzyme_class']\n",
        "\n",
        "            if 'pathways' in metadata and metadata['pathways']:\n",
        "                if isinstance(metadata['pathways'], list):\n",
        "                    enriched_df.at[idx, 'pathways'] = '; '.join(metadata['pathways'])\n",
        "                else:\n",
        "                    enriched_df.at[idx, 'pathways'] = str(metadata['pathways'])\n",
        "\n",
        "            if 'hierarchy' in metadata and metadata['hierarchy']:\n",
        "                if isinstance(metadata['hierarchy'], list):\n",
        "                    enriched_df.at[idx, 'hierarchy'] = '; '.join(metadata['hierarchy'])\n",
        "                else:\n",
        "                    enriched_df.at[idx, 'hierarchy'] = str(metadata['hierarchy'])\n",
        "\n",
        "            # add for the consolidated metals field:\n",
        "            if 'metals_consolidated' in metadata and metadata['metals_consolidated']:\n",
        "                if isinstance(metadata['metals_consolidated'], list):\n",
        "                    enriched_df.at[idx, 'metals_consolidated'] = '; '.join(metadata['metals_consolidated'])\n",
        "                else:\n",
        "                    enriched_df.at[idx, 'metals_consolidated'] = str(metadata['metals_consolidated'])\n",
        "\n",
        "            # add corrosion_relevance category:\n",
        "            if 'corrosion_relevance' in metadata:\n",
        "                enriched_df.at[idx, 'corrosion_relevance'] = metadata['corrosion_relevance']\n",
        "\n",
        "                # Directly use the metadata from ec_records\n",
        "            if 'metals_involved' in metadata and metadata['metals_involved']:\n",
        "                if isinstance(metadata['metals_involved'], list):\n",
        "                    enriched_df.at[idx, 'metals_involved'] = '; '.join(metadata['metals_involved'])\n",
        "                    enriched_df.at[idx, 'has_metal'] = len(metadata['metals_involved']) > 0\n",
        "                else:\n",
        "                    enriched_df.at[idx, 'metals_involved'] = str(metadata['metals_involved'])\n",
        "                    enriched_df.at[idx, 'has_metal'] = bool(metadata['metals_involved'])\n",
        "\n",
        "            if 'corrosion_mechanisms' in metadata and metadata['corrosion_mechanisms']:\n",
        "                if isinstance(metadata['corrosion_mechanisms'], list):\n",
        "                    enriched_df.at[idx, 'corrosion_mechanisms'] = '; '.join(metadata['corrosion_mechanisms'])\n",
        "                else:\n",
        "                    enriched_df.at[idx, 'corrosion_mechanisms'] = str(metadata['corrosion_mechanisms'])\n",
        "\n",
        "            if 'corrosion_relevance_score' in metadata:\n",
        "                try:\n",
        "                    # Ensure score is converted to float\n",
        "                    enriched_df.at[idx, 'corrosion_relevance_score'] = float(metadata['corrosion_relevance_score'])\n",
        "                except (ValueError, TypeError):\n",
        "                    print(f\"Row {idx}: Could not convert score {metadata['corrosion_relevance_score']} to float\")\n",
        "                    enriched_df.at[idx, 'corrosion_relevance_score'] = None\n",
        "\n",
        "    # Final report\n",
        "    end_time = time.time()\n",
        "    total_time = end_time - start_time\n",
        "    print(f\"Completed enrichment in {total_time:.2f} seconds\")\n",
        "    print(f\"Processed {total_rows} rows at {total_rows/total_time:.1f} rows/second\")\n",
        "\n",
        "    # Count non-null values in the metadata columns to see success rate\n",
        "    metadata_counts = {col: enriched_df[col].notnull().sum() for col in metadata_columns}\n",
        "    print(\"\\nMetadata population statistics:\")\n",
        "    for col, count in metadata_counts.items():\n",
        "        print(f\"  {col}: {count} rows ({count/total_rows*100:.1f}%)\")\n",
        "\n",
        "    return enriched_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-16T22:17:52.004087Z",
          "iopub.status.busy": "2025-03-16T22:17:52.003647Z",
          "iopub.status.idle": "2025-03-16T22:35:42.403935Z",
          "shell.execute_reply": "2025-03-16T22:35:42.402457Z",
          "shell.execute_reply.started": "2025-03-16T22:17:52.004052Z"
        },
        "id": "E0B3BgFljnTw",
        "outputId": "2863f5e6-2847-4dd7-e260-29993079477a",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating lookup dictionaries...\n",
            "Created EC dictionary with 8235 entries\n",
            "Created protein name dictionary with 28716 entries\n",
            "Processing 1491288 rows with logging every 10000 rows\n",
            "Performing protein name matches...\n",
            "Found 1216199 protein name matches\n",
            "Performing EC-only matches...\n",
            "Found 275089 EC-only matches\n",
            "Applying metadata to matched rows...\n",
            "Completed enrichment in 1069.25 seconds\n",
            "Processed 1491288 rows at 1394.7 rows/second\n",
            "\n",
            "Metadata population statistics:\n",
            "  enzyme_names: 1491288 rows (100.0%)\n",
            "  enzyme_class: 1488359 rows (99.8%)\n",
            "  pathways: 1425852 rows (95.6%)\n",
            "  hierarchy: 1404044 rows (94.1%)\n",
            "  metals_involved: 1490672 rows (100.0%)\n",
            "  metals_consolidated: 1490672 rows (100.0%)\n",
            "  corrosion_mechanisms: 1412607 rows (94.7%)\n",
            "  corrosion_relevance_score: 1491288 rows (100.0%)\n",
            "  corrosion_relevance: 1491288 rows (100.0%)\n"
          ]
        }
      ],
      "source": [
        "# Enrich the data directly from EC records dictionary > 3-7 minutes\n",
        "ECcontri_Uniprot_enriched= enrich_eccontri_data(ECcontri_Uniprot, ec_records)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-16T22:36:24.055798Z",
          "iopub.status.busy": "2025-03-16T22:36:24.055429Z",
          "iopub.status.idle": "2025-03-16T22:36:24.065229Z",
          "shell.execute_reply": "2025-03-16T22:36:24.064045Z",
          "shell.execute_reply.started": "2025-03-16T22:36:24.055763Z"
        },
        "id": "JRsDCn-kbCye",
        "outputId": "1a77e09c-1ea9-44c8-870c-acf156729b88",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['Sites', 'Genus', 'abund_raw', 'rel_abund_raw', 'genome_EC_count',\n",
              "       'abund_contri', 'rel_abund_contri', 'norm_abund_contri', 'protein_name',\n",
              "       'uniprot_id', 'EC', 'enzyme_names', 'enzyme_class', 'pathways',\n",
              "       'hierarchy', 'metals_involved', 'metals_consolidated',\n",
              "       'corrosion_mechanisms', 'corrosion_relevance_score',\n",
              "       'corrosion_relevance', 'has_metal'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ECcontri_Uniprot_enriched.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-16T22:36:31.076791Z",
          "iopub.status.busy": "2025-03-16T22:36:31.076453Z",
          "iopub.status.idle": "2025-03-16T22:36:32.396140Z",
          "shell.execute_reply": "2025-03-16T22:36:32.394834Z",
          "shell.execute_reply.started": "2025-03-16T22:36:31.076765Z"
        },
        "id": "zOraHS9I8Adf",
        "outputId": "d74bc68f-a3bd-4ce7-a0cd-3b0910bd1a5b",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enzyme Names - Unique Values: 1930\n",
            "Enzyme Names - Top 5 Occurrences:\n",
            " enzyme_names\n",
            "anthocyanidin synthase; leucocyanidin oxygenase; leucocyanidin,2-oxoglutarate:oxygen oxidoreductase; ANS                                                                                                                                 121841\n",
            "glutaryl-7-aminocephalosporanic-acid acylase; 7beta-cephalosporanic acid acylase; cephalosporin C acylase; glutaryl-7-ACA acylase; CA; GCA; GA; cephalosporin acylase; glutaryl-7-aminocephalosporanic acid acylase; GL-7-ACA acylase     24160\n",
            "nitric-oxide synthase; NOS; nitric oxide synthetase; endothelium-derived relaxation factor-forming enzyme; endothelium-derived relaxing factor synthase; NO synthase; NADPH-diaphorase                                                    23215\n",
            "pentalenolactone F synthase; penD; pntD; ptlD                                                                                                                                                                                             18402\n",
            "rifampicin monooxygenase; RIF-O; ROX; RIFMO; rifampicin:NADH:oxygen oxidoreductase                                                                                                                                                        15205\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Protein Names - Unique Values: 4327\n",
            "Protein Names - Top 5 Occurrences:\n",
            " protein_name\n",
            "Uncharacterized protein                                                                                                                             272716\n",
            "Bifunctional protein FolD [Includes: Methylenetetrahydrofolate dehydrogenase ; Methenyltetrahydrofolate cyclohydrolase ]                              4617\n",
            "Coenzyme A biosynthesis bifunctional protein CoaBC [Includes: Phosphopantothenoylcysteine decarboxylase ; Phosphopantothenate--cysteine ligase ]      4326\n",
            "Bifunctional purine biosynthesis protein PurH [Includes: Phosphoribosylaminoimidazolecarboxamide formyltransferase ; IMP cyclohydrolase ]             4310\n",
            "Riboflavin biosynthesis protein [Includes: kinase ; FMN adenylyltransferase ]                                                                         4169\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Count occurrences of each value in both columns\n",
        "enzyme_counts = ECcontri_Uniprot_enriched['enzyme_names'].value_counts()\n",
        "protein_counts = ECcontri_Uniprot_enriched['protein_name'].value_counts()\n",
        "\n",
        "# Get the number of unique values in each column\n",
        "enzyme_unique_count = ECcontri_Uniprot_enriched['enzyme_names'].nunique()\n",
        "protein_unique_count = ECcontri_Uniprot_enriched['protein_name'].nunique()\n",
        "\n",
        "# Print the results\n",
        "print(\"Enzyme Names - Unique Values:\", enzyme_unique_count)\n",
        "print(\"Enzyme Names - Top 5 Occurrences:\\n\", enzyme_counts.head())\n",
        "print(\"\\nProtein Names - Unique Values:\", protein_unique_count)\n",
        "print(\"Protein Names - Top 5 Occurrences:\\n\", protein_counts.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrp1424qRNgT"
      },
      "source": [
        "Enzyme vs. Protein Naming Precision is reflected on this statistics. There are 1,788 unique enzyme names coming from the ec_record combined databases download in this study, in comparison to 4,327 unique protein names coming from the Api retrival from Uniprot. This suggests api retrieval can be more time consuming but rewarding.\n",
        "\n",
        "Distribution Pattern:\n",
        "\n",
        "The top 5 enzyme names all appear exactly 2,407 times each this suggests these enzymes might be universally present across the microorganisms. These top enzymes are primarily involved in basic cellular processes (DNA replication and tRNA charging), which makes sense as they're fundamental to all cellular life\n",
        "\n",
        "Uncharacterized Proteins:\n",
        "\n",
        "\"Uncharacterized protein\" are the name of the fields could no be retrieved and it is present as the most common protein name (272,716 occurrences)\n",
        "\n",
        "Bifunctional Enzymes:\n",
        "\n",
        "Four of the top five protein names are bifunctional enzymes (containing \"[Includes: ... ; ...]\"), which means they perform two distinct enzymatic functions, often in related metabolic pathways.\n",
        "\n",
        "Fundamental vs. Specialized Functions:\n",
        "\n",
        "Top enzymes are involved in fundamental cellular processes (translation, DNA replication). This suggests the dataset has good coverage of core metabolic functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-16T22:37:00.710596Z",
          "iopub.status.busy": "2025-03-16T22:37:00.710206Z",
          "iopub.status.idle": "2025-03-16T22:37:10.526669Z",
          "shell.execute_reply": "2025-03-16T22:37:10.525592Z",
          "shell.execute_reply.started": "2025-03-16T22:37:00.710564Z"
        },
        "id": "YylsorAEE1_k",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Save more efficiently\n",
        "#ECcontri_Uniprot_enriched.to_parquet(output_large / 'ECcontri_Uniprot_enriched.parquet')\n",
        "\n",
        "# Load\n",
        "pre_ECcontri_Uniprot_enriched = pd.read_parquet(output_large / 'ECcontri_Uniprot_enriched.parquet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59LtHY74VlDW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-03-16T21:17:47.401460Z",
          "iopub.status.idle": "2025-03-16T21:17:47.401797Z",
          "shell.execute_reply": "2025-03-16T21:17:47.401658Z"
        },
        "id": "wRAQqvT8E1_l",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# To load ec_records:\n",
        "with open(output_large / \"ec_records.json\", \"r\") as f:\n",
        "    ec_records = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556
        },
        "id": "SB5PTM9GKDsd",
        "outputId": "0b3208e2-4443-4a1d-a120-1c7a0b379b99"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"ec_metadata\",\n  \"rows\": 8235,\n  \"fields\": [\n    {\n      \"column\": \"ec_number\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8235,\n        \"samples\": [\n          \"1.2.7.5\",\n          \"3.4.23.50\",\n          \"5.3.2.2\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"enzyme_names\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"enzyme_class\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 73,\n        \"samples\": [\n          \"Acting on the CH-NH group of donors.\",\n          \"Intramolecular lyases.\",\n          \"Acting on reduced flavodoxin as donor.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pathways\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"hierarchy\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ko\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"reactions\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"compounds\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"modules\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"metals_from_brenda\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"corrosion_metals_from_brenda\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"metals_involved\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"metals_consolidated\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"corrosion_mechanisms\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"metal_binding_info\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"corrosion_relevance_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 9.613695451017927,\n        \"min\": 0.0,\n        \"max\": 40.5,\n        \"num_unique_values\": 77,\n        \"samples\": [\n          1.5,\n          17.0,\n          32.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"corrosion_relevance\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"high\",\n          \"low\",\n          \"medium\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "ec_metadata"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-dd3efb6b-9d8f-4ee2-a1b5-a582e6eb8215\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ec_number</th>\n",
              "      <th>enzyme_names</th>\n",
              "      <th>enzyme_class</th>\n",
              "      <th>pathways</th>\n",
              "      <th>hierarchy</th>\n",
              "      <th>ko</th>\n",
              "      <th>reactions</th>\n",
              "      <th>compounds</th>\n",
              "      <th>modules</th>\n",
              "      <th>metals_from_brenda</th>\n",
              "      <th>corrosion_metals_from_brenda</th>\n",
              "      <th>metals_involved</th>\n",
              "      <th>metals_consolidated</th>\n",
              "      <th>corrosion_mechanisms</th>\n",
              "      <th>metal_binding_info</th>\n",
              "      <th>corrosion_relevance_score</th>\n",
              "      <th>corrosion_relevance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.1.1.1</td>\n",
              "      <td>[alcohol dehydrogenase, aldehyde reductase, AD...</td>\n",
              "      <td>Acting on the CH-OH group of donors.</td>\n",
              "      <td>[Glycolysis / Gluconeogenesis, Fatty acid degr...</td>\n",
              "      <td>[Endocrine and metabolic disease &gt; Alcoholic l...</td>\n",
              "      <td>[K00001, K11440, K13951, K13952, K13953, K1395...</td>\n",
              "      <td>[{'id': 'R00002', 'equation': '16 ATP + 16 H2O...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[selenium, Mo, copper, zinc]</td>\n",
              "      <td>[copper]</td>\n",
              "      <td>[selenium, Mo, copper, zinc, iron, sulfur, hyd...</td>\n",
              "      <td>[Cu, Mn, biofilm, Cl-, Fe, NO2, S, NO3-, phosp...</td>\n",
              "      <td>[biofilm_formation, sulfur_metabolism, direct_...</td>\n",
              "      <td>{'Cu': {'common_residues': ['HIS', 'CYS', 'HOH...</td>\n",
              "      <td>38.0</td>\n",
              "      <td>high</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.1.1.2</td>\n",
              "      <td>[alcohol dehydrogenase, aldehyde reductase, NA...</td>\n",
              "      <td>Acting on the CH-OH group of donors.</td>\n",
              "      <td>[Glycolysis / Gluconeogenesis, Pentose and glu...</td>\n",
              "      <td>[Protein families: signaling and cellular proc...</td>\n",
              "      <td>[K00002, K13979]</td>\n",
              "      <td>[{'id': 'R00002', 'equation': '16 ATP + 16 H2O...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[iron, zinc, Mo]</td>\n",
              "      <td>[iron]</td>\n",
              "      <td>[iron, zinc, Mo, sulfur, hydrogen, manganese, ...</td>\n",
              "      <td>[Cu, Mn, biofilm, Cl-, Fe, NO2, S, NO3-, phosp...</td>\n",
              "      <td>[biofilm_formation, sulfur_metabolism, direct_...</td>\n",
              "      <td>{'Cu': {'common_residues': ['HIS', 'CYS', 'HOH...</td>\n",
              "      <td>36.0</td>\n",
              "      <td>high</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.1.1.3</td>\n",
              "      <td>[homoserine dehydrogenase, HSDH, HSD]</td>\n",
              "      <td>Acting on the CH-OH group of donors.</td>\n",
              "      <td>[Glycine, serine and threonine metabolism, Cys...</td>\n",
              "      <td>[Biosynthesis of other secondary metabolites &gt;...</td>\n",
              "      <td>[K00003]</td>\n",
              "      <td>[{'id': 'R00651', 'equation': 'O-acetyl-L-homo...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[iron, sulfur, hydrogen, cobalt, Mo, selenium,...</td>\n",
              "      <td>[phosphate, Fe, S, Co, Se, Mo, H]</td>\n",
              "      <td>[sulfur_metabolism, direct_eet, o2_consumption...</td>\n",
              "      <td>{'Fe': {'common_residues': ['HIS', 'HEM', 'CYS...</td>\n",
              "      <td>21.5</td>\n",
              "      <td>high</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.1.1.4</td>\n",
              "      <td>[-butanediol dehydrogenase, butyleneglycol deh...</td>\n",
              "      <td>Acting on the CH-OH group of donors.</td>\n",
              "      <td>[Butanoate metabolism, Biosynthesis of seconda...</td>\n",
              "      <td>[Carbohydrate metabolism &gt; Butanoate metabolism]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[{'id': 'R00002', 'equation': '16 ATP + 16 H2O...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[Mo]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[Mo, iron, sulfur, hydrogen, manganese, biofil...</td>\n",
              "      <td>[Cu, Mn, Cl-, Fe, NO2, S, NO3-, phosphate, Co,...</td>\n",
              "      <td>[biofilm_formation, sulfur_metabolism, direct_...</td>\n",
              "      <td>{'Cu': {'common_residues': ['HIS', 'CYS', 'HOH...</td>\n",
              "      <td>31.5</td>\n",
              "      <td>high</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.1.1.5</td>\n",
              "      <td>[Transferred to 1.1.1.303 and 1.1.1.304]</td>\n",
              "      <td>Acting on the CH-OH group of donors.</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[iron]</td>\n",
              "      <td>[Fe]</td>\n",
              "      <td>[]</td>\n",
              "      <td>{'Fe': {'common_residues': ['HIS', 'HEM', 'CYS...</td>\n",
              "      <td>1.5</td>\n",
              "      <td>low</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dd3efb6b-9d8f-4ee2-a1b5-a582e6eb8215')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-dd3efb6b-9d8f-4ee2-a1b5-a582e6eb8215 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-dd3efb6b-9d8f-4ee2-a1b5-a582e6eb8215');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-8fdbd9c5-35f4-40c1-a4ca-e2cbf2be379e\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8fdbd9c5-35f4-40c1-a4ca-e2cbf2be379e')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-8fdbd9c5-35f4-40c1-a4ca-e2cbf2be379e button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "  ec_number                                       enzyme_names  \\\n",
              "0   1.1.1.1  [alcohol dehydrogenase, aldehyde reductase, AD...   \n",
              "1   1.1.1.2  [alcohol dehydrogenase, aldehyde reductase, NA...   \n",
              "2   1.1.1.3              [homoserine dehydrogenase, HSDH, HSD]   \n",
              "3   1.1.1.4  [-butanediol dehydrogenase, butyleneglycol deh...   \n",
              "4   1.1.1.5           [Transferred to 1.1.1.303 and 1.1.1.304]   \n",
              "\n",
              "                           enzyme_class  \\\n",
              "0  Acting on the CH-OH group of donors.   \n",
              "1  Acting on the CH-OH group of donors.   \n",
              "2  Acting on the CH-OH group of donors.   \n",
              "3  Acting on the CH-OH group of donors.   \n",
              "4  Acting on the CH-OH group of donors.   \n",
              "\n",
              "                                            pathways  \\\n",
              "0  [Glycolysis / Gluconeogenesis, Fatty acid degr...   \n",
              "1  [Glycolysis / Gluconeogenesis, Pentose and glu...   \n",
              "2  [Glycine, serine and threonine metabolism, Cys...   \n",
              "3  [Butanoate metabolism, Biosynthesis of seconda...   \n",
              "4                                                 []   \n",
              "\n",
              "                                           hierarchy  \\\n",
              "0  [Endocrine and metabolic disease > Alcoholic l...   \n",
              "1  [Protein families: signaling and cellular proc...   \n",
              "2  [Biosynthesis of other secondary metabolites >...   \n",
              "3   [Carbohydrate metabolism > Butanoate metabolism]   \n",
              "4                                                 []   \n",
              "\n",
              "                                                  ko  \\\n",
              "0  [K00001, K11440, K13951, K13952, K13953, K1395...   \n",
              "1                                   [K00002, K13979]   \n",
              "2                                           [K00003]   \n",
              "3                                                 []   \n",
              "4                                                 []   \n",
              "\n",
              "                                           reactions compounds modules  \\\n",
              "0  [{'id': 'R00002', 'equation': '16 ATP + 16 H2O...        []      []   \n",
              "1  [{'id': 'R00002', 'equation': '16 ATP + 16 H2O...        []      []   \n",
              "2  [{'id': 'R00651', 'equation': 'O-acetyl-L-homo...        []      []   \n",
              "3  [{'id': 'R00002', 'equation': '16 ATP + 16 H2O...        []      []   \n",
              "4                                                 []        []      []   \n",
              "\n",
              "             metals_from_brenda corrosion_metals_from_brenda  \\\n",
              "0  [selenium, Mo, copper, zinc]                     [copper]   \n",
              "1              [iron, zinc, Mo]                       [iron]   \n",
              "2                            []                           []   \n",
              "3                          [Mo]                           []   \n",
              "4                            []                           []   \n",
              "\n",
              "                                     metals_involved  \\\n",
              "0  [selenium, Mo, copper, zinc, iron, sulfur, hyd...   \n",
              "1  [iron, zinc, Mo, sulfur, hydrogen, manganese, ...   \n",
              "2  [iron, sulfur, hydrogen, cobalt, Mo, selenium,...   \n",
              "3  [Mo, iron, sulfur, hydrogen, manganese, biofil...   \n",
              "4                                             [iron]   \n",
              "\n",
              "                                 metals_consolidated  \\\n",
              "0  [Cu, Mn, biofilm, Cl-, Fe, NO2, S, NO3-, phosp...   \n",
              "1  [Cu, Mn, biofilm, Cl-, Fe, NO2, S, NO3-, phosp...   \n",
              "2                  [phosphate, Fe, S, Co, Se, Mo, H]   \n",
              "3  [Cu, Mn, Cl-, Fe, NO2, S, NO3-, phosphate, Co,...   \n",
              "4                                               [Fe]   \n",
              "\n",
              "                                corrosion_mechanisms  \\\n",
              "0  [biofilm_formation, sulfur_metabolism, direct_...   \n",
              "1  [biofilm_formation, sulfur_metabolism, direct_...   \n",
              "2  [sulfur_metabolism, direct_eet, o2_consumption...   \n",
              "3  [biofilm_formation, sulfur_metabolism, direct_...   \n",
              "4                                                 []   \n",
              "\n",
              "                                  metal_binding_info  \\\n",
              "0  {'Cu': {'common_residues': ['HIS', 'CYS', 'HOH...   \n",
              "1  {'Cu': {'common_residues': ['HIS', 'CYS', 'HOH...   \n",
              "2  {'Fe': {'common_residues': ['HIS', 'HEM', 'CYS...   \n",
              "3  {'Cu': {'common_residues': ['HIS', 'CYS', 'HOH...   \n",
              "4  {'Fe': {'common_residues': ['HIS', 'HEM', 'CYS...   \n",
              "\n",
              "   corrosion_relevance_score corrosion_relevance  \n",
              "0                       38.0                high  \n",
              "1                       36.0                high  \n",
              "2                       21.5                high  \n",
              "3                       31.5                high  \n",
              "4                        1.5                 low  "
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ec_metadata = pd.DataFrame(ec_records)\n",
        "ec_metadata.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4hqE4YG4vvU"
      },
      "source": [
        "### Preparing the Enriched DF for entering the Pipeline\n",
        "Before the data enters the pipeline it was noticed that around 18% of the protein-names were Uncharacterized proteins that is because the retrieval from uniprot was no totally suscessful, however in the process of enriching the data, the enzyme name was added with many other entities and so, the already enriched data will have those entries known as Uncharacterized, replaced by the enzyme name that was retrieved from the many other db and compiled on ec_records"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3Cj1FnX5jNA",
        "outputId": "988e175d-4593-48ba-8393-f075ffb9e3c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 272716 'Uncharacterized protein' entries (18.29%)\n",
            "Warning: 47755 duplicate Site-Genus-Protein combinations found. Resolving...\n",
            "✓ All Site-Genus-Protein combinations are now unique\n"
          ]
        }
      ],
      "source": [
        "def correct_uncharacterized_proteins(df):\n",
        "    \"\"\"\n",
        "    Efficiently corrects 'Uncharacterized protein' entries using vectorized operations.\n",
        "\n",
        "    Parameters:\n",
        "        df (pd.DataFrame): ECcontri_Uniprot_enriched dataframe.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Corrected dataframe ready for the pipeline.\n",
        "    \"\"\"\n",
        "    # Create a shallow copy to avoid modifying the original\n",
        "    corrected_df = df.copy(deep=False)\n",
        "\n",
        "    # Identify uncharacterized proteins\n",
        "    unchar_mask = corrected_df['protein_name'] == 'Uncharacterized protein'\n",
        "    unchar_count = unchar_mask.sum()\n",
        "\n",
        "    print(f\"Found {unchar_count} 'Uncharacterized protein' entries ({unchar_count/len(corrected_df)*100:.2f}%)\")\n",
        "\n",
        "    # Replace uncharacterized proteins with enzyme names where possible\n",
        "    valid_enzyme_mask = corrected_df['enzyme_names'].notna() & (corrected_df['enzyme_names'] != '')\n",
        "    corrected_df.loc[unchar_mask & valid_enzyme_mask, 'protein_name'] = corrected_df.loc[unchar_mask & valid_enzyme_mask, 'enzyme_names']\n",
        "\n",
        "    # Recalculate remaining uncharacterized proteins\n",
        "    remaining_unchar_mask = corrected_df['protein_name'] == 'Uncharacterized protein'\n",
        "    remaining_count = remaining_unchar_mask.sum()\n",
        "\n",
        "    if remaining_count > 0:\n",
        "        print(f\"Adding unique identifiers to {remaining_count} remaining uncharacterized proteins\")\n",
        "\n",
        "        # Generate unique IDs for remaining uncharacterized proteins\n",
        "        corrected_df.loc[remaining_unchar_mask, 'protein_name'] = (\n",
        "            'Uncharacterized_protein_' +\n",
        "            corrected_df.loc[remaining_unchar_mask, 'Sites'].astype(str) + '_' +\n",
        "            corrected_df.loc[remaining_unchar_mask, 'Genus'].astype(str) + '_' +\n",
        "            corrected_df.groupby(['Sites', 'Genus']).cumcount()[remaining_unchar_mask].astype(str)\n",
        "        )\n",
        "\n",
        "    # Ensure uniqueness of Site-Genus-Protein combinations\n",
        "    duplicate_counts = corrected_df.groupby(['Sites', 'Genus', 'protein_name']).size()\n",
        "    duplicates = duplicate_counts[duplicate_counts > 1]\n",
        "\n",
        "    if len(duplicates) > 0:\n",
        "        print(f\"Warning: {len(duplicates)} duplicate Site-Genus-Protein combinations found. Resolving...\")\n",
        "\n",
        "        # Append unique suffix using the index position\n",
        "        corrected_df['protein_name'] = corrected_df['protein_name'] + '___' + corrected_df.groupby(['Sites', 'Genus', 'protein_name']).cumcount().astype(str)\n",
        "\n",
        "    print(\"✓ All Site-Genus-Protein combinations are now unique\")\n",
        "\n",
        "    return corrected_df\n",
        "\n",
        "# Apply correction\n",
        "ECcontri_Uniprot_enriched = correct_uncharacterized_proteins(pre_ECcontri_Uniprot_enriched)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-16T22:39:29.455798Z",
          "iopub.status.busy": "2025-03-16T22:39:29.455346Z",
          "iopub.status.idle": "2025-03-16T22:39:29.788695Z",
          "shell.execute_reply": "2025-03-16T22:39:29.787386Z",
          "shell.execute_reply.started": "2025-03-16T22:39:29.455758Z"
        },
        "id": "jAoBN5JDw5AN",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Define category dict outside\n",
        "category_dict = Integrated_T.T.iloc[0, 0:-1].to_dict()\n",
        "\n",
        "# Define colors and categories\n",
        "category_colors = {1: '#008800',  # Dark green\n",
        "                   2: '#FF8C00',  # Dark orange\n",
        "                   3: '#FF0000'}   # Red\n",
        "\n",
        "categories_labels = {1: 'Normal Operation',\n",
        "              2: 'Early Warning',\n",
        "              3: 'System Failure'}\n",
        "# Add Category on eccontry\n",
        "ECcontri_Uniprot_enriched.reset_index(inplace=True)\n",
        "ECcontri_Uniprot_enriched['Category'] = ECcontri_Uniprot_enriched['Sites'].map(category_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BVlakeTzQJKu"
      },
      "outputs": [],
      "source": [
        "# Saving the new corrected dataframe\n",
        "ECcontri_Uniprot_enriched.to_parquet(output_large / 'ECcontri_Uniprot_enriched.parquet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dq8C9vSywZjZ"
      },
      "source": [
        "## 9.5. Filtering pairs Bacteria-Protein by significance to the risk category\n",
        "\n",
        "The analyze_corrosion_proteins function establishes a systematic framework for identifying protein-genus pairs associated with corrosion across different risk categories. The function begins by preparing the data, converting enzyme records to a searchable dictionary and mapping sites to risk categories, then proceeds to track which sites contain each protein-genus combination to maintain traceability throughout the analysis.\n",
        "After the initial preparation, the function orchestrates a series of analytical steps through specialized helper functions that reshape the data, perform statistical tests across risk categories, integrate biological metadata, prioritize markers based on combined statistical and biological significance, and finally organize results into specialized groups for interpretation. This sequential approach ensures a comprehensive assessment that considers both statistical significance and biological relevance, ultimately producing a multifaceted view of protein-genus pairs that may contribute to microbial corrosion processes under varying environmental conditions.\n",
        "\n",
        "A separation of the significative inverse relationship is done and elsewhere analysed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-16T22:39:56.738928Z",
          "iopub.status.busy": "2025-03-16T22:39:56.738494Z",
          "iopub.status.idle": "2025-03-16T22:39:56.751721Z",
          "shell.execute_reply": "2025-03-16T22:39:56.750288Z",
          "shell.execute_reply.started": "2025-03-16T22:39:56.738881Z"
        },
        "id": "aoMYUGIDoMc-",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def analyze_corrosion_proteins(eccontri_df, ec_records, alpha=0.05, balance_genera=False, per_genus_count=10):\n",
        "    \"\"\"\n",
        "    Comprehensive analysis of protein-genus pairs for corrosion relevance\n",
        "\n",
        "    Parameters:\n",
        "    eccontri_df : pandas DataFrame The enriched ECcontri_Uniprot dataframe with EC, protein_name, Genus, abundance, etc.\n",
        "    ec_records : List of dictionaries with EC, protein_name metadata with corrosion relevance information\n",
        "    site_categories : Dictionary mapping Sites IDs to risk categories (e.g., {'site1': '1', 'site2': '2'})\n",
        "    alpha : float,  Significance level for statistical tests (default: 0.05)\n",
        "    balance_genera : bool, Whether to balance representation across genera (default: False)\n",
        "    per_genus_count : intNumber of markers to include per genus if balancing (default: 10)\n",
        "\n",
        "    Returns:\n",
        "     results :  Dictionary containing various analysis results\n",
        "    \"\"\"\n",
        "    print(\"Starting corrosion protein analysis...\")\n",
        "\n",
        "    # Convert ec_records list to dictionary for faster lookup\n",
        "    ec_dict = {}\n",
        "    for record in ec_records:\n",
        "        if 'ec_number' in record:\n",
        "            ec_dict[record['ec_number']] = record\n",
        "\n",
        "    # Use existing category mapping unconditionally to ensure it exists\n",
        "    category_dict = Integrated_T.T.iloc[0, 0:-1].astype(int).to_dict()\n",
        "\n",
        "    # Use existing category map\n",
        "    if 'Category' not in eccontri_df.columns:\n",
        "        eccontri_df['Category'] = eccontri_df['Sites'].map(category_dict)\n",
        "\n",
        "    print(f\"Analyzing {len(eccontri_df)} data points across {len(eccontri_df['Sites'].unique())} Sites...\")\n",
        "\n",
        "    # Create mapping of protein-genus pairs to their sites\n",
        "    protein_genus_sites = {}\n",
        "    site_groups = eccontri_df.groupby(['Genus', 'protein_name'])['Sites'].apply(list)\n",
        "    for (genus, protein), sites in site_groups.items():\n",
        "        protein_genus_sites[(genus, protein)] = list(set(sites))\n",
        "\n",
        "    # Reshape data for analysis\n",
        "    print(\"Preparing data for pattern analysis...\")\n",
        "    pattern_df = prepare_data_for_analysis(eccontri_df)\n",
        "\n",
        "    # create base matrix\n",
        "    base_matrix = create_matrix(eccontri_df)\n",
        "\n",
        "    # Ensuring sites are properly ordered numerically. Get current index and sort it\n",
        "    current_index = base_matrix.index\n",
        "    sorted_index = sorted(current_index, key=lambda x: int(x.split('_')[1]))\n",
        "    base_matrix = base_matrix.reindex(sorted_index)\n",
        "\n",
        "    # Now add the category mapping\n",
        "    if category_dict is not None:\n",
        "        category_mapping = pd.Series(base_matrix.index.map(category_dict),\n",
        "                                  index=base_matrix.index,\n",
        "                                  name='Category')\n",
        "        base_matrix.index = pd.MultiIndex.from_arrays([base_matrix.index, category_mapping],\n",
        "                                                    names=['Sites', 'Category'])\n",
        "\n",
        "    # Perform statistical testing with multiple testing correction\n",
        "    print(\"Performing statistical tests...\")\n",
        "    stat_results = perform_statistical_tests(base_matrix, alpha)\n",
        "\n",
        "    # Integrate with corrosion metadata\n",
        "    print(\"Integrating with corrosion metadata...\")\n",
        "    integrated_results = integrate_with_metadata(pattern_df, stat_results, ec_dict)\n",
        "\n",
        "    # Clasify ubiquitous, niche genus-protein names by mechanism, pathways\n",
        "    classified_results = classify_pathways_by_specificity(integrated_results)\n",
        "\n",
        "    # Separate positive and inverse patterns\n",
        "    print(\"Separating positive and inverse patterns by corr\")\n",
        "    increasing_results, inverse_results, constant_df = separate_by_correlation(classified_results)\n",
        "\n",
        "    # Prioritize positive results only\n",
        "    print(\"Prioritizing markers based on statistical and biological relevance...\")\n",
        "    prioritized_markers = prioritize_markers(increasing_results)\n",
        "\n",
        "    # add genus balancing step in order to filter top 10 protein per genus\n",
        "    if balance_genera:\n",
        "        print(f\"Balancing genus representation (top {per_genus_count} per genus)...\")\n",
        "        balanced_markers = balance_genus_representation(prioritized_markers, per_genus_count)\n",
        "        print(f\"Created balanced dataset with {len(balanced_markers)} markers from {len(prioritized_markers['Genus'].unique())} genera\")\n",
        "\n",
        "        # Create marker groups from balanced markers\n",
        "        print(\"Creating specialized marker groups from balanced markers...\")\n",
        "        marker_groups = create_marker_groups(balanced_markers)\n",
        "    else:\n",
        "        # Create marker groups from prioritized markers (original behavior)\n",
        "        print(\"Creating specialized marker groups...\")\n",
        "        marker_groups = create_marker_groups(prioritized_markers)\n",
        "\n",
        "    # Create additional marker groups based on patterns and mechanism\n",
        "    print(\"Creating specialized marker groups...\")\n",
        "    marker_groups = create_marker_groups(prioritized_markers)\n",
        "\n",
        "    print(\"Analysis complete!\")\n",
        "\n",
        "    return {\n",
        "        'pattern_data': pattern_df,\n",
        "        'statistical_results': stat_results,\n",
        "        'integrated_results': integrated_results,\n",
        "        'classified_results': classified_results,\n",
        "        'increasing_markers': increasing_results,\n",
        "        'prioritized_markers': prioritized_markers,\n",
        "        'marker_groups': marker_groups,\n",
        "        'inverse_markers': inverse_results\n",
        "    }\n",
        "        # Add balanced markers if they were created\n",
        "    if balance_genera:\n",
        "        results_dict['balanced_markers'] = balanced_markers\n",
        "\n",
        "    return results_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDxX599EwamS"
      },
      "source": [
        "### Determining Abundance Patters\n",
        "The determine_abundance_pattern function implements a sophisticated classification system for protein-genus abundance profiles across risk categories, prioritizing patterns with potential corrosion relevance.\n",
        "\n",
        "This classification focuses on corrosion-relevant trends by highlighting proteins that proliferate in higher risk environments, those that spike during transitional conditions (category 2), and those exclusively abundant in severe corrosion conditions (category 3), while deprioritizing proteins with mixed patterns or those predominantly found in normal conditions (category 1).\n",
        "\n",
        "The algorithm identifies distinct patterns\n",
        "with 2 transitions between 3 categories, we have 3² = 9 possible patterns:\n",
        "\n",
        "1. cat1 < cat2 < cat3 (steadily increasing)  # Increasing  \n",
        "2. cat1 < cat2 = cat3 (increases then plateaus)  # Increases  \n",
        "3. cat1 < cat2 > cat3 (peaks at cat2) #Increases  to be decided by biology\n",
        "4. cat1 = cat2 < cat3 (plateaus then increases) # Increases    \n",
        "5. cat1 = cat2 = cat3 (consistent across all)  # Consistent to be droped  \n",
        "6. cat1 = cat2 > cat3 (plateaus then decreases)  # ambigous to be droped  \n",
        "7. cat1 > cat2 < cat3 (valley at cat2)  # mixed to be decided by biology\n",
        "8. cat1 > cat2 = cat3 (decreases then plateaus)  # Inverse df  \n",
        "9. cat1 > cat2 > cat3 (steadily decreasing)  # Inverse df  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "execution": {
          "iopub.execute_input": "2025-03-16T22:39:56.801369Z",
          "iopub.status.busy": "2025-03-16T22:39:56.801039Z",
          "iopub.status.idle": "2025-03-16T22:39:56.818615Z",
          "shell.execute_reply": "2025-03-16T22:39:56.817462Z",
          "shell.execute_reply.started": "2025-03-16T22:39:56.801323Z"
        },
        "id": "K1CbJl7OiFn4",
        "outputId": "156e93c4-2c68-4607-d912-e8b7819581e2",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ncat1 < cat2 < cat3 (increasing) # increasing\\ncat1 < cat2 = cat3 (increases then plateaus) v# increasing\\ncat1 < cat2 > cat3 but cat1 < cat3 (peak at cat2, but still higher at end) # increasing\\ncat1 < cat2 > cat3 and cat1 = cat3 (perfect peak at cat2) # increasing\\ncat1 < cat2 > cat3 and cat1 > cat3 (peak at cat2, ends lower) drop\\ncat1 = cat2 < cat3 (plateaus then increases) # increasing\\ncat1 = cat2 = cat3 (consistent across all) drop\\ncat1 = cat2 > cat3 (plateaus then decreases) drop\\ncat1 > cat2 > cat3 (decreasing) # decrising\\ncat1 > cat2 = cat3 (decreases then plateaus) # decrising\\ncat1 > cat2 < cat3 but cat1 > cat3 (valley at cat2, but still lower at end) #drop\\ncat1 > cat2 < cat3 and cat1 = cat3 (perfect valley at cat2) #drop\\ncat1 > cat2 < cat3 and cat1 < cat3 (valley at cat2, ends higher) # increasing'"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "cat1 < cat2 < cat3 (increasing) # increasing\n",
        "cat1 < cat2 = cat3 (increases then plateaus) v# increasing\n",
        "cat1 < cat2 > cat3 but cat1 < cat3 (peak at cat2, but still higher at end) # increasing\n",
        "cat1 < cat2 > cat3 and cat1 = cat3 (perfect peak at cat2) # increasing\n",
        "cat1 < cat2 > cat3 and cat1 > cat3 (peak at cat2, ends lower) drop\n",
        "cat1 = cat2 < cat3 (plateaus then increases) # increasing\n",
        "cat1 = cat2 = cat3 (consistent across all) drop\n",
        "cat1 = cat2 > cat3 (plateaus then decreases) drop\n",
        "cat1 > cat2 > cat3 (decreasing) # decrising\n",
        "cat1 > cat2 = cat3 (decreases then plateaus) # decrising\n",
        "cat1 > cat2 < cat3 but cat1 > cat3 (valley at cat2, but still lower at end) #drop\n",
        "cat1 > cat2 < cat3 and cat1 = cat3 (perfect valley at cat2) #drop\n",
        "cat1 > cat2 < cat3 and cat1 < cat3 (valley at cat2, ends higher) # increasing'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-16T22:39:56.841622Z",
          "iopub.status.busy": "2025-03-16T22:39:56.841272Z",
          "iopub.status.idle": "2025-03-16T22:39:56.855283Z",
          "shell.execute_reply": "2025-03-16T22:39:56.854040Z",
          "shell.execute_reply.started": "2025-03-16T22:39:56.841595Z"
        },
        "id": "vGydbtkdwb8a",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def determine_abundance_pattern(row, categories, tolerance=0.05):\n",
        "    \"\"\"Determine abundance pattern with correct handling of NaN values and special cases\"\"\"\n",
        "    # Extract mean values for each category\n",
        "    values = []\n",
        "    present_categories = []\n",
        "    for cat in categories:\n",
        "        col = f'mean_{cat}'\n",
        "        if col in row and pd.notna(row[col]) and row[col] > 0:\n",
        "            values.append(row[col])\n",
        "            present_categories.append(cat)\n",
        "        else:\n",
        "            values.append(0)  # Treat NaN as zero for pattern determination\n",
        "\n",
        "    # Special cases for proteins present only in cat 1 and cat 3\n",
        "    if len(present_categories) == 1:\n",
        "        if present_categories[0] == categories[-1]:\n",
        "            return \"only_in_cat3\"  # Positive pattern\n",
        "        elif present_categories[0] == categories[0]:\n",
        "            return \"only_in_cat1\"  # Inverse pattern\n",
        "        else:\n",
        "            return \"only_in_cat2\"  # To be decided based on biology\n",
        "\n",
        "    if len(present_categories) == 0:\n",
        "        return \"insufficient_data\"  # No data in any category\n",
        "\n",
        "    # Check for consistent pattern (to be dropped)\n",
        "    if max(values) < min([v for v in values if v > 0]) * 1.3:  # All values within 30% of each other\n",
        "        return \"consistent_across_all\"\n",
        "\n",
        "    # Calculate correlation with proper handling of zeros\n",
        "    non_zero_indices = [i for i, v in enumerate(values) if v > 0]\n",
        "    non_zero_values = [values[i] for i in non_zero_indices]\n",
        "    non_zero_cats = [categories[i] for i in non_zero_indices]\n",
        "\n",
        "    if len(non_zero_indices) >= 2:\n",
        "        corr = np.corrcoef(non_zero_cats, non_zero_values)[0, 1]\n",
        "    else:\n",
        "        corr = 0\n",
        "\n",
        "    # Strong correlation-based patterns\n",
        "    if corr > 0.7:  # Strong positive correlation\n",
        "        return \"increasing\"\n",
        "    elif corr < -0.7:  # Strong negative correlation\n",
        "        return \"decreasing\"\n",
        "\n",
        "    # For proteins present in cat2 and cat3 but not cat1\n",
        "    if 1 not in present_categories and categories[-1] in present_categories:\n",
        "        # If abundance increases from cat2 to cat3\n",
        "        if values[categories.index(categories[-1])] > values[categories.index(present_categories[0])] * 1.2:\n",
        "            return \"increasing\"\n",
        "        # If abundance decreases from cat2 to cat3\n",
        "        elif values[categories.index(categories[-1])] < values[categories.index(present_categories[0])] * 0.8:\n",
        "            return \"decreasing\"\n",
        "\n",
        "    # Clear increasing patterns\n",
        "    if categories[0] in present_categories and categories[-1] in present_categories:\n",
        "        if values[categories.index(categories[-1])] > values[categories.index(categories[0])] * 1.2:\n",
        "            return \"increasing\"\n",
        "\n",
        "    # Clear decreasing patterns\n",
        "    if categories[0] in present_categories and categories[-1] in present_categories:\n",
        "        if values[categories.index(categories[0])] > values[categories.index(categories[-1])] * 1.2:\n",
        "            return \"decreasing\"\n",
        "\n",
        "    # Ambiguous patterns to be dropped\n",
        "    if len(categories) >= 3 and all(cat in present_categories for cat in categories):\n",
        "        # Plateaus then decreases\n",
        "        if values[0] == values[1] and values[1] > values[2]:\n",
        "            return \"ambiguous\"\n",
        "        # Decreases then plateaus\n",
        "        elif values[0] > values[1] and values[1] == values[2]:\n",
        "            return \"ambiguous\"\n",
        "\n",
        "    # Everything else\n",
        "    return \"mixed\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-16T22:39:56.884148Z",
          "iopub.status.busy": "2025-03-16T22:39:56.883733Z",
          "iopub.status.idle": "2025-03-16T22:39:56.890150Z",
          "shell.execute_reply": "2025-03-16T22:39:56.888943Z",
          "shell.execute_reply.started": "2025-03-16T22:39:56.884114Z"
        },
        "id": "MfCv0dbam-Ya",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def safe_log2fc(value1, value2):\n",
        "    \"\"\"Calculate log2 fold change with pseudocount to avoid division by zero\"\"\"\n",
        "    # If both values are zero or NaN, fold change is zero\n",
        "    if (pd.isna(value1) and pd.isna(value2)) or (value1 == 0 and value2 == 0):\n",
        "        return 0\n",
        "\n",
        "    # If either value is NaN, treat it as zero\n",
        "    value1 = 0 if pd.isna(value1) else value1\n",
        "    value2 = 0 if pd.isna(value2) else value2\n",
        "\n",
        "    # Add pseudocount\n",
        "    pseudo = max(0.01, min(value1, value2) * 0.1) if value1 > 0 and value2 > 0 else 0.01\n",
        "\n",
        "    return np.log2((value1 + pseudo) / (value2 + pseudo))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vK1j45XyoIpE"
      },
      "source": [
        "### Preparing the data for analysis\n",
        "The prepare_data_for_analysis function transforms the long-format data into a wide format by pivoting on Genus and protein_name pairs across risk categories. This restructuring calculates summary statistics for normalized abundance contributions and organizes them by category, creating a tabular view that shows how each protein-genus combination behaves across different risk environments. After pivoting, the function enriches the data with pattern classifications, fold-change calculations, and relevant biological context by efficiently mapping metadata to each protein-genus pair, enabling identification of proteins that may play significant roles in corrosion processes while maintaining site tracking for geographic reference.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "zOFK-jjXOo11",
        "outputId": "643cdaa8-60d4-4220-e05d-7695d089df72"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"# Create a random sample of 1000 rows (or any number you prefer)\\nsample_eccontri = ECcontri_Uniprot_enriched.sample(n=4000, random_state=42)\\n# Define category dict outside\\n\\ncategory_dict = Integrated_T.T.iloc[0, 0:-1].to_dict()\\n\\n# Define colors and categories\\ncategory_colors = {1: '#008800',  # Dark green\\n                   2: '#FF8C00',  # Dark orange\\n                   3: '#FF0000'}   # Red\\n\\ncategories_labels = {1: 'Normal Operation',\\n              2: 'Early Warning',\\n              3: 'System Failure'}\\n# Add Category on eccontry\\nsample_eccontri['Category'] = sample_eccontri['Sites'].map(category_dict)\""
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''# Create a random sample of 1000 rows (or any number you prefer)\n",
        "sample_eccontri = ECcontri_Uniprot_enriched.sample(n=4000, random_state=42)\n",
        "# Define category dict outside\n",
        "\n",
        "category_dict = Integrated_T.T.iloc[0, 0:-1].to_dict()\n",
        "\n",
        "# Define colors and categories\n",
        "category_colors = {1: '#008800',  # Dark green\n",
        "                   2: '#FF8C00',  # Dark orange\n",
        "                   3: '#FF0000'}   # Red\n",
        "\n",
        "categories_labels = {1: 'Normal Operation',\n",
        "              2: 'Early Warning',\n",
        "              3: 'System Failure'}\n",
        "# Add Category on eccontry\n",
        "sample_eccontri['Category'] = sample_eccontri['Sites'].map(category_dict)'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-16T22:39:56.944683Z",
          "iopub.status.busy": "2025-03-16T22:39:56.944305Z",
          "iopub.status.idle": "2025-03-16T22:39:56.958705Z",
          "shell.execute_reply": "2025-03-16T22:39:56.957234Z",
          "shell.execute_reply.started": "2025-03-16T22:39:56.944650Z"
        },
        "id": "Q1pOkXHkpRBH",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def prepare_data_for_analysis(eccontri_df):\n",
        "    \"\"\"Reshape data for pattern analysis and calculate summary statistics.\"\"\"\n",
        "\n",
        "    # Create mapping of protein-genus pairs to their Sites\n",
        "    protein_genus_sites = {}\n",
        "    for idx, row in eccontri_df.iterrows():\n",
        "        key = (row['Genus'], row['protein_name'])\n",
        "        protein_genus_sites.setdefault(key, []).append(row['Sites'])\n",
        "\n",
        "    # Group by protein_name, Genus, and Category to get summary statistics for norm_abund_contri\n",
        "    grouped = eccontri_df.groupby(['Genus', 'protein_name', 'Category'])['norm_abund_contri'].agg(\n",
        "        ['mean', 'median', 'std', 'count']\n",
        "    ).reset_index()\n",
        "\n",
        "    # Pivot to wide format\n",
        "    wide_df = grouped.pivot_table(\n",
        "        index=['Genus', 'protein_name'],\n",
        "        columns='Category',\n",
        "        values=['mean', 'median', 'std', 'count']\n",
        "    )\n",
        "\n",
        "    # Flatten column names (e.g., \"mean_1\", \"median_1\", etc.)\n",
        "    wide_df.columns = [f'{col[0]}_{col[1]}' for col in wide_df.columns]\n",
        "    original_index = wide_df.index.tolist()  # This is a list of tuples: (Genus, protein_name)\n",
        "    wide_df = wide_df.reset_index()\n",
        "\n",
        "    # Add lookup key for context field mapping\n",
        "    wide_df['lookup_key'] = original_index\n",
        "\n",
        "    # Get list of categories and ensure they are sorted\n",
        "    categories = sorted(eccontri_df['Category'].unique())\n",
        "\n",
        "    # Add presence/absence indicators per category\n",
        "    for cat in categories:\n",
        "        wide_df[f'present_in_{cat}'] = ~wide_df[f'mean_{cat}'].isna() & (wide_df[f'mean_{cat}'] > 0)\n",
        "\n",
        "    # Determine abundance pattern using provided function\n",
        "    wide_df['abundance_pattern'] = wide_df.apply(\n",
        "        lambda row: determine_abundance_pattern(row, categories), axis=1)\n",
        "\n",
        "    # Calculate fold changes between consecutive categories\n",
        "    for i in range(len(categories)-1):\n",
        "        cat1, cat2 = categories[i], categories[i+1]\n",
        "        wide_df[f'log2fc_{cat1}_to_{cat2}'] = wide_df.apply(\n",
        "            lambda row: safe_log2fc(row[f'mean_{cat2}'], row[f'mean_{cat1}']), axis=1)\n",
        "\n",
        "    # Calculate maximum fold change across the first and last category\n",
        "    if len(categories) >= 2:\n",
        "        wide_df['max_log2fc'] = wide_df.apply(\n",
        "            lambda row: safe_log2fc(row[f'mean_{categories[-1]}'], row[f'mean_{categories[0]}']),\n",
        "            axis=1)\n",
        "\n",
        "    # List of context metadata fields\n",
        "    context_fields = ['enzyme_names', 'enzyme_class', 'pathways', 'hierarchy',\n",
        "                      'metals_involved', 'metals_consolidated', 'corrosion_mechanisms',\n",
        "                      'corrosion_relevance_score', 'corrosion_relevance', 'has_metal']\n",
        "\n",
        "    # Build metadata lookup dictionary.\n",
        "    # For each protein-genus key, store the first non-null, non-empty value encountered for each field.\n",
        "    print(\"Building metadata lookup dictionary...\")\n",
        "    protein_genus_metadata = {}\n",
        "    for idx, row in eccontri_df.iterrows():\n",
        "        key = (row['Genus'], row['protein_name'])\n",
        "        if key not in protein_genus_metadata:\n",
        "            protein_genus_metadata[key] = {}\n",
        "        for field in context_fields:\n",
        "            # Check if row contains a valid (non-null and non-empty) value for the field.\n",
        "            if field in row and pd.notna(row[field]) and row[field] not in [None, '']:\n",
        "                # Update the value if it hasn't been set yet or if the current stored value is null/empty.\n",
        "                current_val = protein_genus_metadata[key].get(field)\n",
        "                if current_val in [None, '']:\n",
        "                    protein_genus_metadata[key][field] = row[field]\n",
        "\n",
        "    for field in context_fields:\n",
        "        print(f\"Adding field: {field}\")\n",
        "        wide_df[field] = [\n",
        "            protein_genus_metadata.get(key, {}).get(field, None)\n",
        "            for key in original_index]\n",
        "\n",
        "    return wide_df # patterns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fl-3dKzBGGXx",
        "outputId": "99fc0ece-67ca-4ab5-bf6f-6a6e57e9944c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Building metadata lookup dictionary...\n",
            "Adding field: enzyme_names\n",
            "Adding field: enzyme_class\n",
            "Adding field: pathways\n",
            "Adding field: hierarchy\n",
            "Adding field: metals_involved\n",
            "Adding field: metals_consolidated\n",
            "Adding field: corrosion_mechanisms\n",
            "Adding field: corrosion_relevance_score\n",
            "Adding field: corrosion_relevance\n",
            "Adding field: has_metal\n"
          ]
        }
      ],
      "source": [
        "pattern_df = prepare_data_for_analysis(ECcontri_Uniprot_enriched)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55Qn60EQ61r8"
      },
      "source": [
        "### Performing Statistical Tests\n",
        "The perform_statistical_tests function systematically evaluates protein-genus pairs for statistical differences in abundance across risk categories using the non-parametric Kruskal-Wallis test. For each unique pair, it filters the data, performs the statistical test when sufficient samples exist across multiple categories, calculates descriptive statistics (mean, median, standard deviation) for each risk category, and quantifies the magnitude of differences using Cohen's d-like effect sizes. The function incorporates error handling to skip problematic pairs and applies the Benjamini-Hochberg false discovery rate correction to address multiple testing concerns, ultimately producing a dataframe of protein-genus pairs with their statistical metrics and significance indicators that serves as an objective foundation for subsequent biological interpretation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-16T22:39:56.960656Z",
          "iopub.status.busy": "2025-03-16T22:39:56.960296Z",
          "iopub.status.idle": "2025-03-16T22:39:56.984366Z",
          "shell.execute_reply": "2025-03-16T22:39:56.983292Z",
          "shell.execute_reply.started": "2025-03-16T22:39:56.960611Z"
        },
        "id": "emn_Rd_PiHV9",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Making the base matrix\n",
        "def create_matrix(df):\n",
        "    \"\"\"\n",
        "    Creates the base matrix with cleaned protein names\n",
        "    \"\"\"\n",
        "    # Create pivot table\n",
        "    base_matrix = df.pivot_table(\n",
        "        values='norm_abund_contri',\n",
        "        index='Sites',\n",
        "        columns=['Genus', 'protein_name'],\n",
        "        aggfunc='first',\n",
        "        fill_value=0,\n",
        "        observed=True\n",
        "    )\n",
        "\n",
        "    return base_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "execution": {
          "iopub.execute_input": "2025-03-16T22:39:56.986687Z",
          "iopub.status.busy": "2025-03-16T22:39:56.986269Z",
          "iopub.status.idle": "2025-03-16T22:39:57.007721Z",
          "shell.execute_reply": "2025-03-16T22:39:57.006520Z",
          "shell.execute_reply.started": "2025-03-16T22:39:56.986648Z"
        },
        "id": "VoaDmPh0DQhY",
        "outputId": "aec113dc-be7a-4226-c75d-b572f80d5229",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"base_matrix = create_matrix(sample_eccontri)\\n\\n# Ensuring sites are properly ordered numerically. Get current index and sort it\\ncurrent_index = base_matrix.index\\nsorted_index = sorted(current_index, key=lambda x: int(x.split('_')[1]))\\nbase_matrix = base_matrix.reindex(sorted_index)\\n\\n# Now add the category mapping\\nif category_dict is not None:\\n    category_mapping = pd.Series(base_matrix.index.map(category_dict),\\n                              index=base_matrix.index,\\n                              name='Category')\\n    base_matrix.index = pd.MultiIndex.from_arrays([base_matrix.index, category_mapping],\\n                                                names=['Sites', 'Category'])\""
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''base_matrix = create_matrix(ECcontri_Uniprot_enriched)\n",
        "\n",
        "# Ensuring sites are properly ordered numerically. Get current index and sort it\n",
        "current_index = base_matrix.index\n",
        "sorted_index = sorted(current_index, key=lambda x: int(x.split('_')[1]))\n",
        "base_matrix = base_matrix.reindex(sorted_index)\n",
        "\n",
        "# Now add the category mapping\n",
        "if category_dict is not None:\n",
        "    category_mapping = pd.Series(base_matrix.index.map(category_dict),\n",
        "                              index=base_matrix.index,\n",
        "                              name='Category')\n",
        "    base_matrix.index = pd.MultiIndex.from_arrays([base_matrix.index, category_mapping],\n",
        "                                                names=['Sites', 'Category'])'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-16T22:39:57.030541Z",
          "iopub.status.busy": "2025-03-16T22:39:57.030196Z",
          "iopub.status.idle": "2025-03-16T22:39:57.047098Z",
          "shell.execute_reply": "2025-03-16T22:39:57.045758Z",
          "shell.execute_reply.started": "2025-03-16T22:39:57.030515Z"
        },
        "id": "TQbYGLCDGd64",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def perform_statistical_tests(base_matrix, alpha=0.05):\n",
        "    \"\"\"\n",
        "    Performs statistical testing on protein-genus pairs using a MultiIndex DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    base_matrix : pandas DataFrame\n",
        "        MultiIndex DataFrame with (Site, Category) as rows and (Genus, protein_name) as columns.\n",
        "    alpha : float\n",
        "        Significance level for statistical tests (default: 0.05)\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    pd.DataFrame\n",
        "        DataFrame with statistical test results and summary statistics.\n",
        "    \"\"\"\n",
        "    # Get unique categories from the row MultiIndex (at level 1)\n",
        "    categories = sorted(base_matrix.index.get_level_values(1).unique())\n",
        "\n",
        "    # Get all unique protein-genus pairs from the columns\n",
        "    pairs = pd.DataFrame(base_matrix.columns.tolist(), columns=['Genus', 'protein_name'])\n",
        "\n",
        "    results = []\n",
        "    print(f\"Testing {len(pairs)} protein-genus pairs...\")\n",
        "\n",
        "    # Loop through each protein-genus pair\n",
        "    for i, pair in pairs.iterrows():\n",
        "        if i % 1000 == 0 and i > 0:\n",
        "            print(f\"Processed {i}/{len(pairs)} pairs...\")\n",
        "\n",
        "        genus, protein = pair['Genus'], pair['protein_name']\n",
        "        col = (genus, protein)\n",
        "\n",
        "        # Extract data for each category\n",
        "        cat_data = []\n",
        "        cat_stats = {}\n",
        "\n",
        "        for cat in categories:\n",
        "            if cat in base_matrix.index.get_level_values(1):\n",
        "                # Get the series for this category and protein\n",
        "                data = base_matrix.xs(cat, level=1)[col]\n",
        "                cat_data.append(data.values)\n",
        "\n",
        "                if len(data) > 0:\n",
        "                    cat_stats[f\"mean_cat{cat}\"] = data.mean()\n",
        "                    cat_stats[f\"median_cat{cat}\"] = data.median()\n",
        "                    cat_stats[f\"std_cat{cat}\"] = data.std()\n",
        "                    cat_stats[f\"samples_cat{cat}\"] = len(data)\n",
        "                else:\n",
        "                    cat_stats[f\"mean_cat{cat}\"] = np.nan\n",
        "                    cat_stats[f\"median_cat{cat}\"] = np.nan\n",
        "                    cat_stats[f\"std_cat{cat}\"] = np.nan\n",
        "                    cat_stats[f\"samples_cat{cat}\"] = 0\n",
        "\n",
        "        # Filter out categories with no data\n",
        "        valid_data = [d for d in cat_data if len(d) > 0]\n",
        "\n",
        "        # Skip if any category has insufficient data\n",
        "        if len(valid_data) < 2:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # Perform Kruskal-Wallis test\n",
        "            h_stat, p_val = kruskal(*valid_data)\n",
        "\n",
        "            # Calculate effect sizes between consecutive categories\n",
        "            effect_sizes = {}\n",
        "            for j in range(len(categories)-1):\n",
        "                cat1, cat2 = categories[j], categories[j+1]\n",
        "                if cat_stats[f\"samples_cat{cat1}\"] > 0 and cat_stats[f\"samples_cat{cat2}\"] > 0:\n",
        "                    # Cohen's d-like effect size\n",
        "                    mean_diff = cat_stats[f\"mean_cat{cat2}\"] - cat_stats[f\"mean_cat{cat1}\"]\n",
        "                    pooled_std = np.sqrt((cat_stats[f\"std_cat{cat1}\"]**2 + cat_stats[f\"std_cat{cat2}\"]**2) / 2)\n",
        "\n",
        "                    # Avoid division by zero\n",
        "                    if pooled_std > 0:\n",
        "                        effect_sizes[f\"effect_size_{cat1}_to_{cat2}\"] = mean_diff / pooled_std\n",
        "                    else:\n",
        "                        effect_sizes[f\"effect_size_{cat1}_to_{cat2}\"] = 0 if mean_diff == 0 else np.sign(mean_diff) * 1.0\n",
        "                else:\n",
        "                    effect_sizes[f\"effect_size_{cat1}_to_{cat2}\"] = np.nan\n",
        "\n",
        "            # Store results\n",
        "            result = {\n",
        "                'protein_name': protein,\n",
        "                'Genus': genus,\n",
        "                'h_statistic': h_stat,\n",
        "                'p_value': p_val,\n",
        "                **cat_stats,\n",
        "                **effect_sizes\n",
        "            }\n",
        "            results.append(result)\n",
        "\n",
        "        except Exception as e:\n",
        "            # Skip this pair if there's an error\n",
        "            continue\n",
        "\n",
        "    print(f\"Completed testing of {len(pairs)} protein-genus pairs.\")\n",
        "\n",
        "    # Create results DataFrame\n",
        "    if not results:\n",
        "        print(\"Warning: No valid statistical test results were produced.\")\n",
        "        return pd.DataFrame()\n",
        "    results_df = pd.DataFrame(results)\n",
        "\n",
        "    # Apply multiple testing correction\n",
        "    if len(results_df) > 0:\n",
        "        # Define the \"first\" and \"last\" categories\n",
        "        first_cat = categories[0]\n",
        "        last_cat = categories[-1]\n",
        "\n",
        "        # Only apply correction to positive trends (higher in higher categories)\n",
        "        mask_positive = results_df[f\"mean_cat{first_cat}\"] < results_df[f\"mean_cat{last_cat}\"]\n",
        "\n",
        "        # Also only include rows with non-NaN p_value\n",
        "        mask = ~results_df['p_value'].isna() & mask_positive\n",
        "\n",
        "        # Initialize columns\n",
        "        results_df['p_adjusted'] = np.nan\n",
        "        results_df['significant'] = False\n",
        "\n",
        "        if mask.sum() > 0:\n",
        "            reject, pvals_corrected, _, _ = multipletests(\n",
        "                results_df.loc[mask, 'p_value'].values,\n",
        "                alpha=alpha,\n",
        "                method='fdr_bh'  # Benjamini-Hochberg FDR correction\n",
        "            )\n",
        "            results_df.loc[mask, 'p_adjusted'] = pvals_corrected\n",
        "            results_df.loc[mask, 'significant'] = reject\n",
        "        else:\n",
        "            print(\"No significant p-values to correct.\")\n",
        "        print(f\"Found {sum(results_df['significant'])} statistically significant pairs after FDR correction.\")\n",
        "\n",
        "    return results_df.sort_values('p_value') # stat_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3561EstdGfr",
        "outputId": "6b816d45-450d-49b3-f26d-ac7bf384b9e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing 3791 protein-genus pairs...\n",
            "Processed 1000/3791 pairs...\n",
            "Processed 2000/3791 pairs...\n",
            "Processed 3000/3791 pairs...\n",
            "Completed testing of 3791 protein-genus pairs.\n",
            "Found 0 statistically significant pairs after FDR correction.\n"
          ]
        }
      ],
      "source": [
        "stat_df  = perform_statistical_tests(base_matrix, alpha=0.05)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCRBa2lrsLp8"
      },
      "source": [
        "### Integrate_with_metadata function\n",
        "The integrate_with_metadata function serves as a crucial data enrichment step that combines three essential components of the analysis: abundance patterns, statistical significance, and biological context. It performs a left join between the pattern and statistical dataframes using protein-genus pairs as the key, ensuring all pattern data is preserved while incorporating significance metrics where available. Following this merge, the function systematically enriches each row with relevant corrosion-related metadata extracted from the EC records dictionary, including corrosion mechanisms, metal interactions, enzyme classifications, and pathway associations. This comprehensive integration creates a unified dataset where both statistical significance across risk categories and biological relevance to corrosion processes can be evaluated together, enabling more informed prioritization of protein-genus pairs with potential roles in microbial-influenced corrosion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-16T22:39:57.089785Z",
          "iopub.status.busy": "2025-03-16T22:39:57.089402Z",
          "iopub.status.idle": "2025-03-16T22:39:57.103528Z",
          "shell.execute_reply": "2025-03-16T22:39:57.102234Z",
          "shell.execute_reply.started": "2025-03-16T22:39:57.089750Z"
        },
        "id": "8aAYb7gXsI9l",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def integrate_with_metadata(pattern_df, stat_df, ec_dict):\n",
        "    \"\"\"\n",
        "    Integrate pattern and statistical results with corrosion metadata.\n",
        "\n",
        "    Parameters:\n",
        "      pattern_df : pd.DataFrame\n",
        "          DataFrame with pattern analysis results.\n",
        "      stat_df : pd.DataFrame\n",
        "          DataFrame with statistical test results.\n",
        "      ec_dict : list or dict\n",
        "          List of dictionaries or dictionary with EC metadata.\n",
        "\n",
        "    Returns:\n",
        "      pd.DataFrame: Merged DataFrame enriched with corrosion metadata.\n",
        "    \"\"\"\n",
        "    # Merge pattern and statistical results\n",
        "    if len(stat_df) > 0:\n",
        "        merged = pd.merge(pattern_df, stat_df, on=['protein_name', 'Genus'], how='left')\n",
        "    else:\n",
        "        merged = pattern_df.copy(deep=False)\n",
        "        merged['p_value'] = np.nan\n",
        "        merged['p_adjusted'] = np.nan\n",
        "        merged['significant'] = False\n",
        "\n",
        "    # List of metadata fields to integrate\n",
        "    metadata_fields = ['enzyme_names', 'enzyme_class', 'pathways', 'hierarchy',\n",
        "                      'metals_involved', 'metals_consolidated', 'corrosion_mechanisms',\n",
        "                      'corrosion_relevance_score', 'corrosion_relevance', 'has_metal']\n",
        "\n",
        "    # Define a helper function to retrieve a metadata value for a row\n",
        "    def get_metadata_value(row, field, default=None):\n",
        "        # First, if the row already has a non-null value, return it.\n",
        "        if field in row and pd.notna(row[field]) and row[field] not in [None, '']:\n",
        "            # Ensure that if it is a list, join it into a string.\n",
        "            if isinstance(row[field], list):\n",
        "                return \"; \".join(str(v) for v in row[field])\n",
        "            return row[field]\n",
        "\n",
        "        # Handle whether ec_dict is a list or dictionary\n",
        "        if isinstance(ec_dict, dict):\n",
        "            records_to_check = ec_dict.values()\n",
        "        else:\n",
        "            records_to_check = ec_dict\n",
        "\n",
        "        # First try matching by enzyme_names (case-insensitive)\n",
        "        if 'enzyme_names' in row and pd.notna(row['enzyme_names']) and row['enzyme_names']:\n",
        "            row_enzyme = row['enzyme_names'].lower()\n",
        "            for record in records_to_check:\n",
        "                if 'enzyme_names' in record and record['enzyme_names']:\n",
        "                    rec_names = record['enzyme_names']\n",
        "                    if isinstance(rec_names, str):\n",
        "                        rec_names = [rec_names]\n",
        "                    for name in rec_names:\n",
        "                        if isinstance(name, str) and name.lower() in row_enzyme:\n",
        "                            if field in record:\n",
        "                                value = record[field]\n",
        "                                if isinstance(value, list):\n",
        "                                    return \"; \".join(str(v) for v in value)\n",
        "                                return value\n",
        "\n",
        "        # Fallback: try matching by EC number if available\n",
        "        if 'EC' in row and pd.notna(row['EC']):\n",
        "            ec_val = row['EC']\n",
        "\n",
        "            # If ec_dict is a dictionary, look up directly\n",
        "            if isinstance(ec_dict, dict) and ec_val in ec_dict:\n",
        "                record = ec_dict[ec_val]\n",
        "                if field in record:\n",
        "                    value = record[field]\n",
        "                    if isinstance(value, list):\n",
        "                        return \"; \".join(str(v) for v in value)\n",
        "                    return value\n",
        "            # If ec_dict is a list, search through it\n",
        "            else:\n",
        "                for record in records_to_check:\n",
        "                    if 'ec_number' in record and record['ec_number'] == ec_val:\n",
        "                        if field in record:\n",
        "                            value = record[field]\n",
        "                            if isinstance(value, list):\n",
        "                                return \"; \".join(str(v) for v in value)\n",
        "                            return value\n",
        "        return default\n",
        "\n",
        "    print(f\"Adding metadata for {len(merged)} protein-genus pairs...\")\n",
        "\n",
        "    # For each metadata field, update the merged DataFrame using the lookup\n",
        "    for field in metadata_fields:\n",
        "        print(f\"Adding field: {field}\")\n",
        "        merged[field] = merged.apply(lambda row: get_metadata_value(row, field, default=''), axis=1)\n",
        "\n",
        "    # Special check: if metals_consolidated is missing (empty string) but metals_involved is present,\n",
        "    # copy metals_involved to metals_consolidated.\n",
        "    if 'metals_consolidated' in merged.columns and 'metals_involved' in merged.columns:\n",
        "        missing_mets = merged['metals_consolidated'].isin(['', None])\n",
        "        if missing_mets.sum() > 0:\n",
        "            print(f\"Warning: {missing_mets.sum()} entries have metals_involved but missing metals_consolidated\")\n",
        "            merged.loc[missing_mets, 'metals_consolidated'] = merged.loc[missing_mets, 'metals_involved']\n",
        "\n",
        "    print(\"Metadata integration complete.\")\n",
        "    return merged #integrated_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukctHc7LE_N_",
        "outputId": "31de6cfb-9884-4bae-916b-ecd8e6ed00ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Adding metadata for 51152 protein-genus pairs...\n",
            "Adding field: enzyme_names\n",
            "Adding field: enzyme_class\n",
            "Adding field: pathways\n",
            "Adding field: hierarchy\n",
            "Adding field: metals_involved\n",
            "Adding field: metals_consolidated\n",
            "Adding field: corrosion_mechanisms\n",
            "Adding field: corrosion_relevance_score\n",
            "Adding field: corrosion_relevance\n",
            "Adding field: has_metal\n",
            "Warning: 65 entries have metals_involved but missing metals_consolidated\n",
            "Metadata integration complete.\n"
          ]
        }
      ],
      "source": [
        "integrated_results = integrate_with_metadata(pattern_df, stat_df, ec_records)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mR_DPU31yG_U"
      },
      "source": [
        "### Classify Housekeeping, Niche and Mixed Protein depending on pathways, mechanisms and hierarchy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-16T22:39:57.188041Z",
          "iopub.status.busy": "2025-03-16T22:39:57.187652Z",
          "iopub.status.idle": "2025-03-16T22:39:57.209202Z",
          "shell.execute_reply": "2025-03-16T22:39:57.207665Z",
          "shell.execute_reply.started": "2025-03-16T22:39:57.188011Z"
        },
        "id": "W_Ovd0LHyReP",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def classify_pathways_by_specificity(integrated_results):\n",
        "    \"\"\"\n",
        "    Classify pathways as 'universal', 'niche-specific', or 'mixed' based on matching to universal pathways list\n",
        "\n",
        "    Parameters:\n",
        "        integrated_results : DataFrame with pathway information\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with additional columns for pathway classification\n",
        "    \"\"\"\n",
        "    import pandas as pd\n",
        "    import re\n",
        "\n",
        "    # Create a copy to avoid modifying the original\n",
        "    results = integrated_results.copy(deep=False)\n",
        "\n",
        "    # Define the universal pathways based on the document\n",
        "    universal_pathways = {\n",
        "        # Energy Production\n",
        "        \"glycolysis\": \"1.1. Glycolysis\",\n",
        "        \"gluconeogenesis\": \"1.2. Gluconeogenesis\",\n",
        "        \"pentose phosphate\": \"1.3. Pentose Phosphate Pathway\",\n",
        "        \"tca cycle\": \"1.4. Krebs/TCA Cycle\",\n",
        "        \"krebs cycle\": \"1.4. Krebs/TCA Cycle\",\n",
        "        \"citric acid cycle\": \"1.4. Krebs/TCA Cycle\",\n",
        "        \"electron transport chain\": \"1.5. Electron Transport Chain\",\n",
        "        \"etc\": \"1.5. Electron Transport Chain\",\n",
        "        \"fermentation\": \"1.6. Fermentation\",\n",
        "        \"atp synthase\": \"1.7. ATP Synthase\",\n",
        "\n",
        "        # Carbon Storage\n",
        "        \"fatty acid synthesis\": \"2.1. Fatty Acid Synthesis\",\n",
        "        \"fatty acid oxidation\": \"2.2. Fatty Acid Oxidation\",\n",
        "        \"beta-oxidation\": \"2.2. Fatty Acid Oxidation\",\n",
        "        \"amino acid degradation\": \"2.3. Amino Acid Degradation\",\n",
        "        \"glycogenesis\": \"2.4. Carbohydrate Storage\",\n",
        "        \"glycogenolysis\": \"2.4. Carbohydrate Storage\",\n",
        "        \"triacylglycerol synthesis\": \"2.5. Triacylglycerol Synthesis\",\n",
        "\n",
        "        # DNA/RNA/Protein\n",
        "        \"dna replication\": \"3.1. DNA Replication\",\n",
        "        \"dna polymerase\": \"3.1. DNA Replication\",\n",
        "        \"helicase\": \"3.1. DNA Replication\",\n",
        "        \"transcription\": \"3.2. Transcription\",\n",
        "        \"rna polymerase\": \"3.2. Transcription\",\n",
        "        \"translation\": \"3.3. Translation\",\n",
        "        \"ribosome\": \"3.3. Translation\",\n",
        "        \"protein folding\": \"3.4. Protein Folding and Chaperones\",\n",
        "        \"chaperone\": \"3.4. Protein Folding and Chaperones\",\n",
        "        \"proteasome\": \"3.5. Proteasome System\",\n",
        "        \"protease\": \"3.5. Proteasome System\",\n",
        "        \"amino acid biosynthesis\": \"3.6. Amino Acid Biosynthesis\",\n",
        "\n",
        "        # Membrane Transport\n",
        "        \"abc transporter\": \"4.1. ABC Transporters\",\n",
        "        \"pilus\": \"4.2. Pilus and Flagella Formation\",\n",
        "        \"flagella\": \"4.2. Pilus and Flagella Formation\",\n",
        "        \"peptidoglycan\": \"4.3. Cell Wall Maintenance\",\n",
        "        \"s-layer\": \"4.3. Cell Wall Maintenance\",\n",
        "        \"lipid membrane\": \"4.4. Lipid Membrane Synthesis\",\n",
        "        \"glycerophospholipid\": \"4.4. Lipid Membrane Synthesis\",\n",
        "\n",
        "        # Stress Response\n",
        "        \"oxidative stress\": \"5.1. Oxidative Stress Response\",\n",
        "        \"superoxide dismutase\": \"5.1. Oxidative Stress Response\",\n",
        "        \"catalase\": \"5.1. Oxidative Stress Response\",\n",
        "        \"peroxidase\": \"5.1. Oxidative Stress Response\",\n",
        "        \"heat shock\": \"5.2. Heat Shock Proteins\",\n",
        "        \"cold shock\": \"5.3. Cold Shock Proteins\",\n",
        "        \"sos\": \"5.4. SOS DNA Repair System\",\n",
        "        \"dna repair\": \"5.4. SOS DNA Repair System\",\n",
        "\n",
        "        # Biomolecule Synthesis\n",
        "        \"nucleotide biosynthesis\": \"6.2. Nucleotide Biosynthesis\",\n",
        "    }\n",
        "\n",
        "    # Add specific amino acid biosynthesis pathways\n",
        "    amino_acids = [\"isoleucine\", \"valine\", \"leucine\", \"alanine\", \"arginine\",\n",
        "                  \"asparagine\", \"aspartate\", \"cysteine\", \"glutamate\", \"glutamine\",\n",
        "                  \"glycine\", \"histidine\", \"lysine\", \"methionine\", \"phenylalanine\",\n",
        "                  \"proline\", \"serine\", \"threonine\", \"tryptophan\", \"tyrosine\"]\n",
        "\n",
        "    for aa in amino_acids:\n",
        "        universal_pathways[f\"{aa} biosynthesis\"] = \"6.1. Amino Acid Biosynthesis\"\n",
        "\n",
        "    # Initialize classification columns\n",
        "    results[\"pathway_classification\"] = \"niche-specific\"  # Default\n",
        "    results[\"universal_pathways_detected\"] = \"\"\n",
        "    results[\"niche_specific_pathways\"] = \"\"\n",
        "\n",
        "    # Function to classify a pathway\n",
        "    def classify_pathway(pathway_text):\n",
        "        if pd.isna(pathway_text) or pathway_text == \"\":\n",
        "            return \"unknown\", \"\", \"\"\n",
        "\n",
        "        pathway_text = pathway_text.lower()\n",
        "        matched_universals = []\n",
        "\n",
        "        # Check for universal pathway matches\n",
        "        for key, value in universal_pathways.items():\n",
        "            if re.search(r'\\b{}\\b'.format(re.escape(key)), pathway_text):\n",
        "                matched_universals.append(value)\n",
        "\n",
        "        # Determine remaining niche-specific content\n",
        "        niche_specific = pathway_text\n",
        "        for key in universal_pathways:\n",
        "            niche_specific = re.sub(r'\\b{}\\b'.format(re.escape(key)), \"\", niche_specific)\n",
        "\n",
        "        # Clean up niche_specific text\n",
        "        niche_specific = re.sub(r'\\s+', ' ', niche_specific).strip()\n",
        "        niche_specific = re.sub(r'[,;]\\s*[,;]', ',', niche_specific)\n",
        "        niche_specific = re.sub(r'^[,;]\\s*|\\s*[,;]$', '', niche_specific)\n",
        "\n",
        "        # Classify based on matches\n",
        "        if matched_universals and niche_specific:\n",
        "            classification = \"mixed\"\n",
        "        elif matched_universals:\n",
        "            classification = \"universal\"\n",
        "        elif niche_specific:\n",
        "            classification = \"niche-specific\"\n",
        "        else:\n",
        "            classification = \"unknown\"\n",
        "\n",
        "        return classification, \", \".join(set(matched_universals)), niche_specific\n",
        "\n",
        "    # Apply classification to pathways column if it exists\n",
        "    if 'pathways' in results.columns:\n",
        "        classifications = results['pathways'].apply(classify_pathway)\n",
        "        results[\"pathway_classification\"] = classifications.apply(lambda x: x[0])\n",
        "        results[\"universal_pathways_detected\"] = classifications.apply(lambda x: x[1])\n",
        "        results[\"niche_specific_pathways\"] = classifications.apply(lambda x: x[2])\n",
        "\n",
        "    # Count each classification\n",
        "    if 'pathway_classification' in results.columns:\n",
        "        class_counts = results['pathway_classification'].value_counts()\n",
        "        print(\"Pathway classification results:\")\n",
        "        for cls, count in class_counts.items():\n",
        "            print(f\"  - {cls}: {count} ({count/len(results):.1%})\")\n",
        "\n",
        "    return results\n",
        "\n",
        "def generate_specificity_report(classified_results, output_file=None):\n",
        "    \"\"\"\n",
        "    Generate a report of pathway classifications with examples\n",
        "\n",
        "    Parameters:\n",
        "        classified_results : DataFrame with pathway classification\n",
        "        output_file : str, optional path to save Excel report\n",
        "\n",
        "    Returns:\n",
        "        Dict with summary statistics\n",
        "    \"\"\"\n",
        "    import pandas as pd\n",
        "\n",
        "    # Initialize summary statistics\n",
        "    summary = {\n",
        "        'total_pathways': len(classified_results),\n",
        "        'classification_counts': classified_results['pathway_classification'].value_counts().to_dict(),\n",
        "        'universal_pathway_counts': {},\n",
        "        'niche_specific_examples': {}\n",
        "    }\n",
        "\n",
        "    # Count occurrences of each universal pathway\n",
        "    if 'universal_pathways_detected' in classified_results.columns:\n",
        "        all_universal = []\n",
        "        for pathways in classified_results['universal_pathways_detected'].dropna():\n",
        "            if pathways:\n",
        "                all_universal.extend([p.strip() for p in pathways.split(',')])\n",
        "\n",
        "        from collections import Counter\n",
        "        summary['universal_pathway_counts'] = dict(Counter(all_universal))\n",
        "\n",
        "    # Get examples of niche-specific pathways\n",
        "    if 'niche_specific_pathways' in classified_results.columns:\n",
        "        niche_examples = classified_results.loc[\n",
        "            classified_results['pathway_classification'] == 'niche-specific',\n",
        "            'niche_specific_pathways'\n",
        "        ].dropna().unique()\n",
        "\n",
        "        summary['niche_specific_examples'] = list(niche_examples)[:20]  # Top 20 examples\n",
        "\n",
        "    # Create report DataFrames\n",
        "    # 1. Summary of classifications\n",
        "    classification_summary = pd.DataFrame({\n",
        "        'Classification': summary['classification_counts'].keys(),\n",
        "        'Count': summary['classification_counts'].values(),\n",
        "        'Percentage': [count/summary['total_pathways'] for count in summary['classification_counts'].values()]\n",
        "    })\n",
        "\n",
        "    # 2. Universal pathway frequency\n",
        "    universal_frequency = pd.DataFrame({\n",
        "        'Universal Pathway': summary['universal_pathway_counts'].keys(),\n",
        "        'Count': summary['universal_pathway_counts'].values(),\n",
        "    }).sort_values('Count', ascending=False)\n",
        "\n",
        "    # 3. Examples of each classification\n",
        "    examples = {}\n",
        "    for classification in ['universal', 'niche-specific', 'mixed']:\n",
        "        examples[classification] = classified_results.loc[\n",
        "            classified_results['pathway_classification'] == classification,\n",
        "            ['pathways', 'universal_pathways_detected', 'niche_specific_pathways']\n",
        "        ].head(10)\n",
        "\n",
        "    # Export to Excel if requested\n",
        "    if output_file:\n",
        "        with pd.ExcelWriter(output_file) as writer:\n",
        "            classification_summary.to_excel(writer, sheet_name='Classification Summary', index=False)\n",
        "            universal_frequency.to_excel(writer, sheet_name='Universal Pathway Freq', index=False)\n",
        "\n",
        "            for classification, df in examples.items():\n",
        "                if not df.empty:\n",
        "                    df.to_excel(writer, sheet_name=f'{classification.capitalize()} Examples', index=False)\n",
        "\n",
        "            # Additional sheet with all results\n",
        "            columns_to_include = ['Sites', 'Category', 'pathways', 'pathway_classification',\n",
        "                                 'universal_pathways_detected', 'niche_specific_pathways']\n",
        "            columns_available = [col for col in columns_to_include if col in classified_results.columns]\n",
        "            classified_results[columns_available].to_excel(writer, sheet_name='All Results', index=False)\n",
        "\n",
        "        print(f\"Specificity report saved to {output_file}\")\n",
        "\n",
        "    return summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 533
        },
        "id": "Y-oVx4FE0tt-",
        "outputId": "73eff42b-4559-47ac-ae16-ef1a7a410387"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pathway classification results:\n",
            "  - niche-specific: 36096 (70.6%)\n",
            "  - mixed: 14323 (28.0%)\n",
            "  - unknown: 733 (1.4%)\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "classified_results"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-93bfb5ad-de66-4a1b-a004-ff77d35dd90a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Genus</th>\n",
              "      <th>protein_name</th>\n",
              "      <th>count_1</th>\n",
              "      <th>count_2</th>\n",
              "      <th>count_3</th>\n",
              "      <th>mean_1</th>\n",
              "      <th>mean_2</th>\n",
              "      <th>mean_3</th>\n",
              "      <th>median_1</th>\n",
              "      <th>median_2</th>\n",
              "      <th>...</th>\n",
              "      <th>median_cat3</th>\n",
              "      <th>std_cat3</th>\n",
              "      <th>samples_cat3</th>\n",
              "      <th>effect_size_1_to_2</th>\n",
              "      <th>effect_size_2_to_3</th>\n",
              "      <th>p_adjusted</th>\n",
              "      <th>significant</th>\n",
              "      <th>pathway_classification</th>\n",
              "      <th>universal_pathways_detected</th>\n",
              "      <th>niche_specific_pathways</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Acetobacterium</td>\n",
              "      <td>-2-methylmalate dehydratase; citraconate hydra...</td>\n",
              "      <td>13.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.007914</td>\n",
              "      <td>0.020073</td>\n",
              "      <td>0.013688</td>\n",
              "      <td>0.000459</td>\n",
              "      <td>0.004881</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>mixed</td>\n",
              "      <td>6.1. Amino Acid Biosynthesis</td>\n",
              "      <td>valine, leucine and ; c5-branched dibasic acid...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Acetobacterium</td>\n",
              "      <td>-3-amino-2-methylpropionate transaminase; L-3-...</td>\n",
              "      <td>13.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.114986</td>\n",
              "      <td>0.309879</td>\n",
              "      <td>0.199574</td>\n",
              "      <td>0.021473</td>\n",
              "      <td>0.259967</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>niche-specific</td>\n",
              "      <td></td>\n",
              "      <td>valine, leucine and isoleucine degradation; me...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Acetobacterium</td>\n",
              "      <td>-4-hydroxy-3-methylbut-2-enyl-diphosphate synt...</td>\n",
              "      <td>13.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.006940</td>\n",
              "      <td>0.017456</td>\n",
              "      <td>0.012201</td>\n",
              "      <td>0.000542</td>\n",
              "      <td>0.005257</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>niche-specific</td>\n",
              "      <td></td>\n",
              "      <td>terpenoid backbone biosynthesis; metabolic pat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Acetobacterium</td>\n",
              "      <td>1,4-alpha-glucan branching enzyme GlgB -glucan...</td>\n",
              "      <td>13.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.007748</td>\n",
              "      <td>0.020348</td>\n",
              "      <td>0.016439</td>\n",
              "      <td>0.000824</td>\n",
              "      <td>0.008231</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>niche-specific</td>\n",
              "      <td></td>\n",
              "      <td>starch and sucrose metabolism; metabolic pathw...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Acetobacterium</td>\n",
              "      <td>1--5-[methylideneamino] imidazole-4-carboxamid...</td>\n",
              "      <td>13.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.008975</td>\n",
              "      <td>0.028148</td>\n",
              "      <td>0.013812</td>\n",
              "      <td>0.000566</td>\n",
              "      <td>0.005811</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>mixed</td>\n",
              "      <td>6.1. Amino Acid Biosynthesis</td>\n",
              "      <td>histidine metabolism; metabolic pathways; bios...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 53 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-93bfb5ad-de66-4a1b-a004-ff77d35dd90a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-93bfb5ad-de66-4a1b-a004-ff77d35dd90a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-93bfb5ad-de66-4a1b-a004-ff77d35dd90a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f1b27d7c-b829-4a3f-899f-dbdd50c75be5\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f1b27d7c-b829-4a3f-899f-dbdd50c75be5')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f1b27d7c-b829-4a3f-899f-dbdd50c75be5 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "            Genus                                       protein_name  count_1  \\\n",
              "0  Acetobacterium  -2-methylmalate dehydratase; citraconate hydra...     13.0   \n",
              "1  Acetobacterium  -3-amino-2-methylpropionate transaminase; L-3-...     13.0   \n",
              "2  Acetobacterium  -4-hydroxy-3-methylbut-2-enyl-diphosphate synt...     13.0   \n",
              "3  Acetobacterium  1,4-alpha-glucan branching enzyme GlgB -glucan...     13.0   \n",
              "4  Acetobacterium  1--5-[methylideneamino] imidazole-4-carboxamid...     13.0   \n",
              "\n",
              "   count_2  count_3    mean_1    mean_2    mean_3  median_1  median_2  ...  \\\n",
              "0     17.0      9.0  0.007914  0.020073  0.013688  0.000459  0.004881  ...   \n",
              "1     17.0      9.0  0.114986  0.309879  0.199574  0.021473  0.259967  ...   \n",
              "2     17.0      9.0  0.006940  0.017456  0.012201  0.000542  0.005257  ...   \n",
              "3     17.0      9.0  0.007748  0.020348  0.016439  0.000824  0.008231  ...   \n",
              "4     17.0      9.0  0.008975  0.028148  0.013812  0.000566  0.005811  ...   \n",
              "\n",
              "   median_cat3  std_cat3  samples_cat3  effect_size_1_to_2 effect_size_2_to_3  \\\n",
              "0          NaN       NaN           NaN                 NaN                NaN   \n",
              "1          NaN       NaN           NaN                 NaN                NaN   \n",
              "2          NaN       NaN           NaN                 NaN                NaN   \n",
              "3          NaN       NaN           NaN                 NaN                NaN   \n",
              "4          NaN       NaN           NaN                 NaN                NaN   \n",
              "\n",
              "   p_adjusted  significant  pathway_classification  \\\n",
              "0         NaN          NaN                   mixed   \n",
              "1         NaN          NaN          niche-specific   \n",
              "2         NaN          NaN          niche-specific   \n",
              "3         NaN          NaN          niche-specific   \n",
              "4         NaN          NaN                   mixed   \n",
              "\n",
              "    universal_pathways_detected  \\\n",
              "0  6.1. Amino Acid Biosynthesis   \n",
              "1                                 \n",
              "2                                 \n",
              "3                                 \n",
              "4  6.1. Amino Acid Biosynthesis   \n",
              "\n",
              "                             niche_specific_pathways  \n",
              "0  valine, leucine and ; c5-branched dibasic acid...  \n",
              "1  valine, leucine and isoleucine degradation; me...  \n",
              "2  terpenoid backbone biosynthesis; metabolic pat...  \n",
              "3  starch and sucrose metabolism; metabolic pathw...  \n",
              "4  histidine metabolism; metabolic pathways; bios...  \n",
              "\n",
              "[5 rows x 53 columns]"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "classified_results= classify_pathways_by_specificity(integrated_results)\n",
        "classified_results.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUcwwFReAAON"
      },
      "source": [
        "### Patterns Separtation\n",
        "It was interesting to know if the inverse patterns genera had some protein on increasing pattern, a snippet was done and found none: <<Found 0 genera with mixed patterns out of 5780 inverse genera>> that is why it was decided to separate the inverse genera pattern out of the pipeline for further investigation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-16T22:39:57.211260Z",
          "iopub.status.busy": "2025-03-16T22:39:57.210844Z",
          "iopub.status.idle": "2025-03-16T22:39:57.233985Z",
          "shell.execute_reply": "2025-03-16T22:39:57.232968Z",
          "shell.execute_reply.started": "2025-03-16T22:39:57.211228Z"
        },
        "id": "fHfwD3dkAIG0",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def separate_by_correlation(classified_results):\n",
        "    \"\"\"\n",
        "    Separates classified_results into groups based on the correlation\n",
        "    between mean abundances (across risk categories) and the risk label.\n",
        "\n",
        "    Returns:\n",
        "      pos_df : DataFrame with positive correlation (increasing pattern)\n",
        "      inv_df : DataFrame with negative correlation (inverse pattern)\n",
        "      cons_df: DataFrame with zero or undefined correlation (consistent/neutral)\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "\n",
        "    # Get the available category numbers from the column names\n",
        "    mean_cols = [col for col in classified_results.columns if col.startswith('mean_') and col.split('_')[1].isdigit()]\n",
        "    categories = sorted([int(col.split('_')[1]) for col in mean_cols])\n",
        "\n",
        "    def calc_corr(row):\n",
        "        # Extract non-zero mean values for each category\n",
        "        means = []\n",
        "        cats = []\n",
        "\n",
        "        for cat in categories:\n",
        "            col = f'mean_{cat}'\n",
        "            if col in classified_results.columns and pd.notna(row[col]) and row[col] > 0:\n",
        "                means.append(float(row[col]))\n",
        "                cats.append(cat)\n",
        "\n",
        "        # Need at least 2 points for correlation\n",
        "        if len(means) < 2:\n",
        "            # Handle special cases\n",
        "            if 'abundance_pattern' in row:\n",
        "                if row['abundance_pattern'] == 'only_in_cat3':\n",
        "                    return 1.0  # Consider as strong positive correlation\n",
        "                elif row['abundance_pattern'] == 'only_in_cat1':\n",
        "                    return -1.0  # Consider as strong negative correlation\n",
        "            return 0\n",
        "\n",
        "        # Convert to numpy arrays\n",
        "        means_array = np.array(means)\n",
        "        cats_array = np.array(cats)\n",
        "\n",
        "        # Check for zero variance\n",
        "        if np.std(means_array) == 0:\n",
        "            return 0\n",
        "\n",
        "        # Calculate correlation\n",
        "        correlation = np.corrcoef(cats_array, means_array)[0, 1]\n",
        "        return correlation\n",
        "\n",
        "    classified_results['corr'] = classified_results.apply(calc_corr, axis=1)\n",
        "\n",
        "    # Special handling for specific patterns\n",
        "    if 'abundance_pattern' in classified_results.columns:\n",
        "        # Force positive correlation for only_in_cat3\n",
        "        mask_only_cat3 = classified_results['abundance_pattern'] == 'only_in_cat3'\n",
        "        classified_results.loc[mask_only_cat3, 'corr'] = 1.0\n",
        "\n",
        "        # Force negative correlation for only_in_cat1\n",
        "        mask_only_cat1 = classified_results['abundance_pattern'] == 'only_in_cat1'\n",
        "        classified_results.loc[mask_only_cat1, 'corr'] = -1.0\n",
        "\n",
        "        # Handle ambiguous patterns\n",
        "        mask_ambiguous =classified_results['abundance_pattern'].isin(['ambiguous', 'consistent_across_all'])\n",
        "        classified_results.loc[mask_ambiguous, 'corr'] = 0\n",
        "\n",
        "    increasing_df = classified_results[classified_results['corr'] > 0].copy(deep=False)\n",
        "    inverse_df = classified_results[classified_results['corr'] < 0].copy(deep=False)\n",
        "    constant_df = classified_results[classified_results['corr'] == 0].copy(deep=False)\n",
        "\n",
        "    print(f\"Separated {len(increasing_df)} increasing patterns, {len(inverse_df)} inverse patterns, and {len(constant_df)} constant patterns.\")\n",
        "\n",
        "    return increasing_df, inverse_df, constant_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IfGnueLslNS",
        "outputId": "fdab953a-78d5-4dc8-eccc-59bf5427a5ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Separated 28227 increasing patterns, 20477 inverse patterns, and 2448 constant patterns.\n"
          ]
        }
      ],
      "source": [
        "increasing_df, inverse_df, constant_df = separate_by_correlation(classified_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsNzT-EF1gIk"
      },
      "source": [
        "### Checking for consistency on the pattern recognition\n",
        "The functions were improved in an iterative manner to ensure that the patterns specified correspond with the correlation values found.\n",
        "The pattern distribution now shows complete alignment between pattern types and correlation signs:\n",
        "\n",
        "Decreasing patterns have negative correlation values\n",
        "Increasing patterns have positive correlation values\n",
        "Proteins found only in category 3 show positive correlation\n",
        "Proteins found only in category 1 show negative correlation\n",
        "Consistent patterns have zero correlation\n",
        "Mixed patterns are distributed across different correlation types, as expected\n",
        "This approach ensures biological relevance and statistical consistency in the classification of protein abundance patterns across different risk categories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "execution": {
          "iopub.execute_input": "2025-03-16T22:39:57.235995Z",
          "iopub.status.busy": "2025-03-16T22:39:57.235612Z",
          "iopub.status.idle": "2025-03-16T22:39:57.258368Z",
          "shell.execute_reply": "2025-03-16T22:39:57.257301Z",
          "shell.execute_reply.started": "2025-03-16T22:39:57.235904Z"
        },
        "id": "-m4N87ahSpY_",
        "outputId": "bc197525-e8fa-4fb4-cfba-79d1c7425a7a",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'# Checking for consistency on the pattern recognision\\nprint(\"\\nPattern distribution by correlation sign:\")\\ncorr_groups = pd.cut(integrated_results[\\'corr\\'],\\n                    bins=[-1.001, -0.001, 0.001, 1.001],\\n                    labels=[\\'negative\\', \\'zero\\', \\'positive\\'])\\npattern_corr_table = pd.crosstab(integrated_results[\\'abundance_pattern\\'], corr_groups)\\nprint(pattern_corr_table)\\n\\n# Check a few examples from each problematic combination\\nprint(\"\\nSample of \\'increasing\\' pattern with negative correlation:\")\\nsample_increasing_negative = integrated_results[(integrated_results[\\'abundance_pattern\\'] == \\'increasing\\') &\\n                                              (integrated_results[\\'corr\\'] < 0)].head(3)\\nfor _, row in sample_increasing_negative.iterrows():\\n    print(f\"Genus: {row[\\'Genus\\']}, Protein: {row[\\'protein_name\\']}\")\\n    print(f\"Mean values: {[row.get(f\\'mean_{cat}\\', \\'N/A\\') for cat in [1, 2, 3]]}\")\\n    print(f\"Correlation: {row[\\'corr\\']}\")\\n    print()'"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''# Checking for consistency on the pattern recognision\n",
        "print(\"\\nPattern distribution by correlation sign:\")\n",
        "corr_groups = pd.cut(integrated_results['corr'],\n",
        "                    bins=[-1.001, -0.001, 0.001, 1.001],\n",
        "                    labels=['negative', 'zero', 'positive'])\n",
        "pattern_corr_table = pd.crosstab(integrated_results['abundance_pattern'], corr_groups)\n",
        "print(pattern_corr_table)\n",
        "\n",
        "# Check a few examples from each problematic combination\n",
        "print(\"\\nSample of 'increasing' pattern with negative correlation:\")\n",
        "sample_increasing_negative = integrated_results[(integrated_results['abundance_pattern'] == 'increasing') &\n",
        "                                              (integrated_results['corr'] < 0)].head(3)\n",
        "for _, row in sample_increasing_negative.iterrows():\n",
        "    print(f\"Genus: {row['Genus']}, Protein: {row['protein_name']}\")\n",
        "    print(f\"Mean values: {[row.get(f'mean_{cat}', 'N/A') for cat in [1, 2, 3]]}\")\n",
        "    print(f\"Correlation: {row['corr']}\")\n",
        "    print()'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCHfeZHEtB9M"
      },
      "source": [
        "### Priorise Markers\n",
        "The prioritize_markers function implements a scoring system that objectively ranks protein-genus pairs based on their potential relevance to corrosion processes. It evaluates each candidate using a multi-factor approach that integrates statistical evidence with biological context, assigning points for statistical significance (3 points), abundance pattern relevance (up to 2 points, with higher scores for patterns showing increased abundance in corrosion conditions), effect size magnitude (up to 2 points for large fold-changes), corrosion relevance derived from metadata (scaled appropriately), metal involvement (1 bonus point), and participation in multiple corrosion mechanisms (up to 2 additional points). The final combined score creates a comprehensive ranking that balances statistical rigor with biological plausibility, enabling focusing on the most promising corrosion-associated proteins for further investigation while filtering out less relevant background organisms.\n",
        "In this step the inverse patterns are filter out and are left aside for further exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-16T22:39:57.313793Z",
          "iopub.status.busy": "2025-03-16T22:39:57.313415Z",
          "iopub.status.idle": "2025-03-16T22:39:57.336061Z",
          "shell.execute_reply": "2025-03-16T22:39:57.334574Z",
          "shell.execute_reply.started": "2025-03-16T22:39:57.313757Z"
        },
        "id": "IR0N6UBYsvY7",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def prioritize_markers(increasing_results):\n",
        "    \"\"\"Prioritize markers based on statistical and biological significance\"\"\"\n",
        "    # Copy to avoid modifying original\n",
        "    results = increasing_results.copy(deep=False)\n",
        "\n",
        "    # Initialize combined score\n",
        "    results['combined_score'] = 0.0\n",
        "\n",
        "    # Pattern checking (diagnostics)\n",
        "    pattern_counts = results['abundance_pattern'].value_counts()\n",
        "    print(f\"Abundance patterns before filtering: {pattern_counts}\")\n",
        "\n",
        "    # 1. STATISTICAL SIGNIFICANCE COMPONENT (up to 3 points)\n",
        "    if 'significant' in results.columns:\n",
        "        results.loc[results['significant'] == True, 'combined_score'] += 3.0\n",
        "\n",
        "    # 2. PATTERN RELEVANCE COMPONENT (up to 2 points)\n",
        "    pattern_scores = {\n",
        "        'increasing': 2.0,         # Higher in corrosion conditions\n",
        "        'only_in_cat3': 2.0,       # Only present in severe corrosion\n",
        "        'highest_in_cat3': 2.0,    # Highest in severe corrosion\n",
        "        'peak_at_cat2': 1.5,       # Peak at middle risk\n",
        "        'mixed': 0.5               # Mixed patterns get minimal points\n",
        "    }\n",
        "    results['combined_score'] += results['abundance_pattern'].map(pattern_scores).fillna(0.0)\n",
        "\n",
        "    # 3. CORRELATION STRENGTH COMPONENT (up to 2 points)\n",
        "    if 'corr' in results.columns:\n",
        "        # Define correlation scores based on strength\n",
        "        corr_magnitude = results['corr'].abs()\n",
        "        results['corr_score'] = 0.0\n",
        "        results.loc[corr_magnitude >= 0.9, 'corr_score'] = 2.0            # Very strong correlation\n",
        "        results.loc[(corr_magnitude >= 0.7) & (corr_magnitude < 0.9), 'corr_score'] = 1.5  # Strong correlation\n",
        "        results.loc[(corr_magnitude >= 0.5) & (corr_magnitude < 0.7), 'corr_score'] = 1.0  # Moderate correlation\n",
        "        results['combined_score'] += results['corr_score']\n",
        "\n",
        "    # 4. METALS COMPONENT (up to 3 points)\n",
        "    corrosion_metals = ['Fe', 'iron', 'Cu', 'copper', 'Mn', 'manganese',\n",
        "                         'S', 'sulfur', 'Cr', 'chromium', 'NO3-', 'NO2-', 'Cl-']\n",
        "\n",
        "    if 'metals_involved' in results.columns:\n",
        "        # Count mentions of relevant metals (capped at 3 points)\n",
        "        def score_metals(metals_str):\n",
        "            if not isinstance(metals_str, str):\n",
        "                return 0\n",
        "            return min(3.0, sum(0.5 for metal in corrosion_metals if metal.lower() in metals_str.lower()))\n",
        "\n",
        "        results['metals_score'] = results['metals_involved'].apply(score_metals)\n",
        "        results['combined_score'] += results['metals_score']\n",
        "\n",
        "    # 5. PATHWAYS AND HIERARCHY COMPONENT (up to 3 points)\n",
        "    # Combined list of relevant terms to search for in pathways and hierarchy\n",
        "    corrosion_terms = {\n",
        "        # Higher impact terms (0.6 points each)\n",
        "        'metal metabolism': 0.6, 'sulfur metabolism': 0.6, 'iron reduction': 0.6,\n",
        "        'metal': 0.6, 'iron': 0.6, 'sulfur': 0.6, 'redox': 0.6, 'corrosion': 0.6,\n",
        "\n",
        "        # Medium impact terms (0.4 points each)\n",
        "        'electron transfer': 0.4, 'biofilm formation': 0.4, 'oxidative stress': 0.4,\n",
        "        'electron': 0.4, 'biofilm': 0.4, 'anaerobic': 0.4,\n",
        "\n",
        "        # Lower impact terms (0.3 points each)\n",
        "        'metal resistance': 0.3, 'acid production': 0.3, 'aerobic respiration': 0.3,\n",
        "        'anaerobic respiration': 0.3, 'sulfate reduction': 0.3, 'acidic': 0.3\n",
        "    }\n",
        "\n",
        "    # Score pathways\n",
        "    if 'pathways' in results.columns:\n",
        "        def score_terms(text, term_dict, max_score=3.0):\n",
        "            if not isinstance(text, str):\n",
        "                return 0\n",
        "            score = sum(value for term, value in term_dict.items() if term.lower() in text.lower())\n",
        "            return min(max_score, score)\n",
        "\n",
        "        results['pathways_score'] = results['pathways'].apply(lambda x: score_terms(x, corrosion_terms, 3.0))\n",
        "        results['combined_score'] += results['pathways_score']\n",
        "\n",
        "    # Score hierarchy (with same terms but separate cap)\n",
        "    if 'hierarchy' in results.columns:\n",
        "        results['hierarchy_score'] = results['hierarchy'].apply(lambda x: score_terms(x, corrosion_terms, 3.0))\n",
        "        results['combined_score'] += results['hierarchy_score']\n",
        "\n",
        "    # Applying tier based on pathway classiffication\n",
        "    tier_scores ={\n",
        "        'niche-specific': 2.0, # For the ones corrosion specific\n",
        "        'mixed' : 1.0,         # Can be both universal pathway but also specific to corrosion\n",
        "        'universal': 0.5,      # Universal mechanisms can be incremented by corrosion but no that clear\n",
        "    }\n",
        "\n",
        "    # Apply the tier scores based on classification\n",
        "    if 'pathway_classification' in results.columns:\n",
        "        results['tier_score'] = results['pathway_classification'].map(tier_scores).fillna(0.5)\n",
        "        results['combined_score'] += results['tier_score']\n",
        "\n",
        "\n",
        "    # 6. MECHANISMS COMPONENT (up to 4 points)\n",
        "    if 'corrosion_mechanisms' in results.columns:\n",
        "        # Define mechanism weights\n",
        "        mechanism_weights = {\n",
        "            'iron_metabolism': 2.0,\n",
        "            'metal_transformation': 2.0,\n",
        "            'sulfur_metabolism': 1.5,\n",
        "            'direct_eet': 1.5,\n",
        "            'acid_production': 1.0,\n",
        "            'biofilm_formation': 0.8,\n",
        "            'h2_consumption': 0.7,\n",
        "            'o2_consumption': 0.5,\n",
        "            'metal_chelation': 1.0,\n",
        "            'ocre': 1.0\n",
        "        }\n",
        "\n",
        "        def score_mechanisms(mech_str, weights=mechanism_weights, max_score=4.0):\n",
        "            if not isinstance(mech_str, str) or not mech_str:\n",
        "                return 0.0\n",
        "\n",
        "            # Split the mechanisms string and score each mechanism\n",
        "            mechanisms = [m.strip() for m in mech_str.split(';')]\n",
        "            score = sum(weights.get(mech, 0) for mech in mechanisms)\n",
        "\n",
        "            # Add multi-mechanism bonus\n",
        "            if len(mechanisms) >= 3:\n",
        "                score += 1.0\n",
        "            elif len(mechanisms) == 2:\n",
        "                score += 0.5\n",
        "\n",
        "            return min(max_score, score)\n",
        "\n",
        "        results['mechanisms_score'] = results['corrosion_mechanisms'].apply(score_mechanisms)\n",
        "        results['combined_score'] += results['mechanisms_score']\n",
        "\n",
        "    # 7. EFFECT SIZE COMPONENT (up to 2 points)\n",
        "    if 'max_log2fc' in results.columns:\n",
        "        # Convert to absolute values for magnitude\n",
        "        fc_magnitude = results['max_log2fc'].abs()\n",
        "\n",
        "        # Score based on fold change magnitude\n",
        "        results['fc_score'] = 0.0\n",
        "        results.loc[fc_magnitude >= 4, 'fc_score'] = 2.0                          # >16x change\n",
        "        results.loc[(fc_magnitude >= 2) & (fc_magnitude < 4), 'fc_score'] = 1.5   # 4-16x change\n",
        "        results.loc[(fc_magnitude >= 1) & (fc_magnitude < 2), 'fc_score'] = 1.0   # 2-4x change\n",
        "        results.loc[(fc_magnitude > 0) & (fc_magnitude < 1), 'fc_score'] = 0.5    # <2x change\n",
        "\n",
        "        results['combined_score'] += results['fc_score']\n",
        "\n",
        "    # 8. CORROSION RELEVANCE COMPONENT (up to 3 points)\n",
        "    # Use either numerical or categorical scoring, whichever gives higher score\n",
        "    if 'corrosion_relevance_score' in results.columns:\n",
        "        # Convert to numeric first, coercing errors to NaN\n",
        "        results['corrosion_relevance_score'] = pd.to_numeric(results['corrosion_relevance_score'], errors='coerce')\n",
        "        results['corrosion_score_scaled'] = results['corrosion_relevance_score'] / 5.0 * 3.0  # Scale to max 3 points\n",
        "\n",
        "    # Add categorical scoring if available\n",
        "    if 'corrosion_relevance' in results.columns:\n",
        "        corrosion_cat_scores = {\n",
        "            'high': 3.0,\n",
        "            'medium': 2.0,\n",
        "            'low': 1.0\n",
        "        }\n",
        "        results['corrosion_cat_score'] = results['corrosion_relevance'].map(corrosion_cat_scores).fillna(0.0)\n",
        "\n",
        "        # Use the higher score between numerical and categorical if both are available\n",
        "        if 'corrosion_score_scaled' in results.columns:\n",
        "            results['corrosion_final_score'] = results[['corrosion_score_scaled', 'corrosion_cat_score']].max(axis=1)\n",
        "        else:\n",
        "            results['corrosion_final_score'] = results['corrosion_cat_score']\n",
        "\n",
        "        results['combined_score'] += results['corrosion_final_score']\n",
        "    elif 'corrosion_score_scaled' in results.columns:\n",
        "        results['combined_score'] += results['corrosion_score_scaled']\n",
        "\n",
        "    # 9. METAL INVOLVEMENT BONUS (1 point)\n",
        "    if 'has_metal' in results.columns:\n",
        "        results.loc[results['has_metal'] == True, 'combined_score'] += 1.0\n",
        "\n",
        "    # Cap total score at 10 points\n",
        "    results.loc[results['combined_score'] > 10, 'combined_score'] = 10\n",
        "\n",
        "    # Sort by combined score (descending) and adjusted p-value (ascending)\n",
        "    p_value_col = 'p_adjusted' if 'p_adjusted' in results.columns else 'p_value'\n",
        "    sorted_results = results.sort_values(\n",
        "        by=['combined_score', p_value_col],\n",
        "        ascending=[False, True]\n",
        "    )\n",
        "\n",
        "    print(\"Prioritization complete\")\n",
        "\n",
        "    return sorted_results # prioritized_markers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kA_o3_Hstsb1",
        "outputId": "ac1abc20-9fc4-4a31-c055-c5ecb1a03706"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Abundance patterns before filtering: abundance_pattern\n",
            "increasing      25000\n",
            "mixed            1922\n",
            "only_in_cat3     1305\n",
            "Name: count, dtype: int64\n",
            "Prioritization complete\n"
          ]
        }
      ],
      "source": [
        "prioritized_markers = prioritize_markers(increasing_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCZ-qTrpel1y"
      },
      "source": [
        "### Balancing Genus Representation\n",
        "\n",
        "The cher amount of protein per genus is overwehlming so this fuctions aims to prioritize the top 10 protein per genus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5LUD_Oh1W3z"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-16T22:39:57.338280Z",
          "iopub.status.busy": "2025-03-16T22:39:57.337638Z",
          "iopub.status.idle": "2025-03-16T22:39:57.363096Z",
          "shell.execute_reply": "2025-03-16T22:39:57.362043Z",
          "shell.execute_reply.started": "2025-03-16T22:39:57.338234Z"
        },
        "id": "FSMXsGOBepkJ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def balance_genus_representation(prioritized_markers, per_genus_count=10):\n",
        "    \"\"\"\n",
        "    Balance representation by selecting top proteins per genus\n",
        "\n",
        "    Parameters:\n",
        "        prioritized_markers: DataFrame with prioritized markers\n",
        "        per_genus_count: Number of top proteins to include per genus\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with balanced genus representation\n",
        "    \"\"\"\n",
        "    # Group by genus and select top proteins for each\n",
        "    genera = prioritized_markers['Genus'].unique()\n",
        "    balanced_df = pd.DataFrame()\n",
        "\n",
        "    for genus in genera:\n",
        "        genus_df = prioritized_markers[prioritized_markers['Genus'] == genus]\n",
        "        top_genus = genus_df.head(per_genus_count)\n",
        "        balanced_df = pd.concat([balanced_df, top_genus])\n",
        "\n",
        "    return balanced_df.sort_values('combined_score', ascending=False) # balance_markers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7TWqLOHtSyR"
      },
      "source": [
        "### Creating Marker Groups\n",
        "\n",
        "The create_marker_groups function organizes the prioritized results into specialized subsets that facilitate targeted analysis of protein-genus pairs with specific characteristics relevant to corrosion processes. It systematically categorizes markers into distinct groups based on multiple criteria: statistical significance, abundance patterns across risk categories, involvement in specific corrosion mechanisms, association with metals, and combined statistical-biological significance. By extracting and parsing all unique corrosion mechanisms from the dataset, the function creates dedicated groups for each mechanism type such as direct electron transfer or acid production. The function also identifies \"high-confidence\" markers that satisfy both statistical significance and biological relevance thresholds, providing a particularly valuable subset for further investigation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-16T22:39:57.396878Z",
          "iopub.status.busy": "2025-03-16T22:39:57.396526Z",
          "iopub.status.idle": "2025-03-16T22:39:57.413459Z",
          "shell.execute_reply": "2025-03-16T22:39:57.412198Z",
          "shell.execute_reply.started": "2025-03-16T22:39:57.396849Z"
        },
        "id": "ctnaWreptO2n",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def create_marker_groups(markers_df, top_count=100, threshold_percentile=0.75):\n",
        "    \"\"\"\n",
        "    Create specialized marker groups based on various criteria\n",
        "\n",
        "    Parameters:\n",
        "        markers_df: DataFrame with markers (prioritized or balanced)\n",
        "        top_count: Number of top markers to include (default: 100)\n",
        "        threshold_percentile: Percentile for determining high-relevance markers (default: 0.75)\n",
        "\n",
        "    Returns:\n",
        "        Dictionary of marker groups by different criteria\n",
        "    \"\"\"\n",
        "    groups = {}\n",
        "\n",
        "    # 1. OVERALL TOP MARKERS\n",
        "    groups['top_markers'] = markers_df.head(top_count)\n",
        "\n",
        "    # 2. STATISTICAL SIGNIFICANCE GROUPS\n",
        "    if 'significant' in markers_df.columns:\n",
        "        groups['statistically_significant'] = markers_df[\n",
        "            markers_df['significant'] == True]\n",
        "\n",
        "    # 3. CONFIDENCE SCORE GROUPS\n",
        "    # High-confidence markers based on combined score\n",
        "    score_thresholds = {\n",
        "        'high_confidence': 7.0,\n",
        "        'medium_confidence': 5.0,\n",
        "        'low_confidence': 3.0\n",
        "    }\n",
        "\n",
        "    for group_name, threshold in score_thresholds.items():\n",
        "        groups[group_name] = markers_df[markers_df['combined_score'] >= threshold]\n",
        "\n",
        "    # 4. ABUNDANCE PATTERN GROUPS\n",
        "    pattern_groups = {\n",
        "        'increasing': 'Increasing with corrosion severity',\n",
        "        'peak_at_cat2': 'Peak at intermediate corrosion',\n",
        "        'highest_in_cat3': 'Highest in severe corrosion',\n",
        "        'only_in_cat3': 'Only present in severe corrosion',\n",
        "        'mixed': 'Mixed pattern'\n",
        "    }\n",
        "\n",
        "    for pattern, description in pattern_groups.items():\n",
        "        pattern_df = markers_df[markers_df['abundance_pattern'] == pattern]\n",
        "        if len(pattern_df) > 0:\n",
        "            groups[f'pattern_{pattern}'] = pattern_df\n",
        "\n",
        "    # 5. CORRELATION STRENGTH GROUPS\n",
        "    if 'corr' in markers_df.columns:\n",
        "        correlation_thresholds = {\n",
        "            'very_strong_correlation': 0.9,\n",
        "            'strong_correlation': 0.8,\n",
        "            'moderate_correlation': 0.5\n",
        "        }\n",
        "\n",
        "        # Create groups for different correlation strengths\n",
        "        for group_name, threshold in correlation_thresholds.items():\n",
        "            if group_name == 'moderate_correlation':\n",
        "                groups[group_name] = markers_df[\n",
        "                    (markers_df['corr'].abs() >= threshold) &\n",
        "                    (markers_df['corr'].abs() < 0.8)\n",
        "                ]\n",
        "            else:\n",
        "                groups[group_name] = markers_df[markers_df['corr'].abs() >= threshold]\n",
        "\n",
        "    # 6. COMPONENT SCORE GROUPS\n",
        "    # Group by component scores\n",
        "    component_score_fields = {\n",
        "        'metals_score': 'high_metals_relevance',\n",
        "        'pathways_score': 'high_pathway_relevance',\n",
        "        'hierarchy_score': 'high_hierarchy_relevance',\n",
        "        'mechanisms_score': 'high_mechanism_relevance',\n",
        "        'fc_score': 'high_effect_size',\n",
        "        'corr_score': 'high_correlation_score'\n",
        "    }\n",
        "\n",
        "    for score_field, group_name in component_score_fields.items():\n",
        "        if score_field in markers_df.columns and not markers_df[score_field].isna().all():\n",
        "            # Use percentile-based threshold\n",
        "            threshold = markers_df[score_field].quantile(threshold_percentile)\n",
        "            if threshold > 0:  # Only create group if threshold is meaningful\n",
        "                groups[group_name] = markers_df[markers_df[score_field] >= threshold]\n",
        "\n",
        "    # 7. MECHANISM-SPECIFIC GROUPS\n",
        "    if 'corrosion_mechanisms' in markers_df.columns:\n",
        "        # Extract all unique mechanisms\n",
        "        all_mechanisms = set()\n",
        "        for mechs in markers_df['corrosion_mechanisms'].dropna():\n",
        "            if mechs:\n",
        "                all_mechanisms.update([m.strip() for m in mechs.split(';')])\n",
        "\n",
        "        # Create a group for each mechanism\n",
        "        for mechanism in all_mechanisms:\n",
        "            # Skip empty mechanisms\n",
        "            if not mechanism:\n",
        "                continue\n",
        "\n",
        "            mech_df = markers_df[\n",
        "                markers_df['corrosion_mechanisms'].fillna('').str.contains(mechanism)\n",
        "            ]\n",
        "            if len(mech_df) > 0:\n",
        "                # Clean mechanism name for group key\n",
        "                clean_mechanism = mechanism.replace(' ', '_').lower()\n",
        "                groups[f'mechanism_{clean_mechanism}'] = mech_df\n",
        "\n",
        "    # 8. PATHWAY-SPECIFIC GROUPS\n",
        "    if 'pathways' in markers_df.columns:\n",
        "        # Key pathways of interest\n",
        "        key_pathways = [\n",
        "            'metal metabolism', 'sulfur metabolism', 'electron transfer',\n",
        "            'biofilm formation', 'oxidative stress', 'metal resistance',\n",
        "            'acid production'\n",
        "        ]\n",
        "\n",
        "        for pathway in key_pathways:\n",
        "            path_df = markers_df[\n",
        "                markers_df['pathways'].fillna('').str.contains(pathway, case=False)\n",
        "            ]\n",
        "            if len(path_df) > 0:\n",
        "                # Clean pathway name for group key\n",
        "                clean_pathway = pathway.replace(' ', '_').lower()\n",
        "                groups[f'pathway_{clean_pathway}'] = path_df\n",
        "\n",
        "    # 8.5 TIER-BASED GROUPS (PATHWAY CLASSIFICATION)\n",
        "    if 'pathway_classification' in markers_df.columns:\n",
        "        tier_groups = ['niche-specific', 'mixed', 'universal']\n",
        "        for tier in tier_groups:\n",
        "            tier_df = markers_df[markers_df['pathway_classification'] == tier]\n",
        "            if len(tier_df) > 0:\n",
        "                groups[f'tier_{tier}'] = tier_df\n",
        "\n",
        "    # 9. METAL-RELATED GROUPS\n",
        "    # Group by metal involvement flag\n",
        "    if 'has_metal' in markers_df.columns:\n",
        "        groups['metal_involved'] = markers_df[\n",
        "            markers_df['has_metal'] == True]\n",
        "\n",
        "    # If metals_involved column exists, create groups for specific metals\n",
        "    if 'metals_involved' in markers_df.columns:\n",
        "        key_metals = ['Fe', 'iron', 'Cu', 'copper', 'Mn', 'manganese', 'S', 'sulfur']\n",
        "\n",
        "        for metal in key_metals:\n",
        "            metal_df = markers_df[\n",
        "                markers_df['metals_involved'].fillna('').str.contains(metal, case=False)\n",
        "            ]\n",
        "            if len(metal_df) > 0:\n",
        "                # Clean metal name for group key\n",
        "                clean_metal = metal.lower()\n",
        "                groups[f'metal_{clean_metal}'] = metal_df\n",
        "\n",
        "    # 10. EFFECT SIZE GROUPS\n",
        "    if 'max_log2fc' in markers_df.columns:\n",
        "        fc_thresholds = {\n",
        "            'very_high_effect': 4.0,   # >16x change\n",
        "            'high_effect': 2.0,        # 4-16x change\n",
        "            'moderate_effect': 1.0     # 2-4x change\n",
        "        }\n",
        "\n",
        "        for group_name, threshold in fc_thresholds.items():\n",
        "            groups[group_name] = markers_df[\n",
        "                markers_df['max_log2fc'].abs() >= threshold\n",
        "            ]\n",
        "\n",
        "    # Add information about group sizes\n",
        "    group_sizes = {name: len(df) for name, df in groups.items()}\n",
        "    print(\"Marker groups created:\")\n",
        "    for name, size in sorted(group_sizes.items(), key=lambda x: x[1], reverse=True):\n",
        "        print(f\"  - {name}: {size} markers\")\n",
        "\n",
        "    return groups"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRqL49qLQhqm"
      },
      "source": [
        "Analyzing 1491288 data points across 70 sites...\n",
        "Notice that these results only contain positive markers by abundance and functional significance.\n",
        "Completed testing of 35118/37089 protein-genus pairs.\n",
        "Found 943 statistically significant pairs after FDR correction.\n",
        "Integrating with corrosion metadata...\n",
        "Adding metadata for 37089 protein-genus pairs...\n",
        "Metadata integration complete.\n",
        "Prioritizing markers based on statistical and biological relevance...\n",
        "Calculating combined relevance scores...\n",
        "Prioritization complete.\n",
        "Creating specialized marker groups...\n",
        "<ipython-input-238-4958864b2107>:40: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '1.5' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
        "  results.loc[(fc_magnitude >= 2) & (fc_magnitude < 4), 'fc_score'] = 1.5  # 4-16x change\n",
        "Analysis complete!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FSuWh3-rZqp"
      },
      "source": [
        "### Generate Marker Report\n",
        "Generates a report with the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-16T22:39:57.462563Z",
          "iopub.status.busy": "2025-03-16T22:39:57.462218Z",
          "iopub.status.idle": "2025-03-16T22:39:57.483616Z",
          "shell.execute_reply": "2025-03-16T22:39:57.482620Z",
          "shell.execute_reply.started": "2025-03-16T22:39:57.462536Z"
        },
        "id": "43MU0cZEtkLw",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def generate_streamlined_report(analysis_results, output_file=None, top_count=500, include_detailed_scores=True):\n",
        "    \"\"\"\n",
        "    Generate a streamlined report focusing on corrosion-relevant information with\n",
        "    genus/protein pairs and their relationship to corrosion.\n",
        "\n",
        "    Parameters:\n",
        "        analysis_results : dict with analysis results\n",
        "        output_file : str or Path, path to save Excel report (optional)\n",
        "        top_count : int, number of top markers to include in main report (default: 500)\n",
        "        include_detailed_scores : bool, whether to include detailed scoring sheet (default: True)\n",
        "\n",
        "    Returns:\n",
        "        dict with report DataFrames\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract the results\n",
        "    prioritized_markers = analysis_results.get('prioritized_markers')\n",
        "    if prioritized_markers is None:\n",
        "        prioritized_markers = analysis_results.get('increasing_markers')\n",
        "\n",
        "    print(f\"Shape of prioritized_markers: {prioritized_markers.shape}\")\n",
        "\n",
        "    # Get column lists to check\n",
        "    print(\"Available columns in prioritized_markers:\")\n",
        "    print(prioritized_markers.columns.tolist())\n",
        "\n",
        "    # Extract the results - use balanced markers if available\n",
        "    if 'balanced_markers' in analysis_results:\n",
        "        prioritized_markers = analysis_results['balanced_markers']\n",
        "        was_balanced = True\n",
        "    else:\n",
        "        prioritized_markers = analysis_results['prioritized_markers']\n",
        "        was_balanced = False\n",
        "\n",
        "    marker_groups = analysis_results.get('marker_groups', {})\n",
        "\n",
        "    # Get top markers\n",
        "    top_markers = prioritized_markers.head(top_count)\n",
        "\n",
        "    # Define core columns for main report\n",
        "    core_columns = [\n",
        "        'Genus', 'protein_name', 'EC', 'enzyme_names', 'corrosion_mechanisms',\n",
        "        'metals_involved', 'pathway_classification', 'abundance_pattern',\n",
        "        'combined_score', 'corr', 'Sites', 'Category'\n",
        "    ]\n",
        "\n",
        "    # Add category mean columns if they exist\n",
        "    cat_columns = [col for col in top_markers.columns if col.startswith('mean_')]\n",
        "\n",
        "    # Keep only columns that actually exist in the DataFrame\n",
        "    available_core = [col for col in core_columns if col in top_markers.columns]\n",
        "\n",
        "    # Create main report with core columns\n",
        "    main_report = top_markers[available_core].copy(deep=False)\n",
        "\n",
        "    # Add category means in a more organized way - one mean per category\n",
        "    if cat_columns:\n",
        "        category_means = top_markers[cat_columns].copy(deep=False)\n",
        "        # We'll include these separately for the visualizations\n",
        "\n",
        "    # Format column names for readability\n",
        "    pretty_columns = {\n",
        "        'Genus': 'Genus',\n",
        "        'protein_name': 'Protein Name',\n",
        "        'EC': 'EC Number',\n",
        "        'enzyme_names': 'Enzyme Names',\n",
        "        'corrosion_mechanisms': 'Corrosion Mechanisms',\n",
        "        'metals_involved': 'Metals Involved',\n",
        "        'pathway_classification': 'Pathway Classification',\n",
        "        'abundance_pattern': 'Abundance Pattern',\n",
        "        'combined_score': 'Relevance Score',\n",
        "        'corr': 'Correlation with Corrosion',\n",
        "        'Category': 'Risk Category',\n",
        "        'pathways': 'Pathways',\n",
        "        'hierarchy': 'Function Hierarchy',\n",
        "        'niche_specific_pathways': 'Niche-Specific Pathways'\n",
        "    }\n",
        "\n",
        "    # Rename main report columns\n",
        "    main_report.columns = [pretty_columns.get(col, col) for col in main_report.columns]\n",
        "\n",
        "    # Define columns for the visualization sheet\n",
        "    viz_columns = ['Genus', 'protein_name']\n",
        "\n",
        "    # Add pathway, mechanism, and metal columns if they exist\n",
        "    optional_viz_columns = ['pathways', 'corrosion_mechanisms', 'metals_involved',\n",
        "                          'pathway_classification', 'niche_specific_pathways']\n",
        "    viz_columns.extend([col for col in optional_viz_columns if col in top_markers.columns])\n",
        "\n",
        "    # Add category means\n",
        "    viz_columns.extend(cat_columns)\n",
        "\n",
        "    # Create visualization data sheet\n",
        "    available_viz = [col for col in viz_columns if col in top_markers.columns]\n",
        "    viz_data = top_markers[available_viz].copy(deep=False)\n",
        "\n",
        "    # Rename visualization columns\n",
        "    viz_data.columns = [pretty_columns.get(col, col) for col in viz_data.columns]\n",
        "\n",
        "    # Create detailed scoring sheet if requested\n",
        "    if include_detailed_scores:\n",
        "        # Define scoring component columns\n",
        "        score_columns = [\n",
        "            'combined_score', 'corr_score', 'metals_score', 'pathways_score',\n",
        "            'hierarchy_score', 'mechanisms_score', 'fc_score', 'tier_score',\n",
        "            'corrosion_final_score', 'p_value', 'p_adjusted', 'significant'\n",
        "        ]\n",
        "\n",
        "        # Only include columns that exist\n",
        "        available_score = [col for col in score_columns if col in top_markers.columns]\n",
        "\n",
        "        # Create basic identification columns\n",
        "        id_columns = ['Genus', 'protein_name']\n",
        "        available_id = [col for col in id_columns if col in top_markers.columns]\n",
        "\n",
        "        # Combine for detailed scores\n",
        "        detailed_scores = top_markers[available_id + available_score].copy()\n",
        "\n",
        "        # Rename detailed score columns\n",
        "        pretty_score_columns = {\n",
        "            'combined_score': 'Total Score',\n",
        "            'corr_score': 'Correlation Score',\n",
        "            'metals_score': 'Metals Score',\n",
        "            'pathways_score': 'Pathways Score',\n",
        "            'hierarchy_score': 'Hierarchy Score',\n",
        "            'mechanisms_score': 'Mechanisms Score',\n",
        "            'fc_score': 'Effect Size Score',\n",
        "            'tier_score': 'Pathway Specificity Score',\n",
        "            'corrosion_final_score': 'Corrosion Relevance Score',\n",
        "            'p_value': 'P-Value',\n",
        "            'p_adjusted': 'Adjusted P-Value',\n",
        "            'significant': 'Statistically Significant'\n",
        "        }\n",
        "\n",
        "        detailed_scores.columns = [pretty_score_columns.get(col, pretty_columns.get(col, col))\n",
        "                                  for col in detailed_scores.columns]\n",
        "\n",
        "    # Create mechanism-focused data\n",
        "    mechanism_data = None\n",
        "    if 'corrosion_mechanisms' in top_markers.columns:\n",
        "        # Get relevant columns\n",
        "        mech_columns = ['Genus', 'protein_name', 'corrosion_mechanisms', 'combined_score']\n",
        "        available_mech = [col for col in mech_columns if col in top_markers.columns]\n",
        "\n",
        "        # Extract mechanisms data\n",
        "        mechanism_data = top_markers[available_mech].copy()\n",
        "\n",
        "        # Process mechanisms into separate rows\n",
        "        expanded_mechanisms = []\n",
        "\n",
        "        for _, row in mechanism_data.iterrows():\n",
        "            if pd.notna(row.get('corrosion_mechanisms')) and row.get('corrosion_mechanisms'):\n",
        "                mechanisms = [m.strip() for m in row['corrosion_mechanisms'].split(';')]\n",
        "                for mechanism in mechanisms:\n",
        "                    new_row = row.copy()\n",
        "                    new_row['mechanism'] = mechanism\n",
        "                    expanded_mechanisms.append(new_row)\n",
        "            else:\n",
        "                row['mechanism'] = 'unknown'\n",
        "                expanded_mechanisms.append(row)\n",
        "\n",
        "        # Create expanded mechanism DataFrame if there are entries\n",
        "        if expanded_mechanisms:\n",
        "            mechanism_data = pd.DataFrame(expanded_mechanisms)\n",
        "            mechanism_data.columns = [pretty_columns.get(col, col) if col != 'mechanism' else 'Mechanism'\n",
        "                                    for col in mechanism_data.columns]\n",
        "\n",
        "    # Export to Excel if output_file is provided\n",
        "    if output_file:\n",
        "        try:\n",
        "            with pd.ExcelWriter(output_file) as writer:\n",
        "                # Main report\n",
        "                main_report.to_excel(writer, sheet_name='Top Corrosion Markers', index=False)\n",
        "\n",
        "                # Visualization data\n",
        "                viz_data.to_excel(writer, sheet_name='Visualization Data', index=False)\n",
        "\n",
        "                # Detailed scores\n",
        "                if include_detailed_scores and detailed_scores is not None:\n",
        "                    detailed_scores.to_excel(writer, sheet_name='Detailed Scores', index=False)\n",
        "\n",
        "                # Mechanism-focused data\n",
        "                if mechanism_data is not None:\n",
        "                    mechanism_data.to_excel(writer, sheet_name='Mechanisms Focus', index=False)\n",
        "\n",
        "                # Create summary sheet\n",
        "                summary_data = {\n",
        "                    'Item': [\n",
        "                        'Analysis Date',\n",
        "                        'Total Markers Analyzed',\n",
        "                        'Top Markers Selected',\n",
        "                        'Genus Balancing Applied',\n",
        "                        'Pathway Classification Counts',\n",
        "                        'Scoring Components',\n",
        "                        'File Generated'\n",
        "                    ],\n",
        "                    'Value': [\n",
        "                        datetime.now().strftime('%Y-%m-%d %H:%M'),\n",
        "                        len(analysis_results['prioritized_markers']),\n",
        "                        top_count,\n",
        "                        'Yes' if was_balanced else 'No',\n",
        "\n",
        "                        str(top_markers['pathway_classification'].value_counts().to_dict()) if 'pathway_classification' in top_markers else 'N/A',\n",
        "                        ', '.join([col for col in available_score if col not in ['combined_score']]) if include_detailed_scores else 'N/A',\n",
        "                        os.path.basename(str(output_file)) if output_file else 'None'\n",
        "                    ]\n",
        "                }\n",
        "                summary_df = pd.DataFrame(summary_data)\n",
        "                summary_df.to_excel(writer, sheet_name='Summary', index=False)\n",
        "\n",
        "            print(f\"Report successfully generated with {top_count} markers\")\n",
        "            print(f\"Report saved to: {output_file}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating Excel report: {e}\")\n",
        "            print(\"Returning report DataFrames without saving to file.\")\n",
        "    # Return the report DataFrames\n",
        "    report_dict = {\n",
        "        'main_report': main_report,\n",
        "        'visualization_data': viz_data\n",
        "    }\n",
        "\n",
        "    if include_detailed_scores:\n",
        "        report_dict['detailed_scores'] = detailed_scores\n",
        "\n",
        "    if mechanism_data is not None:\n",
        "        report_dict['mechanism_focus'] = mechanism_data\n",
        "\n",
        "    return report_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-03-16T22:39:57.485256Z",
          "iopub.status.busy": "2025-03-16T22:39:57.484975Z"
        },
        "id": "LL_64bSwkKLy",
        "outputId": "e92c88b9-584f-4d79-946c-6ad99a542bdb",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting corrosion protein analysis...\n",
            "Analyzing 1491288 data points across 70 Sites...\n",
            "Preparing data for pattern analysis...\n",
            "Building metadata lookup dictionary...\n",
            "Adding field: enzyme_names\n",
            "Adding field: enzyme_class\n",
            "Adding field: pathways\n",
            "Adding field: hierarchy\n",
            "Adding field: metals_involved\n",
            "Adding field: metals_consolidated\n",
            "Adding field: corrosion_mechanisms\n",
            "Adding field: corrosion_relevance_score\n",
            "Adding field: corrosion_relevance\n",
            "Adding field: has_metal\n",
            "Performing statistical tests...\n",
            "Testing 51152 protein-genus pairs...\n",
            "Processed 1000/51152 pairs...\n",
            "Processed 2000/51152 pairs...\n",
            "Processed 3000/51152 pairs...\n",
            "Processed 4000/51152 pairs...\n",
            "Processed 5000/51152 pairs...\n",
            "Processed 6000/51152 pairs...\n",
            "Processed 7000/51152 pairs...\n",
            "Processed 8000/51152 pairs...\n",
            "Processed 9000/51152 pairs...\n",
            "Processed 10000/51152 pairs...\n",
            "Processed 11000/51152 pairs...\n",
            "Processed 12000/51152 pairs...\n",
            "Processed 13000/51152 pairs...\n",
            "Processed 14000/51152 pairs...\n",
            "Processed 15000/51152 pairs...\n",
            "Processed 16000/51152 pairs...\n",
            "Processed 17000/51152 pairs...\n",
            "Processed 18000/51152 pairs...\n",
            "Processed 19000/51152 pairs...\n",
            "Processed 20000/51152 pairs...\n",
            "Processed 21000/51152 pairs...\n",
            "Processed 22000/51152 pairs...\n",
            "Processed 23000/51152 pairs...\n",
            "Processed 24000/51152 pairs...\n",
            "Processed 25000/51152 pairs...\n",
            "Processed 26000/51152 pairs...\n",
            "Processed 27000/51152 pairs...\n",
            "Processed 28000/51152 pairs...\n",
            "Processed 29000/51152 pairs...\n",
            "Processed 30000/51152 pairs...\n",
            "Processed 31000/51152 pairs...\n",
            "Processed 32000/51152 pairs...\n",
            "Processed 33000/51152 pairs...\n",
            "Processed 34000/51152 pairs...\n",
            "Processed 35000/51152 pairs...\n",
            "Processed 36000/51152 pairs...\n",
            "Processed 37000/51152 pairs...\n",
            "Processed 38000/51152 pairs...\n",
            "Processed 39000/51152 pairs...\n",
            "Processed 40000/51152 pairs...\n",
            "Processed 41000/51152 pairs...\n",
            "Processed 42000/51152 pairs...\n",
            "Processed 43000/51152 pairs...\n",
            "Processed 44000/51152 pairs...\n",
            "Processed 45000/51152 pairs...\n",
            "Processed 46000/51152 pairs...\n",
            "Processed 47000/51152 pairs...\n",
            "Processed 48000/51152 pairs...\n",
            "Processed 49000/51152 pairs...\n",
            "Processed 50000/51152 pairs...\n",
            "Processed 51000/51152 pairs...\n",
            "Completed testing of 51152 protein-genus pairs.\n",
            "Found 0 statistically significant pairs after FDR correction.\n",
            "Integrating with corrosion metadata...\n",
            "Adding metadata for 51152 protein-genus pairs...\n",
            "Adding field: enzyme_names\n",
            "Adding field: enzyme_class\n",
            "Adding field: pathways\n",
            "Adding field: hierarchy\n",
            "Adding field: metals_involved\n",
            "Adding field: metals_consolidated\n",
            "Adding field: corrosion_mechanisms\n",
            "Adding field: corrosion_relevance_score\n",
            "Adding field: corrosion_relevance\n",
            "Adding field: has_metal\n",
            "Warning: 65 entries have metals_involved but missing metals_consolidated\n",
            "Metadata integration complete.\n",
            "Pathway classification results:\n",
            "  - niche-specific: 36096 (70.6%)\n",
            "  - mixed: 14323 (28.0%)\n",
            "  - unknown: 733 (1.4%)\n",
            "Separating positive and inverse patterns by corr\n",
            "Separated 28227 increasing patterns, 20477 inverse patterns, and 2448 constant patterns.\n",
            "Prioritizing markers based on statistical and biological relevance...\n",
            "Abundance patterns before filtering: abundance_pattern\n",
            "increasing      25000\n",
            "mixed            1922\n",
            "only_in_cat3     1305\n",
            "Name: count, dtype: int64\n",
            "Prioritization complete\n",
            "Creating specialized marker groups...\n",
            "Marker groups created:\n",
            "  - low_confidence: 28225 markers\n",
            "  - medium_confidence: 28208 markers\n",
            "  - metal_involved: 28196 markers\n",
            "  - high_confidence: 28078 markers\n",
            "  - metal_s: 27046 markers\n",
            "  - mechanism_h2_consumption: 26388 markers\n",
            "  - mechanism_acid_production: 25135 markers\n",
            "  - pattern_increasing: 25000 markers\n",
            "  - metal_iron: 24835 markers\n",
            "  - mechanism_direct_eet: 20458 markers\n",
            "  - metal_sulfur: 20454 markers\n",
            "  - high_mechanism_relevance: 19847 markers\n",
            "  - tier_niche-specific: 19804 markers\n",
            "  - mechanism_sulfur_metabolism: 17449 markers\n",
            "  - strong_correlation: 15749 markers\n",
            "  - metal_manganese: 15267 markers\n",
            "  - high_metals_relevance: 13764 markers\n",
            "  - high_effect_size: 10871 markers\n",
            "  - moderate_effect: 10871 markers\n",
            "  - very_strong_correlation: 9414 markers\n",
            "  - high_correlation_score: 9414 markers\n",
            "  - mechanism_biofilm_formation: 8599 markers\n",
            "  - tier_mixed: 8033 markers\n",
            "  - mechanism_o2_consumption: 5513 markers\n",
            "  - moderate_correlation: 5191 markers\n",
            "  - high_effect: 4485 markers\n",
            "  - metal_copper: 3238 markers\n",
            "  - pattern_mixed: 1922 markers\n",
            "  - pattern_only_in_cat3: 1305 markers\n",
            "  - very_high_effect: 1103 markers\n",
            "  - pathway_biofilm_formation: 359 markers\n",
            "  - pathway_sulfur_metabolism: 315 markers\n",
            "  - top_markers: 100 markers\n",
            "  - mechanism_iron_metabolism: 22 markers\n",
            "  - statistically_significant: 0 markers\n",
            "Creating specialized marker groups...\n",
            "Marker groups created:\n",
            "  - low_confidence: 28225 markers\n",
            "  - medium_confidence: 28208 markers\n",
            "  - metal_involved: 28196 markers\n",
            "  - high_confidence: 28078 markers\n",
            "  - metal_s: 27046 markers\n",
            "  - mechanism_h2_consumption: 26388 markers\n",
            "  - mechanism_acid_production: 25135 markers\n",
            "  - pattern_increasing: 25000 markers\n",
            "  - metal_iron: 24835 markers\n",
            "  - mechanism_direct_eet: 20458 markers\n",
            "  - metal_sulfur: 20454 markers\n",
            "  - high_mechanism_relevance: 19847 markers\n",
            "  - tier_niche-specific: 19804 markers\n",
            "  - mechanism_sulfur_metabolism: 17449 markers\n",
            "  - strong_correlation: 15749 markers\n",
            "  - metal_manganese: 15267 markers\n",
            "  - high_metals_relevance: 13764 markers\n",
            "  - high_effect_size: 10871 markers\n",
            "  - moderate_effect: 10871 markers\n",
            "  - very_strong_correlation: 9414 markers\n",
            "  - high_correlation_score: 9414 markers\n",
            "  - mechanism_biofilm_formation: 8599 markers\n",
            "  - tier_mixed: 8033 markers\n",
            "  - mechanism_o2_consumption: 5513 markers\n",
            "  - moderate_correlation: 5191 markers\n",
            "  - high_effect: 4485 markers\n",
            "  - metal_copper: 3238 markers\n",
            "  - pattern_mixed: 1922 markers\n",
            "  - pattern_only_in_cat3: 1305 markers\n",
            "  - very_high_effect: 1103 markers\n",
            "  - pathway_biofilm_formation: 359 markers\n",
            "  - pathway_sulfur_metabolism: 315 markers\n",
            "  - top_markers: 100 markers\n",
            "  - mechanism_iron_metabolism: 22 markers\n",
            "  - statistically_significant: 0 markers\n",
            "Analysis complete!\n"
          ]
        }
      ],
      "source": [
        "# Run the analysis 40 min @ 20GB\n",
        "analysis_results = analyze_corrosion_proteins(ECcontri_Uniprot_enriched, ec_records)\n",
        "\n",
        "## calling it with balancing the generacomplete_results\n",
        "#analysis_results = analyze_corrosion_proteins(ECcontri_Uniprot_enriched, ec_records, balance_genera=True, per_genus_count=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdZn8Rdeg3ax",
        "outputId": "672f22fa-4ab9-43e0-c330-7c851f280985"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pattern_data: <class 'pandas.core.frame.DataFrame'>\n",
            "statistical_results: <class 'pandas.core.frame.DataFrame'>\n",
            "integrated_results: <class 'pandas.core.frame.DataFrame'>\n",
            "classified_results: <class 'pandas.core.frame.DataFrame'>\n",
            "increasing_markers: <class 'pandas.core.frame.DataFrame'>\n",
            "prioritized_markers: <class 'pandas.core.frame.DataFrame'>\n",
            "marker_groups: <class 'dict'>\n",
            "inverse_markers: <class 'pandas.core.frame.DataFrame'>\n"
          ]
        }
      ],
      "source": [
        "for key, value in analysis_results.items():\n",
        "    print(f\"{key}: {type(value)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Saving the Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clQoVvdIoakO",
        "outputId": "8faf4183-249f-4fb2-e0a1-b51f3c58062d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Skipping marker_groups: Dict save in excel <class 'dict'>\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=pd.io.pytables.PerformanceWarning)\n",
        "\n",
        "output_file = output_large / \"complete_results.h5\"\n",
        "\n",
        "with pd.HDFStore(output_file) as store:\n",
        "    for key, df in analysis_results.items():\n",
        "\n",
        "        if isinstance(df, (pd.DataFrame, pd.Series)):\n",
        "            store[key] = df  # Store DataFrame or Series\n",
        "        else:\n",
        "            print(f\"Skipping {key}: Dict save in excel {type(df)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-03-16T23:24:36.915783Z",
          "iopub.status.busy": "2025-03-16T23:24:36.915443Z",
          "iopub.status.idle": "2025-03-16T23:24:38.707973Z",
          "shell.execute_reply": "2025-03-16T23:24:38.706637Z",
          "shell.execute_reply.started": "2025-03-16T23:24:36.915757Z"
        },
        "id": "WFHpieA5HF1W",
        "outputId": "4b1435e5-22f1-40e6-db15-ca66fa79a68e",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of prioritized_markers: (28227, 65)\n",
            "Available columns in prioritized_markers:\n",
            "['Genus', 'protein_name', 'count_1', 'count_2', 'count_3', 'mean_1', 'mean_2', 'mean_3', 'median_1', 'median_2', 'median_3', 'std_1', 'std_2', 'std_3', 'lookup_key', 'present_in_1', 'present_in_2', 'present_in_3', 'abundance_pattern', 'log2fc_1_to_2', 'log2fc_2_to_3', 'max_log2fc', 'enzyme_names', 'enzyme_class', 'pathways', 'hierarchy', 'metals_involved', 'metals_consolidated', 'corrosion_mechanisms', 'corrosion_relevance_score', 'corrosion_relevance', 'has_metal', 'h_statistic', 'p_value', 'mean_cat1', 'median_cat1', 'std_cat1', 'samples_cat1', 'mean_cat2', 'median_cat2', 'std_cat2', 'samples_cat2', 'mean_cat3', 'median_cat3', 'std_cat3', 'samples_cat3', 'effect_size_1_to_2', 'effect_size_2_to_3', 'p_adjusted', 'significant', 'pathway_classification', 'universal_pathways_detected', 'niche_specific_pathways', 'corr', 'combined_score', 'corr_score', 'metals_score', 'pathways_score', 'hierarchy_score', 'tier_score', 'mechanisms_score', 'fc_score', 'corrosion_score_scaled', 'corrosion_cat_score', 'corrosion_final_score']\n",
            "Report successfully generated with 500 markers\n",
            "Report saved to: /content/drive/MyDrive/MIC/output_large/corrosion_markers_report.xlsx\n"
          ]
        }
      ],
      "source": [
        "# Generate the Excel report in the same directory\n",
        "report_path =output_large/ \"corrosion_markers_report.xlsx\" #/content/drive/MyDrive/MIC/output_large/corrosion_markers_report.xlsx\n",
        "report = generate_streamlined_report(analysis_results, report_path, top_count=500, include_detailed_scores=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqAXwsa32F0r"
      },
      "source": [
        "## 9.6 Summary of the Results  \n",
        "\n",
        "Analysis Files Description  \n",
        "* statistical_results are statistical selection results, comprises results of comprehensive statistical hypothesis testing including Kruskal-Wallis H statistics, raw p-values, and FDR-corrected p-values (p_adjusted). This non-parametric testing identifies genus-protein pairs that show statistically significant differences in abundance across corrosion risk categories. Each entry includes sample counts per category, mean/median/standard deviation metrics, and effect size measurements between categories.\n",
        "\n",
        "* integrated_results combines statistical significance data with functional metadata from the ec_records dictionary. This dataset enriches statistical findings with biological context including enzyme names, EC numbers, metabolic pathways, functional hierarchies, metal interactions, and corrosion-related mechanisms. It serves as the foundation for subsequent analysis by linking statistical significance to biological function.\n",
        "\n",
        "* classified_results Annotates integrated results with pathway classifications using the comprehensive taxonomy of universal vs. niche-specific pathways. Each entry is labeled as \"universal\" (common across all bacteria/archaea), \"niche-specific\" (specialized for particular environments), or \"mixed\" (containing elements of both). This classification helps distinguish between housekeeping functions and potentially corrosion-specific adaptations. classification_summary is a quantitative breakdown of pathway classifications with counts and percentages for each category, providing insight into the distribution of universal vs. specialized pathways in corrosion environments. Universal_frequency is a detailed frequency analysis of specific universal pathways, showing which core metabolic functions are most prevalent in the dataset.\n",
        "\n",
        "* Correlation Separated Results has tree types of results increasing_results which is a subset of integrated results showing positive correlation with corrosion risk.inverse_results which is a subset showing inverse correlation with corrosion risk, These are preserved for analysis of potentially protective or competitive proteins that might inhibit corrosion and analyse on a different section. Lastly constant_results a subset showing no significant correlation with risk categories.  \n",
        "\n",
        "* prioritized_markers comprising only the increasing_results prioritized according to a comprehensive scoring system that considers: Statistical significance (correlation and pattern significance), biological relevance (mechanisms, hierarchy, pathways), corrosion-specific factors (metal involvement, corrosion terms), effect size components.  It contains following columns: Genus', 'protein_name', 'count_1', 'count_2', 'count_3', 'mean_1','mean_2', 'mean_3', 'median_1', 'median_2', 'median_3', 'std_1', 'std_2', 'std_3', 'lookup_key', 'present_in_1', 'present_in_2', 'present_in_3', 'abundance_pattern', 'log2fc_1_to_2', 'log2fc_2_to_3', 'max_log2fc', 'enzyme_names', 'enzyme_class', 'pathways', 'hierarchy','metals_involved', 'metals_consolidated', 'corrosion_mechanisms', 'corrosion_relevance_score', 'corrosion_relevance', 'has_metal','Sites', 'h_statistic', 'p_value', 'mean_cat1', 'median_cat1', 'std_cat1', 'samples_cat1', 'mean_cat2', 'median_cat2', 'std_cat2',       'samples_cat2', 'mean_cat3', 'median_cat3', 'std_cat3', 'samples_cat3','effect_size_1_to_2', 'effect_size_2_to_3', 'p_adjusted', 'significant','corr', 'combined_score', 'corr_score', 'metals_score','pathways_score', 'hierarchy_score', 'mechanisms_score', 'fc_score','corrosion_score_scaled', 'corrosion_cat_score', 'corrosion_final_score'\n",
        "\n",
        "* balanced_markers is an optional filtered subset of prioritized markers that ensures balanced representation across genera, preventing over-representation of abundant genera while maintaining significant proteins.\n",
        "* marker_groups are categorized groups from the top 100 markers at 75% confidence threshold, organized by biological function and relevance.  \n",
        "\n",
        "* complete_results.joblib is the complete results dictionary contains all analysis results preserved in a flexible parquet format.\n",
        "* corrosion_markers_report.xlsx is the excel report and contains all analysis results in separate sheets for easy exploration and presentation.\n",
        "° Top Corrosion Markers, Visualization Data, Detailed Scores, and Mechanisms Focus.\n",
        "The Top Corrosion Markers sheet, df top_markers contains the most relevant genus-protein pairs with their abundance patterns, pathway classifications, and relevance scores. Columns : Genus', 'Protein Name', 'Enzyme Names', 'Corrosion Mechanisms', 'Metals Involved', 'Abundance Pattern', 'Relevance Score', 'Correlation with Corrosion'.\n",
        "° The Visualization Data sheet, df visualize includes additional columns optimized for creating plots and visual analysis. Columns: 'Genus', 'Protein Name', 'Pathways', 'Corrosion Mechanisms', 'Metals Involved', 'mean_1', 'mean_2', 'mean_3', 'mean_cat1', 'mean_cat2', 'mean_cat3'\n",
        "° The Detailed Scores sheet breaks down the contributing factors to each marker's final score, showing the relative importance of statistical, biological, and corrosion-specific components. Columns: 'Genus', 'Protein Name', 'Total Score', 'Correlation Score', 'Metals Score', 'Pathways Score', 'Hierarchy Score', 'Mechanisms Score', 'Effect Size Score', 'Corrosion Relevance Score', 'P-Value', 'Adjusted P-Value', 'Statistically Significant'.\n",
        "° Finally The Mechanisms Focus sheet df mechanisms, reorganizes the data to highlight specific corrosion mechanisms, allowing for targeted analysis of metal transformation, electron transfer, biofilm formation, and other key processes relevant to microbial corrosion. Columns are: 'Genus', 'Protein Name', 'Corrosion Mechanisms', 'Relevance Score', 'Mechanism'.\n",
        "\n",
        "This pipeline provides a comprehensive analysis framework that progresses from statistical identification to biological classification and prioritization, creating both comprehensive and focused views of corrosion-relevant microbial proteins."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhdDiHBq2F0r"
      },
      "source": [
        "### Reading the Resulsts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "3cqLiBFY2F0s",
        "trusted": true
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "Too few arguments for numpy.complexfloating",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[18], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load\u001b[39;00m\n\u001b[1;32m      2\u001b[0m complete_results \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHDFStore\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_large\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcomplete_results.h5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m store:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m store\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;66;03m# Remove leading '/' from key\u001b[39;00m\n\u001b[1;32m      6\u001b[0m         clean_key \u001b[38;5;241m=\u001b[39m key\u001b[38;5;241m.\u001b[39mlstrip(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "File \u001b[0;32m~/MIC/2_Micro/.venv/lib/python3.10/site-packages/pandas/io/pytables.py:572\u001b[0m, in \u001b[0;36mHDFStore.__init__\u001b[0;34m(self, path, mode, complevel, complib, fletcher32, **kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat is not a defined argument for HDFStore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 572\u001b[0m tables \u001b[38;5;241m=\u001b[39m \u001b[43mimport_optional_dependency\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtables\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m complib \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m complib \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m tables\u001b[38;5;241m.\u001b[39mfilters\u001b[38;5;241m.\u001b[39mall_complibs:\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    576\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomplib only supports \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtables\u001b[38;5;241m.\u001b[39mfilters\u001b[38;5;241m.\u001b[39mall_complibs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m compression.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    577\u001b[0m     )\n",
            "File \u001b[0;32m~/MIC/2_Micro/.venv/lib/python3.10/site-packages/pandas/compat/_optional.py:115\u001b[0m, in \u001b[0;36mimport_optional_dependency\u001b[0;34m(name, extra, errors, min_version)\u001b[0m\n\u001b[1;32m    110\u001b[0m msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing optional dependency \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minstall_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mextra\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse pip or conda to install \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minstall_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    113\u001b[0m )\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 115\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
            "File \u001b[0;32m/usr/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "File \u001b[0;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "File \u001b[0;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "File \u001b[0;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "File \u001b[0;32m~/MIC/2_Micro/.venv/lib/python3.10/site-packages/tables/__init__.py:51\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBlosc2 library not found. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI looked for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(blosc2_search_paths)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     48\u001b[0m     )\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Necessary imports to get versions stored on the cython extension\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilsextension\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_hdf5_version \u001b[38;5;28;01mas\u001b[39;00m _get_hdf5_version\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_version\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m     55\u001b[0m hdf5_version \u001b[38;5;241m=\u001b[39m _get_hdf5_version()\n",
            "File \u001b[0;32m~/MIC/2_Micro/.venv/lib/python3.10/site-packages/tables/utilsextension.pyx:25\u001b[0m, in \u001b[0;36minit tables.utilsextension\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32m~/MIC/2_Micro/.venv/lib/python3.10/site-packages/tables/description.py:9\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Callable, Generator, Literal, Optional, Sequence, Type, Union\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m atom\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_name_validity\n\u001b[1;32m     13\u001b[0m __docformat__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreStructuredText\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
            "File \u001b[0;32m~/MIC/2_Micro/.venv/lib/python3.10/site-packages/tables/atom.py:9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DTypeLike\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SizeType\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmisc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menum\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Enum\n",
            "File \u001b[0;32m~/MIC/2_Micro/.venv/lib/python3.10/site-packages/numpy/typing/__init__.py:158\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03m============================\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mTyping (:mod:`numpy.typing`)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    153\u001b[0m \n\u001b[1;32m    154\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;66;03m# NOTE: The API section will be appended with additional entries\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;66;03m# further down in this file\u001b[39;00m\n\u001b[0;32m--> 158\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_typing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    159\u001b[0m     ArrayLike,\n\u001b[1;32m    160\u001b[0m     DTypeLike,\n\u001b[1;32m    161\u001b[0m     NBitBase,\n\u001b[1;32m    162\u001b[0m     NDArray,\n\u001b[1;32m    163\u001b[0m )\n\u001b[1;32m    165\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArrayLike\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDTypeLike\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNBitBase\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNDArray\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__doc__\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[0;32m~/MIC/2_Micro/.venv/lib/python3.10/site-packages/numpy/_typing/__init__.py:102\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_scalars\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     87\u001b[0m     _CharLike_co \u001b[38;5;28;01mas\u001b[39;00m _CharLike_co,\n\u001b[1;32m     88\u001b[0m     _BoolLike_co \u001b[38;5;28;01mas\u001b[39;00m _BoolLike_co,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     96\u001b[0m     _VoidLike_co \u001b[38;5;28;01mas\u001b[39;00m _VoidLike_co,\n\u001b[1;32m     97\u001b[0m )\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_shape\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     99\u001b[0m     _Shape \u001b[38;5;28;01mas\u001b[39;00m _Shape,\n\u001b[1;32m    100\u001b[0m     _ShapeLike \u001b[38;5;28;01mas\u001b[39;00m _ShapeLike,\n\u001b[1;32m    101\u001b[0m )\n\u001b[0;32m--> 102\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dtype_like\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    103\u001b[0m     DTypeLike \u001b[38;5;28;01mas\u001b[39;00m DTypeLike,\n\u001b[1;32m    104\u001b[0m     _DTypeLike \u001b[38;5;28;01mas\u001b[39;00m _DTypeLike,\n\u001b[1;32m    105\u001b[0m     _SupportsDType \u001b[38;5;28;01mas\u001b[39;00m _SupportsDType,\n\u001b[1;32m    106\u001b[0m     _VoidDTypeLike \u001b[38;5;28;01mas\u001b[39;00m _VoidDTypeLike,\n\u001b[1;32m    107\u001b[0m     _DTypeLikeBool \u001b[38;5;28;01mas\u001b[39;00m _DTypeLikeBool,\n\u001b[1;32m    108\u001b[0m     _DTypeLikeUInt \u001b[38;5;28;01mas\u001b[39;00m _DTypeLikeUInt,\n\u001b[1;32m    109\u001b[0m     _DTypeLikeInt \u001b[38;5;28;01mas\u001b[39;00m _DTypeLikeInt,\n\u001b[1;32m    110\u001b[0m     _DTypeLikeFloat \u001b[38;5;28;01mas\u001b[39;00m _DTypeLikeFloat,\n\u001b[1;32m    111\u001b[0m     _DTypeLikeComplex \u001b[38;5;28;01mas\u001b[39;00m _DTypeLikeComplex,\n\u001b[1;32m    112\u001b[0m     _DTypeLikeTD64 \u001b[38;5;28;01mas\u001b[39;00m _DTypeLikeTD64,\n\u001b[1;32m    113\u001b[0m     _DTypeLikeDT64 \u001b[38;5;28;01mas\u001b[39;00m _DTypeLikeDT64,\n\u001b[1;32m    114\u001b[0m     _DTypeLikeObject \u001b[38;5;28;01mas\u001b[39;00m _DTypeLikeObject,\n\u001b[1;32m    115\u001b[0m     _DTypeLikeVoid \u001b[38;5;28;01mas\u001b[39;00m _DTypeLikeVoid,\n\u001b[1;32m    116\u001b[0m     _DTypeLikeStr \u001b[38;5;28;01mas\u001b[39;00m _DTypeLikeStr,\n\u001b[1;32m    117\u001b[0m     _DTypeLikeBytes \u001b[38;5;28;01mas\u001b[39;00m _DTypeLikeBytes,\n\u001b[1;32m    118\u001b[0m     _DTypeLikeComplex_co \u001b[38;5;28;01mas\u001b[39;00m _DTypeLikeComplex_co,\n\u001b[1;32m    119\u001b[0m )\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_array_like\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    121\u001b[0m     NDArray \u001b[38;5;28;01mas\u001b[39;00m NDArray,\n\u001b[1;32m    122\u001b[0m     ArrayLike \u001b[38;5;28;01mas\u001b[39;00m ArrayLike,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    145\u001b[0m     _UnknownType \u001b[38;5;28;01mas\u001b[39;00m _UnknownType,\n\u001b[1;32m    146\u001b[0m )\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ufunc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    149\u001b[0m     _UFunc_Nin1_Nout1 \u001b[38;5;28;01mas\u001b[39;00m _UFunc_Nin1_Nout1,\n\u001b[1;32m    150\u001b[0m     _UFunc_Nin2_Nout1 \u001b[38;5;28;01mas\u001b[39;00m _UFunc_Nin2_Nout1,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    153\u001b[0m     _GUFunc_Nin2_Nout1 \u001b[38;5;28;01mas\u001b[39;00m _GUFunc_Nin2_Nout1,\n\u001b[1;32m    154\u001b[0m )\n",
            "File \u001b[0;32m~/MIC/2_Micro/.venv/lib/python3.10/site-packages/numpy/_typing/_dtype_like.py:194\u001b[0m\n\u001b[1;32m    162\u001b[0m _DTypeLikeInt: TypeAlias \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28mtype\u001b[39m[\u001b[38;5;28mint\u001b[39m]\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mtype\u001b[39m[np\u001b[38;5;241m.\u001b[39msignedinteger[Any]]\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;241m|\u001b[39m _IntCodes\n\u001b[1;32m    178\u001b[0m )\n\u001b[1;32m    179\u001b[0m _DTypeLikeFloat: TypeAlias \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28mtype\u001b[39m[\u001b[38;5;28mfloat\u001b[39m]\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mtype\u001b[39m[np\u001b[38;5;241m.\u001b[39mfloating[Any]]\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;241m|\u001b[39m _LongDoubleCodes\n\u001b[1;32m    191\u001b[0m )\n\u001b[1;32m    192\u001b[0m _DTypeLikeComplex: TypeAlias \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28mtype\u001b[39m[\u001b[38;5;28mcomplex\u001b[39m]\n\u001b[0;32m--> 194\u001b[0m     \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mtype\u001b[39m[\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomplexfloating\u001b[49m\u001b[43m[\u001b[49m\u001b[43mAny\u001b[49m\u001b[43m]\u001b[49m]\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;241m|\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype[np\u001b[38;5;241m.\u001b[39mcomplexfloating[Any]]\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;241m|\u001b[39m _SupportsDType[np\u001b[38;5;241m.\u001b[39mdtype[np\u001b[38;5;241m.\u001b[39mcomplexfloating[Any]]]\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;241m|\u001b[39m _Complex64Codes\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;241m|\u001b[39m _Complex128Codes\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;241m|\u001b[39m _CSingleCodes\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;241m|\u001b[39m _CDoubleCodes\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;241m|\u001b[39m _CLongDoubleCodes\n\u001b[1;32m    202\u001b[0m )\n\u001b[1;32m    203\u001b[0m _DTypeLikeDT64: TypeAlias \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28mtype\u001b[39m[np\u001b[38;5;241m.\u001b[39mtimedelta64]\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;241m|\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype[np\u001b[38;5;241m.\u001b[39mtimedelta64]\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;241m|\u001b[39m _SupportsDType[np\u001b[38;5;241m.\u001b[39mdtype[np\u001b[38;5;241m.\u001b[39mtimedelta64]]\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;241m|\u001b[39m _TD64Codes\n\u001b[1;32m    208\u001b[0m )\n\u001b[1;32m    209\u001b[0m _DTypeLikeTD64: TypeAlias \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28mtype\u001b[39m[np\u001b[38;5;241m.\u001b[39mdatetime64]\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;241m|\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype[np\u001b[38;5;241m.\u001b[39mdatetime64]\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;241m|\u001b[39m _SupportsDType[np\u001b[38;5;241m.\u001b[39mdtype[np\u001b[38;5;241m.\u001b[39mdatetime64]]\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;241m|\u001b[39m _DT64Codes\n\u001b[1;32m    214\u001b[0m )\n",
            "\u001b[0;31mTypeError\u001b[0m: Too few arguments for numpy.complexfloating"
          ]
        }
      ],
      "source": [
        "# Load\n",
        "complete_results = {}\n",
        "with pd.HDFStore(output_large / \"complete_results.h5\", \"r\") as store:\n",
        "    for key in store.keys():\n",
        "        # Remove leading '/' from key\n",
        "        clean_key = key.lstrip('/')\n",
        "        complete_results[clean_key] = store[key]\n",
        "\n",
        "classified_results= complete_results['classified_results']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Root level keys: ['classified_results', 'increasing_markers', 'integrated_results', 'inverse_markers', 'pattern_data', 'prioritized_markers', 'statistical_results']\n",
            "classified_results <class 'h5py._hl.group.Group'>\n",
            "  Group with keys: ['axis0', 'axis1', 'block0_items', 'block0_values', 'block1_items', 'block1_values', 'block2_items', 'block2_values', 'block3_items', 'block3_values']\n",
            "classified_results/axis0 <class 'h5py._hl.dataset.Dataset'>\n",
            "classified_results/axis1 <class 'h5py._hl.dataset.Dataset'>\n",
            "classified_results/block0_items <class 'h5py._hl.dataset.Dataset'>\n",
            "classified_results/block0_values <class 'h5py._hl.dataset.Dataset'>\n",
            "classified_results/block1_items <class 'h5py._hl.dataset.Dataset'>\n",
            "classified_results/block1_values <class 'h5py._hl.dataset.Dataset'>\n",
            "classified_results/block2_items <class 'h5py._hl.dataset.Dataset'>\n",
            "classified_results/block2_values <class 'h5py._hl.dataset.Dataset'>\n",
            "classified_results/block3_items <class 'h5py._hl.dataset.Dataset'>\n",
            "classified_results/block3_values <class 'h5py._hl.dataset.Dataset'>\n",
            "increasing_markers <class 'h5py._hl.group.Group'>\n",
            "  Group with keys: ['axis0', 'axis1', 'block0_items', 'block0_values', 'block1_items', 'block1_values', 'block2_items', 'block2_values', 'block3_items', 'block3_values']\n",
            "increasing_markers/axis0 <class 'h5py._hl.dataset.Dataset'>\n",
            "increasing_markers/axis1 <class 'h5py._hl.dataset.Dataset'>\n",
            "increasing_markers/block0_items <class 'h5py._hl.dataset.Dataset'>\n",
            "increasing_markers/block0_values <class 'h5py._hl.dataset.Dataset'>\n",
            "increasing_markers/block1_items <class 'h5py._hl.dataset.Dataset'>\n",
            "increasing_markers/block1_values <class 'h5py._hl.dataset.Dataset'>\n",
            "increasing_markers/block2_items <class 'h5py._hl.dataset.Dataset'>\n",
            "increasing_markers/block2_values <class 'h5py._hl.dataset.Dataset'>\n",
            "increasing_markers/block3_items <class 'h5py._hl.dataset.Dataset'>\n",
            "increasing_markers/block3_values <class 'h5py._hl.dataset.Dataset'>\n",
            "integrated_results <class 'h5py._hl.group.Group'>\n",
            "  Group with keys: ['axis0', 'axis1', 'block0_items', 'block0_values', 'block1_items', 'block1_values', 'block2_items', 'block2_values', 'block3_items', 'block3_values']\n",
            "integrated_results/axis0 <class 'h5py._hl.dataset.Dataset'>\n",
            "integrated_results/axis1 <class 'h5py._hl.dataset.Dataset'>\n",
            "integrated_results/block0_items <class 'h5py._hl.dataset.Dataset'>\n",
            "integrated_results/block0_values <class 'h5py._hl.dataset.Dataset'>\n",
            "integrated_results/block1_items <class 'h5py._hl.dataset.Dataset'>\n",
            "integrated_results/block1_values <class 'h5py._hl.dataset.Dataset'>\n",
            "integrated_results/block2_items <class 'h5py._hl.dataset.Dataset'>\n",
            "integrated_results/block2_values <class 'h5py._hl.dataset.Dataset'>\n",
            "integrated_results/block3_items <class 'h5py._hl.dataset.Dataset'>\n",
            "integrated_results/block3_values <class 'h5py._hl.dataset.Dataset'>\n",
            "inverse_markers <class 'h5py._hl.group.Group'>\n",
            "  Group with keys: ['axis0', 'axis1', 'block0_items', 'block0_values', 'block1_items', 'block1_values', 'block2_items', 'block2_values', 'block3_items', 'block3_values']\n",
            "inverse_markers/axis0 <class 'h5py._hl.dataset.Dataset'>\n",
            "inverse_markers/axis1 <class 'h5py._hl.dataset.Dataset'>\n",
            "inverse_markers/block0_items <class 'h5py._hl.dataset.Dataset'>\n",
            "inverse_markers/block0_values <class 'h5py._hl.dataset.Dataset'>\n",
            "inverse_markers/block1_items <class 'h5py._hl.dataset.Dataset'>\n",
            "inverse_markers/block1_values <class 'h5py._hl.dataset.Dataset'>\n",
            "inverse_markers/block2_items <class 'h5py._hl.dataset.Dataset'>\n",
            "inverse_markers/block2_values <class 'h5py._hl.dataset.Dataset'>\n",
            "inverse_markers/block3_items <class 'h5py._hl.dataset.Dataset'>\n",
            "inverse_markers/block3_values <class 'h5py._hl.dataset.Dataset'>\n",
            "pattern_data <class 'h5py._hl.group.Group'>\n",
            "  Group with keys: ['axis0', 'axis1', 'block0_items', 'block0_values', 'block1_items', 'block1_values', 'block2_items', 'block2_values']\n",
            "pattern_data/axis0 <class 'h5py._hl.dataset.Dataset'>\n",
            "pattern_data/axis1 <class 'h5py._hl.dataset.Dataset'>\n",
            "pattern_data/block0_items <class 'h5py._hl.dataset.Dataset'>\n",
            "pattern_data/block0_values <class 'h5py._hl.dataset.Dataset'>\n",
            "pattern_data/block1_items <class 'h5py._hl.dataset.Dataset'>\n",
            "pattern_data/block1_values <class 'h5py._hl.dataset.Dataset'>\n",
            "pattern_data/block2_items <class 'h5py._hl.dataset.Dataset'>\n",
            "pattern_data/block2_values <class 'h5py._hl.dataset.Dataset'>\n",
            "prioritized_markers <class 'h5py._hl.group.Group'>\n",
            "  Group with keys: ['axis0', 'axis1', 'block0_items', 'block0_values', 'block1_items', 'block1_values', 'block2_items', 'block2_values', 'block3_items', 'block3_values']\n",
            "prioritized_markers/axis0 <class 'h5py._hl.dataset.Dataset'>\n",
            "prioritized_markers/axis1 <class 'h5py._hl.dataset.Dataset'>\n",
            "prioritized_markers/block0_items <class 'h5py._hl.dataset.Dataset'>\n",
            "prioritized_markers/block0_values <class 'h5py._hl.dataset.Dataset'>\n",
            "prioritized_markers/block1_items <class 'h5py._hl.dataset.Dataset'>\n",
            "prioritized_markers/block1_values <class 'h5py._hl.dataset.Dataset'>\n",
            "prioritized_markers/block2_items <class 'h5py._hl.dataset.Dataset'>\n",
            "prioritized_markers/block2_values <class 'h5py._hl.dataset.Dataset'>\n",
            "prioritized_markers/block3_items <class 'h5py._hl.dataset.Dataset'>\n",
            "prioritized_markers/block3_values <class 'h5py._hl.dataset.Dataset'>\n",
            "statistical_results <class 'h5py._hl.group.Group'>\n",
            "  Group with keys: ['axis0', 'axis1', 'block0_items', 'block0_values', 'block1_items', 'block1_values', 'block2_items', 'block2_values', 'block3_items', 'block3_values']\n",
            "statistical_results/axis0 <class 'h5py._hl.dataset.Dataset'>\n",
            "statistical_results/axis1 <class 'h5py._hl.dataset.Dataset'>\n",
            "statistical_results/block0_items <class 'h5py._hl.dataset.Dataset'>\n",
            "statistical_results/block0_values <class 'h5py._hl.dataset.Dataset'>\n",
            "statistical_results/block1_items <class 'h5py._hl.dataset.Dataset'>\n",
            "statistical_results/block1_values <class 'h5py._hl.dataset.Dataset'>\n",
            "statistical_results/block2_items <class 'h5py._hl.dataset.Dataset'>\n",
            "statistical_results/block2_values <class 'h5py._hl.dataset.Dataset'>\n",
            "statistical_results/block3_items <class 'h5py._hl.dataset.Dataset'>\n",
            "statistical_results/block3_values <class 'h5py._hl.dataset.Dataset'>\n"
          ]
        }
      ],
      "source": [
        "import h5py\n",
        "\n",
        "# Try to open the file and explore its structure\n",
        "with h5py.File(output_large /'complete_results.h5', 'r') as f:\n",
        "    # Print the keys (dataset names) at the root level\n",
        "    print(\"Root level keys:\", list(f.keys()))\n",
        "    \n",
        "    # Explore the structure recursively\n",
        "    def print_structure(name, obj):\n",
        "        print(name, type(obj))\n",
        "        if isinstance(obj, h5py.Group):\n",
        "            print(\"  Group with keys:\", list(obj.keys()))\n",
        "    \n",
        "    # Apply the function to all items in the file\n",
        "    f.visititems(print_structure)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keys in prioritized_markers group: ['axis0', 'axis1', 'block0_items', 'block0_values', 'block1_items', 'block1_values', 'block2_items', 'block2_values', 'block3_items', 'block3_values']\n",
            "(65, 28227)\n",
            "[4911, 5451, 5640, 5810, 7521]\n"
          ]
        }
      ],
      "source": [
        "import h5py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def read_dataframe_from_h5(file_path, key):\n",
        "    \"\"\"\n",
        "    Read a specific DataFrame from an HDF5 file with improved type handling.\n",
        "    \"\"\"\n",
        "    with h5py.File(file_path, 'r') as f:\n",
        "        # Get the group\n",
        "        group = f[key]\n",
        "        \n",
        "        # Print the keys to understand the structure\n",
        "        print(f\"Keys in {key} group:\", list(group.keys()))\n",
        "        \n",
        "        # Handle both string and numeric indices\n",
        "        try:\n",
        "            # Try to extract axes information\n",
        "            if 'axis0' in group:\n",
        "                # Check if they're strings or numbers\n",
        "                axis0_data = group['axis0'][:]\n",
        "                if isinstance(axis0_data[0], (bytes, np.bytes_)):\n",
        "                    axis0 = [x.decode('utf-8') for x in axis0_data]\n",
        "                else:\n",
        "                    axis0 = axis0_data.tolist()\n",
        "            else:\n",
        "                axis0 = None\n",
        "                \n",
        "            if 'axis1' in group:\n",
        "                axis1_data = group['axis1'][:]\n",
        "                if isinstance(axis1_data[0], (bytes, np.bytes_)):\n",
        "                    axis1 = [x.decode('utf-8') for x in axis1_data]\n",
        "                else:\n",
        "                    axis1 = axis1_data.tolist()\n",
        "            else:\n",
        "                # If no axis1, try to infer columns from blocks\n",
        "                col_items = []\n",
        "                for k in group.keys():\n",
        "                    if k.endswith('_items') and k != 'axis0_items':\n",
        "                        items_data = group[k][:]\n",
        "                        if isinstance(items_data[0], (bytes, np.bytes_)):\n",
        "                            col_items.extend([x.decode('utf-8') for x in items_data])\n",
        "                        else:\n",
        "                            col_items.extend(items_data.tolist())\n",
        "                axis1 = col_items if col_items else None\n",
        "                \n",
        "            # If we can't determine both axes, try a different approach\n",
        "            if axis0 is None or axis1 is None:\n",
        "                # Look for values directly\n",
        "                data_dict = {}\n",
        "                for k in group.keys():\n",
        "                    if k.endswith('_values'):\n",
        "                        values = group[k][:]\n",
        "                        items_key = k.replace('_values', '_items')\n",
        "                        if items_key in group:\n",
        "                            items_data = group[items_key][:]\n",
        "                            if isinstance(items_data[0], (bytes, np.bytes_)):\n",
        "                                items = [x.decode('utf-8') for x in items_data]\n",
        "                            else:\n",
        "                                items = items_data.tolist()\n",
        "                            for i, item in enumerate(items):\n",
        "                                if values.ndim > 1:\n",
        "                                    data_dict[item] = values[:, i]\n",
        "                                else:\n",
        "                                    data_dict[item] = values\n",
        "                \n",
        "                # Create DataFrame from the dictionary\n",
        "                df = pd.DataFrame(data_dict)\n",
        "                return df\n",
        "            \n",
        "            # Create a DataFrame with the axes\n",
        "            df = pd.DataFrame(index=axis0, columns=axis1)\n",
        "            \n",
        "            # Fill with values from blocks\n",
        "            for block_key in [k for k in group.keys() if k.endswith('_values')]:\n",
        "                items_key = block_key.replace('_values', '_items')\n",
        "                if items_key in group:\n",
        "                    items_data = group[items_key][:]\n",
        "                    if isinstance(items_data[0], (bytes, np.bytes_)):\n",
        "                        items = [x.decode('utf-8') for x in items_data]\n",
        "                    else:\n",
        "                        items = items_data.tolist()\n",
        "                    \n",
        "                    values = group[block_key][:]\n",
        "                    \n",
        "                    # Assign values based on items\n",
        "                    for i, item in enumerate(items):\n",
        "                        if item in axis1:\n",
        "                            col_idx = axis1.index(item)\n",
        "                            if values.ndim > 1:\n",
        "                                df.iloc[:, col_idx] = values[:, i]\n",
        "                            else:\n",
        "                                df.iloc[:, col_idx] = values\n",
        "            \n",
        "            return df\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"Error with standard approach: {e}\")\n",
        "            print(\"Trying alternative method...\")\n",
        "            \n",
        "            # Alternative method: try to extract all data at once\n",
        "            try:\n",
        "                # Get values and column names\n",
        "                data = {}\n",
        "                \n",
        "                # Process each block\n",
        "                for block_key in [k for k in group.keys() if k.endswith('_values')]:\n",
        "                    values = group[block_key][:]\n",
        "                    \n",
        "                    # Get column names for this block\n",
        "                    items_key = block_key.replace('_values', '_items')\n",
        "                    if items_key in group:\n",
        "                        items_data = group[items_key][:]\n",
        "                        if isinstance(items_data[0], (bytes, np.bytes_)):\n",
        "                            items = [x.decode('utf-8') for x in items_data]\n",
        "                        else:\n",
        "                            items = items_data.tolist()\n",
        "                            \n",
        "                        # Add to data dictionary\n",
        "                        if values.ndim == 1:\n",
        "                            data[items[0]] = values\n",
        "                        else:\n",
        "                            for i, item in enumerate(items):\n",
        "                                data[item] = values[:, i]\n",
        "                \n",
        "                # Create DataFrame\n",
        "                df = pd.DataFrame(data)\n",
        "                return df\n",
        "            \n",
        "            except Exception as e2:\n",
        "                print(f\"Alternative method failed: {e2}\")\n",
        "                # Last resort: just extract metadata\n",
        "                return pd.DataFrame({'error': [f\"Failed to read {key}: {e}, {e2}\"]})\n",
        "\n",
        "# Example usage:\n",
        "try:\n",
        "    # Full path to your file\n",
        "    file_path = output_large / 'complete_results.h5'\n",
        "    \n",
        "    prioritized_markers = read_dataframe_from_h5(file_path, 'prioritized_markers')\n",
        "    print(prioritized_markers.shape)\n",
        "    print(prioritized_markers.columns.tolist()[:5])  # Show first 5 columns\n",
        "except Exception as e:\n",
        "    print(f\"Overall error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uo8Tc3_YCFb"
      },
      "source": [
        "### Reading the Reports on Pathway Classification from Specificity Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iW3dhr_ITcyI",
        "outputId": "4a67d255-0174-4158-81ad-d1a45a140eae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Specificity report saved to /content/drive/MyDrive/MIC/output_large/classification_report.xlsx\n"
          ]
        }
      ],
      "source": [
        "classification_report_path = output_large / \"classification_report.xlsx\"\n",
        "classification_summary = generate_specificity_report(classified_results, classification_report_path)\n",
        "pathway_classification_path = output_large / \"pathway_classification_report.xlsx\"\n",
        "pathway_classification = pd.read_excel(pathway_classification_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPvtOi9N2F0s",
        "outputId": "37318d4b-bc36-4169-b1df-f0e8e523eafa",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'total_pathways': 51152,\n",
              " 'classification_counts': {'niche-specific': 36096,\n",
              "  'mixed': 14323,\n",
              "  'unknown': 733},\n",
              " 'universal_pathway_counts': {'6.1. Amino Acid Biosynthesis': 4448,\n",
              "  '4.4. Lipid Membrane Synthesis': 1012,\n",
              "  '3.3. Translation': 2287,\n",
              "  '1.2. Gluconeogenesis': 1795,\n",
              "  '1.1. Glycolysis': 1795,\n",
              "  '1.4. Krebs/TCA Cycle': 1110,\n",
              "  '1.3. Pentose Phosphate Pathway': 1415,\n",
              "  '4.3. Cell Wall Maintenance': 1310,\n",
              "  '3.1. DNA Replication': 296,\n",
              "  '3.2. Transcription': 595,\n",
              "  '5.4. SOS DNA Repair System': 1826,\n",
              "  '3.5. Proteasome System': 9},\n",
              " 'niche_specific_examples': ['valine, leucine and isoleucine degradation; metabolic pathways; amino acid related enzymes [br:ko01007]',\n",
              "  'terpenoid backbone biosynthesis; metabolic pathways; biosynthesis of secondary metabolites',\n",
              "  'starch and sucrose metabolism; metabolic pathways; biosynthesis of secondary metabolites; exosome [br:ko04147]',\n",
              "  'fructose and mannose metabolism; metabolic pathways; phosphotransferase system (pts)',\n",
              "  'carotenoid biosynthesis; metabolic pathways; biosynthesis of secondary metabolites; prenyltransferases [br:ko01006]',\n",
              "  'chromosome and associated proteins [br:ko03036]',\n",
              "  'pantothenate and coa biosynthesis; metabolic pathways; biosynthesis of secondary metabolites',\n",
              "  'phenylalanine metabolism; metabolic pathways; microbial metabolism in diverse environments',\n",
              "  'thiamine metabolism; metabolic pathways',\n",
              "  'enzymes with ec numbers',\n",
              "  'fatty acid biosynthesis; biotin metabolism; metabolic pathways; lipid biosynthesis proteins [br:ko01004]',\n",
              "  'valine, leucine and isoleucine degradation; butanoate metabolism; metabolic pathways',\n",
              "  'starch and sucrose metabolism; metabolic pathways',\n",
              "  'alanine, aspartate and glutamate metabolism; beta-alanine metabolism; propanoate metabolism; butanoate metabolism; metabolic pathways; microbial metabolism in diverse environments; amino acid related enzymes [br:ko01007]',\n",
              "  'benzoate degradation; metabolic pathways; microbial metabolism in diverse environments',\n",
              "  'vitamin b6 metabolism; metabolic pathways',\n",
              "  'purine metabolism; pyrimidine metabolism; nicotinate and nicotinamide metabolism; metabolic pathways; biosynthesis of secondary metabolites; glycosylphosphatidylinositol (gpi)-anchored proteins [br:ko00537]; enzymes with ec numbers',\n",
              "  'one carbon pool by folate; metabolic pathways; folate transport and metabolism',\n",
              "  'other carbon fixation pathways; metabolic pathways; microbial metabolism in diverse environments',\n",
              "  'methane metabolism']}"
            ]
          },
          "execution_count": 112,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "classification_summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RV62Z9oquSo3"
      },
      "source": [
        "Niche-specific pathways are the most common (25,762 entries), followed by mixed pathways (11,004), with only a small number of unknown classifications (323). The universal pathway counts provide valuable insights into which core metabolic functions are most represented:\n",
        "\n",
        "Amino Acid Biosynthesis (3,587) is the most common universal pathway Translation (1,583) is the second most common Glycolysis/Gluconeogenesis (1,426 each) are also highly represented SOS DNA Repair System (1,409) is surprisingly prevalent\n",
        "\n",
        "The niche-specific examples reveal many specialized metabolic pathways, including:\n",
        "\n",
        "Secondary metabolite biosynthesis Vitamin metabolism (thiamine, B6, riboflavin) Specialized carbon metabolism Several pathways involved in environmental adaptation\n",
        "\n",
        "This results are at first sight surprising since the top pathway reported by picrust2 results gave aerobic respiration(cytochrome c)as the dominant metabolic pathway. This could be explained by the calculation, whiles picrust output ranks pathways by total abundance across all samples, the classification_summary counts the number of unique genus-protein pairs associated with each universal pathway category. So aerobic respiration is the most abundant expressed pathway with high activity levels since is a high flux energy generating pathway. Whiles amino acid biosynthesis involves more distintic proteins or greater diversity of functions that actually comprises many different enzymes and reactions for creating an array of aminoacids."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "deHmNODSYPb5",
        "outputId": "78220577-ae46-43b4-b1cd-84661057cd86"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"pathway_classification\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"Classification\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"niche-specific\",\n          \"mixed\",\n          \"unknown\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 14861,\n        \"min\": 23,\n        \"max\": 28860,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          28860,\n          8206,\n          23\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Percentage\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.400688747133461,\n        \"min\": 0.0006201299576693898,\n        \"max\": 0.7781282860147214,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.7781282860147214,\n          0.2212515840276093,\n          0.0006201299576693898\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "pathway_classification"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-29661289-736d-46bd-b00f-4201346c7bb6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Classification</th>\n",
              "      <th>Count</th>\n",
              "      <th>Percentage</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>niche-specific</td>\n",
              "      <td>28860</td>\n",
              "      <td>0.778128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>mixed</td>\n",
              "      <td>8206</td>\n",
              "      <td>0.221252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>unknown</td>\n",
              "      <td>23</td>\n",
              "      <td>0.000620</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-29661289-736d-46bd-b00f-4201346c7bb6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-29661289-736d-46bd-b00f-4201346c7bb6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-29661289-736d-46bd-b00f-4201346c7bb6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-00d7b581-514b-4f52-b8e6-0422c7bfeed3\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-00d7b581-514b-4f52-b8e6-0422c7bfeed3')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-00d7b581-514b-4f52-b8e6-0422c7bfeed3 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_30e79906-39af-496f-aca4-7dca0abed838\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('pathway_classification')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_30e79906-39af-496f-aca4-7dca0abed838 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('pathway_classification');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "   Classification  Count  Percentage\n",
              "0  niche-specific  28860    0.778128\n",
              "1           mixed   8206    0.221252\n",
              "2         unknown     23    0.000620"
            ]
          },
          "execution_count": 113,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pathway_classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        },
        "id": "ziaiXDQUlY2V",
        "outputId": "5c3e0fe1-dd43-4979-a571-3877c937f60d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "classified_results"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-813b14b4-6a76-43ed-8dff-28fee008dfff\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Genus</th>\n",
              "      <th>protein_name</th>\n",
              "      <th>count_1</th>\n",
              "      <th>count_2</th>\n",
              "      <th>count_3</th>\n",
              "      <th>mean_1</th>\n",
              "      <th>mean_2</th>\n",
              "      <th>mean_3</th>\n",
              "      <th>median_1</th>\n",
              "      <th>median_2</th>\n",
              "      <th>...</th>\n",
              "      <th>std_cat3</th>\n",
              "      <th>samples_cat3</th>\n",
              "      <th>effect_size_1_to_2</th>\n",
              "      <th>effect_size_2_to_3</th>\n",
              "      <th>p_adjusted</th>\n",
              "      <th>significant</th>\n",
              "      <th>pathway_classification</th>\n",
              "      <th>universal_pathways_detected</th>\n",
              "      <th>niche_specific_pathways</th>\n",
              "      <th>corr</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Acetobacterium</td>\n",
              "      <td>-2-methylmalate dehydratase; citraconate hydra...</td>\n",
              "      <td>13.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.007914</td>\n",
              "      <td>0.020073</td>\n",
              "      <td>0.013688</td>\n",
              "      <td>0.000459</td>\n",
              "      <td>0.004881</td>\n",
              "      <td>...</td>\n",
              "      <td>0.013571</td>\n",
              "      <td>21</td>\n",
              "      <td>0.168781</td>\n",
              "      <td>-0.189809</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "      <td>mixed</td>\n",
              "      <td>6.1. Amino Acid Biosynthesis</td>\n",
              "      <td>valine, leucine and ; c5-branched dibasic acid...</td>\n",
              "      <td>0.474688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Acetobacterium</td>\n",
              "      <td>-3-amino-2-methylpropionate transaminase; L-3-...</td>\n",
              "      <td>13.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.114986</td>\n",
              "      <td>0.309879</td>\n",
              "      <td>0.199574</td>\n",
              "      <td>0.021473</td>\n",
              "      <td>0.259967</td>\n",
              "      <td>...</td>\n",
              "      <td>0.186917</td>\n",
              "      <td>21</td>\n",
              "      <td>0.304434</td>\n",
              "      <td>-0.331781</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "      <td>niche-specific</td>\n",
              "      <td></td>\n",
              "      <td>valine, leucine and isoleucine degradation; me...</td>\n",
              "      <td>0.432768</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Acetobacterium</td>\n",
              "      <td>-4-hydroxy-3-methylbut-2-enyl-diphosphate synt...</td>\n",
              "      <td>13.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.006940</td>\n",
              "      <td>0.017456</td>\n",
              "      <td>0.012201</td>\n",
              "      <td>0.000542</td>\n",
              "      <td>0.005257</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010797</td>\n",
              "      <td>21</td>\n",
              "      <td>0.190106</td>\n",
              "      <td>-0.216180</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "      <td>niche-specific</td>\n",
              "      <td></td>\n",
              "      <td>terpenoid backbone biosynthesis; metabolic pat...</td>\n",
              "      <td>0.500270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Acetobacterium</td>\n",
              "      <td>1,4-alpha-glucan branching enzyme GlgB -glucan...</td>\n",
              "      <td>13.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.007748</td>\n",
              "      <td>0.020348</td>\n",
              "      <td>0.016439</td>\n",
              "      <td>0.000824</td>\n",
              "      <td>0.008231</td>\n",
              "      <td>...</td>\n",
              "      <td>0.015593</td>\n",
              "      <td>21</td>\n",
              "      <td>0.210203</td>\n",
              "      <td>-0.167399</td>\n",
              "      <td>0.748214</td>\n",
              "      <td>False</td>\n",
              "      <td>niche-specific</td>\n",
              "      <td></td>\n",
              "      <td>starch and sucrose metabolism; metabolic pathw...</td>\n",
              "      <td>0.673780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Acetobacterium</td>\n",
              "      <td>1--5-[methylideneamino] imidazole-4-carboxamid...</td>\n",
              "      <td>13.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.008975</td>\n",
              "      <td>0.028148</td>\n",
              "      <td>0.013812</td>\n",
              "      <td>0.000566</td>\n",
              "      <td>0.005811</td>\n",
              "      <td>...</td>\n",
              "      <td>0.012307</td>\n",
              "      <td>21</td>\n",
              "      <td>0.211607</td>\n",
              "      <td>-0.251869</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "      <td>mixed</td>\n",
              "      <td>6.1. Amino Acid Biosynthesis</td>\n",
              "      <td>histidine metabolism; metabolic pathways; bios...</td>\n",
              "      <td>0.242560</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 54 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-813b14b4-6a76-43ed-8dff-28fee008dfff')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-813b14b4-6a76-43ed-8dff-28fee008dfff button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-813b14b4-6a76-43ed-8dff-28fee008dfff');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-4ef83ef6-dc10-4bf2-93f0-7467e5a37b36\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4ef83ef6-dc10-4bf2-93f0-7467e5a37b36')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-4ef83ef6-dc10-4bf2-93f0-7467e5a37b36 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "            Genus                                       protein_name  count_1  \\\n",
              "0  Acetobacterium  -2-methylmalate dehydratase; citraconate hydra...     13.0   \n",
              "1  Acetobacterium  -3-amino-2-methylpropionate transaminase; L-3-...     13.0   \n",
              "2  Acetobacterium  -4-hydroxy-3-methylbut-2-enyl-diphosphate synt...     13.0   \n",
              "3  Acetobacterium  1,4-alpha-glucan branching enzyme GlgB -glucan...     13.0   \n",
              "4  Acetobacterium  1--5-[methylideneamino] imidazole-4-carboxamid...     13.0   \n",
              "\n",
              "   count_2  count_3    mean_1    mean_2    mean_3  median_1  median_2  ...  \\\n",
              "0     17.0      9.0  0.007914  0.020073  0.013688  0.000459  0.004881  ...   \n",
              "1     17.0      9.0  0.114986  0.309879  0.199574  0.021473  0.259967  ...   \n",
              "2     17.0      9.0  0.006940  0.017456  0.012201  0.000542  0.005257  ...   \n",
              "3     17.0      9.0  0.007748  0.020348  0.016439  0.000824  0.008231  ...   \n",
              "4     17.0      9.0  0.008975  0.028148  0.013812  0.000566  0.005811  ...   \n",
              "\n",
              "   std_cat3  samples_cat3  effect_size_1_to_2  effect_size_2_to_3 p_adjusted  \\\n",
              "0  0.013571            21            0.168781           -0.189809        NaN   \n",
              "1  0.186917            21            0.304434           -0.331781        NaN   \n",
              "2  0.010797            21            0.190106           -0.216180        NaN   \n",
              "3  0.015593            21            0.210203           -0.167399   0.748214   \n",
              "4  0.012307            21            0.211607           -0.251869        NaN   \n",
              "\n",
              "   significant  pathway_classification   universal_pathways_detected  \\\n",
              "0        False                   mixed  6.1. Amino Acid Biosynthesis   \n",
              "1        False          niche-specific                                 \n",
              "2        False          niche-specific                                 \n",
              "3        False          niche-specific                                 \n",
              "4        False                   mixed  6.1. Amino Acid Biosynthesis   \n",
              "\n",
              "                             niche_specific_pathways      corr  \n",
              "0  valine, leucine and ; c5-branched dibasic acid...  0.474688  \n",
              "1  valine, leucine and isoleucine degradation; me...  0.432768  \n",
              "2  terpenoid backbone biosynthesis; metabolic pat...  0.500270  \n",
              "3  starch and sucrose metabolism; metabolic pathw...  0.673780  \n",
              "4  histidine metabolism; metabolic pathways; bios...  0.242560  \n",
              "\n",
              "[5 rows x 54 columns]"
            ]
          },
          "execution_count": 110,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "classified_results.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "cQgbzN5c2F0s",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Calling the report on\n",
        "report_path =output_large/ \"corrosion_markers_report.xlsx\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "rmR7f3AOP0tn",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "top_markers = pd.read_excel(report_path, sheet_name=\"Top Corrosion Markers\", engine=\"openpyxl\")\n",
        "visualize = pd.read_excel(report_path, sheet_name=\"Visualization Data\", engine=\"openpyxl\")\n",
        "scores = pd.read_excel(report_path, sheet_name=\"Detailed Scores\", engine=\"openpyxl\")\n",
        "mechanisms = pd.read_excel(report_path, sheet_name=\"Mechanisms Focus\", engine=\"openpyxl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3tpFO3N2F0s",
        "outputId": "7d762c03-cb76-4d26-a66c-9e23d7dbf628",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['Genus', 'Protein Name', 'Corrosion Mechanisms', 'Relevance Score',\n",
              "       'Mechanism'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 116,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mechanisms.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ynN-YKf2F0t",
        "outputId": "39f82dd8-370c-4d9d-a03f-413fc32258b3",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['Genus', 'Protein Name', 'Pathways', 'Corrosion Mechanisms',\n",
              "       'Metals Involved', 'Pathway Classification', 'Niche-Specific Pathways',\n",
              "       'mean_1', 'mean_2', 'mean_3', 'mean_cat1', 'mean_cat2', 'mean_cat3'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 117,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "visualize.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "id": "xLo5kiZg2F0t",
        "outputId": "aa49ca23-8586-4ebd-e589-f8ecac5f2da5",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"scores\",\n  \"rows\": 500,\n  \"fields\": [\n    {\n      \"column\": \"Genus\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"Azospira\",\n          \"Bacillus\",\n          \"Halomonas\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Protein Name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 484,\n        \"samples\": [\n          \"3-mercaptopyruvate sulfurtransferase___0\",\n          \"Amidophosphoribosyltransferase___0\",\n          \"Transferred to 7.1.2.2___0\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Total Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 10,\n        \"max\": 10,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          10\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Correlation Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.1430556879670268,\n        \"min\": 0.0,\n        \"max\": 2.0,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Metals Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.5857834792558908,\n        \"min\": 0.5,\n        \"max\": 2.5,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          2.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Pathways Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2888562199982264,\n        \"min\": 0.0,\n        \"max\": 2.6,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Hierarchy Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.11026948225794364,\n        \"min\": 0.0,\n        \"max\": 1.4,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.8\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Mechanisms Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.079512452113704,\n        \"min\": 0.0,\n        \"max\": 4.0,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          3.7\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Effect Size Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.42984827448549395,\n        \"min\": 0.5,\n        \"max\": 2.0,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Pathway Specificity Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.4747438925642475,\n        \"min\": 0.5,\n        \"max\": 2.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          2.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Corrosion Relevance Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4.908212756689531,\n        \"min\": 1.0,\n        \"max\": 24.3,\n        \"num_unique_values\": 64,\n        \"samples\": [\n          12.3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"P-Value\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.002733819198607476,\n        \"min\": 0.00261618588349748,\n        \"max\": 0.01210760470246854,\n        \"num_unique_values\": 119,\n        \"samples\": [\n          0.01034155826741369\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Adjusted P-Value\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6.806926359059046e-16,\n        \"min\": 0.108875187117088,\n        \"max\": 0.108875187117088,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.108875187117088\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Statistically Significant\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          false\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "scores"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-e72d4570-ee4e-44ab-9de1-f082db0666bd\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Genus</th>\n",
              "      <th>Protein Name</th>\n",
              "      <th>Total Score</th>\n",
              "      <th>Correlation Score</th>\n",
              "      <th>Metals Score</th>\n",
              "      <th>Pathways Score</th>\n",
              "      <th>Hierarchy Score</th>\n",
              "      <th>Mechanisms Score</th>\n",
              "      <th>Effect Size Score</th>\n",
              "      <th>Pathway Specificity Score</th>\n",
              "      <th>Corrosion Relevance Score</th>\n",
              "      <th>P-Value</th>\n",
              "      <th>Adjusted P-Value</th>\n",
              "      <th>Statistically Significant</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Azospira</td>\n",
              "      <td>5-carboxymethyl-2-hydroxymuconate Delta-isomer...</td>\n",
              "      <td>10</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>2.0</td>\n",
              "      <td>9.6</td>\n",
              "      <td>0.007990</td>\n",
              "      <td>0.108875</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Azospira</td>\n",
              "      <td>propionate CoA-transferase; propionate coenzym...</td>\n",
              "      <td>10</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>2.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>0.007402</td>\n",
              "      <td>0.108875</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Bacillus</td>\n",
              "      <td>Beta-lactamase family protein___0</td>\n",
              "      <td>10</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.5</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.9</td>\n",
              "      <td>0.011441</td>\n",
              "      <td>0.108875</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Bacillus</td>\n",
              "      <td>Kynurenine formamidase___0</td>\n",
              "      <td>10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.2</td>\n",
              "      <td>0.5</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.1</td>\n",
              "      <td>0.005744</td>\n",
              "      <td>0.108875</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Brachybacterium</td>\n",
              "      <td>23S rRNA /16S -methyltransferase___0</td>\n",
              "      <td>10</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>12.9</td>\n",
              "      <td>0.011060</td>\n",
              "      <td>0.108875</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e72d4570-ee4e-44ab-9de1-f082db0666bd')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e72d4570-ee4e-44ab-9de1-f082db0666bd button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e72d4570-ee4e-44ab-9de1-f082db0666bd');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-361b3c0c-7825-46c4-a895-1f396e960ea1\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-361b3c0c-7825-46c4-a895-1f396e960ea1')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-361b3c0c-7825-46c4-a895-1f396e960ea1 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "             Genus                                       Protein Name  \\\n",
              "0         Azospira  5-carboxymethyl-2-hydroxymuconate Delta-isomer...   \n",
              "1         Azospira  propionate CoA-transferase; propionate coenzym...   \n",
              "2         Bacillus                  Beta-lactamase family protein___0   \n",
              "3         Bacillus                         Kynurenine formamidase___0   \n",
              "4  Brachybacterium               23S rRNA /16S -methyltransferase___0   \n",
              "\n",
              "   Total Score  Correlation Score  Metals Score  Pathways Score  \\\n",
              "0           10                1.0           1.5             0.6   \n",
              "1           10                1.0           2.0             0.6   \n",
              "2           10                1.5           0.5             0.0   \n",
              "3           10                0.0           0.5             0.0   \n",
              "4           10                2.0           2.0             0.0   \n",
              "\n",
              "   Hierarchy Score  Mechanisms Score  Effect Size Score  \\\n",
              "0              0.0               4.0                0.5   \n",
              "1              0.0               4.0                0.5   \n",
              "2              0.0               0.7                0.5   \n",
              "3              0.0               2.2                0.5   \n",
              "4              0.0               4.0                0.5   \n",
              "\n",
              "   Pathway Specificity Score  Corrosion Relevance Score   P-Value  \\\n",
              "0                        2.0                        9.6  0.007990   \n",
              "1                        2.0                       18.0  0.007402   \n",
              "2                        2.0                        3.9  0.011441   \n",
              "3                        2.0                        5.1  0.005744   \n",
              "4                        1.0                       12.9  0.011060   \n",
              "\n",
              "   Adjusted P-Value  Statistically Significant  \n",
              "0          0.108875                      False  \n",
              "1          0.108875                      False  \n",
              "2          0.108875                      False  \n",
              "3          0.108875                      False  \n",
              "4          0.108875                      False  "
            ]
          },
          "execution_count": 118,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "scores.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "rq6xw6nW2F0t",
        "outputId": "554c5438-e530-4a54-d86a-d8bfb85aba1e",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"mechanisms\",\n  \"rows\": 1833,\n  \"fields\": [\n    {\n      \"column\": \"Genus\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"Azospira\",\n          \"Bacillus\",\n          \"Halomonas\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Protein Name\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 484,\n        \"samples\": [\n          \"3-mercaptopyruvate sulfurtransferase___0\",\n          \"Amidophosphoribosyltransferase___0\",\n          \"Transferred to 7.1.2.2___0\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Corrosion Mechanisms\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 16,\n        \"samples\": [\n          \"direct_eet; h2_consumption; acid_production\",\n          \"biofilm_formation; sulfur_metabolism; direct_eet; h2_consumption; acid_production\",\n          \"sulfur_metabolism; direct_eet; o2_consumption; h2_consumption; acid_production\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Relevance Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 10,\n        \"max\": 10,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          10\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Mechanism\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"direct_eet\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "mechanisms"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-cca1b6eb-8a0d-4c2a-ba8e-b2fe8f14b41d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Genus</th>\n",
              "      <th>Protein Name</th>\n",
              "      <th>Corrosion Mechanisms</th>\n",
              "      <th>Relevance Score</th>\n",
              "      <th>Mechanism</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Azospira</td>\n",
              "      <td>5-carboxymethyl-2-hydroxymuconate Delta-isomer...</td>\n",
              "      <td>direct_eet; h2_consumption; acid_production</td>\n",
              "      <td>10</td>\n",
              "      <td>direct_eet</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Azospira</td>\n",
              "      <td>5-carboxymethyl-2-hydroxymuconate Delta-isomer...</td>\n",
              "      <td>direct_eet; h2_consumption; acid_production</td>\n",
              "      <td>10</td>\n",
              "      <td>h2_consumption</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Azospira</td>\n",
              "      <td>5-carboxymethyl-2-hydroxymuconate Delta-isomer...</td>\n",
              "      <td>direct_eet; h2_consumption; acid_production</td>\n",
              "      <td>10</td>\n",
              "      <td>acid_production</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Azospira</td>\n",
              "      <td>propionate CoA-transferase; propionate coenzym...</td>\n",
              "      <td>biofilm_formation; sulfur_metabolism; direct_e...</td>\n",
              "      <td>10</td>\n",
              "      <td>biofilm_formation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Azospira</td>\n",
              "      <td>propionate CoA-transferase; propionate coenzym...</td>\n",
              "      <td>biofilm_formation; sulfur_metabolism; direct_e...</td>\n",
              "      <td>10</td>\n",
              "      <td>sulfur_metabolism</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cca1b6eb-8a0d-4c2a-ba8e-b2fe8f14b41d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-cca1b6eb-8a0d-4c2a-ba8e-b2fe8f14b41d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-cca1b6eb-8a0d-4c2a-ba8e-b2fe8f14b41d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-07927082-6651-4de0-ba80-96b8eecb082d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-07927082-6651-4de0-ba80-96b8eecb082d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-07927082-6651-4de0-ba80-96b8eecb082d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "      Genus                                       Protein Name  \\\n",
              "0  Azospira  5-carboxymethyl-2-hydroxymuconate Delta-isomer...   \n",
              "1  Azospira  5-carboxymethyl-2-hydroxymuconate Delta-isomer...   \n",
              "2  Azospira  5-carboxymethyl-2-hydroxymuconate Delta-isomer...   \n",
              "3  Azospira  propionate CoA-transferase; propionate coenzym...   \n",
              "4  Azospira  propionate CoA-transferase; propionate coenzym...   \n",
              "\n",
              "                                Corrosion Mechanisms  Relevance Score  \\\n",
              "0        direct_eet; h2_consumption; acid_production               10   \n",
              "1        direct_eet; h2_consumption; acid_production               10   \n",
              "2        direct_eet; h2_consumption; acid_production               10   \n",
              "3  biofilm_formation; sulfur_metabolism; direct_e...               10   \n",
              "4  biofilm_formation; sulfur_metabolism; direct_e...               10   \n",
              "\n",
              "           Mechanism  \n",
              "0         direct_eet  \n",
              "1     h2_consumption  \n",
              "2    acid_production  \n",
              "3  biofilm_formation  \n",
              "4  sulfur_metabolism  "
            ]
          },
          "execution_count": 119,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mechanisms.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Assuming you've loaded your HDF5 data:\n",
        "# with pd.HDFStore('complete_results.h5', 'r') as store:\n",
        "#     increasing_markers = store['increasing_markers']\n",
        "#     prioritized_markers = store['prioritized_markers']\n",
        "#     classified_results = store['classified_results']\n",
        "\n",
        "# 1. FUNCTIONAL LANDSCAPE HEATMAP\n",
        "def plot_functional_landscape(prioritized_markers, top_n=30):\n",
        "    \"\"\"\n",
        "    Creates a clustered heatmap showing functional categories across risk categories.\n",
        "    \"\"\"\n",
        "    # Select top markers by score\n",
        "    top_markers = prioritized_markers.sort_values('combined_score', ascending=False).head(top_n)\n",
        "    \n",
        "    # Create a pivot table of corrosion mechanisms vs risk categories\n",
        "    # Extract mechanisms into separate rows\n",
        "    mechanism_data = []\n",
        "    for _, row in top_markers.iterrows():\n",
        "        if isinstance(row['corrosion_mechanisms'], str) and row['corrosion_mechanisms']:\n",
        "            mechanisms = [m.strip() for m in row['corrosion_mechanisms'].split(';')]\n",
        "            for mech in mechanisms:\n",
        "                mechanism_data.append({\n",
        "                    'Genus': row['Genus'],\n",
        "                    'protein_name': row['protein_name'],\n",
        "                    'mechanism': mech,\n",
        "                    'Category 1': row['mean_cat1'],\n",
        "                    'Category 2': row['mean_cat2'],\n",
        "                    'Category 3': row['mean_cat3'],\n",
        "                    'combined_score': row['combined_score']\n",
        "                })\n",
        "    \n",
        "    # Convert to DataFrame\n",
        "    mech_df = pd.DataFrame(mechanism_data)\n",
        "    \n",
        "    # Create pivot table\n",
        "    pivot_data = mech_df.pivot_table(\n",
        "        index=['mechanism'], \n",
        "        columns=['Category 1', 'Category 2', 'Category 3'],\n",
        "        values='combined_score',\n",
        "        aggfunc='mean'\n",
        "    )\n",
        "    \n",
        "    # Set up the matplotlib figure\n",
        "    plt.figure(figsize=(14, 10))\n",
        "    \n",
        "    # Generate a custom colormap (blue to red)\n",
        "    cmap = LinearSegmentedColormap.from_list(\n",
        "        'BlueRed', ['#1E88E5', '#FFFFFF', '#E53935']\n",
        "    )\n",
        "    \n",
        "    # Create the heatmap\n",
        "    sns.clustermap(\n",
        "        pivot_data, \n",
        "        cmap=cmap,\n",
        "        linewidths=0.5, \n",
        "        figsize=(14, 10),\n",
        "        dendrogram_ratio=0.1,\n",
        "        cbar_kws={'label': 'Combined Score'},\n",
        "        xticklabels=True,\n",
        "        yticklabels=True\n",
        "    )\n",
        "    \n",
        "    plt.title('Functional Landscape of Corrosion Mechanisms Across Risk Categories', fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    return plt.gcf()\n",
        "\n",
        "# 2. PATHWAY ENRICHMENT ANALYSIS\n",
        "def plot_pathway_enrichment(classified_results):\n",
        "    \"\"\"\n",
        "    Creates a bubble chart showing pathway enrichment by risk category.\n",
        "    \"\"\"\n",
        "    # Extract top pathways from the dataset\n",
        "    pathway_data = []\n",
        "    \n",
        "    for _, row in classified_results.iterrows():\n",
        "        if isinstance(row['pathways'], str) and row['pathways']:\n",
        "            pathways = [p.strip() for p in row['pathways'].split(';')]\n",
        "            # Take only the first 3 pathways to avoid overwhelming the plot\n",
        "            for pathway in pathways[:3]:\n",
        "                pathway_data.append({\n",
        "                    'pathway': pathway,\n",
        "                    'category': 'Category 1',\n",
        "                    'mean_abundance': row['mean_cat1'],\n",
        "                    'classification': row['pathway_classification']\n",
        "                })\n",
        "                pathway_data.append({\n",
        "                    'pathway': pathway,\n",
        "                    'category': 'Category 2',\n",
        "                    'mean_abundance': row['mean_cat2'],\n",
        "                    'classification': row['pathway_classification']\n",
        "                })\n",
        "                pathway_data.append({\n",
        "                    'pathway': pathway,\n",
        "                    'category': 'Category 3',\n",
        "                    'mean_abundance': row['mean_cat3'],\n",
        "                    'classification': row['pathway_classification']\n",
        "                })\n",
        "    \n",
        "    # Convert to DataFrame\n",
        "    pathway_df = pd.DataFrame(pathway_data)\n",
        "    \n",
        "    # Group by pathway and category, calculate mean abundance\n",
        "    grouped = pathway_df.groupby(['pathway', 'category', 'classification'], as_index=False)['mean_abundance'].mean()\n",
        "    \n",
        "    # Calculate fold change from Category 1 to 3\n",
        "    pivot = grouped.pivot_table(index='pathway', columns='category', values='mean_abundance').reset_index()\n",
        "    pivot['fold_change'] = pivot['Category 3'] / pivot['Category 1'].replace(0, np.nan)\n",
        "    pivot['log2fc'] = np.log2(pivot['fold_change'])\n",
        "    \n",
        "    # Merge back with groupby results\n",
        "    enrichment = pd.merge(grouped, pivot[['pathway', 'fold_change', 'log2fc']], on='pathway')\n",
        "    \n",
        "    # Filter for significant enrichment and top pathways\n",
        "    significant = enrichment[~np.isnan(enrichment['log2fc'])]\n",
        "    significant = significant.sort_values('log2fc', ascending=False)\n",
        "    \n",
        "    # Get top 15 enriched and top 15 depleted pathways\n",
        "    top_enriched = significant.nlargest(15, 'log2fc')\n",
        "    top_depleted = significant.nsmallest(15, 'log2fc')\n",
        "    plot_data = pd.concat([top_enriched, top_depleted])\n",
        "    \n",
        "    # Create a bubble chart\n",
        "    fig = px.scatter(\n",
        "        plot_data,\n",
        "        x='log2fc',\n",
        "        y='pathway',\n",
        "        size='mean_abundance',\n",
        "        color='classification',\n",
        "        facet_col='category',\n",
        "        hover_data=['fold_change', 'mean_abundance'],\n",
        "        height=800,\n",
        "        width=1200,\n",
        "        title='Pathway Enrichment Analysis Across Risk Categories',\n",
        "        labels={'log2fc': 'Log2 Fold Change (Cat3/Cat1)', 'pathway': 'Metabolic Pathway'},\n",
        "        color_discrete_map={'niche-specific': '#FF5722', 'mixed': '#2196F3', 'universal': '#4CAF50'}\n",
        "    )\n",
        "    \n",
        "    fig.update_layout(yaxis={'categoryorder': 'total ascending'})\n",
        "    return fig\n",
        "\n",
        "# 3. NETWORK ANALYSIS OF CORROSION MECHANISMS\n",
        "def plot_mechanism_network(prioritized_markers, threshold=0.7):\n",
        "    \"\"\"\n",
        "    Creates a network visualization showing relationships between genera, proteins, and mechanisms.\n",
        "    \"\"\"\n",
        "    import networkx as nx\n",
        "    \n",
        "    # Create a networkx graph\n",
        "    G = nx.Graph()\n",
        "    \n",
        "    # Add nodes for genera (blue), proteins (green), and mechanisms (red)\n",
        "    genera = set()\n",
        "    proteins = set()\n",
        "    mechanisms = set()\n",
        "    \n",
        "    # Extract top markers\n",
        "    top_markers = prioritized_markers.sort_values('combined_score', ascending=False).head(50)\n",
        "    \n",
        "    # Process each row\n",
        "    for _, row in top_markers.iterrows():\n",
        "        genus = row['Genus']\n",
        "        protein = row['protein_name']\n",
        "        \n",
        "        genera.add(genus)\n",
        "        proteins.add(protein)\n",
        "        \n",
        "        # Add genus-protein edge\n",
        "        G.add_edge(genus, protein, weight=row['combined_score'])\n",
        "        \n",
        "        # Add mechanism nodes and edges\n",
        "        if isinstance(row['corrosion_mechanisms'], str) and row['corrosion_mechanisms']:\n",
        "            for mech in row['corrosion_mechanisms'].split(';'):\n",
        "                mech = mech.strip()\n",
        "                mechanisms.add(mech)\n",
        "                G.add_edge(protein, mech, weight=1)\n",
        "    \n",
        "    # Node colors\n",
        "    color_map = []\n",
        "    for node in G.nodes():\n",
        "        if node in genera:\n",
        "            color_map.append('skyblue')\n",
        "        elif node in proteins:\n",
        "            color_map.append('limegreen')\n",
        "        else:\n",
        "            color_map.append('tomato')\n",
        "    \n",
        "    # Node sizes\n",
        "    size_map = []\n",
        "    for node in G.nodes():\n",
        "        if node in genera:\n",
        "            size_map.append(300)  # Genera\n",
        "        elif node in proteins:\n",
        "            size_map.append(200)  # Proteins\n",
        "        else:\n",
        "            size_map.append(400)  # Mechanisms\n",
        "    \n",
        "    # Create plot\n",
        "    plt.figure(figsize=(20, 20))\n",
        "    pos = nx.spring_layout(G, k=0.5, iterations=50)\n",
        "    \n",
        "    # Draw the network\n",
        "    nx.draw_networkx_nodes(G, pos, node_color=color_map, node_size=size_map, alpha=0.8)\n",
        "    nx.draw_networkx_edges(G, pos, width=1.0, alpha=0.5)\n",
        "    nx.draw_networkx_labels(G, pos, font_size=10, font_family='sans-serif')\n",
        "    \n",
        "    plt.title('Network of Genera, Proteins, and Corrosion Mechanisms', fontsize=20)\n",
        "    plt.axis('off')\n",
        "    return plt.gcf()\n",
        "\n",
        "# 4. METAL INVOLVEMENT ANALYSIS\n",
        "def plot_metal_involvement(prioritized_markers):\n",
        "    \"\"\"\n",
        "    Creates a heatmap showing metal involvement by genus across risk categories.\n",
        "    \"\"\"\n",
        "    # Process metal involvement data\n",
        "    metal_data = []\n",
        "    \n",
        "    for _, row in prioritized_markers.iterrows():\n",
        "        if isinstance(row['metals_involved'], str) and row['metals_involved']:\n",
        "            metals = [m.strip() for m in row['metals_involved'].split(',')]\n",
        "            for metal in metals:\n",
        "                metal_data.append({\n",
        "                    'Genus': row['Genus'],\n",
        "                    'metal': metal,\n",
        "                    'Category 1': row['mean_cat1'],\n",
        "                    'Category 2': row['mean_cat2'],\n",
        "                    'Category 3': row['mean_cat3'],\n",
        "                    'combined_score': row['combined_score']\n",
        "                })\n",
        "    \n",
        "    # Convert to DataFrame\n",
        "    metal_df = pd.DataFrame(metal_data)\n",
        "    \n",
        "    # Group by genus and metal\n",
        "    grouped = metal_df.groupby(['Genus', 'metal']).agg({\n",
        "        'Category 1': 'mean',\n",
        "        'Category 2': 'mean',\n",
        "        'Category 3': 'mean',\n",
        "        'combined_score': 'mean'\n",
        "    }).reset_index()\n",
        "    \n",
        "    # Create a pivot table for the heatmap\n",
        "    pivot = grouped.pivot_table(\n",
        "        index='Genus',\n",
        "        columns='metal',\n",
        "        values='combined_score',\n",
        "        aggfunc='mean'\n",
        "    ).fillna(0)\n",
        "    \n",
        "    # Sort by sum of values\n",
        "    pivot = pivot.loc[pivot.sum(axis=1).sort_values(ascending=False).index]\n",
        "    pivot = pivot[pivot.sum().sort_values(ascending=False).index]\n",
        "    \n",
        "    # Create heatmap\n",
        "    plt.figure(figsize=(14, 10))\n",
        "    sns.heatmap(\n",
        "        pivot,\n",
        "        cmap='YlOrRd',\n",
        "        linewidths=0.5,\n",
        "        cbar_kws={'label': 'Combined Score'}\n",
        "    )\n",
        "    \n",
        "    plt.title('Metal Involvement by Genus', fontsize=16)\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    return plt.gcf()\n",
        "\n",
        "# 5. TEMPORAL TRANSITION ANALYSIS\n",
        "def plot_temporal_transition(classified_results):\n",
        "    \"\"\"\n",
        "    Creates a Sankey diagram showing the flow of proteins between risk categories.\n",
        "    \"\"\"\n",
        "    from plotly.graph_objects import Sankey\n",
        "    \n",
        "    # Filter for significant proteins\n",
        "    sig_results = classified_results[classified_results['significant'] == True].copy()\n",
        "    \n",
        "    # Create category dominance for each protein-genus pair\n",
        "    sig_results['dominant_category'] = sig_results[['mean_cat1', 'mean_cat2', 'mean_cat3']].idxmax(axis=1)\n",
        "    sig_results['dominant_category'] = sig_results['dominant_category'].map({\n",
        "        'mean_cat1': 'Category 1', \n",
        "        'mean_cat2': 'Category 2', \n",
        "        'mean_cat3': 'Category 3'\n",
        "    })\n",
        "    \n",
        "    # Group by genus and corrosion mechanism\n",
        "    flow_data = []\n",
        "    for genus, group in sig_results.groupby('Genus'):\n",
        "        for mech in group['corrosion_mechanisms'].dropna().unique():\n",
        "            if isinstance(mech, str) and mech:\n",
        "                mechanisms = [m.strip() for m in mech.split(';')]\n",
        "                for m in mechanisms:\n",
        "                    # Count proteins in each category\n",
        "                    cat1_count = ((group['dominant_category'] == 'Category 1') & \n",
        "                                 (group['corrosion_mechanisms'].str.contains(m, na=False))).sum()\n",
        "                    cat2_count = ((group['dominant_category'] == 'Category 2') & \n",
        "                                 (group['corrosion_mechanisms'].str.contains(m, na=False))).sum()\n",
        "                    cat3_count = ((group['dominant_category'] == 'Category 3') & \n",
        "                                 (group['corrosion_mechanisms'].str.contains(m, na=False))).sum()\n",
        "                    \n",
        "                    # Add flows between each category\n",
        "                    if cat1_count > 0 and cat2_count > 0:\n",
        "                        flow_data.append({\n",
        "                            'source': 'Category 1', \n",
        "                            'target': 'Category 2', \n",
        "                            'value': min(cat1_count, cat2_count),\n",
        "                            'mechanism': m\n",
        "                        })\n",
        "                    if cat2_count > 0 and cat3_count > 0:\n",
        "                        flow_data.append({\n",
        "                            'source': 'Category 2', \n",
        "                            'target': 'Category 3', \n",
        "                            'value': min(cat2_count, cat3_count),\n",
        "                            'mechanism': m\n",
        "                        })\n",
        "    \n",
        "    # Convert to DataFrame\n",
        "    flow_df = pd.DataFrame(flow_data)\n",
        "    \n",
        "    # Create node lists\n",
        "    nodes = ['Category 1', 'Category 2', 'Category 3']\n",
        "    mechanisms = flow_df['mechanism'].unique()\n",
        "    all_nodes = nodes + list(mechanisms)\n",
        "    \n",
        "    # Map source and target to indices\n",
        "    node_indices = {node: i for i, node in enumerate(all_nodes)}\n",
        "    flow_df['source_idx'] = flow_df['source'].map(node_indices)\n",
        "    flow_df['target_idx'] = flow_df['target'].map(node_indices)\n",
        "    \n",
        "    # Create Sankey diagram\n",
        "    fig = go.Figure(data=[go.Sankey(\n",
        "        node=dict(\n",
        "            pad=15,\n",
        "            thickness=20,\n",
        "            line=dict(color=\"black\", width=0.5),\n",
        "            label=all_nodes,\n",
        "            color=['rgba(31, 119, 180, 0.8)', 'rgba(255, 127, 14, 0.8)', 'rgba(214, 39, 40, 0.8)'] + \n",
        "                  ['rgba(148, 103, 189, 0.8)'] * len(mechanisms)\n",
        "        ),\n",
        "        link=dict(\n",
        "            source=flow_df['source_idx'],\n",
        "            target=flow_df['target_idx'],\n",
        "            value=flow_df['value'],\n",
        "            label=flow_df['mechanism'],\n",
        "        )\n",
        "    )])\n",
        "    \n",
        "    fig.update_layout(\n",
        "        title_text=\"Metabolic Transitions Between Risk Categories\",\n",
        "        font_size=12,\n",
        "        height=800,\n",
        "        width=1200\n",
        "    )\n",
        "    \n",
        "    return fig\n",
        "\n",
        "# 6. MECHANISM DISTRIBUTION BY RISK CATEGORY\n",
        "def plot_mechanism_distribution(prioritized_markers):\n",
        "    \"\"\"\n",
        "    Creates stacked bar charts showing mechanism distribution across risk categories.\n",
        "    \"\"\"\n",
        "    # Process mechanism data\n",
        "    mech_data = []\n",
        "    \n",
        "    for _, row in prioritized_markers.iterrows():\n",
        "        if isinstance(row['corrosion_mechanisms'], str) and row['corrosion_mechanisms']:\n",
        "            mechanisms = [m.strip() for m in row['corrosion_mechanisms'].split(';')]\n",
        "            for mech in mechanisms:\n",
        "                mech_data.append({\n",
        "                    'mechanism': mech,\n",
        "                    'Category 1': row['mean_cat1'],\n",
        "                    'Category 2': row['mean_cat2'],\n",
        "                    'Category 3': row['mean_cat3']\n",
        "                })\n",
        "    \n",
        "    # Convert to DataFrame\n",
        "    mech_df = pd.DataFrame(mech_data)\n",
        "    \n",
        "    # Group by mechanism\n",
        "    grouped = mech_df.groupby('mechanism').agg({\n",
        "        'Category 1': 'sum',\n",
        "        'Category 2': 'sum',\n",
        "        'Category 3': 'sum'\n",
        "    }).reset_index()\n",
        "    \n",
        "    # Calculate total abundance and sort\n",
        "    grouped['total'] = grouped['Category 1'] + grouped['Category 2'] + grouped['Category 3']\n",
        "    grouped = grouped.sort_values('total', ascending=False).head(10)\n",
        "    \n",
        "    # Prepare data for stacked bar chart\n",
        "    data = []\n",
        "    \n",
        "    for category in ['Category 1', 'Category 2', 'Category 3']:\n",
        "        data.append(\n",
        "            go.Bar(\n",
        "                name=category,\n",
        "                x=grouped['mechanism'],\n",
        "                y=grouped[category],\n",
        "                text=grouped[category].round(2),\n",
        "                textposition='auto'\n",
        "            )\n",
        "        )\n",
        "    \n",
        "    # Create figure\n",
        "    fig = go.Figure(data=data)\n",
        "    \n",
        "    # Update layout\n",
        "    fig.update_layout(\n",
        "        barmode='stack',\n",
        "        title='Top 10 Corrosion Mechanisms by Risk Category',\n",
        "        xaxis_title='Mechanism',\n",
        "        yaxis_title='Total Abundance',\n",
        "        legend_title='Risk Category',\n",
        "        height=600,\n",
        "        width=1000\n",
        "    )\n",
        "    \n",
        "    return fig\n",
        "\n",
        "# 7. PROTEIN ABUNDANCE TRAJECTORIES\n",
        "def plot_protein_trajectories(prioritized_markers, top_n=20):\n",
        "    \"\"\"\n",
        "    Creates a line chart showing protein abundance trajectories across risk categories.\n",
        "    \"\"\"\n",
        "    # Select top proteins\n",
        "    top_proteins = prioritized_markers.sort_values('combined_score', ascending=False).head(top_n)\n",
        "    \n",
        "    # Prepare data for trajectories\n",
        "    trajectories = []\n",
        "    \n",
        "    for i, row in top_proteins.iterrows():\n",
        "        trajectories.append({\n",
        "            'id': f\"{row['Genus']} - {row['protein_name']}\",\n",
        "            'Category 1': row['mean_cat1'],\n",
        "            'Category 2': row['mean_cat2'],\n",
        "            'Category 3': row['mean_cat3'],\n",
        "            'pattern': row['abundance_pattern'],\n",
        "            'score': row['combined_score']\n",
        "        })\n",
        "    \n",
        "    # Convert to DataFrame\n",
        "    traj_df = pd.DataFrame(trajectories)\n",
        "    \n",
        "    # Melt for plotting\n",
        "    melted = pd.melt(\n",
        "        traj_df, \n",
        "        id_vars=['id', 'pattern', 'score'], \n",
        "        value_vars=['Category 1', 'Category 2', 'Category 3'],\n",
        "        var_name='category', \n",
        "        value_name='abundance'\n",
        "    )\n",
        "    \n",
        "    # Map categories to numeric values for plotting\n",
        "    melted['category_num'] = melted['category'].map({\n",
        "        'Category 1': 1, \n",
        "        'Category 2': 2, \n",
        "        'Category 3': 3\n",
        "    })\n",
        "    \n",
        "    # Create line chart\n",
        "    fig = px.line(\n",
        "        melted,\n",
        "        x='category_num',\n",
        "        y='abundance',\n",
        "        color='id',\n",
        "        line_group='id',\n",
        "        hover_data=['pattern', 'score'],\n",
        "        markers=True,\n",
        "        title='Protein Abundance Trajectories Across Risk Categories',\n",
        "        labels={'category_num': 'Risk Category', 'abundance': 'Mean Abundance'},\n",
        "        height=600,\n",
        "        width=1000\n",
        "    )\n",
        "    \n",
        "    # Update x-axis\n",
        "    fig.update_xaxes(\n",
        "        tickvals=[1, 2, 3],\n",
        "        ticktext=['Category 1', 'Category 2', 'Category 3']\n",
        "    )\n",
        "    \n",
        "    return fig\n",
        "\n",
        "# 8. UMAP WITH PROTEIN METADATA\n",
        "def plot_umap_with_metadata(classified_results):\n",
        "    \"\"\"\n",
        "    Creates a UMAP visualization colored by protein metadata.\n",
        "    \"\"\"\n",
        "    # Prepare data for UMAP\n",
        "    features = classified_results[['mean_cat1', 'mean_cat2', 'mean_cat3']].copy()\n",
        "    \n",
        "    # Standardize features\n",
        "    scaler = StandardScaler()\n",
        "    scaled_features = scaler.fit_transform(features)\n",
        "    \n",
        "    # Compute UMAP embedding\n",
        "    reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)\n",
        "    embedding = reducer.fit_transform(scaled_features)\n",
        "    \n",
        "    # Add embedding to dataframe\n",
        "    umap_df = classified_results.copy()\n",
        "    umap_df['umap_x'] = embedding[:, 0]\n",
        "    umap_df['umap_y'] = embedding[:, 1]\n",
        "    \n",
        "    # Add dominant category\n",
        "    umap_df['dominant_category'] = umap_df[['mean_cat1', 'mean_cat2', 'mean_cat3']].idxmax(axis=1)\n",
        "    umap_df['dominant_category'] = umap_df['dominant_category'].map({\n",
        "        'mean_cat1': 'Category 1', \n",
        "        'mean_cat2': 'Category 2', \n",
        "        'mean_cat3': 'Category 3'\n",
        "    })\n",
        "    \n",
        "    # Create UMAP visualization with four facets\n",
        "    fig = make_subplots(\n",
        "        rows=2, \n",
        "        cols=2,\n",
        "        subplot_titles=(\n",
        "            'Colored by Dominant Category', \n",
        "            'Colored by Pathway Classification',\n",
        "            'Colored by Has Metal',\n",
        "            'Colored by Corrosion Relevance'\n",
        "        )\n",
        "    )\n",
        "    \n",
        "    # Facet 1: Colored by dominant category\n",
        "    fig.add_trace(\n",
        "        go.Scatter(\n",
        "            x=umap_df['umap_x'],\n",
        "            y=umap_df['umap_y'],\n",
        "            mode='markers',\n",
        "            marker=dict(\n",
        "                size=5,\n",
        "                color=umap_df['dominant_category'].map({\n",
        "                    'Category 1': 'blue',\n",
        "                    'Category 2': 'orange',\n",
        "                    'Category 3': 'red'\n",
        "                }),\n",
        "                opacity=0.7\n",
        "            ),\n",
        "            text=umap_df['Genus'] + ' - ' + umap_df['protein_name'],\n",
        "            hoverinfo='text',\n",
        "            showlegend=False\n",
        "        ),\n",
        "        row=1, col=1\n",
        "    )\n",
        "    \n",
        "    # Facet 2: Colored by pathway classification\n",
        "    fig.add_trace(\n",
        "        go.Scatter(\n",
        "            x=umap_df['umap_x'],\n",
        "            y=umap_df['umap_y'],\n",
        "            mode='markers',\n",
        "            marker=dict(\n",
        "                size=5,\n",
        "                color=umap_df['pathway_classification'].map({\n",
        "                    'niche-specific': 'green',\n",
        "                    'mixed': 'purple',\n",
        "                    'universal': 'gray'\n",
        "                }),\n",
        "                opacity=0.7\n",
        "            ),\n",
        "            text=umap_df['Genus'] + ' - ' + umap_df['protein_name'],\n",
        "            hoverinfo='text',\n",
        "            showlegend=False\n",
        "        ),\n",
        "        row=1, col=2\n",
        "    )\n",
        "    \n",
        "    # Facet 3: Colored by has_metal\n",
        "    fig.add_trace(\n",
        "        go.Scatter(\n",
        "            x=umap_df['umap_x'],\n",
        "            y=umap_df['umap_y'],\n",
        "            mode='markers',\n",
        "            marker=dict(\n",
        "                size=5,\n",
        "                color=umap_df['has_metal'].map({\n",
        "                    True: 'gold',\n",
        "                    False: 'lightgray'\n",
        "                }),\n",
        "                opacity=0.7\n",
        "            ),\n",
        "            text=umap_df['Genus'] + ' - ' + umap_df['protein_name'],\n",
        "            hoverinfo='text',\n",
        "            showlegend=False\n",
        "        ),\n",
        "        row=2, col=1\n",
        "    )\n",
        "    \n",
        "    # Facet 4: Colored by corrosion_relevance\n",
        "    fig.add_trace(\n",
        "        go.Scatter(\n",
        "            x=umap_df['umap_x'],\n",
        "            y=umap_df['umap_y'],\n",
        "            mode='markers',\n",
        "            marker=dict(\n",
        "                size=5,\n",
        "                color=umap_df['corrosion_relevance'].map({\n",
        "                    'high': 'red',\n",
        "                    'medium': 'orange',\n",
        "                    'low': 'yellow'\n",
        "                }),\n",
        "                opacity=0.7\n",
        "            ),\n",
        "            text=umap_df['Genus'] + ' - ' + umap_df['protein_name'],\n",
        "            hoverinfo='text',\n",
        "            showlegend=False\n",
        "        ),\n",
        "        row=2, col=2\n",
        "    )\n",
        "    \n",
        "    # Update layout\n",
        "    fig.update_layout(\n",
        "        title='UMAP Visualization of Protein-Genus Pairs with Metadata',\n",
        "        height=800,\n",
        "        width=1200\n",
        "    )\n",
        "    \n",
        "    return fig\n",
        "\n",
        "# Example usage:\n",
        "# functional_landscape = plot_functional_landscape(prioritized_markers)\n",
        "# pathway_enrichment = plot_pathway_enrichment(classified_results)\n",
        "# mechanism_network = plot_mechanism_network(prioritized_markers)\n",
        "# metal_involvement = plot_metal_involvement(prioritized_markers)\n",
        "# temporal_transition = plot_temporal_transition(classified_results)\n",
        "# mechanism_distribution = plot_mechanism_distribution(prioritized_markers)\n",
        "# protein_trajectories = plot_protein_trajectories(prioritized_markers)\n",
        "# umap_metadata = plot_umap_with_metadata(classified_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "U7ljhBfNa3tB",
        "outputId": "9bf2cdd2-f71b-4897-a93f-98f39079c699"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'plt' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-29fe59ba2248>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# @title Genus vs Mechanism\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Ensure 'Genus' and 'Mechanism' are treated as categorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmechanisms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Genus'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmechanisms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Genus'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
          ]
        }
      ],
      "source": [
        "# @title Genus vs Mechanism\n",
        "plt.subplots(figsize=(8, 8))\n",
        "\n",
        "# Ensure 'Genus' and 'Mechanism' are treated as categorical\n",
        "mechanisms['Genus'] = pd.Categorical(mechanisms['Genus'])\n",
        "mechanisms['Mechanism'] = pd.Categorical(mechanisms['Mechanism'])\n",
        "\n",
        "df_2dhist = pd.DataFrame({\n",
        "    x_label: grp['Mechanism'].value_counts()\n",
        "    for x_label, grp in mechanisms.groupby('Genus')\n",
        "})\n",
        "\n",
        "# Convert index and columns to lists to avoid MultiIndex issue\n",
        "df_2dhist = df_2dhist.reindex(index=df_2dhist.index.tolist(), columns=df_2dhist.columns.tolist())\n",
        "\n",
        "sns.heatmap(df_2dhist, cmap='viridis')\n",
        "plt.xlabel('Genus')\n",
        "_ = plt.ylabel('Mechanism')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Owo7FeqpaRwJ"
      },
      "outputs": [],
      "source": [
        "inverse_markers = complete_results['inverse_markers']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCzmUKAB0O1X",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Define category colors\n",
        "category_colors = {\n",
        "    1: '#008800',  # Dark green\n",
        "    2: '#FF8C00',  # Dark orange\n",
        "    3: '#FF0000'   # Red\n",
        "}\n",
        "\n",
        "categories_labels = {\n",
        "    1: 'Normal Operation',\n",
        "    2: 'Early Warning',\n",
        "    3: 'System Failure'\n",
        "}\n",
        "\n",
        "def plot_top_protein_heatmap(df, n_top=10, sort_by='corrosion_final_score'):\n",
        "    \"\"\"\n",
        "    Create a heatmap of top protein-genera combinations across categories.\n",
        "\n",
        "    Parameters:\n",
        "    - df: DataFrame with processed results (prioritized_markers)\n",
        "    - n_top: Number of top entries to display per category\n",
        "    - sort_by: Column to sort by for selecting top entries\n",
        "    \"\"\"\n",
        "    # Filter for significant results\n",
        "    if 'significant' in df.columns:\n",
        "        significant_df = df[df['significant'] == True].copy(deep=False)\n",
        "    else:\n",
        "        significant_df = df.copy(deep=False)\n",
        "\n",
        "    # Get top entries for each category\n",
        "    try:\n",
        "        # Try to filter by abundance pattern\n",
        "        top_cat1 = significant_df[significant_df['abundance_pattern'].str.contains('higher in cat 1', na=False)].nlargest(n_top, sort_by)\n",
        "        top_cat2 = significant_df[significant_df['abundance_pattern'].str.contains('higher in cat 2', na=False)].nlargest(n_top, sort_by)\n",
        "        top_cat3 = significant_df[significant_df['abundance_pattern'].str.contains('higher in cat 3', na=False)].nlargest(n_top, sort_by)\n",
        "    except:\n",
        "        # Alternative approach - determine dominant category by mean values\n",
        "        significant_df['dominant_category'] = significant_df[['mean_cat1', 'mean_cat2', 'mean_cat3']].idxmax(axis=1)\n",
        "        significant_df['dominant_category'] = significant_df['dominant_category'].str.replace('mean_cat', '')\n",
        "\n",
        "        top_cat1 = significant_df[significant_df['dominant_category'] == '1'].nlargest(n_top, sort_by)\n",
        "        top_cat2 = significant_df[significant_df['dominant_category'] == '2'].nlargest(n_top, sort_by)\n",
        "        top_cat3 = significant_df[significant_df['dominant_category'] == '3'].nlargest(n_top, sort_by)\n",
        "\n",
        "    # Combine top entries\n",
        "    top_combined = pd.concat([top_cat1, top_cat2, top_cat3])\n",
        "\n",
        "    # Create a unique identifier for each protein-genera combo\n",
        "    top_combined['protein_genera'] = top_combined['Genus'] + ' - ' + top_combined['protein_name'].str[:20]\n",
        "\n",
        "    # Create a pivot table for the heatmap\n",
        "    pivot_data = top_combined.pivot_table(\n",
        "        index='protein_genera',\n",
        "        values=['mean_cat1', 'mean_cat2', 'mean_cat3']\n",
        "    )\n",
        "\n",
        "    # Normalize for better visualization\n",
        "    # This helps see patterns even when absolute values differ greatly\n",
        "    norm_data = pivot_data.div(pivot_data.max(axis=1), axis=0)\n",
        "\n",
        "    # Set up the plot with a good height based on number of entries\n",
        "    plt.figure(figsize=(10, max(8, 0.3 * len(norm_data))))\n",
        "\n",
        "    # Create a heatmap with custom color gradient\n",
        "    cmap = sns.color_palette(\"YlOrRd\", as_cmap=True)\n",
        "\n",
        "    # Create the heatmap\n",
        "    ax = sns.heatmap(\n",
        "        norm_data,\n",
        "        cmap=cmap,\n",
        "        annot=pivot_data.round(2),  # Show original values but display normalized colors\n",
        "        fmt=\".2f\",\n",
        "        linewidths=0.5,\n",
        "        cbar_kws={'label': 'Normalized Abundance'}\n",
        "    )\n",
        "\n",
        "    # Style the plot\n",
        "    ax.set_title('Top Protein-Genera Combinations by Category', fontsize=16)\n",
        "\n",
        "    # Rename columns for better readability\n",
        "    new_labels = [categories_labels[1], categories_labels[2], categories_labels[3]]\n",
        "    ax.set_xticklabels(new_labels, rotation=45, ha='right')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    return plt.gcf()\n",
        "\n",
        "def plot_category_specific_heatmaps(df, n_top=10, sort_by='corrosion_final_score'):\n",
        "    \"\"\"\n",
        "    Create three separate heatmaps highlighting the top protein-genera combinations\n",
        "    that are dominant in each category.\n",
        "\n",
        "    Parameters:\n",
        "    - df: DataFrame with processed results (prioritized_markers)\n",
        "    - n_top: Number of top entries to display per category\n",
        "    - sort_by: Column to sort by for selecting top entries\n",
        "    \"\"\"\n",
        "    # Filter for significant results\n",
        "    if 'significant' in df.columns:\n",
        "        significant_df = df[df['significant'] == True].copy(deep=False)\n",
        "    else:\n",
        "        significant_df = df.copy(deep=False)\n",
        "\n",
        "    # Create figure with three subplots\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 10), sharey=False)\n",
        "\n",
        "    # Process each category\n",
        "    for i, cat in enumerate([1, 2, 3]):\n",
        "        # Try to filter by abundance pattern first\n",
        "        try:\n",
        "            cat_data = significant_df[significant_df['abundance_pattern'].str.contains(f'higher in cat {cat}', na=False)]\n",
        "            if cat_data.empty:\n",
        "                raise ValueError(\"No data found with pattern\")\n",
        "        except:\n",
        "            # Alternative approach - determine dominant category by mean values\n",
        "            cat_col = f'mean_cat{cat}'\n",
        "            other_cols = [f'mean_cat{j}' for j in [1, 2, 3] if j != cat]\n",
        "            cat_data = significant_df[significant_df[cat_col] > significant_df[other_cols].max(axis=1)]\n",
        "\n",
        "        # Get top entries\n",
        "        top_data = cat_data.nlargest(n_top, sort_by).copy()\n",
        "\n",
        "        # Create a unique identifier\n",
        "        top_data['protein_genera'] = top_data['Genus'] + ' - ' + top_data['protein_name'].str[:20]\n",
        "\n",
        "        # Extract values for this category and set as dataframe\n",
        "        values_df = top_data.set_index('protein_genera')[[f'mean_cat{cat}']]\n",
        "        values_df.columns = [f'Category {cat}']\n",
        "\n",
        "        # Create a custom colormap\n",
        "        cat_cmap = LinearSegmentedColormap.from_list(\n",
        "            f'cat{cat}_cmap',\n",
        "            ['white', category_colors[cat]]\n",
        "        )\n",
        "\n",
        "        # Plot the heatmap\n",
        "        ax = sns.heatmap(\n",
        "            values_df,\n",
        "            annot=True,\n",
        "            fmt=\".2f\",\n",
        "            cmap=cat_cmap,\n",
        "            ax=axes[i],\n",
        "            cbar_kws={'label': 'Abundance'}\n",
        "        )\n",
        "\n",
        "        # Style the plot\n",
        "        ax.set_title(f'{categories_labels[cat]}', fontsize=14)\n",
        "        ax.set_xlabel('')\n",
        "\n",
        "        # Only show y-axis labels on the first subplot\n",
        "        if i > 0:\n",
        "            ax.set_ylabel('')\n",
        "\n",
        "    # Add overall title\n",
        "    plt.suptitle('Top Protein-Genera Combinations by Category', fontsize=16, y=1.05)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    return fig\n",
        "\n",
        "# Running on prioritized markers\n",
        "fig1 = plot_top_protein_heatmap(top_markers)\n",
        "\n",
        "fig2 = plot_category_specific_heatmaps(prioritized_markers)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OILrTfIJ2F0u",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "plot_top_protein_heatmap(prioritized_markers, n_top=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-Y3uidSKXkW",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Select top pathways based on combined_score or corrosion_final_score\n",
        "def create_forest_plot(markers_df, n_top=15, sort_by='combined_score'):\n",
        "    # Sort and select top pathways\n",
        "    top_markers = markers_df.sort_values(by=sort_by, ascending=False).head(n_top)\n",
        "\n",
        "    # Calculate difference between categories (similar to log2fc values)\n",
        "    # Using mean values across categories\n",
        "    top_markers = top_markers.assign(\n",
        "        diff_2_vs_1=top_markers['mean_2'] - top_markers['mean_1'],\n",
        "        diff_3_vs_2=top_markers['mean_3'] - top_markers['mean_2']\n",
        "    )\n",
        "\n",
        "    # Create figure\n",
        "    fig, ax = plt.subplots(figsize=(10, 12))\n",
        "\n",
        "    # Create a custom label combining genus and protein\n",
        "    labels = [f\"{g} - {p[:30]}...\" if len(p) > 30 else f\"{g} - {p}\"\n",
        "             for g, p in zip(top_markers['Genus'], top_markers['protein_name'])]\n",
        "\n",
        "    # Position on y-axis\n",
        "    y_pos = np.arange(len(labels))\n",
        "\n",
        "    # Create a categorical color palette based on hierarchies (from hierarchy column)\n",
        "    # Extract the top level of hierarchy for coloring\n",
        "    hierarchies = top_markers['hierarchy'].apply(lambda x: x.split('/')[0] if isinstance(x, str) and '/' in x else 'Other')\n",
        "    unique_hierarchies = hierarchies.unique()\n",
        "\n",
        "    color_palette = sns.color_palette(\"husl\", len(unique_hierarchies))\n",
        "    color_map = dict(zip(unique_hierarchies, color_palette))\n",
        "    point_colors = [color_map[h] for h in hierarchies]\n",
        "\n",
        "    # Plot the difference between category 3 and 1 (overall change)\n",
        "    diff_3_vs_1 = top_markers['mean_3'] - top_markers['mean_1']\n",
        "\n",
        "    # Plot points\n",
        "    scatter = ax.scatter(\n",
        "        diff_3_vs_1,\n",
        "        y_pos,\n",
        "        c=point_colors,\n",
        "        s=80,\n",
        "        zorder=3\n",
        "    )\n",
        "\n",
        "    # Add confidence intervals/error bars using standard deviation\n",
        "    for i, (idx, row) in enumerate(top_markers.iterrows()):\n",
        "        # Calculate combined standard error for the difference\n",
        "        se = np.sqrt((row['std_1']**2 / row['count_1']) + (row['std_3']**2 / row['count_3']))\n",
        "        # 95% confidence interval\n",
        "        ci = 1.96 * se\n",
        "\n",
        "        # Plot error bars\n",
        "        ax.plot(\n",
        "            [diff_3_vs_1.iloc[i] - ci, diff_3_vs_1.iloc[i] + ci],\n",
        "            [y_pos[i], y_pos[i]],\n",
        "            'k-',\n",
        "            alpha=0.7,\n",
        "            zorder=2\n",
        "        )\n",
        "\n",
        "    # Add vertical line at x=0\n",
        "    ax.axvline(x=0, color='gray', linestyle='-', alpha=0.7, zorder=1)\n",
        "\n",
        "    # Customize the plot\n",
        "    ax.set_yticks(y_pos)\n",
        "    ax.set_yticklabels(labels)\n",
        "    ax.set_xlabel('Difference in mean abundance (Category 3 - Category 1)')\n",
        "    ax.set_title('Differential Abundance of Key Pathways Between Risk Categories')\n",
        "    ax.grid(axis='x', linestyle='--', alpha=0.3)\n",
        "\n",
        "    # Add significance markers\n",
        "    for i, p_val in enumerate(top_markers['p_value']):\n",
        "        if p_val <= 0.05:\n",
        "            marker = '*' if p_val <= 0.05 else ''\n",
        "            marker = '**' if p_val <= 0.01 else marker\n",
        "            marker = '***' if p_val <= 0.001 else marker\n",
        "\n",
        "            ax.text(\n",
        "                diff_3_vs_1.iloc[i] + (0.05 * max(abs(diff_3_vs_1))),\n",
        "                y_pos[i],\n",
        "                marker,\n",
        "                fontsize=12,\n",
        "                va='center'\n",
        "            )\n",
        "\n",
        "    # Add a legend for hierarchies\n",
        "    handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color, markersize=8)\n",
        "              for color in color_map.values()]\n",
        "    ax.legend(handles, color_map.keys(), title='Pathway Hierarchy',\n",
        "             loc='lower right', bbox_to_anchor=(1.15, 0))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    #plt.savefig('pathway_differential_abundance.png', dpi=300, bbox_inches='tight')\n",
        "    #plt.show()\n",
        "\n",
        "    return fig\n",
        "\n",
        "# Example usage:\n",
        "create_forest_plot(top_markers, n_top=15, sort_by='combined_score')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iVidoKkJ-gIk",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def plot_functional_group_heatmap(classified_results, mechanism_focus=True, top_n=100):\n",
        "    \"\"\"\n",
        "    Create a heatmap grouped by functional categories (mechanisms or pathway classification)\n",
        "\n",
        "    Parameters:\n",
        "        classified_results: DataFrame with classified pathways\n",
        "        mechanism_focus: Whether to focus on mechanisms (True) or pathway classification (False)\n",
        "        top_n: Number of top pairs to use (default: 100)\n",
        "\n",
        "    Returns:\n",
        "        matplotlib figure\n",
        "    \"\"\"\n",
        "    # Get top markers\n",
        "    top_markers = classified_results.head(top_n)\n",
        "\n",
        "    if mechanism_focus and 'corrosion_mechanisms' in top_markers.columns:\n",
        "        # Extract all mechanisms\n",
        "        all_mechanisms = set()\n",
        "        for mechs in top_markers['corrosion_mechanisms'].dropna():\n",
        "            if isinstance(mechs, str):\n",
        "                all_mechanisms.update([m.strip() for m in mechs.split(';')])\n",
        "\n",
        "        # Create a matrix: rows=genera, columns=mechanisms\n",
        "        genera = top_markers['Genus'].unique()\n",
        "        mechanisms = sorted(list(all_mechanisms))\n",
        "\n",
        "        matrix = np.zeros((len(genera), len(mechanisms)))\n",
        "\n",
        "        # Fill matrix with counts\n",
        "        for i, genus in enumerate(genera):\n",
        "            genus_markers = top_markers[top_markers['Genus'] == genus]\n",
        "\n",
        "            for _, row in genus_markers.iterrows():\n",
        "                if pd.notna(row['corrosion_mechanisms']) and isinstance(row['corrosion_mechanisms'], str):\n",
        "                    marker_mechanisms = [m.strip() for m in row['corrosion_mechanisms'].split(';')]\n",
        "                    for mechanism in marker_mechanisms:\n",
        "                        if mechanism in mechanisms:\n",
        "                            j = mechanisms.index(mechanism)\n",
        "                            matrix[i, j] += 1\n",
        "\n",
        "        # Plot\n",
        "        plt.figure(figsize=(4, 5))\n",
        "\n",
        "        # Use a better colormap for mechanisms\n",
        "        cmap = LinearSegmentedColormap.from_list('corrosion', ['#FFFFFF', '#FF9966', '#CC3300'])\n",
        "\n",
        "        ax = sns.heatmap(matrix, cmap=cmap, linewidths=0.5, linecolor='gray',\n",
        "                      xticklabels=mechanisms, yticklabels=genera)\n",
        "\n",
        "        plt.title('Genus-Mechanism Relationship (Count of Proteins)')\n",
        "        plt.xlabel('Corrosion Mechanism')\n",
        "        plt.ylabel('Genus')\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "    else:\n",
        "        # Use pathway classification\n",
        "        if 'pathway_classification' not in top_markers.columns:\n",
        "            return None\n",
        "\n",
        "        # Create a matrix: rows=genera, columns=classifications\n",
        "        genera = top_markers['Genus'].unique()\n",
        "        classifications = ['universal', 'mixed', 'niche-specific']\n",
        "\n",
        "        matrix = np.zeros((len(genera), len(classifications)))\n",
        "\n",
        "        # Fill matrix with counts\n",
        "        for i, genus in enumerate(genera):\n",
        "            genus_markers = top_markers[top_markers['Genus'] == genus]\n",
        "\n",
        "            for classification in classifications:\n",
        "                j = classifications.index(classification)\n",
        "                count = sum(genus_markers['pathway_classification'] == classification)\n",
        "                matrix[i, j] = count\n",
        "\n",
        "        # Plot\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        cmap = LinearSegmentedColormap.from_list('pathways', ['#FFFFFF', '#66CCFF', '#003366'])\n",
        "\n",
        "        ax = sns.heatmap(matrix, cmap=cmap, linewidths=0.5, linecolor='gray',\n",
        "                      xticklabels=classifications, yticklabels=genera)\n",
        "\n",
        "        plt.title('Genus-Pathway Classification Relationship (Count of Proteins)')\n",
        "        plt.xlabel('Pathway Classification')\n",
        "        plt.ylabel('Genus')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return plt.gcf()\n",
        "\n",
        "plot_functional_group_heatmap(classified_results, mechanism_focus=True, top_n=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AyamylmAK3Ve",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def create_pathway_site_heatmap(data_frame, n_pathways=20, group_by='corrosion_mechanisms'):\n",
        "    \"\"\"\n",
        "    Create a heatmap showing pathway abundance across sites, grouped by corrosion mechanism\n",
        "\n",
        "    Parameters: pandas DataFrame prioritized_markers\n",
        "    n_pathways : int Number of top pathways to include\n",
        "    group_by : str column to use for grouping (e.g., 'corrosion_mechanisms', 'metals_involved')\n",
        "    \"\"\"\n",
        "    # Use a different sorting metric if combined_score isn't available\n",
        "    sort_columns = ['corrosion_final_score', 'combined_score', 'corr', 'h_statistic']\n",
        "    sort_col = next((col for col in sort_columns if col in data_frame.columns), None)\n",
        "\n",
        "    if sort_col:\n",
        "        top_pathways = data_frame.sort_values(by=sort_col, ascending=False).head(n_pathways)\n",
        "    else:\n",
        "        # If none of the sort columns exist, use the first n_pathways rows\n",
        "        top_pathways = data_frame.head(n_pathways)\n",
        "\n",
        "    # Create a unique identifier for each pathway\n",
        "    top_pathways['pathway_id'] = top_pathways['Genus'] + ' - ' + top_pathways['protein_name'].apply(\n",
        "        lambda x: x[:20] + '...' if len(x) > 20 else x)\n",
        "\n",
        "    # Create a long-format DataFrame for the heatmap\n",
        "    heatmap_data = []\n",
        "\n",
        "    for _, row in top_pathways.iterrows():\n",
        "        # Parse the Sites column - handle different formats\n",
        "        if 'Sites' in row:\n",
        "            sites_str = row['Sites']\n",
        "            # Handle different formats of the Sites column\n",
        "            if isinstance(sites_str, str):\n",
        "                if ';' in sites_str:\n",
        "                    # Handle semicolon-separated format\n",
        "                    sites = [s.strip() for s in sites_str.split(';')]\n",
        "                elif ',' in sites_str:\n",
        "                    # Handle comma-separated format\n",
        "                    sites = [s.strip() for s in sites_str.split(',')]\n",
        "                elif '[' in sites_str:\n",
        "                    # Try to safely evaluate as a list\n",
        "                    try:\n",
        "                        sites = eval(sites_str)\n",
        "                    except:\n",
        "                        sites = [sites_str]\n",
        "                else:\n",
        "                    # Single site\n",
        "                    sites = [sites_str]\n",
        "            elif isinstance(sites_str, list):\n",
        "                sites = sites_str\n",
        "            else:\n",
        "                sites = []\n",
        "        else:\n",
        "            # If Sites column doesn't exist, use an empty list\n",
        "            sites = []\n",
        "\n",
        "        # Get abundance for each category\n",
        "        for category in [1, 2, 3]:\n",
        "            # Check if the mean columns exist\n",
        "            mean_col = f'mean_{category}'\n",
        "            if mean_col in row:\n",
        "                mean_value = row[mean_col]\n",
        "            else:\n",
        "                # Try alternative column names\n",
        "                alt_mean_col = f'mean_cat{category}'\n",
        "                if alt_mean_col in row:\n",
        "                    mean_value = row[alt_mean_col]\n",
        "                else:\n",
        "                    # If no abundance data for this category, skip\n",
        "                    continue\n",
        "\n",
        "            # Process the group_by column\n",
        "            if group_by in row:\n",
        "                mechanisms = row[group_by]\n",
        "                if isinstance(mechanisms, str):\n",
        "                    # Handle different potential formats\n",
        "                    if '[' in mechanisms:\n",
        "                        try:\n",
        "                            mechanisms = eval(mechanisms)\n",
        "                        except:\n",
        "                            mechanisms = [m.strip() for m in mechanisms.split(',') if m.strip()]\n",
        "                    elif ',' in mechanisms:\n",
        "                        mechanisms = [m.strip() for m in mechanisms.split(',') if m.strip()]\n",
        "                    else:\n",
        "                        mechanisms = [mechanisms]\n",
        "                elif isinstance(mechanisms, list):\n",
        "                    pass  # Already a list\n",
        "                else:\n",
        "                    mechanisms = ['Unknown']\n",
        "            else:\n",
        "                mechanisms = ['Unknown']\n",
        "\n",
        "            primary_mechanism = mechanisms[0] if mechanisms else 'Unknown'\n",
        "\n",
        "            heatmap_data.append({\n",
        "                'pathway_id': row['pathway_id'],\n",
        "                'category': f'Category {category}',\n",
        "                'abundance': mean_value,\n",
        "                'mechanism': primary_mechanism,\n",
        "                'genus': row['Genus']\n",
        "            })\n",
        "\n",
        "    # If no data could be processed, return with a message\n",
        "    if not heatmap_data:\n",
        "        print(\"No suitable data found for heatmap visualization\")\n",
        "        return None\n",
        "\n",
        "    heatmap_df = pd.DataFrame(heatmap_data)\n",
        "\n",
        "    # Create a pivot table for the heatmap\n",
        "    pivot_data = heatmap_df.pivot_table(\n",
        "        index=['mechanism', 'pathway_id'],\n",
        "        columns='category',\n",
        "        values='abundance',\n",
        "        aggfunc='mean'\n",
        "    )\n",
        "\n",
        "    # Sort by mechanism and then by abundance in Category 3\n",
        "    if 'Category 3' in pivot_data.columns:\n",
        "        pivot_data = pivot_data.sort_values(by=['mechanism', 'Category 3'], ascending=[True, False])\n",
        "\n",
        "    # Create the figure\n",
        "    plt.figure(figsize=(12, n_pathways * 0.4 + 4))\n",
        "\n",
        "    # Set diverging color palette centered at the median\n",
        "    cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
        "\n",
        "    # Create the heatmap\n",
        "    ax = sns.heatmap(\n",
        "        pivot_data,\n",
        "        cmap=cmap,\n",
        "        center=pivot_data.values.mean(),\n",
        "        annot=True,\n",
        "        fmt=\".2f\",\n",
        "        linewidths=.5,\n",
        "        cbar_kws={\"label\": \"Abundance\"},\n",
        "        yticklabels=True\n",
        "    )\n",
        "\n",
        "    # Customize\n",
        "    plt.title(f'Pathway Abundance Across Risk Categories Grouped by {group_by.replace(\"_\", \" \").title()}')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    return ax\n",
        "\n",
        "# Example usage:\n",
        "create_pathway_site_heatmap(prioritized_markers, n_pathways=10, group_by='corrosion_mechanisms')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQf6YwTFLEyn",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def create_bubble_chart(data_frame, n_top=20):\n",
        "    \"\"\"\n",
        "    Create a bubble chart showing relationships between pathways, risk categories, and metals.\n",
        "\n",
        "    Parameters:ata_frame : prioritized_markers DataFrame\n",
        "    n_top : int Number of top pathways to include\n",
        "    \"\"\"\n",
        "    # Select top pathways based on combined_score\n",
        "    top_paths = data_frame.sort_values('combined_score', ascending=False).head(n_top)\n",
        "\n",
        "    # Create a long-form DataFrame for the bubble chart\n",
        "    bubble_data = []\n",
        "\n",
        "    for _, row in top_paths.iterrows():\n",
        "        # Create a simplified pathway name (first 20 chars of protein name + genus)\n",
        "        pathway_name = f\"{row['Genus']}: {row['protein_name'][:20]}...\"\n",
        "\n",
        "        # Process metals involved\n",
        "        metals = row['metals_involved']\n",
        "        if isinstance(metals, str):\n",
        "            if '[' in metals:\n",
        "                metals = eval(metals)\n",
        "            elif ',' in metals:\n",
        "                metals = [m.strip() for m in metals.split(',')]\n",
        "            else:\n",
        "                metals = [metals]\n",
        "\n",
        "        # If no metals or empty list, use \"None\"\n",
        "        primary_metal = metals[0] if isinstance(metals, list) and metals else \"None\"\n",
        "\n",
        "        # Add data points for each category\n",
        "        for category in [1, 2, 3]:\n",
        "            bubble_data.append({\n",
        "                'pathway': pathway_name,\n",
        "                'category': f'Category {category}',\n",
        "                'abundance': row[f'mean_{category}'],\n",
        "                'primary_metal': primary_metal,\n",
        "                'corrosion_score': row['corrosion_final_score'],\n",
        "                'log2fc': row[f'log2fc_1_to_2'] if category == 2 else row[f'log2fc_2_to_3'] if category == 3 else 0\n",
        "            })\n",
        "\n",
        "    bubble_df = pd.DataFrame(bubble_data)\n",
        "\n",
        "    # Create a categorical order for the pathways based on overall abundance\n",
        "    pathway_order = bubble_df.groupby('pathway')['abundance'].mean().sort_values(ascending=False).index\n",
        "\n",
        "    # Create a categorical order for metals based on their correlation with abundance\n",
        "    metal_order = bubble_df.groupby('primary_metal')['abundance'].mean().sort_values(ascending=False).index\n",
        "\n",
        "    # Set up the figure\n",
        "    plt.figure(figsize=(16, 12))\n",
        "\n",
        "    # Create bubble chart\n",
        "    ax = sns.scatterplot(\n",
        "        data=bubble_df,\n",
        "        x='category',\n",
        "        y='pathway',\n",
        "        size='abundance',\n",
        "        hue='primary_metal',\n",
        "        palette='viridis',\n",
        "        sizes=(20, 1000),  # Min and max bubble size\n",
        "        alpha=0.7,\n",
        "        legend='brief',\n",
        "        order=['Category 1', 'Category 2', 'Category 3'],\n",
        "        hue_order=metal_order,\n",
        "    )\n",
        "\n",
        "    # Add pathway labels\n",
        "    for label in pathway_order:\n",
        "        subset = bubble_df[bubble_df['pathway'] == label]\n",
        "        if not subset.empty:\n",
        "            max_abundance_idx = subset['abundance'].idxmax()\n",
        "            max_row = subset.loc[max_abundance_idx]\n",
        "\n",
        "            # Add text annotations for significant log2fc values\n",
        "            if abs(max_row['log2fc']) > 1:  # Only add for meaningful fold changes\n",
        "                cat_index = ['Category 1', 'Category 2', 'Category 3'].index(max_row['category'])\n",
        "                plt.text(\n",
        "                    cat_index,\n",
        "                    list(pathway_order).index(label),\n",
        "                    f\" FC:{max_row['log2fc']:.1f}\",\n",
        "                    fontsize=8,\n",
        "                    verticalalignment='center'\n",
        "                )\n",
        "\n",
        "    # Adjust legend\n",
        "    plt.legend(title='Primary Metal', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "    # Set title and labels\n",
        "    plt.title('Pathway Abundance Across Risk Categories by Metal Involvement', fontsize=14)\n",
        "    plt.xlabel('Risk Category', fontsize=12)\n",
        "    plt.ylabel('Pathway', fontsize=12)\n",
        "\n",
        "    # Adjust layout\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('pathway_metal_bubble_chart.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    return ax\n",
        "\n",
        "# Example usage:\n",
        "create_bubble_chart(prioritized_markers, n_top=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itKFXFy8ue71"
      },
      "outputs": [],
      "source": [
        "prior"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LSHPyikRLP-j",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def create_sunburst_chart(data_frame, category=3, output_file='pathway_sunburst.html'):\n",
        "    \"\"\"\n",
        "    Create a sunburst chart showing hierarchical representation of pathways\n",
        "    for a specific risk category.\n",
        "\n",
        "    Parameters: data_frame classified_results\n",
        "    category : int Risk category to visualize (1, 2, or 3)\n",
        "    output_file : str file name to save the interactive HTML plot\n",
        "    \"\"\"\n",
        "    # Filter data to significant pathways\n",
        "    sig_df = data_frame[data_frame['significant'] == True].copy(deep=False)\n",
        "\n",
        "    # Prepare data for sunburst chart\n",
        "    sunburst_data = []\n",
        "\n",
        "    for _, row in sig_df.iterrows():\n",
        "        # Extract hierarchy levels - assuming hierarchy is in path format\n",
        "        hierarchy = row['hierarchy']\n",
        "        if not isinstance(hierarchy, str) or not hierarchy:\n",
        "            hierarchy = \"Unclassified\"\n",
        "\n",
        "        # Split hierarchy path\n",
        "        hierarchy_levels = hierarchy.split('/')\n",
        "        levels = []\n",
        "\n",
        "        # Build all path components\n",
        "        for i in range(len(hierarchy_levels)):\n",
        "            levels.append('/'.join(hierarchy_levels[:i+1]))\n",
        "\n",
        "        # Calculate abundance value\n",
        "        abundance = row[f'mean_{category}']\n",
        "\n",
        "        # Add the full path with the value\n",
        "        sunburst_data.append({\n",
        "            'id': '/'.join(hierarchy_levels),\n",
        "            'parent': '/'.join(hierarchy_levels[:-1]) if len(hierarchy_levels) > 1 else '',\n",
        "            'labels': hierarchy_levels[-1],\n",
        "            'values': abundance,\n",
        "            'genus': row['Genus'],\n",
        "            'protein': row['protein_name'],\n",
        "            'corrosion_relevance': row['corrosion_relevance'],\n",
        "            'pathway_classification': row['pathway_classification'] if 'pathway_classification' in row else 'Unknown'\n",
        "        })\n",
        "\n",
        "        # Add all parent paths\n",
        "        for i in range(len(hierarchy_levels)-1, 0, -1):\n",
        "            parent_path = '/'.join(hierarchy_levels[:i])\n",
        "            path = '/'.join(hierarchy_levels[:i+1])\n",
        "\n",
        "            # Check if this parent path is already in data\n",
        "            if not any(d['id'] == path for d in sunburst_data):\n",
        "                sunburst_data.append({\n",
        "                    'id': path,\n",
        "                    'parent': parent_path if i > 1 else '',\n",
        "                    'labels': hierarchy_levels[i],\n",
        "                    'values': abundance,  # This will get summed up in plotly\n",
        "                    'genus': '',\n",
        "                    'protein': '',\n",
        "                    'corrosion_relevance': '',\n",
        "                    'pathway_classification': ''\n",
        "                })\n",
        "\n",
        "        # Add the root node if needed\n",
        "        if len(hierarchy_levels) > 0 and not any(d['id'] == hierarchy_levels[0] for d in sunburst_data):\n",
        "            sunburst_data.append({\n",
        "                'id': hierarchy_levels[0],\n",
        "                'parent': '',\n",
        "                'labels': hierarchy_levels[0],\n",
        "                'values': abundance,\n",
        "                'genus': '',\n",
        "                'protein': '',\n",
        "                'corrosion_relevance': '',\n",
        "                'pathway_classification': ''\n",
        "            })\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    sunburst_df = pd.DataFrame(sunburst_data)\n",
        "\n",
        "    # Create the sunburst chart with Plotly\n",
        "    fig = px.sunburst(\n",
        "        sunburst_df,\n",
        "        ids='id',\n",
        "        parents='parent',\n",
        "        names='labels',\n",
        "        values='values',\n",
        "        color='values',\n",
        "        hover_data=['genus', 'protein', 'corrosion_relevance', 'pathway_classification'],\n",
        "        color_continuous_scale='Viridis',\n",
        "        title=f'Hierarchical Pathway Distribution for Category {category}'\n",
        "    )\n",
        "\n",
        "    # Update layout\n",
        "    fig.update_layout(\n",
        "        width=900,\n",
        "        height=900,\n",
        "        margin=dict(t=50, l=0, r=0, b=10)\n",
        "    )\n",
        "\n",
        "    # Save to HTML file\n",
        "    pio.write_html(fig, file=output_file, auto_open=True)\n",
        "\n",
        "    return fig\n",
        "\n",
        "# Example usage:\n",
        "create_sunburst_chart(classified_results, category=3, output_file='pathway_sunburst_cat3.html')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FG7RGsVBIN-A",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def create_network_graph(data_frame, n_top=15, threshold=0.5, category=3):\n",
        "    \"\"\"\n",
        "    Create a network graph showing relationships between genera, pathways, and corrosion mechanisms.\n",
        "    Parameters: data_frame : pandas DataFrame prioritized_markers DataFrame\n",
        "    n_top : int   Number of top genera-pathway combinations to include\n",
        "    threshold : float  Minimum combined_score to include (0-1)\n",
        "    category : int  Risk category to visualize (1, 2, or 3)\n",
        "    \"\"\"\n",
        "    # Filter data\n",
        "    filtered_df = data_frame[\n",
        "        (data_frame['combined_score'] >= threshold) &\n",
        "        (data_frame[f'mean_{category}'] > 0)\n",
        "    ].sort_values('combined_score', ascending=False).head(n_top)\n",
        "\n",
        "    # Create a graph\n",
        "    G = nx.Graph()\n",
        "\n",
        "    # Add nodes and edges\n",
        "    for _, row in filtered_df.iterrows():\n",
        "        genus = row['Genus']\n",
        "        protein = row['protein_name'][:20] + '...' if len(row['protein_name']) > 20 else row['protein_name']\n",
        "\n",
        "        # Process corrosion mechanisms\n",
        "        mechanisms = row['corrosion_mechanisms']\n",
        "        if isinstance(mechanisms, str):\n",
        "            if '[' in mechanisms:\n",
        "                mechanisms = eval(mechanisms)\n",
        "            elif ',' in mechanisms:\n",
        "                mechanisms = [m.strip() for m in mechanisms.split(',')]\n",
        "            else:\n",
        "                mechanisms = [mechanisms]\n",
        "\n",
        "        # Add nodes if they don't exist\n",
        "        if not G.has_node(genus):\n",
        "            G.add_node(genus, type='genus',\n",
        "                      abundance=row[f'mean_{category}'],\n",
        "                      corr_score=row['corr'])\n",
        "\n",
        "        if not G.has_node(protein):\n",
        "            G.add_node(protein, type='protein',\n",
        "                      score=row['combined_score'],\n",
        "                      hierarchy=row['hierarchy'].split('/')[0] if isinstance(row['hierarchy'], str) else 'Unknown')\n",
        "\n",
        "        # Add edge between genus and protein\n",
        "        G.add_edge(genus, protein, weight=row[f'mean_{category}'])\n",
        "\n",
        "        # Add mechanism nodes and edges\n",
        "        for mech in mechanisms:\n",
        "            if mech and isinstance(mech, str):\n",
        "                if not G.has_node(mech):\n",
        "                    G.add_node(mech, type='mechanism')\n",
        "\n",
        "                # Connect mechanism to protein\n",
        "                G.add_edge(protein, mech, weight=1)\n",
        "\n",
        "    # Set up the plot\n",
        "    plt.figure(figsize=(16, 12))\n",
        "\n",
        "    # Set positions using spring layout\n",
        "    pos = nx.spring_layout(G, k=0.5, iterations=50, seed=42)\n",
        "\n",
        "    # Prepare node colors by type\n",
        "    node_colors = []\n",
        "    node_sizes = []\n",
        "    node_alphas = []\n",
        "\n",
        "    for node in G.nodes():\n",
        "        if G.nodes[node]['type'] == 'genus':\n",
        "            # Color genera by correlation score (red for positive, blue for negative)\n",
        "            corr = G.nodes[node]['corr_score']\n",
        "            if corr > 0:\n",
        "                color = (1, 0, 0, min(1, abs(corr)))  # Red with alpha based on correlation strength\n",
        "            else:\n",
        "                color = (0, 0, 1, min(1, abs(corr)))  # Blue with alpha based on correlation strength\n",
        "            node_colors.append(color)\n",
        "            node_sizes.append(300 + G.nodes[node]['abundance'] * 100)\n",
        "            node_alphas.append(min(1, abs(corr)))\n",
        "\n",
        "        elif G.nodes[node]['type'] == 'protein':\n",
        "            # Color proteins by hierarchy\n",
        "            hierarchy = G.nodes[node]['hierarchy']\n",
        "            # Create a deterministic color based on hierarchy string\n",
        "            h_hash = sum([ord(c) for c in hierarchy]) % 100 / 100.0\n",
        "            color = plt.cm.tab20(h_hash)\n",
        "            node_colors.append(color)\n",
        "            node_sizes.append(200 + G.nodes[node]['score'] * 300)\n",
        "            node_alphas.append(G.nodes[node]['score'])\n",
        "\n",
        "        elif G.nodes[node]['type'] == 'mechanism':\n",
        "            # Use a distinct color for mechanisms\n",
        "            node_colors.append('green')\n",
        "            node_sizes.append(150)\n",
        "            node_alphas.append(0.8)\n",
        "\n",
        "    # Get edge weights for width\n",
        "    edge_weights = [G[u][v]['weight'] for u, v in G.edges()]\n",
        "    max_weight = max(edge_weights)\n",
        "    normalized_weights = [w/max_weight * 3 for w in edge_weights]\n",
        "\n",
        "    # Draw the network\n",
        "    nodes = nx.draw_networkx_nodes(\n",
        "        G, pos,\n",
        "        node_color=node_colors,\n",
        "        node_size=node_sizes,\n",
        "        alpha=0.8\n",
        "    )\n",
        "\n",
        "    edges = nx.draw_networkx_edges(\n",
        "        G, pos,\n",
        "        width=normalized_weights,\n",
        "        alpha=0.6,\n",
        "        edge_color='gray'\n",
        "    )\n",
        "\n",
        "    labels = nx.draw_networkx_labels(\n",
        "        G, pos,\n",
        "        font_size=10,\n",
        "        font_weight='bold'\n",
        "    )\n",
        "\n",
        "    # Add legend\n",
        "    genus_patch = mpatches.Patch(color='lightcoral', label='Genus (red=positive corr, blue=negative)')\n",
        "    protein_patch = mpatches.Patch(color='lightblue', label='Protein (color=hierarchy)')\n",
        "    mech_patch = mpatches.Patch(color='lightgreen', label='Corrosion Mechanism')\n",
        "    plt.legend(handles=[genus_patch, protein_patch, mech_patch], loc='upper left', bbox_to_anchor=(1, 1))\n",
        "\n",
        "    # Add title\n",
        "    plt.title(f'Network of Genera, Proteins, and Corrosion Mechanisms for Category {category}', fontsize=14)\n",
        "\n",
        "    # Remove axis\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Adjust layout\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'network_graph_cat{category}.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    return G\n",
        "classified_results= complete_results['classified_results']\n",
        "prioritized_markers = complete_results['prioritized_markers']\n",
        "# Example usage:\n",
        "create_network_graph(prioritized_markers, n_top=15, threshold=0.5, category=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1B1NzKQt7U2q",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def improved_genus_enzyme_plot(increasing_results, eccontri_df, genus_name, top_n=5):\n",
        "    \"\"\"\n",
        "    Create an improved plot showing genus and enzyme class abundance across ordered sites.\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "\n",
        "    # Filter data for the specified genus\n",
        "    genus_markers = increasing_results[increasing_results['Genus'] == genus_name]\n",
        "    genus_data = eccontri_df[eccontri_df['Genus'] == genus_name]\n",
        "\n",
        "    if len(genus_data) == 0:\n",
        "        print(f\"No data found for genus {genus_name}\")\n",
        "        return\n",
        "\n",
        "    # Get categories and sort sites within each category\n",
        "    categories = sorted(genus_data['Category'].unique())\n",
        "    sites_by_category = {}\n",
        "\n",
        "    for cat in categories:\n",
        "        # Extract sites for this category and sort them numerically\n",
        "        cat_sites = genus_data[genus_data['Category'] == cat]['Sites'].unique()\n",
        "        # Extract numeric part of site name for sorting\n",
        "        cat_sites_sorted = sorted(cat_sites, key=lambda x: int(x.split('_')[1]))\n",
        "        sites_by_category[cat] = cat_sites_sorted\n",
        "\n",
        "    # Flatten the ordered sites list\n",
        "    ordered_sites = []\n",
        "    for cat in categories:\n",
        "        ordered_sites.extend(sites_by_category[cat])\n",
        "\n",
        "    # Calculate genus abundance by site\n",
        "    genus_abundance = {}\n",
        "    for site in ordered_sites:\n",
        "        site_data = genus_data[genus_data['Sites'] == site]\n",
        "        genus_abundance[site] = site_data['norm_abund_contri'].sum()\n",
        "\n",
        "    # Get top enzyme classes\n",
        "    if 'enzyme_class' in genus_markers.columns and genus_markers['enzyme_class'].notna().sum() > 0:\n",
        "        valid_classes = genus_markers[genus_markers['enzyme_class'].notna() &\n",
        "                                     (genus_markers['enzyme_class'] != '')]\n",
        "\n",
        "        if len(valid_classes) > 0:\n",
        "            enzyme_counts = valid_classes['enzyme_class'].value_counts()\n",
        "            top_classes = enzyme_counts.head(top_n).index\n",
        "            using_proteins = False\n",
        "        else:\n",
        "            protein_counts = genus_markers['protein_name'].value_counts()\n",
        "            top_classes = protein_counts.head(top_n).index\n",
        "            using_proteins = True\n",
        "    else:\n",
        "        protein_counts = genus_markers['protein_name'].value_counts()\n",
        "        top_classes = protein_counts.head(top_n).index\n",
        "        using_proteins = True\n",
        "\n",
        "    # Calculate abundance by site for each class/protein\n",
        "    class_abundance = {}\n",
        "    for cls in top_classes:\n",
        "        class_abundance[cls] = {}\n",
        "\n",
        "        for site in ordered_sites:\n",
        "            if using_proteins:\n",
        "                site_data = genus_data[(genus_data['Sites'] == site) &\n",
        "                                     (genus_data['protein_name'] == cls)]\n",
        "            else:\n",
        "                class_proteins = genus_markers[genus_markers['enzyme_class'] == cls]['protein_name'].unique()\n",
        "                site_data = genus_data[(genus_data['Sites'] == site) &\n",
        "                                     (genus_data['protein_name'].isin(class_proteins))]\n",
        "\n",
        "            class_abundance[cls][site] = site_data['norm_abund_contri'].sum()\n",
        "\n",
        "    # Create figure with two y-axes\n",
        "    fig, ax1 = plt.subplots(figsize=(16, 8))\n",
        "    ax2 = ax1.twinx()\n",
        "\n",
        "    # Prepare x-axis\n",
        "    x = np.arange(len(ordered_sites))\n",
        "\n",
        "    # Plot genus abundance\n",
        "    genus_values = [genus_abundance.get(site, 0) for site in ordered_sites]\n",
        "    ax1.plot(x, genus_values, 'b-', linewidth=2.5, marker='o', markersize=6,\n",
        "             label=f\"{genus_name} (Genus)\")\n",
        "\n",
        "    # Plot enzyme classes with offsets for better visibility\n",
        "    colors = plt.cm.tab10(np.linspace(0, 1, len(top_classes)))\n",
        "    max_class_value = 0\n",
        "\n",
        "    for cls_values in class_abundance.values():\n",
        "        for site in ordered_sites:\n",
        "            max_class_value = max(max_class_value, cls_values.get(site, 0))\n",
        "\n",
        "    offset_step = max_class_value * 0.1  # 10% offset between classes\n",
        "\n",
        "    for i, cls in enumerate(top_classes):\n",
        "        offset = i * offset_step\n",
        "        class_values = [class_abundance[cls].get(site, 0) + offset for site in ordered_sites]\n",
        "        label = f\"{cls}\" if len(cls) < 30 else f\"{cls[:27]}...\"\n",
        "        ax2.plot(x, class_values, '--', color=colors[i], linewidth=1.5, marker='o',\n",
        "                markersize=4, label=f\"{label} (offset: +{offset:.2f})\")\n",
        "\n",
        "    # Set axis labels\n",
        "    ax1.set_ylabel('Genus Abundance', color='b', fontsize=12)\n",
        "    ax1.tick_params(axis='y', labelcolor='b')\n",
        "\n",
        "    ax2.set_ylabel('Enzyme Class Abundance', color='r', fontsize=12)\n",
        "    ax2.tick_params(axis='y', labelcolor='r')\n",
        "\n",
        "    # Add category dividers\n",
        "    site_indices = [0]  # Start with the first site\n",
        "    category_labels = []\n",
        "\n",
        "    for i, cat in enumerate(categories):\n",
        "        if i > 0:\n",
        "            idx = site_indices[-1] + len(sites_by_category[categories[i-1]])\n",
        "            site_indices.append(idx)\n",
        "            # Add vertical line\n",
        "            plt.axvline(x=idx-0.5, color='black', linestyle='-', linewidth=2)\n",
        "\n",
        "    # Add category labels at the top\n",
        "    for i, cat in enumerate(categories):\n",
        "        start_idx = site_indices[i]\n",
        "        end_idx = start_idx + len(sites_by_category[cat]) - 1\n",
        "        mid_point = (start_idx + end_idx) / 2\n",
        "        plt.text(mid_point, ax1.get_ylim()[1] * 1.05, f\"Category {cat}\",\n",
        "                horizontalalignment='center', fontsize=14, fontweight='bold')\n",
        "\n",
        "    # Set x-ticks for sites\n",
        "    plt.xticks(x, [site.split('_')[1] for site in ordered_sites], rotation=90)\n",
        "    plt.xlabel('Site Number', fontsize=12)\n",
        "\n",
        "    # Add grid\n",
        "    ax1.grid(True, axis='y', linestyle='--', alpha=0.3)\n",
        "\n",
        "    # Add title and legend\n",
        "    plt.title(f\"Genus vs Enzyme Class Abundance for {genus_name}\", fontsize=16)\n",
        "    ax1.legend(loc='upper left', fontsize=10)\n",
        "    ax2.legend(loc='upper right', fontsize=9)\n",
        "\n",
        "    # Adjust layout\n",
        "    plt.tight_layout()\n",
        "    plt.subplots_adjust(top=0.9)  # Make room for category labels\n",
        "\n",
        "    # Print category information\n",
        "    for cat in categories:\n",
        "        print(f\"Category {cat} Sites: {sites_by_category[cat]}\")\n",
        "\n",
        "    plt.show()\n",
        "#['Pseudomonas', 'Simplicispira', 'Acidovorax', 'Dechloromonas', 'Desulfobulbus', 'Silanimonas', 'Propionivibrio', 'Ruminiclostridium_1', 'Achromobacter', 'Blastomonas', 'Bacillus', 'Bradyrhizobium', 'Sphingomonas', 'Brevundimonas', 'Paracoccus', 'Variovorax', 'Nitrospira', 'Desulfovibrio', 'Phenylobacterium', 'Pseudoxanthomonas', 'Erysipelothrix', 'Geothrix', 'Acetobacterium', 'Shewanella', 'Gelria', 'Micrococcus', 'Desulfotomaculum', 'Streptococcus', 'Pseudorhodoferax', 'Propionibacterium', 'Methylocystis', 'Azospira', 'Smithella', 'Sphingopyxis', 'Caulobacter', 'Novosphingobium', 'Staphylococcus', 'Afipia', 'Porphyrobacter', 'Sphingobium', 'Tepidimonas', 'Halomonas', 'Desulfomicrobium', 'Thiobacillus', 'Phreatobacter', 'Chryseobacterium', 'Oxalobacteraceae_unclassified', 'Anoxybacillus', 'Tessaracoccus', 'Hydrogenophaga', 'Legionella', 'Corynebacterium', 'Mycobacterium', 'Desulfosporosinus', 'Enhydrobacter', 'Opitutus', 'Sediminibacterium', 'Thermincola', 'Clostridium_sensu_stricto_12', 'Acidisoma', 'Gallionella', 'Enterococcus', 'Clostridium', 'Mycoplana', 'Syntrophus', 'Bulleidia', 'Neisseria', 'Treponema', 'Psb-m-3', 'Wchb1-05', 'Pseudoalteromonas', 'Flavisolibacter', 'Prevotella', 'Brevibacterium', 'Ralstonia', 'Brachybacterium', 'Oxobacter', 'Oerskovia', 'Cutibacterium', 'Aestuariimicrobium', 'Herbaspirillum', 'Beta_proteobacterium', 'Candidatus_desulforudis', 'Pseudarthrobacter', 'Desulfobacterium']\n",
        "improved_genus_enzyme_plot(prioritized_markers, ECcontri_Uniprot_enriched, 'Pseudomonas', top_n=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kC6ERUGAreDR"
      },
      "source": [
        "### Top Markers by Score -\n",
        "Bar chart showing the highest scoring markers\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSDtFpVLqEZh",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def plot_top_markers_by_score(prioritized_markers, top_n=20, figsize=(8, 6)):\n",
        "    \"\"\"\n",
        "    Create a horizontal bar chart of top markers by score\n",
        "    \"\"\"\n",
        "    # Get top markers\n",
        "    top_markers = prioritized_markers.head(top_n)\n",
        "\n",
        "    # Create labels\n",
        "    labels = [f\"{row['Genus']} - {row['protein_name'][:30]}\"\n",
        "              for _, row in top_markers.iterrows()]\n",
        "\n",
        "    # Create figure\n",
        "    plt.figure(figsize=figsize)\n",
        "\n",
        "    # Plot horizontal bars\n",
        "    plt.barh(range(len(labels)), top_markers['combined_score'], color='steelblue')\n",
        "\n",
        "    # Add labels and title\n",
        "    plt.yticks(range(len(labels)), labels)\n",
        "    plt.xlabel('Combined Score')\n",
        "    plt.title(f'Top {top_n} Corrosion Markers by Score')\n",
        "    plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return plt.gcf()\n",
        "\n",
        "plot_top_markers_by_score(prioritized_markers, top_n=20, figsize=(8, 6))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsj41VblqH0R"
      },
      "source": [
        "### Genus-Mechanism Heatmap\n",
        "Shows relationships between top genera and corrosion mechanisms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTlyG6qMqMb3",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def plot_genus_mechanism_heatmap(prioritized_markers, top_genera=10, top_markers_per_genus=5, figsize=None):\n",
        "    \"\"\"\n",
        "    Create a heatmap showing top genera and their corrosion mechanisms\n",
        "    \"\"\"\n",
        "    # Ensure corrosion_mechanisms column exists\n",
        "    if 'corrosion_mechanisms' not in prioritized_markers.columns:\n",
        "        raise ValueError(\"DataFrame must contain 'corrosion_mechanisms' column\")\n",
        "\n",
        "    # Get top genera\n",
        "    top_genera_list = prioritized_markers['Genus'].value_counts().head(top_genera).index.tolist()\n",
        "\n",
        "    # Filter for top genera\n",
        "    genus_df = prioritized_markers[prioritized_markers['Genus'].isin(top_genera_list)].copy(deep=False)\n",
        "\n",
        "    # Get top markers per genus\n",
        "    top_markers = []\n",
        "    for genus in top_genera_list:\n",
        "        genus_markers = genus_df[genus_df['Genus'] == genus].head(top_markers_per_genus)\n",
        "        top_markers.append(genus_markers)\n",
        "\n",
        "    top_markers_df = pd.concat(top_markers)\n",
        "\n",
        "    # Extract all unique mechanisms\n",
        "    all_mechanisms = set()\n",
        "    for mechs in top_markers_df['corrosion_mechanisms'].dropna():\n",
        "        if isinstance(mechs, str):\n",
        "            all_mechanisms.update([m.strip() for m in mechs.split(';')])\n",
        "\n",
        "    all_mechanisms = sorted(list(all_mechanisms))\n",
        "\n",
        "    # Create matrix for heatmap (genus x mechanism)\n",
        "    heatmap_data = np.zeros((len(top_genera_list), len(all_mechanisms)))\n",
        "\n",
        "    # Fill matrix with scores\n",
        "    for i, genus in enumerate(top_genera_list):\n",
        "        genus_markers = top_markers_df[top_markers_df['Genus'] == genus]\n",
        "\n",
        "        for _, row in genus_markers.iterrows():\n",
        "            if pd.notna(row['corrosion_mechanisms']):\n",
        "                mechanisms = [m.strip() for m in row['corrosion_mechanisms'].split(';')]\n",
        "                score = row['combined_score']\n",
        "\n",
        "                for mechanism in mechanisms:\n",
        "                    if mechanism in all_mechanisms:\n",
        "                        j = all_mechanisms.index(mechanism)\n",
        "                        heatmap_data[i, j] += score\n",
        "\n",
        "    # Create heatmap\n",
        "    plt.figure(figsize=figsize)\n",
        "    sns.heatmap(heatmap_data, annot=True, fmt='.1f', cmap='YlGnBu',\n",
        "                xticklabels=all_mechanisms, yticklabels=top_genera_list)\n",
        "\n",
        "    plt.title('Genus-Mechanism Relationship (Combined Score)')\n",
        "    plt.ylabel('Genus')\n",
        "    plt.xlabel('Corrosion Mechanism')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    return plt.gcf()\n",
        "\n",
        "plot_genus_mechanism_heatmap(prioritized_markers, top_genera=10, top_markers_per_genus=5, figsize=(8, 6))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWqnGHDmqPg4"
      },
      "source": [
        "### Abundance Across Categories\n",
        "Line plots showing how abundance changes across corrosion categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sepU-uzRhOY2",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def plot_abundance_across_categories(prioritized_markers, top_n=5, figsize=None):\n",
        "    \"\"\"\n",
        "    Plot abundance patterns across corrosion categories for top markers\n",
        "    \"\"\"\n",
        "    # Get mean columns\n",
        "    mean_cols = [col for col in prioritized_markers.columns if col.startswith('mean_')]\n",
        "\n",
        "    if not mean_cols:\n",
        "        raise ValueError(\"No mean abundance columns found (should start with 'mean_')\")\n",
        "\n",
        "    # Get top markers\n",
        "    top_markers = prioritized_markers.head(top_n)\n",
        "\n",
        "    # Extract category numbers from column names\n",
        "    categories = [col.replace('mean_', '') for col in mean_cols]\n",
        "\n",
        "    # Create figure\n",
        "    plt.figure(figsize=figsize)\n",
        "\n",
        "    # Plot abundance patterns for each marker\n",
        "    for i, (_, row) in enumerate(top_markers.iterrows()):\n",
        "        label = f\"{row['Genus']} - {row['protein_name'][:20]}\"\n",
        "        values = [row[col] for col in mean_cols]\n",
        "        plt.plot(categories, values, marker='o', linewidth=2, label=label)\n",
        "\n",
        "    # Add labels and title\n",
        "    plt.xlabel('Corrosion Category')\n",
        "    plt.ylabel('Mean Abundance')\n",
        "    plt.title(f'Abundance Patterns Across Corrosion Categories (Top {top_n} Markers)')\n",
        "    plt.grid(linestyle='--', alpha=0.7)\n",
        "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return plt.gcf()\n",
        "\n",
        "plot_abundance_across_categories(prioritized_markers, top_n=5, figsize=(8, 6))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2xnVZOYqYGP"
      },
      "source": [
        "### Pathway Classification Breakdown\n",
        "Shows distribution of universal vs niche-specific pathways\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXTXJ0gQm2es",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def plot_pathway_classification_breakdown(prioritized_markers, figsize=(8, 6)):\n",
        "    \"\"\"\n",
        "    Create pie and bar charts showing pathway classification breakdown\n",
        "    \"\"\"\n",
        "    if 'pathway_classification' not in prioritized_markers.columns:\n",
        "        raise ValueError(\"DataFrame must contain 'pathway_classification' column\")\n",
        "\n",
        "    # Count pathway classifications\n",
        "    class_counts = prioritized_markers['pathway_classification'].value_counts()\n",
        "\n",
        "    # Create figure with subplots\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)\n",
        "\n",
        "    # Pie chart\n",
        "    ax1.pie(class_counts, labels=class_counts.index, autopct='%1.1f%%',\n",
        "           shadow=True, startangle=90, colors=['#ff9999','#66b3ff','#99ff99'])\n",
        "    ax1.set_title('Pathway Classification Distribution')\n",
        "\n",
        "    # Bar chart\n",
        "    ax2.bar(class_counts.index, class_counts.values, color=['#ff9999','#66b3ff','#99ff99'])\n",
        "    ax2.set_title('Pathway Classification Counts')\n",
        "    ax2.set_ylabel('Count')\n",
        "\n",
        "    #plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "classified_results= complete_results['classified_results']\n",
        "plot_pathway_classification_breakdown(classified_results, figsize=(12, 8))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6cJpbQ5jmh0",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def prepare_genera_pca(base_matrix, category_mapping=None):\n",
        "    \"\"\"\n",
        "    Prepare genera data for PCA with handling of multi-index categories\n",
        "\n",
        "    Parameters: base_matrix : pandas.DataFrame, Matrix with multi-index (Sites, Category)\n",
        "                category_mapping : pandas.Series,  Category mapping\n",
        "\n",
        "    Returns:    X_pca : numpy.ndarray, PCA transformed data\n",
        "                explained_variance_ratio : numpy.ndarray  Explained variance ratios\n",
        "                loadings : pandas.DataFrame, PCA loadings\n",
        "                categories : pandas.Series,  Categories for each site\n",
        "    \"\"\"\n",
        "    # Extract categories if they're in the multi-index\n",
        "    if isinstance(base_matrix.index, pd.MultiIndex):\n",
        "        categories = base_matrix.index.get_level_values('Category')\n",
        "        # No need to drop category as it's in the index\n",
        "        X = base_matrix\n",
        "    else:\n",
        "        # Use provided category mapping or None\n",
        "        categories = category_mapping\n",
        "        X = base_matrix\n",
        "\n",
        "    # No need for iloc[1:] as we don't have enzyme names as first row anymore\n",
        "    X_for_scaling = X.astype(float)\n",
        "\n",
        "    # Standardize\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X_for_scaling)\n",
        "\n",
        "    # PCA\n",
        "    pca = PCA(n_components=2)\n",
        "    X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "    # Create loadings DataFrame with proper multi-index columns\n",
        "    loadings = pd.DataFrame(\n",
        "        pca.components_.T,\n",
        "        index=X.columns,  # preserving multi-index columns (Genus, protein_name)\n",
        "        columns=['PC1', 'PC2']\n",
        "    )\n",
        "\n",
        "    return X_pca, pca.explained_variance_ratio_, loadings, categories\n",
        "def plot_pca_results(X_pca, explained_variance, Category, title, category_colors, categories_labels,\n",
        "                     pc1_idx=0, pc2_idx=1):  # Add parameters for component indices\n",
        "    \"\"\"\n",
        "    Plot PCA with risk categories\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    X_pca : numpy array   PCA transformed data\n",
        "    explained_variance : numpy array        Explained variance ratios\n",
        "    Category : array-like  Category labels for each sample\n",
        "    title : str   Plot title\n",
        "    category_colors : dict  Mapping of categories to colors\n",
        "    categories_labels : dict  Mapping of categories to display labels\n",
        "    pc1_idx : int        Index of the first PC to plot (default 0 for PC1)\n",
        "    pc2_idx : int        Index of the second PC to plot (default 1 for PC2)\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 8))\n",
        "\n",
        "    # Plot using specified components\n",
        "    for category in sorted(set(Category)):\n",
        "        mask = Category == category\n",
        "        plt.scatter(\n",
        "            X_pca[mask, pc1_idx],  # Specified PC for x-axis\n",
        "            X_pca[mask, pc2_idx],  # Specified PC for y-axis\n",
        "            c=category_colors[category],\n",
        "            label=categories_labels[category],\n",
        "            alpha=0.7,\n",
        "            s=100\n",
        "        )\n",
        "\n",
        "    plt.xlabel(f'PC{pc1_idx+1} ({explained_variance[pc1_idx]:.1%} variance explained)')\n",
        "    plt.ylabel(f'PC{pc2_idx+1} ({explained_variance[pc2_idx]:.1%} variance explained)')\n",
        "    plt.title(title)\n",
        "    plt.legend(title='Risk Category')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def prepare_flexible_pca(data_matrix, categories=None, n_components=None):\n",
        "    \"\"\"\n",
        "    Prepare PCA with flexible number of components\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    data_matrix : pandas DataFrame   Input data with samples as rows and features as columns\n",
        "    categories : array-like,   Category labels for each sample\n",
        "    n_components : int,   Number of components to calculate (None for all possible)\n",
        "    n_plot : int   Number of components to return for plotting\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    X_pca : numpy array   PCA transformed data (first n_plot components)\n",
        "    explained_variance : numpy array  Explained variance ratios for all components\n",
        "    loadings : pandas DataFrame  PCA loadings with feature names as index\n",
        "    \"\"\"\n",
        "    # Standardize\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(data_matrix)\n",
        "\n",
        "    # PCA\n",
        "    pca = PCA(n_components=n_components)\n",
        "    X_pca_full = pca.fit_transform(X_scaled)\n",
        "\n",
        "    # Get loadings for all components\n",
        "    loadings = pd.DataFrame(\n",
        "        pca.components_.T,\n",
        "        index=data_matrix.columns,\n",
        "        columns=[f'PC{i+1}' for i in range(pca.n_components_)]\n",
        "    )\n",
        "\n",
        "    # Return only requested components for plotting\n",
        "    X_pca = X_pca_full\n",
        "\n",
        "    # Return all calculated components\n",
        "    return X_pca_full, pca.explained_variance_ratio_, loadings\n",
        "\n",
        "# Calculate PCA with all components\n",
        "X_pca_all, var_ratio_all, loadings_all = prepare_flexible_pca(base_matrix)\n",
        "\n",
        "# Plot different component combinations\n",
        "# PC1 vs PC2 (default)\n",
        "plot_pca_results(X_pca_all, var_ratio_all, categories.values, \"PC1 vs PC2\",\n",
        "                 category_colors, categories_labels)\n",
        "\n",
        "# PC2 vs PC3\n",
        "plot_pca_results(X_pca_all, var_ratio_all, categories.values, \"PC2 vs PC3\",\n",
        "                 category_colors, categories_labels,\n",
        "                 pc1_idx=1, pc2_idx=2)\n",
        "\n",
        "# PC3 vs PC4\n",
        "plot_pca_results(X_pca_all, var_ratio_all, categories.values, \"PC3 vs PC4\",\n",
        "                 category_colors, categories_labels,\n",
        "                 pc1_idx=2, pc2_idx=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_830Bc_bGY3w"
      },
      "source": [
        "## Adapting for Source Groups"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJiXA5rnlid7"
      },
      "source": [
        "## 9.5 Retrieving Statistically Significant Groups\n",
        "\n",
        "From notebook 3_Feature_selection the file finalist.xlsx contain the groups worked and that were statistically significant in relation to the risk label. This groups posses interest since the relationship to the label could show better understanding in contrast with the different groups of known bacteria, core taxa, checked bacteria and the mixed groups.\n",
        "The idea is to understand if the core taxa which make up a large influence on the comunities on the water and cooling systems are also influencing corrosion.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvd8IvkvlcPL",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "source_groups = {\n",
        "    \"known_bacteria\": known_bacteria_list,\n",
        "    \"pure_checked\": pure_checked_list,\n",
        "    \"pure_core\": pure_core_list,\n",
        "    \"checked_core\": checked_core_list\n",
        "}\n",
        "\n",
        "Influencers_uniques_path = base_dir / \"finalist_dfs.xlsx\"\n",
        "# Integrated taxa from origin genus as headers with levels 6 for the genera, 7 for the GID, muss be cleaned\n",
        "Influencers_uniques = pd.read_excel(Influencers_uniques_path, sheet_name='Influencers_uniques', header=[0,1,2,3,4,5,6,7], engine ='openpyxl')\n",
        "# Drop first row (index 0) and first column in one chain\n",
        "Influencers_uniques = Influencers_uniques.drop(index=0)\n",
        "Influencers_uniques = Influencers_uniques.drop(Influencers_uniques.columns[0], axis=1)\n",
        "Influencers_uniques = Influencers_uniques.astype({'Sites': str})\n",
        "# Remove 'Unnamed' level names\n",
        "Influencers_uniques.columns = Influencers_uniques.columns.map(lambda x: tuple('' if \"Unnamed\" in str(level) else level for level in x))\n",
        "Influencers_uniques_list= Influencers_uniques.columns.get_level_values(6)\n",
        "Influencers_uniques_list= Influencers_uniques_list[Influencers_uniques_list !='']\n",
        "\n",
        "### Updating the groups to visualise\n",
        "source_groups = {\n",
        "    \"known_bacteria\": known_bacteria_list,\n",
        "    \"pure_checked\": pure_checked_list,\n",
        "    \"pure_core\": pure_core_list,\n",
        "    \"checked_core\": checked_core_list,\n",
        "    \"Influencers_uniques\": Influencers_uniques_list,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JaOwWSY1DFP9",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Suppose known_bacteria_list is our list of genera\n",
        "group_cols_known = [col for col in base_matrix.columns if col[0] in known_bacteria_list]\n",
        "base_matrix_known = base_matrix.loc[:, group_cols_known]\n",
        "\n",
        "plot_top_proteins_across_categories(\n",
        "    base_matrix_known,\n",
        "    categories=[1,2,3],\n",
        "    n_top=5,\n",
        "    n_genera=10,\n",
        "    category_level=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tLvr9eYYel7"
      },
      "source": [
        "# 10. Pathways Analysis\n",
        "\n",
        "## 10.1. Pathways distribution by Risk Category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dakf67H0Yel7",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def prepare_pathway_pca(metabolic_info, use_col='Pathways'):\n",
        "    \"\"\"\n",
        "    Convert pathway strings to numeric features for PCA\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    metabolic_info : pandas.DataFrame,   DataFrame with 'Pathways' column containing comma-separated pathway strings\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    X_pca : numpy.ndarray,    PCA transformed data\n",
        "    explained_variance_ratio : numpy.ndarray,     Explained variance ratios\n",
        "    loadings : pandas.DataFrame,   PCA loadings with pathway names as index\n",
        "    pathway_matrix : pandas.Dataframe,  Binary matrix of pathway presence/absence (useful for further analysis)\n",
        "    \"\"\"\n",
        "    # Handle NaN values first\n",
        "    valid_data = metabolic_info[metabolic_info[use_col].notna()]\n",
        "\n",
        "    # Create set of unique items with explicit string handling\n",
        "    all_items = set()\n",
        "    for item_str in valid_data[use_col]:\n",
        "        if isinstance(item_str, str):  # Ensure it's a string\n",
        "            items = [i.strip() for i in item_str.strip('[]').split(',') if i.strip()]\n",
        "            all_items.update(items)\n",
        "\n",
        "    # Create binary matrix with explicit index preservation\n",
        "    data_dict = {}\n",
        "    original_index = metabolic_info.index\n",
        "\n",
        "    for item in all_items:\n",
        "        if item:  # Skip empty strings\n",
        "            item_escaped = re.escape(item)\n",
        "            data_dict[item] = metabolic_info[use_col].str.contains(\n",
        "                item_escaped,\n",
        "                regex=True,\n",
        "                na=False\n",
        "            ).astype(int)\n",
        "\n",
        "    data_matrix = pd.DataFrame(data_dict, index=original_index)\n",
        "\n",
        "    # Print debug info\n",
        "    print(f\"Created matrix with {data_matrix.shape[1]} features\")\n",
        "    print(f\"Non-zero entries: {data_matrix.astype(bool).sum().sum()}\")\n",
        "\n",
        "    # Run PCA with explicit scaling\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(data_matrix)\n",
        "    pca = PCA(n_components=2)\n",
        "    X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "    loadings = pd.DataFrame(\n",
        "        pca.components_.T,\n",
        "        index=data_matrix.columns,\n",
        "        columns=['PC1', 'PC2']\n",
        "    )\n",
        "\n",
        "    return X_pca, pca.explained_variance_ratio_, loadings, data_matrix\n",
        "\n",
        "def plot_metabolic_pca_results(X_pca, explained_variance, metabolic_sites_info, category_dict, title, category_colors, categories_labels):\n",
        "    \"\"\"\n",
        "    Plot PCA results for pathways with risk categories\n",
        "\n",
        "    Parameters:     X_pca : numpy array  PCA transformed coordinates\n",
        "                    explained_variance : numpy array   Explained variance ratios\n",
        "                    metabolic_info : pandas DataFrame   The metabolic info DataFrame with Sites index\n",
        "                    category_dict : dict     Mapping of sites to categories\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 8))\n",
        "\n",
        "    # Get categories for each site in metabolic_info\n",
        "    if isinstance(metabolic_sites_info.index, pd.MultiIndex):\n",
        "        Sites = metabolic_sites_info.index.get_level_values('Sites')\n",
        "    else:\n",
        "        Sites = metabolic_sites_info.index\n",
        "\n",
        "    plot_categories = pd.Series(Sites).map(category_dict)\n",
        "\n",
        "    # Plot each category\n",
        "    for category in sorted(set(plot_categories)):\n",
        "        mask = plot_categories == category\n",
        "        plt.scatter( X_pca[mask, 0], X_pca[mask, 1], c=category_colors[category],\n",
        "            label=categories_labels[category], alpha=0.7, s=100)\n",
        "\n",
        "    plt.xlabel(f'PC1 ({explained_variance[0]:.1%} variance explained)')\n",
        "    plt.ylabel(f'PC2 ({explained_variance[1]:.1%} variance explained)')\n",
        "    plt.title(title)\n",
        "    plt.legend(title='Risk Category')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "# For pathway PCA\n",
        "X_pca_path, var_ratio_path, loadings_path, pathway_matrix = prepare_pathway_pca(metabolic_sites_info, use_col='Pathways')\n",
        "\n",
        "plot_metabolic_pca_results( X_pca_path, var_ratio_path,  metabolic_sites_info, category_dict, \"Pathways PCA by Risk Category\", category_colors, categories_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqmkIJeaYel7"
      },
      "source": [
        "## 10.2. Top Pathways Loadings by Category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gY-sQA13Yel7",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def plot_pca_loadings_heatmap(loadings, top_n=20):\n",
        "    \"\"\"Plot a heatmap of pathway loadings for PC1 and PC2.\n",
        "       Parameters:     loadings: DataFrame with PCA loadings\n",
        "       top_n: Number of top pathways to display     \"\"\"\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    # Select top pathways based on absolute contribution to PC1 and PC2\n",
        "    top_pathways = (loadings[['PC1', 'PC2']].abs().sum(axis=1).nlargest(top_n).index)\n",
        "    # Filter the loadings dataframe\n",
        "    heatmap_data = loadings.loc[top_pathways, ['PC1', 'PC2']]\n",
        "    sns.heatmap(heatmap_data, annot=True, cmap='coolwarm', center=0)\n",
        "    plt.title('Top Pathway Contributions to PC1 and PC2')\n",
        "    plt.xlabel('Principal Components')\n",
        "    plt.ylabel('Pathways')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_pca_loadings_heatmap(loadings_genera)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXRurjWFuvgW"
      },
      "source": [
        "## 10.3. Pathways patterns by source groups"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GH3EfeFrYel8"
      },
      "source": [
        "|Sites|---|site_1|site_1|site_1|site_2|site_2|site_2|site_2|\n",
        "|---|---|---|---|---|---|---|---|---|\n",
        "|Genus|---|genus_1|genus_2|genus3|genus_2|genus_70|genus_154|genus_520|\n",
        "|Pathways|---|---|---|---|---|---|---|---|\n",
        "|pathway_1|---|---|---|---|---|---|---|---|\n",
        "|pathway_2|---|---|---|---|---|---|---|---|\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0i9EqyPYel8",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def analyze_bacterial_groups(base_matrix, metabolic_sites_info, source_groups):\n",
        "    \"\"\"\n",
        "    Analyze relationships between bacterial groups and functional patterns.\n",
        "\n",
        "    Parameters:\n",
        "    - base_matrix: DataFrame with sites and functional data.\n",
        "      (Columns are multi-indexed (Site, Genus) or similar structure.)\n",
        "    - metabolic_sites_info: DataFrame with site-genus level information.\n",
        "    - source_groups: dict with group names as keys and list of genera as values.\n",
        "\n",
        "    Returns:\n",
        "    - results: dict with analysis results for each group.\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "\n",
        "    for source_name, genus_list in source_groups.items():\n",
        "\n",
        "        # Filter columns where the first level (e.g., site or genus) is in the group list.\n",
        "        group_cols = [col for col in base_matrix.columns if col[0] in genus_list]\n",
        "        group_data = base_matrix.loc[:, group_cols]\n",
        "\n",
        "        # Standardize the data\n",
        "        scaler = MinMaxScaler() # Changing from standard scaler to robustscaler\n",
        "        scaled_data = scaler.fit_transform(group_data)\n",
        "\n",
        "        # PCA analysis\n",
        "        pca = PCA(n_components=5)\n",
        "        pca_result = pca.fit_transform(scaled_data)\n",
        "\n",
        "        print(\"\\nPCA Variance Explained:\")\n",
        "        for i, var in enumerate(pca.explained_variance_ratio_):\n",
        "            print(f\"PC{i+1}: {var:.2%}\")\n",
        "        print(f\"Total variance explained: {sum(pca.explained_variance_ratio_):.2%}\")\n",
        "\n",
        "        # UMAP analysis\n",
        "        reducer = umap.UMAP(random_state=42)\n",
        "        umap_result = reducer.fit_transform(scaled_data)\n",
        "\n",
        "        # Save results for current group\n",
        "        results[source_name] = {\n",
        "            'pca': pca_result,\n",
        "            'umap': umap_result,\n",
        "            'pca_explained': pca.explained_variance_ratio_,\n",
        "            'data': group_data\n",
        "        }\n",
        "\n",
        "        # Plottinextract categories from base_matrix index if available.\n",
        "        try:\n",
        "            categories = base_matrix.index.get_level_values('Category')\n",
        "        except (KeyError, AttributeError):\n",
        "            # If no 'Category' level, assign a default category (e.g., all 1)\n",
        "            categories = pd.Series(np.ones(group_data.shape[0]), index=group_data.index)\n",
        "\n",
        "        # PCA plot\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "        for cat in sorted(set(categories)):\n",
        "            mask = categories == cat\n",
        "            ax1.scatter(pca_result[mask, 0],\n",
        "                        pca_result[mask, 1],\n",
        "                        c=category_colors.get(cat, '#000000'),\n",
        "                        label=categories_labels.get(cat, f'Cat {cat}'),\n",
        "                        alpha=0.7)\n",
        "        ax1.set_title(f'PCA - {source_name}')\n",
        "        ax1.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
        "        ax1.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
        "        ax1.legend()\n",
        "\n",
        "        # UMAP plot\n",
        "        for cat in sorted(set(categories)):\n",
        "            mask = categories == cat\n",
        "            ax2.scatter(umap_result[mask, 0],\n",
        "                        umap_result[mask, 1],\n",
        "                        c=category_colors.get(cat, '#000000'),\n",
        "                        label=categories_labels.get(cat, f'Cat {cat}'),\n",
        "                        alpha=0.7)\n",
        "        ax2.set_title(f'UMAP - {source_name}')\n",
        "        ax2.set_xlabel('UMAP 1')\n",
        "        ax2.set_ylabel('UMAP 2')\n",
        "        ax2.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # PCA Explained Variance plot\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(range(1, len(pca.explained_variance_ratio_) + 1),\n",
        "                 np.cumsum(pca.explained_variance_ratio_), 'bo-')\n",
        "        plt.xlabel('Number of Components')\n",
        "        plt.ylabel('Cumulative Explained Variance Ratio')\n",
        "        plt.title(f'PCA Explained Variance - {source_name}')\n",
        "        plt.grid(True)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BjwgGTo22fJB",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "print(f\"{known_bacteria}: group_data.shape = {group_data.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNhqT9G92y19",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "known_bacteria_list = ['Clostridium', 'Corynebacterium', 'Novosphingobium', 'Streptococcus', 'Thiobacillus', 'Acetobacterium', 'Bacillus', 'Desulfotomaculum', 'Desulfovibrio', 'Micrococcus', 'Propionibacterium',\n",
        " 'Pseudomonas', 'Staphylococcus', 'Desulfobacterium', 'Desulfobulbus', 'Gallionella', 'Shewanella']\n",
        "\n",
        "group_cols_known = [col for col in base_matrix.columns if col[0] in known_bacteria_list]\n",
        "group_data_known = base_matrix.loc[:, group_cols_known]\n",
        "\n",
        "group_data_known.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJteNaoZ46f7",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "print(group_data_known.head(), group_data_known.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWvbkrMc6_Yu",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "for gname, glist in source_groups.items():\n",
        "    group_cols = [col for col in base_matrix.columns if col[0] in glist]\n",
        "    tmp_data = base_matrix.loc[:, group_cols]\n",
        "    print(gname, tmp_data.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ad6mjcN7MhN",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "corr_matrix = group_data_known.corr()\n",
        "corr_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z55hs_rh7ULF",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "corr_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eH_ifJ0wnnLo"
      },
      "source": [
        "## Bacterial Groups Analysis Component"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IsAIvUBYntEu",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def analyze_combined_groups(base_matrix, source_groups, group_names=['checked_core', 'Influencers_uniques']):\n",
        "    \"\"\"\n",
        "    Analyze combined bacterial groups while preserving their individual contributions.\n",
        "\n",
        "    Parameters:\n",
        "    - base_matrix: DataFrame with sites and functional data\n",
        "    - source_groups: dict with group names as keys and list of genera as values\n",
        "    - group_names: list of group names to combine\n",
        "\n",
        "    Returns:\n",
        "    - Combined analysis results including PCA, UMAP and variance explained\n",
        "    \"\"\"\n",
        "    # Filter for selected groups\n",
        "    selected_genera = []\n",
        "    for group in group_names:\n",
        "        selected_genera.extend(source_groups[group])\n",
        "\n",
        "    # Remove duplicates while preserving order\n",
        "    selected_genera = list(dict.fromkeys(selected_genera))\n",
        "\n",
        "    # Filter columns for selected genera\n",
        "    group_cols = [col for col in base_matrix.columns if col[0] in selected_genera]\n",
        "    combined_data = base_matrix.loc[:, group_cols]\n",
        "\n",
        "    # Remove zero columns\n",
        "    combined_data = combined_data.loc[:, (combined_data != 0).any(axis=0)]\n",
        "\n",
        "    # Standardize\n",
        "    scaler = StandardScaler()\n",
        "    scaled_data = scaler.fit_transform(combined_data)\n",
        "\n",
        "    # PCA\n",
        "    pca = PCA(n_components=3)\n",
        "    pca_result = pca.fit_transform(scaled_data)\n",
        "\n",
        "    # UMAP\n",
        "    reducer = umap.UMAP(random_state=42)\n",
        "    umap_result = reducer.fit_transform(scaled_data)\n",
        "\n",
        "    results = {\n",
        "        'pca': pca_result,\n",
        "        'umap': umap_result,\n",
        "        'pca_explained': pca.explained_variance_ratio_,\n",
        "        'data': combined_data,\n",
        "        'genera': selected_genera\n",
        "    }\n",
        "\n",
        "    # Plottinextract categories from base_matrix index if available.\n",
        "    try:\n",
        "        categories = base_matrix.index.get_level_values('Category')\n",
        "    except (KeyError, AttributeError):\n",
        "        # If no 'Category' level, assign a default category (e.g., all 1)\n",
        "        categories = pd.Series(np.ones(combined_data.shape[0]), index=combined_data.index)\n",
        "\n",
        "    # PCA plot\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "    for cat in sorted(set(categories)):\n",
        "        mask = categories == cat\n",
        "        ax1.scatter(pca_result[mask, 0],\n",
        "                    pca_result[mask, 1],\n",
        "                    c=category_colors.get(cat, '#000000'),\n",
        "                    label=categories_labels.get(cat, f'Cat {cat}'),\n",
        "                    alpha=0.7)\n",
        "    ax1.set_title(f'PCA - {combined_data}')\n",
        "    ax1.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
        "    ax1.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
        "    ax1.legend()\n",
        "\n",
        "    # UMAP plot\n",
        "    for cat in sorted(set(categories)):\n",
        "        mask = categories == cat\n",
        "        ax2.scatter(umap_result[mask, 0],\n",
        "                    umap_result[mask, 1],\n",
        "                    c=category_colors.get(cat, '#000000'),\n",
        "                    label=categories_labels.get(cat, f'Cat {cat}'),\n",
        "                    alpha=0.7)\n",
        "    ax2.set_title(f'UMAP - {group_names}')\n",
        "    ax2.set_xlabel('UMAP 1')\n",
        "    ax2.set_ylabel('UMAP 2')\n",
        "    ax2.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # PCA Explained Variance plot\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(range(1, len(pca.explained_variance_ratio_) + 1),\n",
        "              np.cumsum(pca.explained_variance_ratio_), 'bo-')\n",
        "    plt.xlabel('Number of Components')\n",
        "    plt.ylabel('Cumulative Explained Variance Ratio')\n",
        "    plt.title(f'PCA Explained Variance - {group_names}')\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Voj9pVqlAR3E",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "results = analyze_combined_groups(base_matrix, source_groups, group_names=['checked_core', 'Influencers_uniques'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuOHFlIQJ1YD"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJhbme4LoEgY",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from scipy import stats\n",
        "from statsmodels.stats.multitest import multipletests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ytCoE1icYel8",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def analyze_category_enrichment(base_matrix, category_dict, source_groups):\n",
        "    \"\"\"\n",
        "    Analyze pathway enrichment within each risk category.\n",
        "    Uses relative abundance and statistical testing to identify\n",
        "    significantly enriched proteins in each category.\n",
        "    \"\"\"\n",
        "    # Get the sites and categories from the MultiIndex\n",
        "    sites_categories = pd.Series(\n",
        "        base_matrix.index.get_level_values('Category'),\n",
        "        index=base_matrix.index.get_level_values('Sites')\n",
        "    )\n",
        "\n",
        "    def get_enrichment_for_group(group_data, category):\n",
        "        # Get data for this category\n",
        "        cat_mask = group_data.index.get_level_values(\"Category\")  == category\n",
        "        cat_data = group_data[cat_mask]\n",
        "        other_data= group_data[~cat_mask]\n",
        "\n",
        "        # Calculate mean abundances\n",
        "        cat_means = cat_data.mean()\n",
        "        other_means = other_data.mean()\n",
        "\n",
        "        # Calculate fold change\n",
        "        fold_change = np.log2(cat_means / other_means)\n",
        "\n",
        "        # Perform statistical test (Mann-Whitney U)\n",
        "        pvalues = []\n",
        "        for col in group_data.columns:\n",
        "            stat, pval = stats.mannwhitneyu(\n",
        "                cat_data[col],\n",
        "                other_data[col],\n",
        "                alternative='greater'\n",
        "            )\n",
        "            pvalues.append(pval)\n",
        "\n",
        "        # Create results DataFrame\n",
        "        results = pd.DataFrame({\n",
        "            'fold_change': fold_change,\n",
        "            'pvalue': pvalues,\n",
        "            'mean_abundance': cat_means\n",
        "        })\n",
        "\n",
        "        # Add multiple testing correction\n",
        "        results['padj'] = multipletests(results['pvalue'], method='fdr_bh')[1]\n",
        "\n",
        "        return results\n",
        "\n",
        "    enrichment_results = {}\n",
        "\n",
        "    # Analyze each source group\n",
        "    for group_name, genera in source_groups.items():\n",
        "        print(f\"\\nAnalyzing {group_name}...\")\n",
        "\n",
        "        # Filter for genera in this group\n",
        "        group_cols = [col for col in base_matrix.columns if col[0] in genera]\n",
        "        if not group_cols:\n",
        "            continue\n",
        "\n",
        "        group_data = base_matrix[group_cols]\n",
        "\n",
        "        # Get enrichment for each category\n",
        "        group_results = {}\n",
        "        for cat in [1, 2, 3]:\n",
        "            results = get_enrichment_for_group(group_data, cat)\n",
        "            group_results[cat] = results\n",
        "\n",
        "        enrichment_results[group_name] = group_results\n",
        "\n",
        "        # Plot volcano plots for each category\n",
        "        plt.figure(figsize=(15, 5))\n",
        "        plt.suptitle(f\"Protein Enrichment Analysis - {group_name}\", y=1.05)\n",
        "\n",
        "        for i, cat in enumerate([1, 2, 3], 1):\n",
        "            results = group_results[cat]\n",
        "\n",
        "            plt.subplot(1, 3, i)\n",
        "\n",
        "            # Create volcano plot\n",
        "            plt.scatter(\n",
        "                results['fold_change'],\n",
        "                -np.log10(results['padj']),\n",
        "                alpha=0.6,\n",
        "                c=category_colors[cat],\n",
        "                s= results['mean_abundance']*1000\n",
        "            )\n",
        "\n",
        "            # Add significance lines\n",
        "            plt.axhline(-np.log10(0.05), color='red', linestyle='--', alpha=0.3)\n",
        "            plt.axvline(0, color='black', linestyle='--', alpha=0.3)\n",
        "\n",
        "            plt.title(f\"{categories_labels[cat]}\")\n",
        "            plt.xlabel(\"Log2 Fold Change\")\n",
        "            plt.ylabel(\"-log10(adjusted p-value)\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Print top enriched proteins\n",
        "        for cat in [1, 2, 3]:\n",
        "            results = group_results[cat]\n",
        "            significant = results[results['padj'] < 0.05].sort_values('fold_change', ascending=False)\n",
        "\n",
        "            print(f\"\\nTop enriched proteins in {categories_labels[cat]} for {group_name}:\")\n",
        "            if len(significant) > 0:\n",
        "                print(significant.head(10))\n",
        "            else:\n",
        "                print(\"No significantly enriched proteins found\")\n",
        "\n",
        "    return enrichment_results\n",
        "\n",
        "# Run the analysis\n",
        "enrichment_results = analyze_category_enrichment(base_matrix, category_dict, source_groups)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpDMQg0fZVGn",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def create_comparison_table(enrichment_results):\n",
        "    \"\"\"\n",
        "    Create a structured comparison table from enrichment results\n",
        "    \"\"\"\n",
        "    # Create empty list to store rows\n",
        "    comparison_rows = []\n",
        "\n",
        "    for group_name, group_results in enrichment_results.items():\n",
        "        for category in [1, 2, 3]:\n",
        "            if category in group_results:\n",
        "                results = group_results[category]\n",
        "                significant = results[results['padj'] < 0.05]\n",
        "\n",
        "                if len(significant) > 0:\n",
        "                    for idx, row in significant.head(10).iterrows():\n",
        "                        comparison_rows.append({\n",
        "                            'Group': group_name,\n",
        "                            'Category': categories_labels[category],\n",
        "                            'Genus': idx[0],\n",
        "                            'Protein': idx[1],\n",
        "                            'Fold_Change': row['fold_change'],\n",
        "                            'Padj': row['padj'],\n",
        "                            'Mean_Abundance': row['mean_abundance']\n",
        "                        })\n",
        "\n",
        "    # Create DataFrame\n",
        "    comparison_df = pd.DataFrame(comparison_rows)\n",
        "\n",
        "    # Save to CSV with proper formatting\n",
        "    comparison_df.to_csv('enrichment_comparison.csv', index=False)\n",
        "\n",
        "    return comparison_df\n",
        "\n",
        "# Create comparison table\n",
        "comparison_table = create_comparison_table(enrichment_results)\n",
        "\n",
        "# Display formatted table\n",
        "print(\"\\nComparison Table Preview:\")\n",
        "print(comparison_table.to_string())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zoPuDB4Yel8"
      },
      "source": [
        "__________________________________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxMZgylFpBX4"
      },
      "source": [
        "https://www.youtube.com/watch?v=jQVNsyAnDMo\n",
        "\n",
        "https://microreact.org/\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWCQOr4KuvgW",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def create_integrated_visualization(df, results, metadata=None):\n",
        "    \"\"\"\n",
        "    Create an integrated visualization combining PCA, clustering, and metadata\n",
        "\n",
        "    Parameters:\n",
        "    df: Original pathway data\n",
        "    results: Results from explore_pathway_patterns\n",
        "    metadata: DataFrame with risk labels, materials, etc.\n",
        "    \"\"\"\n",
        "    fig = plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # 1. PCA with clustering\n",
        "    pca_data = results['pca']['components']\n",
        "    clusters = results['clustering'][5]['kmeans']  # Using k=5 clusters\n",
        "\n",
        "    plt.subplot(2, 2, 1)\n",
        "    scatter = plt.scatter(pca_data[:, 0], pca_data[:, 1],\n",
        "                         c=clusters, cmap='Set2', alpha=0.6)\n",
        "    plt.title('PCA Components with Clusters')\n",
        "    plt.xlabel('PC1')\n",
        "    plt.ylabel('PC2')\n",
        "    plt.colorbar(scatter, label='Cluster')\n",
        "\n",
        "    # 2. Top pathway contributions\n",
        "    plt.subplot(2, 2, 2)\n",
        "    top_loadings = abs(results['pca']['loadings']['PC1']).nlargest(10)\n",
        "    sns.barplot(x=top_loadings.values, y=top_loadings.index)\n",
        "    plt.title('Top 10 Pathways Contributing to PC1')\n",
        "    plt.xlabel('Absolute Loading')\n",
        "\n",
        "    # 3. Correlation structure summary\n",
        "    plt.subplot(2, 2, 3)\n",
        "    corr_summary = results['correlation'].abs().mean()\n",
        "    sns.histplot(corr_summary, bins=50)\n",
        "    plt.title('Distribution of Mean Correlation Strengths')\n",
        "    plt.xlabel('Mean |Correlation|')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZvr6vjWuvgW",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "create_integrated_visualization(base_matrix, results_patterns, metadata=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bk9_ZGhfuvgW"
      },
      "source": [
        "## 9.3 Analysing Pathways Organic Fate\n",
        "\n",
        "Now the task is to identify the most abundant pathways in the samples, focusing specifically on organic matter-related metabolism. Ultimately creating visualizations to understand pathway distributions and analyze correlations between pathways."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11HYL0iNuvgW",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def analyze_metabolic_pathways(df):\n",
        "    \"\"\"\n",
        "    Analyze metabolic pathways from PICRUSt output\n",
        "\n",
        "    Parameters:\n",
        "    df: pandas DataFrame with pathways as index and samples as columns\n",
        "    \"\"\"\n",
        "    # Calculate mean abundance across samples for each pathway\n",
        "    mean_abundance = df.mean(axis=1).sort_values(ascending=False)\n",
        "\n",
        "    # Get top 20 most abundant pathways\n",
        "    top_pathways = mean_abundance.head(20)\n",
        "\n",
        "    # Create heatmap of top pathways across samples\n",
        "    plt.figure(figsize=(15, 8))\n",
        "    sns.heatmap(df.loc[top_pathways.index],\n",
        "                cmap='YlOrRd',\n",
        "                center=0,\n",
        "                robust=True,\n",
        "                xticklabels=True,\n",
        "                yticklabels=True)\n",
        "    plt.title('Top 20 Most Abundant Pathways Across Samples')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Filter for organic matter metabolism related pathways\n",
        "    organic_terms = ['carbon', 'carbohydrate', 'lipid', 'fatty acid',\n",
        "                    'organic acid', 'amino acid', 'degradation']\n",
        "\n",
        "    organic_pathways = df.index[df.index.str.lower().str.contains('|'.join(organic_terms))]\n",
        "    organic_data = df.loc[organic_pathways]\n",
        "\n",
        "    # Calculate summary statistics for organic matter pathways\n",
        "    pathway_stats = pd.DataFrame({\n",
        "        'mean_abundance': organic_data.mean(axis=1),\n",
        "        'std_abundance': organic_data.std(axis=1),\n",
        "        'cv': organic_data.std(axis=1) / organic_data.mean(axis=1) * 100\n",
        "    }).sort_values('mean_abundance', ascending=False)\n",
        "\n",
        "    return pathway_stats, organic_data\n",
        "\n",
        "def plot_pathway_distribution(pathway_stats):\n",
        "    \"\"\"Plot distribution of pathway abundances\"\"\"\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.boxplot(data=pathway_stats.reset_index(),\n",
        "                x='mean_abundance',\n",
        "                y='index',\n",
        "                order=pathway_stats.index[:15])\n",
        "    plt.title('Top 15 Organic Matter Related Pathways')\n",
        "    plt.xlabel('Mean Abundance')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Calling the function\n",
        "stats, organic_data = analyze_metabolic_pathways(Picrust_Result)\n",
        "plot_pathway_distribution(stats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZkQ1j5X5fHL",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def analyze_pathway_patterns(df):\n",
        "    \"\"\"\n",
        "    Analyze pathway patterns using sites vs pathways abundances\n",
        "    \"\"\"\n",
        "    # Create the correct matrix: sites vs pathways with abundances\n",
        "    pathway_matrix = df.pivot_table(\n",
        "        values='norm_abund_contri',\n",
        "        index='Sites',          # Sites as rows\n",
        "        columns='Pathways',     # Pathways as columns\n",
        "        aggfunc='sum',          # Sum abundances\n",
        "        fill_value=0\n",
        "    )\n",
        "\n",
        "    print(\"Matrix shape:\", pathway_matrix.shape)\n",
        "\n",
        "    # Standardize data\n",
        "    scaler = StandardScaler()\n",
        "    scaled_data = scaler.fit_transform(pathway_matrix)\n",
        "\n",
        "    # PCA\n",
        "    pca = PCA(n_components=5)\n",
        "    X_pca = pca.fit_transform(scaled_data)\n",
        "\n",
        "    # UMAP\n",
        "    reducer = umap.UMAP(random_state=42)\n",
        "    umap_result = reducer.fit_transform(scaled_data)\n",
        "\n",
        "    # Get categories for sites\n",
        "    categories = pd.Series(pathway_matrix.index).map(lambda x: category_dict[x])\n",
        "\n",
        "    # Plot both PCA and UMAP\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
        "\n",
        "    # PCA explained variance\n",
        "    ax1.plot(range(1, 6), pca.explained_variance_ratio_, 'bo-')\n",
        "    print(\"\\nPCA Variance Explained:\")\n",
        "    print(f\"Total (5 components): {sum(pca.explained_variance_ratio_):.2%}\")\n",
        "    ax1.set_title('PCA Explained Variance')\n",
        "    ax1.set_xlabel('Component')\n",
        "    ax1.set_ylabel('Explained Variance Ratio')\n",
        "\n",
        "    # PCA scatter\n",
        "    for cat in category_colors.keys():\n",
        "        mask = categories == cat\n",
        "        ax2.scatter(X_pca[mask, 0],\n",
        "                   X_pca[mask, 1],\n",
        "                   c=category_colors[cat],\n",
        "                   label=categories_labels[cat],\n",
        "                   alpha=0.7)\n",
        "\n",
        "    ax2.set_title('PCA First Two Components')\n",
        "    ax2.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
        "    ax2.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
        "    ax2.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # UMAP plot\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    for cat in category_colors.keys():\n",
        "        mask = categories == cat\n",
        "        plt.scatter(umap_result[mask, 0],\n",
        "                   umap_result[mask, 1],\n",
        "                   c=category_colors[cat],\n",
        "                   label=categories_labels[cat],\n",
        "                   alpha=0.7)\n",
        "\n",
        "    plt.title('UMAP Projection of Pathways')\n",
        "    plt.xlabel('UMAP1')\n",
        "    plt.ylabel('UMAP2')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Run the analysis\n",
        "results = analyze_pathway_patterns(metabolic_sites_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9GJgu3yuvgW",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# To analyze specific pathways of interest:\n",
        "def analyze_specific_pathways(df, pathway_list):\n",
        "    \"\"\"\n",
        "    Analyze specific pathways of interest\n",
        "\n",
        "    Parameters:\n",
        "    df: DataFrame with pathway data\n",
        "    pathway_list: list of pathway names to analyze\n",
        "    \"\"\"\n",
        "    specific_data = df.loc[df.index.str.contains('|'.join(pathway_list), case=False)]\n",
        "\n",
        "    # Create correlation matrix for these pathways\n",
        "    corr = specific_data.T.corr()\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(corr, annot=True, cmap='coolwarm', center=0)\n",
        "    plt.title('Correlation between Selected Pathways')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return specific_data.describe()\n",
        "\n",
        "# Calling the funtion\n",
        "Description = analyze_specific_pathways(Picrust_Result, Picrust_Result.index.tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HEIwON2uvgW"
      },
      "source": [
        "## 9.4. Pathways Relevant to Corrosion\n",
        "This code witll categorise pathways into key groups: sulfur metabolism (critical for sulfate-reducing bacteria), Metal-related pathways (iron, manganese, etc.); organic acid production (which can influence local pH); biofilm formation (important for corrosion processes) and electron transfer mechanisms. Then it would analyse correlations between these different categories to understand potential synergistic effects, identifying the most abundant pathways in each category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8FN-ChvuvgW",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def analyze_corrosion_pathways(df):\n",
        "    \"\"\"\n",
        "    Analyze pathways relevant to microbially influenced corrosion (MIC)\n",
        "\n",
        "    Parameters:\n",
        "    df: pandas DataFrame with pathways as index and samples as columns\n",
        "    \"\"\"\n",
        "    # Define relevant pathway terms for different corrosion mechanisms\n",
        "    pathway_categories = {\n",
        "        'sulfur': ['sulfur', 'sulfate', 'sulfide', 'thiosulfate', 'sulfite', 'sulfonate'],\n",
        "        'metal': ['iron', 'metal', 'Fe', 'manganese', 'chromium', 'nickel'],\n",
        "        'organic_acid': ['organic acid', 'acetate', 'formate', 'lactate', 'pyruvate'],\n",
        "        'biofilm': ['biofilm', 'exopolysaccharide', 'EPS', 'adhesion'],\n",
        "        'electron_transfer': ['cytochrome', 'electron transport', 'oxidoreductase']\n",
        "    }\n",
        "\n",
        "    # Function to filter pathways by category\n",
        "    def get_category_pathways(terms):\n",
        "        return df.index[df.index.str.lower().str.contains('|'.join(terms), regex=True)]\n",
        "\n",
        "    # Analyze each category\n",
        "    category_data = {}\n",
        "    category_stats = {}\n",
        "\n",
        "    for category, terms in pathway_categories.items():\n",
        "        pathways = get_category_pathways(terms)\n",
        "        if len(pathways) > 0:\n",
        "            category_data[category] = df.loc[pathways]\n",
        "            category_stats[category] = pd.DataFrame({\n",
        "                'mean_abundance': category_data[category].mean(axis=1),\n",
        "                'std_abundance': category_data[category].std(axis=1),\n",
        "                'cv': category_data[category].std(axis=1) / category_data[category].mean(axis=1) * 100\n",
        "            }).sort_values('mean_abundance', ascending=False)\n",
        "\n",
        "    return category_data, category_stats\n",
        "\n",
        "def plot_corrosion_pathways(category_data, category_stats):\n",
        "    \"\"\"\n",
        "    Create visualizations for corrosion-related pathways\n",
        "    \"\"\"\n",
        "    # Plot top pathways for each category\n",
        "    for category, data in category_stats.items():\n",
        "        if len(data) > 0:\n",
        "            plt.figure(figsize=(12, min(6, max(3, len(data)*0.3))))\n",
        "            sns.barplot(data=data.head(10).reset_index(),\n",
        "                       x='mean_abundance',\n",
        "                       y='index',\n",
        "                       palette='YlOrRd')\n",
        "            plt.title(f'Top {min(10, len(data))} {category.replace(\"_\", \" \").title()} Related Pathways')\n",
        "            plt.xlabel('Mean Abundance')\n",
        "            plt.ylabel('Pathway')\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "    # Create correlation heatmap between categories\n",
        "    category_means = pd.DataFrame({\n",
        "        cat: data.mean(axis=1) for cat, data in category_data.items()\n",
        "    })\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(category_means.corr(),\n",
        "                annot=True,\n",
        "                cmap='coolwarm',\n",
        "                center=0,\n",
        "                vmin=-1,\n",
        "                vmax=1)\n",
        "    plt.title('Correlation between Pathway Categories')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def analyze_pathway_interactions(df, category_data):\n",
        "    \"\"\"\n",
        "    Analyze interactions between different pathway categories\n",
        "    \"\"\"\n",
        "    # Calculate mean abundance for each category\n",
        "    category_abundances = pd.DataFrame({\n",
        "        category: data.mean(axis=0)\n",
        "        for category, data in category_data.items()\n",
        "    })\n",
        "\n",
        "    # Calculate correlations between categories\n",
        "    correlations = category_abundances.corr()\n",
        "\n",
        "    # Identify potential synergistic relationships\n",
        "    high_correlations = correlations.unstack()\n",
        "    high_correlations = high_correlations[high_correlations != 1.0]\n",
        "    high_correlations = high_correlations[abs(high_correlations) > 0.5]\n",
        "\n",
        "    return category_abundances, correlations, high_correlations.sort_values(ascending=False)\n",
        "\n",
        "# Analysing Corrosion Pathways\n",
        "category_data, category_stats = analyze_corrosion_pathways(Picrust_Result)\n",
        "plot_corrosion_pathways(category_data, category_stats)\n",
        "abundances, correlations, high_corr = analyze_pathway_interactions(Picrust_Result, category_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bi4wuyN7uvgW"
      },
      "source": [
        "## 9.5. Heating and Cooling Systems Pathway Analysis\n",
        "Creating independent analyses:\n",
        "\n",
        "Failure analysis (based on human assessment/estimation)\n",
        "Microbiological analysis (16S rRNA)\n",
        "Physicochemical parameters\n",
        "\n",
        "\n",
        "Using physicochemical parameters as labels/indicators of corrosion state - this is quite clever because it gives us an objective measure without directly mixing in the biological data\n",
        "Then planning to correlate the microbial communities with these states through machine learning\n",
        "\n",
        "And now to use PICRUSt's functional predictions to validate our assumptions about organic matter metabolism. It can help confirm if the bacteria identified through correlations actually have the metabolic capacity to influence corrosion\n",
        "It might reveal unexpected metabolic pathways that could explain the correlations. The following script will Validate our organic matter assumptions by:\n",
        "\n",
        "Breaking down different types of organic matter processing\n",
        "Looking at both degradation and synthesis pathways\n",
        "Identifying transport mechanisms\n",
        "\n",
        "Connect with our physicochemical parameters by analyzing pathways that could influence:\n",
        "\n",
        "pH modulation\n",
        "Temperature response\n",
        "Metal interactions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWBttrFbuvgW",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def analyze_system_pathways(df):\n",
        "    \"\"\"\n",
        "    Analyze pathways relevant to heating/cooling system corrosion\n",
        "\n",
        "    Parameters:\n",
        "    df: pandas DataFrame with pathways as index and samples as columns\n",
        "    \"\"\"\n",
        "    # Define pathway categories relevant to system conditions\n",
        "    pathway_categories = {\n",
        "        # Water chemistry influence\n",
        "        'ph_modulation': ['acid', 'alkaline', 'proton pump', 'pH homeostasis'],\n",
        "\n",
        "        # Temperature adaptation\n",
        "        'temp_response': ['heat shock', 'cold shock', 'temperature response'],\n",
        "\n",
        "        # Organic matter processing\n",
        "        'carbon_metabolism': [\n",
        "            'carbon fixation', 'carbon utilization',\n",
        "            'organic acid', 'fatty acid',\n",
        "            'carbohydrate metabolism'\n",
        "        ],\n",
        "\n",
        "        # Corrosion-related\n",
        "        'metal_interaction': [\n",
        "            'iron', 'metal', 'oxidation-reduction',\n",
        "            'electron transport', 'metal binding'\n",
        "        ],\n",
        "\n",
        "        # Biofilm formation\n",
        "        'surface_attachment': [\n",
        "            'biofilm', 'adhesion', 'exopolysaccharide',\n",
        "            'extracellular matrix'\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    # Filter and analyze pathways\n",
        "    def get_category_pathways(terms):\n",
        "        return df.index[df.index.str.lower().str.contains('|'.join(terms), regex=True)]\n",
        "\n",
        "    category_data = {}\n",
        "    category_stats = {}\n",
        "\n",
        "    for category, terms in pathway_categories.items():\n",
        "        pathways = get_category_pathways(terms)\n",
        "        if len(pathways) > 0:\n",
        "            category_data[category] = df.loc[pathways]\n",
        "\n",
        "            # Calculate basic statistics\n",
        "            category_stats[category] = pd.DataFrame({\n",
        "                'mean_abundance': category_data[category].mean(axis=1),\n",
        "                'std_abundance': category_data[category].std(axis=1),\n",
        "                'cv': category_data[category].std(axis=1) / category_data[category].mean(axis=1) * 100,\n",
        "                'presence': (category_data[category] > 0).mean(axis=1) * 100  # % of samples with pathway\n",
        "            }).sort_values('mean_abundance', ascending=False)\n",
        "\n",
        "    return category_data, category_stats\n",
        "\n",
        "def analyze_organic_matter_pathways(df):\n",
        "    \"\"\"\n",
        "    Detailed analysis of organic matter related pathways\n",
        "    \"\"\"\n",
        "    # Specific organic matter categories\n",
        "    organic_categories = {\n",
        "        'degradation': ['degradation', 'breakdown', 'catabolism'],\n",
        "        'synthesis': ['biosynthesis', 'anabolism', 'synthesis'],\n",
        "        'transport': ['transport', 'uptake', 'export'],\n",
        "        'modification': ['modification', 'conversion', 'transformation']\n",
        "    }\n",
        "\n",
        "    organic_data = {}\n",
        "\n",
        "    for category, terms in organic_categories.items():\n",
        "        pathways = df.index[df.index.str.lower().str.contains(\n",
        "            '|'.join(terms), regex=True\n",
        "        ) & df.index.str.lower().str.contains(\n",
        "            'organic|carbon|fatty acid|lipid|protein|amino acid'\n",
        "        )]\n",
        "        if len(pathways) > 0:\n",
        "            organic_data[category] = df.loc[pathways]\n",
        "\n",
        "    return organic_data\n",
        "\n",
        "def plot_pathway_distributions(category_stats, category_data):\n",
        "    \"\"\"\n",
        "    Create visualizations for pathway distributions\n",
        "    \"\"\"\n",
        "    for category, stats in category_stats.items():\n",
        "        if len(stats) > 0:\n",
        "            # Create subplot with dual axis\n",
        "            fig, ax1 = plt.subplots(figsize=(12, min(8, max(4, len(stats)*0.3))))\n",
        "            ax2 = ax1.twinx()\n",
        "\n",
        "            # Plot mean abundance\n",
        "            sns.barplot(data=stats.head(10).reset_index(),\n",
        "                       x='mean_abundance',\n",
        "                       y='index',\n",
        "                       color='skyblue',\n",
        "                       ax=ax1)\n",
        "\n",
        "            # Plot presence percentage\n",
        "            stats.head(10)['presence'].plot(\n",
        "                marker='o',\n",
        "                color='red',\n",
        "                ax=ax2\n",
        "            )\n",
        "\n",
        "            ax1.set_title(f'{category.replace(\"_\", \" \").title()} Pathways')\n",
        "            ax1.set_xlabel('Mean Abundance')\n",
        "            ax2.set_xlabel('Presence (%)')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "# Calling the analysis\n",
        "category_data, category_stats = analyze_system_pathways(Picrust_Result)\n",
        "organic_data = analyze_organic_matter_pathways(Picrust_Result)\n",
        "plot_pathway_distributions(category_stats, category_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFrF4oTRuvgX"
      },
      "source": [
        "I have a big gap on the cation anion account and then used mackensy, 2012 method from the usgs to check ec measured Vs calculated and cation Vs ions. It is a big gap still, but I have a lot of OM so I could no assume as normally that OM is CH4 so I attribute it to small organic acids and put acetate and oxalate as OM representatives, I have a small study of small acids form on failure analysis and also report of a mass that has a magnetic consistency, so I infere that those muss be some organic metalic compound but only accounted for AC- and Ox-2, I thought better to chose this other compounds Fe rich but I don't know how to do it actually. So in my bacteria I actually found lots of them with Ac- metabolism whiles I was looking at the families I realise no only oxobacter accendants, but others similar, also got important biofilm formers, there is also halogen related and should be, big deal of difference make the material and location cause water treatment, unfortunately the annotations are no to be taken as parameters but can serve as annotations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXMFmbSruvgX"
      },
      "source": [
        "validate assumptions about:\n",
        "\n",
        "Organic acid presence (by showing metabolic capability)\n",
        "Metal-organic complex formation (through siderophore and metal-binding pathways)\n",
        "Biofilm formation potential (which can influence local chemistry)\n",
        "\n",
        "Validate acetate/oxalate assumptions by showing if these metabolic pathways are actually present\n",
        "Look for other potential organic acid pathways might want to consider\n",
        "Identify metal-organic interaction pathways that could explain magnetic mass observation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3sujtH1uvgX",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def analyze_organic_metal_pathways(df):\n",
        "    \"\"\"\n",
        "    Analyze pathways related to organic acid metabolism and metal interactions\n",
        "\n",
        "    Parameters:\n",
        "    df: pandas DataFrame with pathways as index and samples as columns\n",
        "    \"\"\"\n",
        "    # Define specific pathway categories\n",
        "    pathway_categories = {\n",
        "        'organic_acid_metabolism': [\n",
        "            'acetate', 'acetic acid', 'acetyl',\n",
        "            'oxalate', 'oxalic acid',\n",
        "            'organic acid', 'fatty acid',\n",
        "            'carboxylic acid'\n",
        "        ],\n",
        "\n",
        "        'metal_organic_interaction': [\n",
        "            'siderophore', 'metal binding',\n",
        "            'iron complex', 'metal transport',\n",
        "            'metallophore', 'metal organic'\n",
        "        ],\n",
        "\n",
        "        'biofilm_formation': [\n",
        "            'biofilm', 'exopolysaccharide',\n",
        "            'extracellular matrix', 'adhesion'\n",
        "        ],\n",
        "\n",
        "        'halogen_related': [\n",
        "            'halogen', 'chloride', 'bromide',\n",
        "            'halide', 'dehalogenation'\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    # Analyze each category\n",
        "    def get_category_pathways(terms):\n",
        "        return df.index[df.index.str.lower().str.contains('|'.join(terms), regex=True)]\n",
        "\n",
        "    pathway_data = {}\n",
        "    pathway_stats = {}\n",
        "\n",
        "    for category, terms in pathway_categories.items():\n",
        "        pathways = get_category_pathways(terms)\n",
        "        if len(pathways) > 0:\n",
        "            pathway_data[category] = df.loc[pathways]\n",
        "\n",
        "            # Calculate comprehensive statistics\n",
        "            pathway_stats[category] = pd.DataFrame({\n",
        "                'mean_abundance': pathway_data[category].mean(axis=1),\n",
        "                'std_abundance': pathway_data[category].std(axis=1),\n",
        "                'cv': pathway_data[category].std(axis=1) / pathway_data[category].mean(axis=1) * 100,\n",
        "                'presence': (pathway_data[category] > 0).mean(axis=1) * 100,  # % of samples with pathway\n",
        "                'relative_abundance': pathway_data[category].mean(axis=1) / df.mean(axis=1).mean() * 100\n",
        "            }).sort_values('mean_abundance', ascending=False)\n",
        "\n",
        "    return pathway_data, pathway_stats\n",
        "\n",
        "def analyze_pathway_relationships(pathway_data):\n",
        "    \"\"\"\n",
        "    Analyze relationships between different pathway categories\n",
        "    \"\"\"\n",
        "    # Calculate mean abundance for each category across samples\n",
        "    category_means = pd.DataFrame({\n",
        "        category: data.mean(axis=0)\n",
        "        for category, data in pathway_data.items()\n",
        "    })\n",
        "\n",
        "    # Calculate correlations\n",
        "    correlations = category_means.corr()\n",
        "\n",
        "    # Identify potential functional relationships\n",
        "    high_correlations = correlations.unstack()\n",
        "    high_correlations = high_correlations[high_correlations != 1.0]\n",
        "    high_correlations = high_correlations[abs(high_correlations) > 0.5]\n",
        "\n",
        "    return category_means, correlations, high_correlations.sort_values(ascending=False)\n",
        "\n",
        "def plot_pathway_analysis(pathway_stats, pathway_data):\n",
        "    \"\"\"\n",
        "    Create visualizations for pathway analysis\n",
        "    \"\"\"\n",
        "    for category, stats in pathway_stats.items():\n",
        "        if len(stats) > 0:\n",
        "            # Create subplot with dual axis\n",
        "            fig, ax1 = plt.subplots(figsize=(12, min(8, max(4, len(stats)*0.3))))\n",
        "            ax2 = ax1.twinx()\n",
        "\n",
        "            # Plot abundance and relative abundance\n",
        "            sns.barplot(data=stats.head(10).reset_index(),\n",
        "                       x='relative_abundance',\n",
        "                       y='index',\n",
        "                       color='skyblue',\n",
        "                       ax=ax1)\n",
        "\n",
        "            stats.head(10)['presence'].plot(\n",
        "                marker='o',\n",
        "                color='red',\n",
        "                ax=ax2\n",
        "            )\n",
        "\n",
        "            ax1.set_title(f'{category.replace(\"_\", \" \").title()} Pathways')\n",
        "            ax1.set_xlabel('Relative Abundance (%)')\n",
        "            ax2.set_xlabel('Presence (%)')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "# calling the function\n",
        "pathway_data, pathway_stats = analyze_organic_metal_pathways(Picrust_Result)category_means, correlations, high_corr = analyze_pathway_relationships(pathway_data)\n",
        "plot_pathway_analysis(pathway_stats, pathway_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvV8Sor3uvgX"
      },
      "source": [
        "## 9.6. Corrosion Relevant Pathways\n",
        "\n",
        "Focus on corrosion-relevant pathways by categorizing them into:\n",
        "\n",
        "Organic acid metabolism (relevant to our acetate/oxalate observations)\n",
        "Sulfur metabolism\n",
        "Metal interactions\n",
        "Biofilm formation\n",
        "\n",
        "\n",
        "Handle the high-dimensional data by:\n",
        "\n",
        "Using dimensionality reduction (PCA)\n",
        "Calculating summary statistics\n",
        "Visualizing key patterns\n",
        "\n",
        "\n",
        "Address our specific interests:\n",
        "\n",
        "Organic matter metabolism pathways\n",
        "Metal-organic interactions\n",
        "Correlations with physicochemical parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EThn4vkFuvgX",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def analyze_corrosion_pathways(df):\n",
        "    \"\"\"\n",
        "    Analyze pathways relevant to microbially influenced corrosion\n",
        "    \"\"\"\n",
        "    # Define pathway categories relevant to corrosion\n",
        "    pathway_categories = {\n",
        "        'organic_acid': [\n",
        "            'CENTFERM-PWY',  # Central fermentation pathways\n",
        "            'FERMENTATION-PWY',  # Mixed acid fermentation\n",
        "            'GLYCOLYSIS',  # Glucose fermentation\n",
        "            'PWY-5100',  # Pyruvate fermentation\n",
        "            'GALACTUROCAT-PWY'  # Galacturonate degradation\n",
        "        ],\n",
        "        'sulfur': [\n",
        "            'PWY-6932',  # Sulfate reduction\n",
        "            'SO4ASSIM-PWY',  # Sulfate assimilation\n",
        "            'SULFATE-CYS-PWY'  # Sulfate to cysteine\n",
        "        ],\n",
        "        'metal_interaction': [\n",
        "            'PWY-7219',  # Iron oxidation\n",
        "            'PWY-7221',  # Iron reduction\n",
        "            'HEME-BIOSYNTHESIS-II',  # Iron-containing compounds\n",
        "            'P125-PWY'  # Metal resistance\n",
        "        ],\n",
        "        'biofilm': [\n",
        "            'COLANSYN-PWY',  # Colanic acid (biofilm)\n",
        "            'EXOPOLYSACC-PWY',  # Exopolysaccharide\n",
        "            'GLUCOSE1PMETAB-PWY'  # UDP-glucose synthesis\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    # Extract relevant pathways and their abundances\n",
        "    relevant_pathways = {}\n",
        "    for category, pathways in pathway_categories.items():\n",
        "        category_data = df[df.index.isin(pathways)]\n",
        "        if not category_data.empty:\n",
        "            relevant_pathways[category] = category_data\n",
        "\n",
        "    # Calculate summary statistics\n",
        "    summary_stats = {}\n",
        "    for category, data in relevant_pathways.items():\n",
        "        summary_stats[category] = {\n",
        "            'mean_abundance': data.mean().mean(),\n",
        "            'std_abundance': data.mean().std(),\n",
        "            'present_in_samples': (data > 0).mean().mean() * 100,\n",
        "            'pathways_found': len(data)\n",
        "        }\n",
        "\n",
        "    # Dimension reduction for visualization\n",
        "    if df.shape[0] > 0:\n",
        "        # Standardize the data\n",
        "        scaler = StandardScaler()\n",
        "        scaled_data = scaler.fit_transform(df.T)\n",
        "\n",
        "        # PCA\n",
        "        pca = PCA(n_components=2)\n",
        "        pca_result = pca.fit_transform(scaled_data)\n",
        "\n",
        "        return relevant_pathways, summary_stats, pca_result, pca.explained_variance_ratio_\n",
        "\n",
        "    return relevant_pathways, summary_stats, None, None\n",
        "\n",
        "def plot_pathway_analysis(relevant_pathways, summary_stats, pca_result=None, explained_variance=None):\n",
        "    \"\"\"\n",
        "    Create visualizations for pathway analysis\n",
        "    \"\"\"\n",
        "    # Plot mean abundances by category\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    categories = list(summary_stats.keys())\n",
        "    means = [stats['mean_abundance'] for stats in summary_stats.values()]\n",
        "    presence = [stats['present_in_samples'] for stats in summary_stats.values()]\n",
        "\n",
        "    ax1 = plt.gca()\n",
        "    ax2 = ax1.twinx()\n",
        "\n",
        "    bars = ax1.bar(categories, means, alpha=0.6, color='skyblue')\n",
        "    ax1.set_ylabel('Mean Abundance')\n",
        "\n",
        "    line = ax2.plot(categories, presence, 'ro-', label='Presence %')\n",
        "    ax2.set_ylabel('Presence in Samples (%)')\n",
        "\n",
        "    plt.title('Pathway Categories Overview')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # If PCA results available, plot them\n",
        "    if pca_result is not None:\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        plt.scatter(pca_result[:, 0], pca_result[:, 1], alpha=0.6)\n",
        "        plt.xlabel(f'PC1 ({explained_variance[0]*100:.1f}%)')\n",
        "        plt.ylabel(f'PC2 ({explained_variance[1]*100:.1f}%)')\n",
        "        plt.title('PCA of Pathway Abundances')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# Calling the functions\n",
        "relevant_pathways, summary_stats, pca_result, explained_variance = analyze_corrosion_pathways(Picrust_Result)\n",
        "plot_pathway_analysis(relevant_pathways, summary_stats, pca_result, explained_variance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZgXC50euvgX"
      },
      "source": [
        "## 9.7. Functional Pathway Clustering Analysis\n",
        "Hierarchical Clustering:\n",
        "\n",
        "Groups pathways based on their abundance patterns\n",
        "Creates a dendrogram to visualize relationships\n",
        "Automatically determines optimal number of clusters\n",
        "\n",
        "\n",
        "Correlation-based Analysis:\n",
        "\n",
        "Identifies pathways that behave similarly across samples\n",
        "Creates correlation heatmap to visualize relationships\n",
        "Helps identify functional modules\n",
        "\n",
        "\n",
        "Feature Creation:\n",
        "\n",
        "Generates new features based on cluster statistics:\n",
        "\n",
        "Mean abundance per cluster\n",
        "Total abundance per cluster\n",
        "Pathway diversity within clusters\n",
        "\n",
        "Reduce dimensionality while maintaining biological meaning\n",
        "Identify functional modules that might be working together\n",
        "Create more robust features for our ML analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6NsHQCquvgX",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def cluster_pathways(df, n_clusters=None, corr_threshold=0.7):\n",
        "    \"\"\"\n",
        "    Cluster pathways based on their functional similarity\n",
        "\n",
        "    Parameters:\n",
        "    df: DataFrame with pathways as rows and samples as columns\n",
        "    n_clusters: Number of clusters (if None, determined automatically)\n",
        "    corr_threshold: Correlation threshold for considering pathways related\n",
        "    \"\"\"\n",
        "    # Standardize the data\n",
        "    scaler = StandardScaler()\n",
        "    scaled_data = scaler.fit_transform(df.T).T\n",
        "\n",
        "    # Calculate correlation matrix\n",
        "    corr_matrix = np.corrcoef(scaled_data)\n",
        "\n",
        "    # Create linkage matrix for hierarchical clustering\n",
        "    linkage_matrix = hierarchy.linkage(pdist(scaled_data), method='ward')\n",
        "\n",
        "    if n_clusters is None:\n",
        "        # Automatically determine number of clusters using elbow method\n",
        "        last = linkage_matrix[-10:, 2]\n",
        "        acceleration = np.diff(last, 2)\n",
        "        n_clusters = len(last) - np.argmax(acceleration) + 1\n",
        "\n",
        "    # Perform clustering\n",
        "    clustering = AgglomerativeClustering(n_clusters=n_clusters)\n",
        "    cluster_labels = clustering.fit_predict(scaled_data)\n",
        "\n",
        "    # Create cluster summary\n",
        "    cluster_summary = pd.DataFrame({\n",
        "        'pathway': df.index,\n",
        "        'cluster': cluster_labels\n",
        "    })\n",
        "\n",
        "    return cluster_labels, linkage_matrix, corr_matrix, cluster_summary\n",
        "\n",
        "def analyze_pathway_clusters(df, cluster_labels):\n",
        "    \"\"\"\n",
        "    Analyze the characteristics of each pathway cluster\n",
        "    \"\"\"\n",
        "    cluster_stats = {}\n",
        "\n",
        "    for cluster in np.unique(cluster_labels):\n",
        "        # Get pathways in this cluster\n",
        "        cluster_paths = df.index[cluster_labels == cluster]\n",
        "        cluster_data = df.loc[cluster_paths]\n",
        "\n",
        "        # Calculate statistics\n",
        "        cluster_stats[cluster] = {\n",
        "            'size': len(cluster_paths),\n",
        "            'mean_abundance': cluster_data.mean().mean(),\n",
        "            'std_abundance': cluster_data.mean().std(),\n",
        "            'pathways': list(cluster_paths),\n",
        "            'correlation': np.corrcoef(cluster_data),\n",
        "            'total_abundance': cluster_data.sum().mean()\n",
        "        }\n",
        "\n",
        "    return cluster_stats\n",
        "\n",
        "def plot_pathway_clusters(df, linkage_matrix, corr_matrix, cluster_labels, cluster_stats):\n",
        "    \"\"\"\n",
        "    Create visualizations for pathway clusters\n",
        "    \"\"\"\n",
        "    # Plot dendrogram\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    plt.title('Pathway Clustering Dendrogram')\n",
        "    hierarchy.dendrogram(linkage_matrix, labels=df.index, leaf_rotation=90)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Plot correlation heatmap\n",
        "    plt.figure(figsize=(12, 12))\n",
        "    sns.heatmap(pd.DataFrame(corr_matrix, index=df.index, columns=df.index),\n",
        "                cmap='coolwarm', center=0, vmin=-1, vmax=1)\n",
        "    plt.title('Pathway Correlation Heatmap')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Plot cluster sizes and abundances\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    clusters = list(cluster_stats.keys())\n",
        "    sizes = [stats['size'] for stats in cluster_stats.values()]\n",
        "    abundances = [stats['mean_abundance'] for stats in cluster_stats.values()]\n",
        "\n",
        "    ax1 = plt.gca()\n",
        "    ax2 = ax1.twinx()\n",
        "\n",
        "    ax1.bar(clusters, sizes, alpha=0.6, color='skyblue')\n",
        "    ax1.set_ylabel('Number of Pathways')\n",
        "\n",
        "    ax2.plot(clusters, abundances, 'ro-')\n",
        "    ax2.set_ylabel('Mean Abundance')\n",
        "\n",
        "    plt.title('Cluster Sizes and Abundances')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def create_cluster_features(df, cluster_labels):\n",
        "    \"\"\"\n",
        "    Create new features based on pathway clusters\n",
        "    \"\"\"\n",
        "    n_clusters = len(np.unique(cluster_labels))\n",
        "    cluster_features = pd.DataFrame(index=df.columns)\n",
        "\n",
        "    for cluster in range(n_clusters):\n",
        "        # Get pathways in this cluster\n",
        "        cluster_paths = df.index[cluster_labels == cluster]\n",
        "\n",
        "        # Calculate mean abundance for cluster\n",
        "        cluster_features[f'cluster_{cluster}'] = df.loc[cluster_paths].mean()\n",
        "\n",
        "        # Calculate total abundance for cluster\n",
        "        cluster_features[f'cluster_{cluster}_total'] = df.loc[cluster_paths].sum()\n",
        "\n",
        "        # Calculate diversity within cluster\n",
        "        cluster_features[f'cluster_{cluster}_diversity'] = (df.loc[cluster_paths] > 0).sum()\n",
        "\n",
        "    return cluster_features\n",
        "\n",
        "# Calling the fUNCTION\n",
        "cluster_labels, linkage_matrix, corr_matrix, cluster_summary = cluster_pathways(Picrust_Result)\n",
        "cluster_stats = analyze_pathway_clusters(Picrust_Result, cluster_labels)\n",
        "plot_pathway_clusters(Picrust_Result, linkage_matrix, corr_matrix, cluster_labels, cluster_stats)\n",
        "cluster_features = create_cluster_features(Picrust_Result, cluster_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Os65js9EuvgX"
      },
      "source": [
        "# 13 Organic Metal Pathways"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vgw9Pwp8uvgX",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "def analyze_organic_metal_pathways(df):\n",
        "    \"\"\"\n",
        "    Analyze pathways related to organic acid metabolism and metal interactions\n",
        "\n",
        "    Parameters:\n",
        "    df: pandas DataFrame with pathways as index and samples as columns\n",
        "    \"\"\"\n",
        "    # Define specific pathway categories\n",
        "    pathway_categories = {\n",
        "        'organic_acid_metabolism': [\n",
        "            'acetate', 'acetic acid', 'acetyl',\n",
        "            'oxalate', 'oxalic acid',\n",
        "            'organic acid', 'fatty acid',\n",
        "            'carboxylic acid'\n",
        "        ],\n",
        "\n",
        "        'metal_organic_interaction': [\n",
        "            'siderophore', 'metal binding',\n",
        "            'iron complex', 'metal transport',\n",
        "            'metallophore', 'metal organic'\n",
        "        ],\n",
        "\n",
        "        'biofilm_formation': [\n",
        "            'biofilm', 'exopolysaccharide',\n",
        "            'extracellular matrix', 'adhesion'\n",
        "        ],\n",
        "\n",
        "        'halogen_related': [\n",
        "            'halogen', 'chloride', 'bromide',\n",
        "            'halide', 'dehalogenation'\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    # Analyze each category\n",
        "    def get_category_pathways(terms):\n",
        "        return df.index[df.index.str.lower().str.contains('|'.join(terms), regex=True)]\n",
        "\n",
        "    pathway_data = {}\n",
        "    pathway_stats = {}\n",
        "\n",
        "    for category, terms in pathway_categories.items():\n",
        "        pathways = get_category_pathways(terms)\n",
        "        if len(pathways) > 0:\n",
        "            pathway_data[category] = df.loc[pathways]\n",
        "\n",
        "            # Calculate comprehensive statistics\n",
        "            pathway_stats[category] = pd.DataFrame({\n",
        "                'mean_abundance': pathway_data[category].mean(axis=1),\n",
        "                'std_abundance': pathway_data[category].std(axis=1),\n",
        "                'cv': pathway_data[category].std(axis=1) / pathway_data[category].mean(axis=1) * 100,\n",
        "                'presence': (pathway_data[category] > 0).mean(axis=1) * 100,  # % of samples with pathway\n",
        "                'relative_abundance': pathway_data[category].mean(axis=1) / df.mean(axis=1).mean() * 100\n",
        "            }).sort_values('mean_abundance', ascending=False)\n",
        "\n",
        "    return pathway_data, pathway_stats\n",
        "\n",
        "def analyze_pathway_relationships(pathway_data):\n",
        "    \"\"\"\n",
        "    Analyze relationships between different pathway categories\n",
        "    \"\"\"\n",
        "    # Calculate mean abundance for each category across samples\n",
        "    category_means = pd.DataFrame({\n",
        "        category: data.mean(axis=0)\n",
        "        for category, data in pathway_data.items()\n",
        "    })\n",
        "\n",
        "    # Calculate correlations\n",
        "    correlations = category_means.corr()\n",
        "\n",
        "    # Identify potential functional relationships\n",
        "    high_correlations = correlations.unstack()\n",
        "    high_correlations = high_correlations[high_correlations != 1.0]\n",
        "    high_correlations = high_correlations[abs(high_correlations) > 0.5]\n",
        "\n",
        "    return category_means, correlations, high_correlations.sort_values(ascending=False)\n",
        "\n",
        "def plot_pathway_analysis(pathway_stats, pathway_data):\n",
        "    \"\"\"\n",
        "    Create visualizations for pathway analysis\n",
        "    \"\"\"\n",
        "    for category, stats in pathway_stats.items():\n",
        "        if len(stats) > 0:\n",
        "            # Create subplot with dual axis\n",
        "            fig, ax1 = plt.subplots(figsize=(12, min(8, max(4, len(stats)*0.3))))\n",
        "            ax2 = ax1.twinx()\n",
        "\n",
        "            # Plot abundance and relative abundance\n",
        "            sns.barplot(data=stats.head(10).reset_index(),\n",
        "                       x='relative_abundance',\n",
        "                       y='index',\n",
        "                       color='skyblue',\n",
        "                       ax=ax1)\n",
        "\n",
        "            stats.head(10)['presence'].plot(\n",
        "                marker='o',\n",
        "                color='red',\n",
        "                ax=ax2\n",
        "            )\n",
        "\n",
        "            ax1.set_title(f'{category.replace(\"_\", \" \").title()} Pathways')\n",
        "            ax1.set_xlabel('Relative Abundance (%)')\n",
        "            ax2.set_xlabel('Presence (%)')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Le_mzeY_uvgX",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Calling the function\n",
        "pathway_data, pathway_stats = analyze_organic_metal_pathways(Picrust_Result)\n",
        "category_means, correlations, high_corr = analyze_pathway_relationships(pathway_data)\n",
        "plot_pathway_analysis(pathway_stats, pathway_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wf76SqphuvgX"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EQUT3fwuvgX"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djNOlMWkuvgX",
        "trusted": true
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 6825079,
          "sourceId": 10969174,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 6825231,
          "sourceId": 10969366,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 6825260,
          "sourceId": 10996867,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30886,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
