{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cDED5j4zCqR_"
   },
   "source": [
    "## 1. Introduction\n",
    "This notebook analyzes the functional and sequence relationships from the study on bacteria from operational heating and cooling water systems, primarily in Germany. Using 16S rRNA data (bootstrap-validated from Notebook 5), this analysis employs PICRUSt2 to predict metabolic functions and compare functional profiles between different bacterial groups.\n",
    "\n",
    "### Analysis Approaches\n",
    "The notebook start by importing libraries, preparing the directories paths in section 1, section 2 prepares the data for the picrust pipeline on fasta, biom formates. Section 3-6 shows the process utilising directly algoritm for advance computer capabilities. The author approach utilises the Galaxy platform, where the original product files are found https://usegalaxy.eu/u/magicalex238/h/picrust2-functional. The results are imported on section 7 where a broad analysis is done on the global data exploring pathways,algoritm implementation, top general pathways and reactions. Section 8 involves retrieving the protein names for the granular data (EC contribution, ECcontri) from Galaxy results and for that utilises Uniprot Database. ECcontri_Uniprot data is analysed for the point of cutoff where the protein abundance has biological activity (Knee_abundance) for later use. Section 9 import several known and autoritative databases that are use to compile a new database (EC_records) comprising enzyme_names, mechanisms, pathways, metal_involved, protein hierarchi, reactions, amongst others. Then enriches the granular data with the newly created EC_records, put identifiers idx and introduce Sites and Categories. Section 10 is dedicated to the Filtering of protein-genus pairs pipeline on ECcontri_Uniprot_enriched. The data is classified by patterns, pathways (housekeeping, niche and mixed), separated between increasing, decreasing and mixed patterns, and the increasing patterns pairs are taken to the next step. The data is then prioritized by biological and statistical significance (prioritized_markers). Lastly is filtered with the knee abundance for biological activity (balanced_markers) and divided on marker_groups.\n",
    "Then Section 11 visualise the results, made some clustering between related pairs and choses the top 10 markers to bring into the machine learning compendium repository where is join with the physicochemical data for a more comprensive prediction on corrosion by microbiologically induced corrosion.\n",
    "\n",
    "### Directory Structure:\n",
    "``` text\n",
    "Sequence Analysis and Functional Prediction Pipeline\n",
    "├── Introduction  \n",
    "│   ├── 16S rRNA Data (bootstrap-validated)\n",
    "│   └── imports, paths, preparation data\n",
    "└── Analysis Approaches  \n",
    "    ├── Direct Algorithm (Sections 3-6)  \n",
    "    └── Galaxy Platform Analysis                             \n",
    "         ├── Section 7: Broad Analysis (pathways, reactions)\n",
    "         ├── Section 8: Protein Name Retrieval, data preparation\n",
    "         ├── Section 9: Database Creation and Data enrichment\n",
    "         │       ├──  EC_records: enzyme_names, mechanisms, pathways,\n",
    "         │       └──  metal_involved, reactions, hierarchy...\n",
    "         ├── Section 10: Filtering Protein-Genus Pairs pipeline\n",
    "         │       ├── patterns_data, classified_results,\n",
    "         │       └── prioritized_results...\n",
    "         └── Section 11:Visualization & Machine Learning  \n",
    "                 ├── Clustering related pairs  \n",
    "                 ├── Top 10 markers  \n",
    "                 └── further to ML (corrosion prediction)   \n",
    "\n",
    "Picrust2 works using its reference database that was installed with the package   ~/miniconda3/envs/picrust2/lib/python3.9/site-packages/picrust2/default_files/prokaryotic/pro_ref\n",
    "\n",
    "About picrust2  \n",
    "https://evomics.org/wp-content/uploads/2015/01/presentation_evomics-05-picrust_01-18-15.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LeIn7RIBCqSD"
   },
   "source": [
    "# 2. Loading and Preparing the Data\n",
    "\n",
    "## 2.1 Colab Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:07:24.262288Z",
     "iopub.status.busy": "2025-04-19T16:07:24.261554Z",
     "iopub.status.idle": "2025-04-19T16:07:24.271232Z",
     "shell.execute_reply": "2025-04-19T16:07:24.269761Z",
     "shell.execute_reply.started": "2025-04-19T16:07:24.262182Z"
    },
    "id": "ATJF6joLK02g",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Suppresses INFO and WARNING messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:07:24.273136Z",
     "iopub.status.busy": "2025-04-19T16:07:24.272778Z",
     "iopub.status.idle": "2025-04-19T16:07:24.294301Z",
     "shell.execute_reply": "2025-04-19T16:07:24.293033Z",
     "shell.execute_reply.started": "2025-04-19T16:07:24.273103Z"
    },
    "id": "m2gkBXEzwKkG",
    "outputId": "d8a77406-6902-4fac-8b7c-8cffb3e6dcf9",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#os.path.exists('/content/drive/MyDrive')\n",
    "\n",
    "#from google.colab import drive\n",
    "#drive.flush_and_unmount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:07:24.297680Z",
     "iopub.status.busy": "2025-04-19T16:07:24.297236Z",
     "iopub.status.idle": "2025-04-19T16:07:24.318405Z",
     "shell.execute_reply": "2025-04-19T16:07:24.316487Z",
     "shell.execute_reply.started": "2025-04-19T16:07:24.297642Z"
    },
    "id": "5U6gm_R_JQjt",
    "outputId": "aadee8f2-bcb9-418b-96a5-093645340734",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "'''# Colab specific\n",
    "from google.colab import drive\n",
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "change the path\n",
    "os.chdir('/content/drive/MyDrive/MIC/2_Micro/data_picrust')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T09:47:57.946272Z",
     "iopub.status.busy": "2025-04-21T09:47:57.945882Z",
     "iopub.status.idle": "2025-04-21T09:48:23.306492Z",
     "shell.execute_reply": "2025-04-21T09:48:23.305278Z",
     "shell.execute_reply.started": "2025-04-21T09:47:57.946234Z"
    },
    "id": "xcyAS5xNUyNx",
    "outputId": "73de5e85-92f2-43de-a2ab-d3458289cd71",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "'''# Making sure to use same python version for compatibility\n",
    "!sudo apt-get update -y\n",
    "!sudo apt-get install python3.10\n",
    "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.10 1\n",
    "!python --version'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VWqOI6IDD1qW"
   },
   "source": [
    "__Importing PICRUST IN COLAB__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:07:33.212589Z",
     "iopub.status.busy": "2025-04-19T16:07:33.212201Z",
     "iopub.status.idle": "2025-04-19T16:07:33.220977Z",
     "shell.execute_reply": "2025-04-19T16:07:33.219859Z",
     "shell.execute_reply.started": "2025-04-19T16:07:33.212551Z"
    },
    "id": "uKimriI3hmTq",
    "outputId": "b81742b6-750f-4e9d-9432-1abd583b987e",
    "trusted": true,
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "'''# Install miniconda and initialize:\n",
    "!wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
    "!bash Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local/miniconda3\n",
    "!conda config --add channels defaults\n",
    "!conda config --add channels bioconda\n",
    "!conda config --add channels conda-forge\n",
    "# Imports for colab\n",
    "import condacolab\n",
    "import sys\n",
    "sys.path.append('/usr/local/miniconda3/lib/python3.7/site-packages/')\n",
    "\n",
    "# Install PICRUSt2 and its dependencies\n",
    "%conda install -c bioconda -c conda-forge picrust2=2.4.1 -y\n",
    "# Verify installations%\n",
    "%conda list | grep picrust2'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HzaNXN9suvgG"
   },
   "source": [
    "### Using Pro colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:07:33.222521Z",
     "iopub.status.busy": "2025-04-19T16:07:33.222104Z",
     "iopub.status.idle": "2025-04-19T16:07:33.239965Z",
     "shell.execute_reply": "2025-04-19T16:07:33.238847Z",
     "shell.execute_reply.started": "2025-04-19T16:07:33.222480Z"
    },
    "id": "qF94FGxn2iUL",
    "outputId": "e045632e-f70a-4db6-bc15-7dc76a9a6aa9",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "'''import sys\n",
    "print([module for module in sys.modules if 'tensorflow' in module])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:07:33.241521Z",
     "iopub.status.busy": "2025-04-19T16:07:33.241175Z",
     "iopub.status.idle": "2025-04-19T16:07:33.259758Z",
     "shell.execute_reply": "2025-04-19T16:07:33.258604Z",
     "shell.execute_reply.started": "2025-04-19T16:07:33.241493Z"
    },
    "id": "3gWJfdx3Ni1f",
    "outputId": "4bed6970-4438-4ad9-9a03-3d5d821229bb",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "'''# Set up memory footprint support libraries\n",
    "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
    "!pip install gputil\n",
    "!pip install psutil\n",
    "%pip install humanize\n",
    "%pip install memory_profiler\n",
    "import psutil\n",
    "import humanize\n",
    "import os\n",
    "import GPUtil as GPU\n",
    "GPUs = GPU.getGPUs()\n",
    "from psutil import virtual_memory\n",
    "ram_gb = virtual_memory().total / 1e9\n",
    "print('Runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
    "\n",
    "if ram_gb < 20:\n",
    "  print('Not using a high-RAM runtime')\n",
    "else:\n",
    "  print('Using a high-RAM runtime!')'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTKk2e3eYelt"
   },
   "source": [
    "### Kaggle / Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T09:48:23.307919Z",
     "iopub.status.busy": "2025-04-21T09:48:23.307594Z",
     "iopub.status.idle": "2025-04-21T09:50:07.304282Z",
     "shell.execute_reply": "2025-04-21T09:50:07.303043Z",
     "shell.execute_reply.started": "2025-04-21T09:48:23.307887Z"
    },
    "id": "CfyrnbtLIh2D",
    "outputId": "c7abb7d1-5df9-4f90-d52d-4e64cb98963c",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install psutil\n",
    "import psutil\n",
    "!pip install biopython\n",
    "!pip install biom-format\n",
    "!pip install umap-learn\n",
    "!pip install fuzzywuzzy\n",
    "!pip install lxml pandas\n",
    "!pip install pyarrow\n",
    "!pip install openpyxl\n",
    "!pip install scipy\n",
    "!pip install python-Levenshtein\n",
    "!pip install -U kaleido\n",
    "!pip install statsmodels\n",
    "!pip install kneed\n",
    "!pip install natsort\n",
    "!python3 --version\n",
    "!pip install adjustText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FiHOVZiIuvgH"
   },
   "source": [
    "# 2.2. Importing Libraries,  Making Directories and Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T09:50:07.306311Z",
     "iopub.status.busy": "2025-04-21T09:50:07.305788Z",
     "iopub.status.idle": "2025-04-21T09:50:49.873925Z",
     "shell.execute_reply": "2025-04-21T09:50:49.872830Z",
     "shell.execute_reply.started": "2025-04-21T09:50:07.306244Z"
    },
    "id": "eVWYFfyQIh2E",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "import ast\n",
    "import subprocess\n",
    "import logging\n",
    "import time\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "from io import StringIO\n",
    "from pathlib import Path\n",
    "import re\n",
    "from IPython import get_ipython\n",
    "from IPython.display import display\n",
    "\n",
    "# Data processing and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "%matplotlib inline\n",
    "from matplotlib.colors import to_rgba, LinearSegmentedColormap\n",
    "import matplotlib.patches as mpatches\n",
    "from collections import defaultdict\n",
    "from collections import Counter \n",
    "\n",
    "# Machine learning and statistical analysis\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA, NMF\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
    "from scipy.cluster import hierarchy\n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "import scipy\n",
    "from scipy import stats\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.stats import spearmanr, kruskal, mannwhitneyu\n",
    "from kneed import KneeLocator\n",
    "from scipy.signal import savgol_filter\n",
    "from community import community_louvain\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Bioinformatics\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from biom import Table, load_table\n",
    "from biom.util import biom_open\n",
    "import xml.etree.ElementTree as ET\n",
    "# Retrieval and requesting \n",
    "from lxml import etree\n",
    "import requests\n",
    "\n",
    "# Utility libraries\n",
    "import gzip\n",
    "import random\n",
    "from natsort import natsorted\n",
    "from typing import Dict, List, Tuple, Set, Optional\n",
    "import pickle\n",
    "import gc\n",
    "import joblib\n",
    "import h5py\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "os.environ['DISPLAY'] = ':0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T10:25:01.918769Z",
     "iopub.status.busy": "2025-04-21T10:25:01.918408Z",
     "iopub.status.idle": "2025-04-21T10:25:01.927304Z",
     "shell.execute_reply": "2025-04-21T10:25:01.926048Z",
     "shell.execute_reply.started": "2025-04-21T10:25:01.918739Z"
    },
    "id": "Jhg73Rb9CqSF",
    "outputId": "6f722459-1c1f-4a24-b5e8-c18d67388f09",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#========================== COLAB =================================\\n# for colab\\n# Create output directory if it doesn\\'t exist\\nbase_dir = Path(\"/content/drive/MyDrive/MIC/2_Micro/data_picrust\")\\nbase_dir.mkdir(parents=True, exist_ok=True)\\nabundance_excel= Path(\"/content/drive/MyDrive/MIC/2_Micro/data_picrustmerged_to_sequence.xlsx\")\\nfasta_file_final = Path(\"/content/drive/MyDrive/MIC/2_Micro/data_picrust/final_sequences_gg.fasta\")\\naligned_fasta = Path(\"/content/drive/MyDrive/MIC/2_Micro/data_picrust/aligned-dna-sequences_gg.fasta\")\\n\\noutput_base = base_dir  # Separate output directory\\noutput_base.mkdir(parents=True, exist_ok=True)\\nlarge_dir = Path(\"/content/drive/MyDrive/MIC/\")\\nlarge_dir.mkdir(parents=True, exist_ok=True)\\ndb_dir = Path(\"/content/drive/MyDrive/MIC/Databases\")\\ndb_dir.mkdir(parents=True, exist_ok=True)\\ninput_galaxy = large_dir / \"data_galaxies\"\\n# Directory to output large files # eccontris, compilated db\\noutput_large = large_dir / \"output_large\"\\noutput_large.mkdir(parents=True, exist_ok=True)\\n\\n## ========================= KAGGLE ==================================\\n# For Kaggle work\\n# Input datasets (read-only in Kaggle)\\nbase_dir = Path(\"/kaggle/input/new-picrust\") #base dir for small files to git /kaggle/input/new-picrust\\n\\n# Files in small input directory\\nabundance_excel= base_dir / \"merged_to_sequence.xlsx\" # inside input small sizes input\\nfasta_file_final = base_dir  / \"final_sequences_gg.fasta\" # inside input small sizes\\n\\n# Output for small files has to be changed for vscode no to push it to git\\noutput_base = Path(\"/kaggle/working/output_base\")\\noutput_base.mkdir(parents=True, exist_ok=True)\\n#datasets large galaxies and databases\\ndb_dir = Path(\"/kaggle/input/databases/Databases\")\\ninput_galaxy = Path(\"/kaggle/input/data-galaxies\")\\n\\n# Directory to output large files # eccontris, compilated db\\nlarge_dir =  Path(\"/kaggle/working/\")\\n\\n# Directory to output large files # eccontris, compilated db\\noutput_large = large_dir / \"output_large\"\\noutput_large.mkdir(parents=True, exist_ok=True)'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "# Directory Structure Definitions\n",
    "SIMPLE_BASE = {\n",
    "    'known': 'simple_known_mic',\n",
    "    'other': 'simple_candidate_mic'\n",
    "}\n",
    "\n",
    "DETAILED_BASE = {\n",
    "    'known': 'detailed_known_mic',\n",
    "    'pure_checked': 'detailed_pure_checked_mic',\n",
    "    'pure_core': 'detailed_pure_core_mic',\n",
    "    'checked_core': 'detailed_checked_core_mic'\n",
    "}\n",
    "\n",
    "SUBDIRS = [\n",
    "    'EC_predictions',\n",
    "    'pathway_predictions',\n",
    "    'KO_predictions',\n",
    "    'other_picrust_files'\n",
    "]\n",
    "\n",
    "\n",
    "# Base Paths\n",
    "if \"google.colab\" in sys.modules:\n",
    "    base_dir = Path(\"/content/drive/MyDrive/MIC/data_picrust\")\n",
    "else:\n",
    "    base_dir = Path(\"/home/beatriz/MIC/2_Micro/data_picrust\")\n",
    "# ====================== VSCODE =====================================\n",
    "#base dir for small files to git\n",
    "base_dir = Path(\"/home/beatriz/MIC/2_Micro/data_picrust\")\n",
    "\n",
    "abundance_excel= Path(\"/home/beatriz/MIC/2_Micro/data_Ref/merged_to_sequence.xlsx\")\n",
    "fasta_file_final = Path(\"/home/beatriz/MIC/2_Micro/data_qiime/results_match_gg/final_sequences_gg.fasta\")\n",
    "aligned_fasta = Path(\"/home/beatriz/MIC/2_Micro/data_qiime/results_match_gg/aligned-dna-sequences_gg.fasta\")\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "output_base = base_dir / \"output_base\"\n",
    "output_base.mkdir(parents=True, exist_ok=True)\n",
    "# large galaxies input and output #large size dir for large files hosted instead in kaggle\n",
    "large_dir = Path(\"/home/beatriz/MIC\")\n",
    "large_dir.mkdir(parents=True, exist_ok=True)\n",
    "# databases\n",
    "db_dir = large_dir / \"Databases\"\n",
    "# input galaxies and uniprots\n",
    "input_galaxy = large_dir / \"data_galaxies\"\n",
    "# Directory to output large files # eccontris, compilated db\n",
    "output_large = large_dir / \"output_large\"\n",
    "output_large.mkdir(parents=True, exist_ok=True)\n",
    "'''\n",
    "#========================== COLAB =================================\n",
    "# for colab\n",
    "# Create output directory if it doesn't exist\n",
    "base_dir = Path(\"/content/drive/MyDrive/MIC/2_Micro/data_picrust\")\n",
    "base_dir.mkdir(parents=True, exist_ok=True)\n",
    "abundance_excel= Path(\"/content/drive/MyDrive/MIC/2_Micro/data_picrustmerged_to_sequence.xlsx\")\n",
    "fasta_file_final = Path(\"/content/drive/MyDrive/MIC/2_Micro/data_picrust/final_sequences_gg.fasta\")\n",
    "aligned_fasta = Path(\"/content/drive/MyDrive/MIC/2_Micro/data_picrust/aligned-dna-sequences_gg.fasta\")\n",
    "\n",
    "output_base = base_dir  # Separate output directory\n",
    "output_base.mkdir(parents=True, exist_ok=True)\n",
    "large_dir = Path(\"/content/drive/MyDrive/MIC/\")\n",
    "large_dir.mkdir(parents=True, exist_ok=True)\n",
    "db_dir = Path(\"/content/drive/MyDrive/MIC/Databases\")\n",
    "db_dir.mkdir(parents=True, exist_ok=True)\n",
    "input_galaxy = large_dir / \"data_galaxies\"\n",
    "# Directory to output large files # eccontris, compilated db\n",
    "output_large = large_dir / \"output_large\"\n",
    "output_large.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "## ========================= KAGGLE ==================================\n",
    "# For Kaggle work\n",
    "# Input datasets (read-only in Kaggle)\n",
    "base_dir = Path(\"/kaggle/input/new-picrust\") #base dir for small files to git /kaggle/input/new-picrust\n",
    "\n",
    "# Files in small input directory\n",
    "abundance_excel= base_dir / \"merged_to_sequence.xlsx\" # inside input small sizes input\n",
    "fasta_file_final = base_dir  / \"final_sequences_gg.fasta\" # inside input small sizes\n",
    "\n",
    "# Output for small files has to be changed for vscode no to push it to git\n",
    "output_base = Path(\"/kaggle/working/output_base\")\n",
    "output_base.mkdir(parents=True, exist_ok=True)\n",
    "#datasets large galaxies and databases\n",
    "db_dir = Path(\"/kaggle/input/databases/Databases\")\n",
    "input_galaxy = Path(\"/kaggle/input/data-galaxies\")\n",
    "\n",
    "# Directory to output large files # eccontris, compilated db\n",
    "large_dir =  Path(\"/kaggle/working/\")\n",
    "\n",
    "# Directory to output large files # eccontris, compilated db\n",
    "output_large = large_dir / \"output_large\"\n",
    "output_large.mkdir(parents=True, exist_ok=True)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "COW1kGFZCqSG"
   },
   "source": [
    "The fasta file come from the Alternative Sequences finding from the Greenes Genes Database, from the taxonomy in this study made in section 7 in the 5_Sequences_qiime notebook: final_sequences_gg.fasta. Abundance dataframe come from the data from notebook 4 merged_to_sequence.xlsx sheet=core_check_usual_taxa which is a unified df between 3 different groups explained previously: cora_taxa (>20% 60 abundance features), usual_taxa (17 high literature ranking bacteria influencing corrosion) and checked_taxa (30 statistically significant to the corrosion risk label) in total 85 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T10:25:04.699245Z",
     "iopub.status.busy": "2025-04-21T10:25:04.698881Z",
     "iopub.status.idle": "2025-04-21T10:25:05.007008Z",
     "shell.execute_reply": "2025-04-21T10:25:05.005842Z",
     "shell.execute_reply.started": "2025-04-21T10:25:04.699212Z"
    },
    "id": "qzMCSdlPuvgI",
    "outputId": "78005138-1033-43a6-cd44-b88b3db330fa",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Integrated taxa from origin genus as headers with levels 6 for the genera, 7 for the GID, muss be cleaned\n",
    "Integrated_T = pd.read_excel(abundance_excel, sheet_name='core_check_usual_taxa', header=[0,1,2,3,4,5,6,7], engine ='openpyxl')\n",
    "# Drop first row (index 0) and first column in one chain\n",
    "Integrated_T = Integrated_T.drop(index=0).drop(Integrated_T.columns[0], axis=1)\n",
    "Integrated_T= Integrated_T.astype({'Sites': str})\n",
    "Integrated_T['Sites'] = Integrated_T['Sites'].fillna('Source')\n",
    "# Remove 'Unnamed' level names\n",
    "Integrated_T.columns = Integrated_T.columns.map(lambda x: tuple('' if 'Unnamed' in str(level) else level for level in x))\n",
    "# Changing dtypes to category whiles respecting structure\n",
    "Integrated_T[\"Category\"] = Integrated_T[\"Category\"].astype(\"Int64\")\n",
    "Integrated_T= Integrated_T.set_index(\"Sites\")\n",
    "pre_Integrated = Integrated_T.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sdOLY5SPuvgI"
   },
   "source": [
    "## 2.3. Making Sequences for Picrust fasta file\n",
    "\n",
    "Picrust Functional Analyiss requires a biom table with otus as index, samples as headers and abundance as values. The present biom has genus names but is needs instead Otus instead. The other input file for picrust is the representative sequences table that consist of the sequences per genera followed by the frequency of that genera on the whole sample, this is done directly by the software. The fasta file requires the otus instead of the genera names and the sequences non aligned coming from notebook 5. The following scrips will formate the data to picrust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:08:43.091047Z",
     "iopub.status.busy": "2025-04-19T16:08:43.090537Z",
     "iopub.status.idle": "2025-04-19T16:08:43.109376Z",
     "shell.execute_reply": "2025-04-19T16:08:43.108008Z",
     "shell.execute_reply.started": "2025-04-19T16:08:43.091005Z"
    },
    "id": "cizMvGCGuvgI",
    "outputId": "dadd5c4f-c930-4699-f1b3-6246ffddccaf",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Read and modify sequences\n",
    "new_records = []\n",
    "for record in SeqIO.parse(fasta_file_final, \"fasta\"):\n",
    "    match = re.search(r\"\\s(\\d+)\\s\", record.description)  # Look for digits surrounded by spaces\n",
    "    if match:\n",
    "        otu_id = match.group(1)\n",
    "    else:\n",
    "        print(f\"Warning: Could not extract OTU ID from description: {record.description}\")\n",
    "        continue  # Skip this record if OTU ID not found\n",
    "\n",
    "    # Create new record with only OTU as ID\n",
    "    new_record = SeqRecord(\n",
    "        record.seq,\n",
    "        id=otu_id,\n",
    "        description=\"\"  # Empty description to keep only ID\n",
    "    )\n",
    "    new_records.append(new_record)\n",
    "\n",
    "# Write modified FASTA\n",
    "output_fasta_path = output_base / \"sequences_for_picrust.fasta\"\n",
    "\n",
    "SeqIO.write(new_records, output_fasta_path, \"fasta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RN-pFp1cuvgI"
   },
   "source": [
    "## 2.4. Making of Dataframes for 2 Different Pipelines\n",
    "The following script is the path to the biom file but also to the Integrate dataframe which create dataframes that discriminate its origin in order to pass then through picrust different pipelines, to know: Simple_Base that compares the known bacteria namely usual_taxa against the other features to understand their relationships on the function of their metabolism, an additional group is put forward as simply_candidate_mic which corresponds to the bacteria no previously linked to corrosion but showing an statistical significance with the risk label, those come from the checked_taxa and in this study are: genera(GID): Bulleida (154); Mycoplana (471), Oxobacter (512) and Oerskovia (). Also as showing an favor behaviour against corrosion are presented: Phenylobacterium (549), Gelria(334), Porphyrobacteria (564) and Tepidimonas (712)\n",
    "SIMPLE_BASE = {'known': 'simple_known_mic', 'other': 'simple_candidate_mic'}\n",
    "The second pipeline comprises a more detailed separation of the bacteria and that is: The Known bacteria as previously, pure_checked corresponding to the statistical significant genera, pure_core correspondent to the core taxa on the systems and the combination of the core and checked taxa.\n",
    "DETAILED_BASE = {'known': 'detailed_known_mic','pure_checked': 'detailed_pure_checked_mic',\n",
    "    'pure_core': 'detailed_pure_core_mic', 'checked_core': 'detailed_checked_core_mic'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "akl8MkZjuvgI"
   },
   "source": [
    "__Making the Integrated dataframe__\n",
    "The original dataframe has a column for source, indicating from which df  came from (core, usual, checked), this script proceses that datadrame into individual dfs and the combined preserving the source for further analysis. The Integrated dataframe continues to be process on the next step to become the biom abundance df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:08:43.111158Z",
     "iopub.status.busy": "2025-04-19T16:08:43.110826Z",
     "iopub.status.idle": "2025-04-19T16:08:43.187472Z",
     "shell.execute_reply": "2025-04-19T16:08:43.186024Z",
     "shell.execute_reply.started": "2025-04-19T16:08:43.111130Z"
    },
    "id": "MWukq8fUCqSH",
    "outputId": "4c8c6cff-dc4a-4f67-912f-ff77850a3140",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def process_integrated_data(df):\n",
    "    \"\"\"\n",
    "    Process the integrated DataFrame to create a new DataFrame with clear column names\n",
    "    and preserve all values including source information.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Input DataFrame with MultiIndex index and site columns\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: Processed DataFrame with clear structure\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract genera and GIDs from the index MultiIndex\n",
    "    genera = df.index.get_level_values(6)[1:]  # Skip first row\n",
    "    gids = pd.to_numeric(df.index.get_level_values(7)[1:], errors='coerce')\n",
    "\n",
    "    # Create a new DataFrame with the extracted information\n",
    "    result_df = pd.DataFrame({\n",
    "        'Genus': genera,\n",
    "        'GID': gids\n",
    "    })\n",
    "\n",
    "    # Add the site values from the original DataFrame\n",
    "    for col in df.columns:\n",
    "        result_df[col] = df.iloc[1:][col].values\n",
    "\n",
    "    # Clean up the DataFrame\n",
    "    result_df['GID'] = pd.to_numeric(result_df['GID'], errors='coerce')\n",
    "    result_df = result_df.dropna(subset=['GID'])\n",
    "    result_df['GID'] = result_df['GID'].astype(int)\n",
    "\n",
    "    return result_df\n",
    "\n",
    "def get_taxa_groups(df):\n",
    "    \"\"\"\n",
    "    Separate the processed DataFrame into different taxa groups based on Source column\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Processed DataFrame from process_integrated_data()\n",
    "\n",
    "    Returns:\n",
    "    dict: Dictionary containing DataFrames for different taxa groups\n",
    "    \"\"\"\n",
    "    # Split the data into groups based on 'Source' column patterns\n",
    "\n",
    "    # Known corrosion bacteria (any pattern with 'us')\n",
    "    known_bacteria = df[df['Source'].str.contains('us', case=False, na=False)]\n",
    "\n",
    "    # Pure checked bacteria (only 'chk' without 'core' or 'us')\n",
    "    pure_checked = df[\n",
    "        df['Source'].str.contains('chk', case=False, na=False) &\n",
    "        ~df['Source'].str.contains('core|us', case=False, na=False)\n",
    "    ]\n",
    "\n",
    "    # Pure core bacteria (only 'core' without 'chk' or 'us')\n",
    "    pure_core = df[\n",
    "        df['Source'].str.contains('core', case=False, na=False) &\n",
    "        ~df['Source'].str.contains('chk|us', case=False, na=False)\n",
    "    ]\n",
    "\n",
    "    # Checked-core bacteria (contains both 'core' and 'chk' but no 'us')\n",
    "    checked_core = df[\n",
    "        df['Source'].str.contains('chk.*core|core.*chk', case=False, na=False) &\n",
    "        ~df['Source'].str.contains('us', case=False, na=False)\n",
    "    ]\n",
    "\n",
    "    # Create groups dictionary\n",
    "    taxa_groups = {\n",
    "        'known_bacteria': known_bacteria,\n",
    "        'pure_checked': pure_checked,\n",
    "        'pure_core': pure_core,\n",
    "        'checked_core': checked_core\n",
    "    }\n",
    "\n",
    "    # Print summary statistics\n",
    "    print(\"\\nDetailed Classification Results:\")\n",
    "    print(f\"Known corrosion bacteria: {len(known_bacteria)}\")\n",
    "    print(f\"Pure checked bacteria: {len(pure_checked)}\")\n",
    "    print(f\"Pure core bacteria: {len(pure_core)}\")\n",
    "    print(f\"Checked-core bacteria: {len(checked_core)}\")\n",
    "\n",
    "    # Verify total matches expected\n",
    "    total_classified = len(known_bacteria) + len(pure_checked) + len(pure_core) + len(checked_core)\n",
    "    print(f\"\\nTotal classified taxa: {total_classified}\")\n",
    "    print(f\"Total in dataset: {len(df)}\")\n",
    "\n",
    "    return taxa_groups\n",
    "\n",
    "# Usage example:\n",
    "Integrated = process_integrated_data(pre_Integrated)\n",
    "\n",
    "# Get the groups\n",
    "taxa_groups = get_taxa_groups(Integrated)\n",
    "\n",
    "# Access individual groups -\n",
    "known_bacteria = taxa_groups['known_bacteria']\n",
    "pure_core = taxa_groups['pure_core']\n",
    "pure_checked = taxa_groups['pure_checked']\n",
    "checked_core = taxa_groups['checked_core']\n",
    "\n",
    "Integrated = process_integrated_data(pre_Integrated)\n",
    "taxa_groups = get_taxa_groups(Integrated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g56zbUtiuvgJ"
   },
   "source": [
    "## 2.5. Making the Abundanc Biom dataframe for Picrust\n",
    "\n",
    "The final biom should have as index the Otus numbers no the genera names and a clean formate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:08:43.188887Z",
     "iopub.status.busy": "2025-04-19T16:08:43.188500Z",
     "iopub.status.idle": "2025-04-19T16:08:43.208983Z",
     "shell.execute_reply": "2025-04-19T16:08:43.207574Z",
     "shell.execute_reply.started": "2025-04-19T16:08:43.188844Z"
    },
    "id": "Qn6xPmvfuvgJ",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# droping source and genus and putting GID as index\n",
    "pre_biom= Integrated.drop(columns=[\"Source\", \"GID\"])\n",
    "pre_biom= pre_biom.set_index(\"Genus\").astype(str)\n",
    "# Ensure all data values are float\n",
    "pre_biom = pre_biom.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mI5nSqwXuvgJ"
   },
   "source": [
    "__changing genera to otus__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:08:43.211222Z",
     "iopub.status.busy": "2025-04-19T16:08:43.210872Z",
     "iopub.status.idle": "2025-04-19T16:08:43.228232Z",
     "shell.execute_reply": "2025-04-19T16:08:43.227000Z",
     "shell.execute_reply.started": "2025-04-19T16:08:43.211194Z"
    },
    "id": "amKUleGLuvgJ",
    "outputId": "406f51f2-a5d3-4420-f03e-e439dc3eeea9",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create genus to OTU mapping from FASTA headers\n",
    "genus_to_otu = {}\n",
    "for record in SeqIO.parse(fasta_file_final, \"fasta\"):\n",
    "    parts = record.description.split()\n",
    "    if len(parts) >= 3:\n",
    "        genus = parts[0]\n",
    "        otu = parts[1]  # We'll use the first OTU number\n",
    "        genus_to_otu[genus] = otu\n",
    "\n",
    "# Print a few mappings to verify\n",
    "print(\"Sample genus to OTU mappings:\")\n",
    "for i, (genus, otu) in enumerate(list(genus_to_otu.items())[:5]):\n",
    "    print(f\"{genus} -> {otu}\")\n",
    "\n",
    "# Replace genus with OTU in the index\n",
    "pre_biom.index = pre_biom.index.map(lambda x: genus_to_otu.get(x, x))\n",
    "\n",
    "# Remove the 'Genus' name from the index\n",
    "pre_biom.index.name = \"OTU\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eFYJS2KuuvgJ"
   },
   "source": [
    "__Calculation counts for picrust2__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:08:43.230039Z",
     "iopub.status.busy": "2025-04-19T16:08:43.229596Z",
     "iopub.status.idle": "2025-04-19T16:08:43.280124Z",
     "shell.execute_reply": "2025-04-19T16:08:43.278847Z",
     "shell.execute_reply.started": "2025-04-19T16:08:43.229997Z"
    },
    "id": "bwLLzgWLuvgJ",
    "outputId": "5471c694-b6c1-4743-f556-a4b65b049c05",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "scaling_factor = 10000\n",
    "# Multiply by scaling factor and round to nearest integer\n",
    "count_pre_biom = np.round(pre_biom * scaling_factor).astype(int)\n",
    "count_pre_biom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gu2uuKLHuvgJ"
   },
   "source": [
    "__Creating the biom table formate__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:08:43.282134Z",
     "iopub.status.busy": "2025-04-19T16:08:43.281696Z",
     "iopub.status.idle": "2025-04-19T16:08:43.322548Z",
     "shell.execute_reply": "2025-04-19T16:08:43.321357Z",
     "shell.execute_reply.started": "2025-04-19T16:08:43.282090Z"
    },
    "id": "PsCMsci7w8R7",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create BIOM table with type specification\n",
    "biom_table = Table(data=count_pre_biom.values,\n",
    "                  observation_ids=count_pre_biom.index.astype(str),\n",
    "                  sample_ids=count_pre_biom.columns.astype(str),\n",
    "                  type=\"OTU table\",\n",
    "                  create_date=datetime.now().isoformat(),\n",
    "                  generated_by=\"BIOM-Format\",\n",
    "                  matrix_type=\"sparse\",\n",
    "                  matrix_element_type=\"float\")\n",
    "\n",
    "# Save with explicit format\n",
    "output_path = output_base / \"count_abundance_85.biom\"\n",
    "\n",
    "with biom_open(output_path, 'w') as f:\n",
    "    biom_table.to_hdf5(f, generated_by=\"BIOM-Format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:08:43.323834Z",
     "iopub.status.busy": "2025-04-19T16:08:43.323530Z",
     "iopub.status.idle": "2025-04-19T16:08:47.007487Z",
     "shell.execute_reply": "2025-04-19T16:08:47.006077Z",
     "shell.execute_reply.started": "2025-04-19T16:08:43.323809Z"
    },
    "id": "gV5uEFS_uvgJ",
    "outputId": "528f1a14-3917-4f78-bdff-494dc7c00218",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Validate the table structure\n",
    "print(\"\\nValidating table...\")\n",
    "!biom validate-table -i {output_path}\n",
    "#/home/beatriz/MIC/2_Micro/data_picrust/count_abundance_85.biom\n",
    "\n",
    "# Show table info\n",
    "!biom summarize-table -i {output_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KzS6DEGb-PJG"
   },
   "source": [
    "Validating table...\n",
    "\n",
    "The input file is a valid BIOM-formatted file.\n",
    "Num samples: 70\n",
    "Num observations: 85\n",
    "Total count: 56747993\n",
    "Table density (fraction of non-zero values): 0.405\n",
    "\n",
    "Counts/sample summary:\n",
    " Min: 181800.000\n",
    " Max: 990578.000\n",
    " Median: 851078.500\n",
    " Mean: 810685.614\n",
    " Std. dev.: 157876.192\n",
    " Sample Metadata Categories: None provided\n",
    " Observation Metadata Categories: None provided\n",
    "\n",
    "Counts/sample detail:\n",
    "site_69: 181800.000\n",
    "site_67: 217903.000\n",
    "site_70: 270600.000\n",
    "site_26: 582999.000\n",
    "site_21: 589725.000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YmMvQVofuvgK"
   },
   "source": [
    "# 3. Making the representative sequences\n",
    "\n",
    "__Convert Abundance Biom table and the Sequences into a QIIME2 artifact__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:08:47.009629Z",
     "iopub.status.busy": "2025-04-19T16:08:47.009234Z",
     "iopub.status.idle": "2025-04-19T16:08:47.033396Z",
     "shell.execute_reply": "2025-04-19T16:08:47.032214Z",
     "shell.execute_reply.started": "2025-04-19T16:08:47.009594Z"
    },
    "id": "yXcOOPN7uvgK",
    "outputId": "2ea050f2-b1b6-402c-b992-31ae7d0112b5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_rep_seqs_with_freq(sequence_file, pre_biom_df, output_fasta):\n",
    "    \"\"\"\n",
    "    Create representative sequences with frequencies written to output\n",
    "\n",
    "    Args:\n",
    "        sequence_file: Path to FASTA file with OTU sequences\n",
    "        pre_biom_df: DataFrame with abundance data\n",
    "        output_fasta: Path to save sequences with frequencies\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Calculate total frequency for each OTU\n",
    "        total_frequencies = round(pre_biom_df.sum(axis=1), 2)\n",
    "\n",
    "        with open(output_fasta, 'w') as out:\n",
    "            for record in SeqIO.parse(sequence_file, \"fasta\"):\n",
    "                otu_id = record.id\n",
    "\n",
    "                if otu_id in total_frequencies.index:\n",
    "                    freq = total_frequencies[otu_id]\n",
    "                    sequence = str(record.seq)\n",
    "\n",
    "                    # Write sequence with frequency to FASTA\n",
    "                    out.write(f\">{otu_id} {sequence} {freq}\\n\")\n",
    "\n",
    "        # First lines of the file\n",
    "        print(\"Representative Sequences head:\")\n",
    "        with open(output_fasta, 'r') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i < 1:  # Show first 3 sequences (header + sequence lines)\n",
    "                    print(line.strip())\n",
    "        return output_fasta\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "# Representative sequences\n",
    "sequences_for_picrust = output_base / \"sequences_for_picrust.fasta\"\n",
    "\n",
    "output_fasta = output_base / \"representative_sequences\"\n",
    "\n",
    "repres_sequ = create_rep_seqs_with_freq(sequences_for_picrust, pre_biom, output_fasta)\n",
    "repres_sequ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ku8lUve9uvgK"
   },
   "source": [
    "__Disclamer:__ These notebook was mean to do the analysis of the functional mechanisms of bacteria using picrust2, however the capacity of the laptop was no sufficient to run it, nor colab on public library, nor a virtual machine, that is the reason why the analysis was undertaken in the galaxy website, where the data resides.\n",
    "https://usegalaxy.eu/  \n",
    "username= magicalex238"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zv-yUkfmCqSN"
   },
   "source": [
    "## 3.1. Classifying Bacteria by their Source DataFrame\n",
    "Two distinct classification approaches are implemented to categorize bacteria. The simple approach (get_bacteria_sources_simple) divides bacteria into known corrosion-causers (usual_taxa) and candidates (all others). The detailed approach (get_bacteria_sources_detailed) provides finer categorization by separating bacteria into known corrosion-causers, pure checked taxa, pure core taxa, and those present in both checked and core datasets. Please notice that this function uses df Integrated for source clasification and no abundance.biom which will be used for the picrust2 pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:08:47.034775Z",
     "iopub.status.busy": "2025-04-19T16:08:47.034503Z",
     "iopub.status.idle": "2025-04-19T16:08:47.049490Z",
     "shell.execute_reply": "2025-04-19T16:08:47.047697Z",
     "shell.execute_reply.started": "2025-04-19T16:08:47.034753Z"
    },
    "id": "5bDVrPwWCqSR",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_bacteria_sources_comprehensive(Integrated_df):\n",
    "    \"\"\"\n",
    "    Comprehensive classification with all necessary combinations, returning lists of genus names.\n",
    "    Also includes a combined list of specified categories.\n",
    "    \"\"\"\n",
    "    genera = Integrated_df[\"Genus\"]\n",
    "    gids = Integrated_df[\"GID\"]\n",
    "    sources = Integrated_df['Source'] if 'Source' in Integrated_df.columns else None\n",
    "    \n",
    "    # Initialize empty lists for each category\n",
    "    usual_bacteria = []  # anything with 'us'\n",
    "    pure_checked = []    # only 'chk'\n",
    "    pure_core = []       # only 'core'\n",
    "    pure_usual = []      # only 'us'\n",
    "    checked_core = []    # both 'chk' and 'core'\n",
    "    checked_usual = []   # both 'chk' and 'us'\n",
    "    core_usual = []      # both 'core' and 'us'\n",
    "    all_three = []       # all three markers\n",
    "    \n",
    "    sources_found = set()\n",
    "    \n",
    "    for i, (genus, gid) in enumerate(zip(genera, gids)):\n",
    "        if sources is not None:  # Check if source exists for this genus\n",
    "            source = str(sources.iloc[i]).strip().lower()\n",
    "            sources_found.add(source)\n",
    "            \n",
    "            # Check patterns\n",
    "            has_us = 'us' in source\n",
    "            has_chk = 'chk' in source\n",
    "            has_core = 'core' in source\n",
    "            \n",
    "            # Overlapping categories (for union operations)\n",
    "            if has_us:\n",
    "                usual_bacteria.append(genus)\n",
    "                \n",
    "                if has_chk:\n",
    "                    checked_usual.append(genus)\n",
    "                \n",
    "                if has_core:\n",
    "                    core_usual.append(genus)\n",
    "                \n",
    "                if has_chk and has_core:\n",
    "                    all_three.append(genus)\n",
    "                    \n",
    "                if not has_chk and not has_core:\n",
    "                    pure_usual.append(genus)\n",
    "            \n",
    "            if has_chk and has_core:\n",
    "                checked_core.append(genus)\n",
    "            \n",
    "            # Pure categories\n",
    "            if has_chk and not has_core and not has_us:\n",
    "                pure_checked.append(genus)\n",
    "            \n",
    "            if has_core and not has_chk and not has_us:\n",
    "                pure_core.append(genus)\n",
    "\n",
    "    # Sort all lists\n",
    "    pure_checked.sort()\n",
    "    pure_core.sort()\n",
    "    pure_usual.sort()\n",
    "    checked_core.sort()\n",
    "    checked_usual.sort()\n",
    "    core_usual.sort()\n",
    "    all_three.sort()\n",
    "    usual_bacteria.sort()\n",
    "    # Create combined list of most interesting bacteria\n",
    "    components = list(set(\n",
    "        pure_checked + \n",
    "        checked_core + \n",
    "        checked_usual + \n",
    "        core_usual + \n",
    "        all_three\n",
    "    ))\n",
    "    components.sort() \n",
    "    # Print summary statistics\n",
    "    print(\"\\nComprehensive Classification Results:\")\n",
    "    print(f\"Usual bacteria (any with 'us'): {len(usual_bacteria)}\")\n",
    "    print(f\"Pure checked bacteria (only 'chk'): {len(pure_checked)}\")\n",
    "    print(f\"Pure core bacteria (only 'core'): {len(pure_core)}\")\n",
    "    print(f\"Pure usual bacteria (only 'us'): {len(pure_usual)}\")\n",
    "    \n",
    "    print(f\"Checked-core bacteria (both): {len(checked_core)}\")\n",
    "    print(f\"Checked-usual bacteria (both): {len(checked_usual)}\")\n",
    "    print(f\"Core-usual bacteria (both): {len(core_usual)}\")\n",
    "    print(f\"In all three categories: {len(all_three)}\")\n",
    "    print(f\"\\nCombined bacteria (components): {len(components)}\")\n",
    "    \n",
    "    # Print expected intersections\n",
    "    print(\"\\nIntersection Counts:\")\n",
    "    print(f\"chk-us: {len(checked_usual)}\")\n",
    "    print(f\"core-us: {len(core_usual)}\")\n",
    "    print(f\"chk-core: {len(checked_core)}\")\n",
    "    print(f\"all three: {len(all_three)}\")\n",
    "    print(f\"components:{len(components)}\")\n",
    "    \n",
    "    # Verify total matches expected\n",
    "    unique_genera = set(pure_checked + pure_core + pure_usual + \n",
    "                      [g for g in checked_core if g not in all_three] + \n",
    "                      [g for g in checked_usual if g not in all_three] + \n",
    "                      [g for g in core_usual if g not in all_three] + \n",
    "                      all_three)\n",
    "    \n",
    "    print(f\"\\nTotal unique genera: {len(unique_genera)}\")\n",
    "    print(f\"Total in dataset: {len(Integrated_df)}\")\n",
    "    print(\"\\nSources found:\", sources_found)\n",
    "    \n",
    "    return {\n",
    "        'usual_bacteria': usual_bacteria,\n",
    "        'pure_checked': pure_checked,\n",
    "        'pure_core': pure_core,\n",
    "        'pure_usual': pure_usual,\n",
    "        'checked_core': checked_core,\n",
    "        'checked_usual': checked_usual,\n",
    "        'core_usual': core_usual,\n",
    "        'all_three': all_three,\n",
    "        'components': components\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:08:47.051469Z",
     "iopub.status.busy": "2025-04-19T16:08:47.050989Z",
     "iopub.status.idle": "2025-04-19T16:08:47.082347Z",
     "shell.execute_reply": "2025-04-19T16:08:47.081179Z",
     "shell.execute_reply.started": "2025-04-19T16:08:47.051427Z"
    },
    "id": "LSxCpNd8Yelz",
    "outputId": "a1390814-9c67-498a-93f3-50a86c5bab00",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "results_dict = get_bacteria_sources_comprehensive(Integrated)\n",
    "# Create a dictionary to hold the data for each sheet\n",
    "sheet_data = {}\n",
    "\n",
    "# For each category, create a DataFrame\n",
    "for category, bacteria_list in results_dict.items():\n",
    "    # Create a DataFrame with just the genus names\n",
    "    df = pd.DataFrame({\n",
    "        'Genus': bacteria_list\n",
    "    })\n",
    "    sheet_data[category] = df\n",
    "\n",
    "# Save to Excel with one sheet per category\n",
    "bacteria_clas_path = output_base / \"bacteria_classification.xlsx\"\n",
    "with pd.ExcelWriter(bacteria_clas_path) as writer:\n",
    "    for sheet_name, df in sheet_data.items():\n",
    "        df.to_excel(writer, sheet_name=sheet_name, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZCA_PPxRYel0"
   },
   "source": [
    "The lists will be utilised later in order to groupby this list int he analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "23P2k1QwCqSR"
   },
   "source": [
    "## 3.2. Prepare picrust data and Creating Directories for PICRUSt2 Input\n",
    "The check_missing_genera function processes the integrated data and handles data quality control. Known problematic genera (e.g., 'Clostridium_sensu_stricto_12', 'Oxalobacteraceae_unclassified') are flagged for exclusion to prevent analysis errors. The function also creates an organized directory structure as outlined in the introduction, with separate paths for different bacterial classifications (known_mic, candidate_mic, etc.) and their respective analysis outputs (EC_predictions, pathway_predictions, KO_predictions). Following function prepares the data for picrust analysis but both dataframes the abundance.biom and Integrated have some bacteria that were no sequenciated mostly cause are no known specimens. So it is necesary to do same procedure to both dfs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:08:47.097166Z",
     "iopub.status.busy": "2025-04-19T16:08:47.096805Z",
     "iopub.status.idle": "2025-04-19T16:08:47.118683Z",
     "shell.execute_reply": "2025-04-19T16:08:47.117656Z",
     "shell.execute_reply.started": "2025-04-19T16:08:47.097135Z"
    },
    "id": "bNfnbXfKCqSS",
    "outputId": "23c7cc54-a6e5-448f-9f45-c69b3a2eaf41",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def prepare_picrust_data(Integrated_df, aligned_file, function_type='simple'):\n",
    "    \"\"\"\n",
    "    Prepare data for PICRUSt analysis with choice of  function_type method\n",
    "\n",
    "    Args:\n",
    "        Integrated_df: Input DataFrame\n",
    "        aligned_file: Path to aligned sequences\n",
    "        function_type: 'simple' or 'detailed'\n",
    "    \"\"\"\n",
    "    # Get bacteria source_groups based on chosen  function_type\n",
    "    if  function_type == 'simple':\n",
    "        source_groups = get_bacteria_sources_simple(Integrated_df)\n",
    "    else:\n",
    "        source_groups= get_bacteria_sources_detailed(Integrated_df)\n",
    "\n",
    "    # Create appropriate directory structure\n",
    "    create_directory_structure(function_type)\n",
    "\n",
    "    return source_groups\n",
    "\n",
    "def create_directory_structure(function_type='simple'):\n",
    "    \"\"\"Create directory structure for PICRUSt analysis\"\"\"\n",
    "    base_dir = Path(\"/home/beatriz/MIC/2_Micro/data_picrust\")\n",
    "    base_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if function_type == 'simple':\n",
    "        directories = SIMPLE_BASE\n",
    "    else:\n",
    "        directories = DETAILED_BASE\n",
    "\n",
    "    # Create all required directories\n",
    "    for dir_name in directories.values():\n",
    "        for subdir in SUBDIRS:\n",
    "            (base_dir / dir_name / subdir).mkdir(parents=True, exist_ok=True)\n",
    "    logging.info(\"Directory structure created successfully\")\n",
    "\n",
    "    return True\n",
    "\n",
    "'''  except Exception as e:\n",
    "    logging.error(f\"Error creating directory structure: {str(e)}\")\n",
    "    return False'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:08:47.120303Z",
     "iopub.status.busy": "2025-04-19T16:08:47.119952Z",
     "iopub.status.idle": "2025-04-19T16:08:47.142549Z",
     "shell.execute_reply": "2025-04-19T16:08:47.141290Z",
     "shell.execute_reply.started": "2025-04-19T16:08:47.120239Z"
    },
    "id": "q4v7JVU8uvgK",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def verify_input_files():\n",
    "    \"\"\"Verify that input files exist and are readable\"\"\"\n",
    "    missing_files = []\n",
    "\n",
    "    if not fasta_file.exists():\n",
    "        missing_files.append(str(fasta_file))\n",
    "    if not biom_table.exists():\n",
    "        missing_files.append(str(biom_table))\n",
    "\n",
    "    if missing_files:\n",
    "        logging.error(f\"Missing input files: {', '.join(missing_files)}\")\n",
    "        return False\n",
    "\n",
    "    logging.info(\"All input files found\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUwADMKZCqSS"
   },
   "source": [
    "# 4. PICRUSt Pipeline Definition\n",
    "The pipeline processes the aligned sequence data from notebook 5 that has or not undergo cleaning of the sequences as previously done on section 2. Also processes the biom_table in order to account on this anylsis on abundance. It queries the PICRUSt database to predict potential metabolic pathways for each genus. This prediction is based on evolutionary relationships and known genomic capabilities of related organisms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:08:47.144272Z",
     "iopub.status.busy": "2025-04-19T16:08:47.143871Z",
     "iopub.status.idle": "2025-04-19T16:08:47.155409Z",
     "shell.execute_reply": "2025-04-19T16:08:47.154374Z",
     "shell.execute_reply.started": "2025-04-19T16:08:47.144220Z"
    },
    "id": "srMpS5DkCqSS",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def run_picrust2_pipeline(fasta_file, biom_file, output_dir):\n",
    "    \"\"\"\n",
    "    Run the main PICRUSt2 pipeline on input sequences and BIOM table.\n",
    "\n",
    "    Args:\n",
    "        fasta_file: Path to the aligned sequences FASTA file.\n",
    "        biom_file: Path to the BIOM table (without extra columns).\n",
    "        output_dir: Directory for PICRUSt2 output.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Run main PICRUSt2 pipeline\n",
    "        cmd = [\n",
    "            'picrust2_pipeline.py',\n",
    "            '-s', fasta_file,        # Input FASTA file with aligned sequences\n",
    "            '-i', biom_file,         # BIOM table with abundance data\n",
    "            '-o', output_dir,        # Output directory\n",
    "            '--processes', '4',      # Parallel processes\n",
    "            '--verbose',\n",
    "            '--min_align', '0.25'    # Note the split here\n",
    "        ]\n",
    "        subprocess.run(cmd, check=True)\n",
    "\n",
    "        # Add pathway descriptions if the pathway file exists\n",
    "        pathway_file = os.path.join(output_dir, 'pathways_out/path_abun_unstrat.tsv.gz')\n",
    "        if os.path.exists(pathway_file):\n",
    "            cmd_desc = [\n",
    "                'add_descriptions.py',\n",
    "                '-i', pathway_file,\n",
    "                '-m', 'PATHWAY',\n",
    "                '-o', os.path.join(output_dir, 'pathways_with_descriptions.tsv')\n",
    "            ]\n",
    "            subprocess.run(cmd_desc, check=True)\n",
    "\n",
    "        print(f\"PICRUSt2 pipeline completed successfully for {output_dir}\")\n",
    "        return True\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error running PICRUSt2: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rx_DyzHbCqSS"
   },
   "source": [
    "# 5. Analysis of Pathways\n",
    "The analysis focuses on metabolic pathways known to be involved in microbially influenced corrosion, including sulfur metabolism, organic acid production, iron metabolism, and biofilm formation. These pathways were selected based on documented mechanisms of known corrosion-inducing bacteria. Separate pipeline runs for simple and detailed classifications ensure proper pathway analysis for each bacterial group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:08:47.163274Z",
     "iopub.status.busy": "2025-04-19T16:08:47.162861Z",
     "iopub.status.idle": "2025-04-19T16:08:47.181528Z",
     "shell.execute_reply": "2025-04-19T16:08:47.180238Z",
     "shell.execute_reply.started": "2025-04-19T16:08:47.163214Z"
    },
    "id": "8eP8MAidCqSS",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def analyze_functional_profiles(picrust_output_dir, bacteria_list):\n",
    "    \"\"\"\n",
    "    Analyze functional profiles with focus on corrosion-relevant pathways\n",
    "\n",
    "    Parameters:\n",
    "    picrust_output_dir: directory containing PICRUSt2 output\n",
    "    bacteria_list: list of bacteria names to analyze\n",
    "    \"\"\"\n",
    "    # Define corrosion-relevant pathways\n",
    "    relevant_pathways = [\n",
    "        'Sulfur metabolism',\n",
    "        'Iron metabolism',\n",
    "        'Energy metabolism',\n",
    "        'Biofilm formation',\n",
    "        'Metal transport',\n",
    "        'ochre formation',\n",
    "        'iron oxide deposits',\n",
    "        'iron precipitation',\n",
    "        'rust formation',\n",
    "        'organic acid production',\n",
    "        'acetate production',\n",
    "        'lactate metabolism',\n",
    "        'formate production',\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        # Read PICRUSt2 output\n",
    "        pathway_file = os.path.join(picrust_output_dir, 'pathways_with_descriptions.tsv')\n",
    "        pathways_df = pd.read_csv(pathway_file, sep='\\t')\n",
    "\n",
    "        # Filter for relevant pathways\n",
    "        filtered_pathways = pathways_df[\n",
    "            pathways_df['description'].str.contains('|'.join(relevant_pathways),\n",
    "                                                  case=False,\n",
    "                                                  na=False)]\n",
    "\n",
    "        # Calculate pathway abundances per bacteria\n",
    "        pathway_abundances = filtered_pathways.groupby('description').sum()\n",
    "\n",
    "        # Calculate pathway similarities between bacteria\n",
    "        pathway_similarities = {}\n",
    "        for bacteria in bacteria_list:\n",
    "            if bacteria in pathways_df.columns:\n",
    "                similarities = pathways_df[bacteria].corr(pathways_df[list(bacteria_list)])\n",
    "                pathway_similarities[bacteria] = similarities\n",
    "\n",
    "        # Predict functional potential\n",
    "        functional_predictions = {}\n",
    "        for pathway in relevant_pathways:\n",
    "            pathway_presence = filtered_pathways[\n",
    "                filtered_pathways['description'].str.contains(pathway, case=False)\n",
    "            ]\n",
    "            if not pathway_presence.empty:\n",
    "                functional_predictions[pathway] = {\n",
    "                    'presence': len(pathway_presence),\n",
    "                    'mean_abundance': pathway_presence.mean().mean(),\n",
    "                    'max_abundance': pathway_presence.max().max()\n",
    "                }\n",
    "\n",
    "        # Calculate correlation scores\n",
    "        correlation_scores = {}\n",
    "        for bacteria in bacteria_list:\n",
    "            if bacteria in pathways_df.columns:\n",
    "                correlations = pathways_df[bacteria].corr(\n",
    "                    pathways_df[filtered_pathways.index]\n",
    "                )\n",
    "                correlation_scores[bacteria] = {\n",
    "                    'mean_correlation': correlations.mean(),\n",
    "                    'max_correlation': correlations.max(),\n",
    "                    'key_pathways': correlations.nlargest(5).index.tolist()\n",
    "                }\n",
    "\n",
    "        comparison_results = {\n",
    "            'pathway_similarities': pathway_similarities,\n",
    "            'functional_predictions': functional_predictions,\n",
    "            'correlation_scores': correlation_scores,\n",
    "            'pathway_abundances': pathway_abundances.to_dict()\n",
    "        }\n",
    "\n",
    "        return filtered_pathways, comparison_results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in pathway analysis: {str(e)}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-EEla9jCqSS"
   },
   "source": [
    "## 5.2. Testing the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:08:47.184846Z",
     "iopub.status.busy": "2025-04-19T16:08:47.184519Z",
     "iopub.status.idle": "2025-04-19T16:08:47.209012Z",
     "shell.execute_reply": "2025-04-19T16:08:47.207735Z",
     "shell.execute_reply.started": "2025-04-19T16:08:47.184817Z"
    },
    "id": "mnwckS6sCqST",
    "outputId": "8fb19d4c-2ffa-4677-c513-3e34213ef4f3",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "'''# ---- RUNNING THE PIPELINE ----\n",
    "\n",
    "# Set paths\n",
    "fasta_file = Path('/home/beatriz/MIC/2_Micro/data_tree/accession_sequences.fasta')\n",
    "abundance_biom_file =  Path('/home/beatriz/MIC/2_Micro/data_picrust/abundance_accession.biom')\n",
    "output_dir = 'picrust_output'\n",
    "\n",
    "# List of bacteria to analyze\n",
    "bacteria_of_interest = ['Azospira', 'Brachybacterium', 'Bulleidia']\n",
    "\n",
    "# Run PICRUSt2\n",
    "if run_picrust2_pipeline(aligned_fasta_file,\n",
    "                         abundance_biom_file,\n",
    "                         output_dir\n",
    "                        ):\n",
    "    # Analyze functional profiles if the pipeline completes successfully\n",
    "    filtered_pathways, abundances = analyze_functional_profiles(output_dir, bacteria_of_interest)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03KIf3UaCqST"
   },
   "source": [
    "# 6. Functional Analysis\n",
    "## 6.1 Running picrust full pipeline 1\n",
    "The analysis workflow begins by categorizing bacteria into source groups using the classification functions. These categorized data are then processed through the PICRUSt pipeline to predict metabolic capabilities. The functional analysis examines pathway presence, abundance, and correlations between different bacterial groups to identify potential corrosion-related metabolic patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:08:47.210998Z",
     "iopub.status.busy": "2025-04-19T16:08:47.210567Z",
     "iopub.status.idle": "2025-04-19T16:08:47.230671Z",
     "shell.execute_reply": "2025-04-19T16:08:47.229389Z",
     "shell.execute_reply.started": "2025-04-19T16:08:47.210957Z"
    },
    "id": "uHpbek-BCqST",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def run_functional_analysis(df, Integrated_df, aligned_file, analysis_type='simple'):\n",
    "    \"\"\"\n",
    "    Run complete functional analysis pipeline for either simple or detailed classification\n",
    "\n",
    "    Parameters:\n",
    "    df: Input DataFrame\n",
    "    aligned_file: Path to aligned sequences file\n",
    "    analysis_type: 'simple' or 'detailed'\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Starting {analysis_type} classification analysis\")\n",
    "        print(f\"{'='*50}\")\n",
    "\n",
    "        # Prepare data and get source groups\n",
    "        print(\"\\nStep 1: Preparing data...\")\n",
    "\n",
    "        source_groups = prepare_picrust_data(Integrated_df, aligned_file, function_type=analysis_type)\n",
    "\n",
    "        if not source_groups:\n",
    "            raise ValueError(\"Failed to prepare data: No source groups returned\")\n",
    "\n",
    "        # Base directory for PICRUSt output\n",
    "        base_dir = Path(\"~MIC/2_Micro/data_picrust\")\n",
    "\n",
    "        results = {}\n",
    "\n",
    "        if analysis_type == 'simple':\n",
    "            # Run analysis for simple classification\n",
    "            # Known bacteria\n",
    "            known_output_dir = base_dir /SIMPLE_BASE['known']\n",
    "            success_known = run_picrust2_pipeline(aligned_file, df, str(known_output_dir))\n",
    "            if success_known:\n",
    "                results_known = analyze_functional_profiles(str(known_output_dir),\n",
    "                                                        source_groups['known_bacteria'].keys())\n",
    "\n",
    "            # Other bacteria\n",
    "            other_output_dir = base_dir / SIMPLE_BASE['other']\n",
    "            success_other = run_picrust2_pipeline(aligned_file, str(other_output_dir))\n",
    "            if success_other:\n",
    "                results_other = analyze_functional_profiles(str(other_output_dir),\n",
    "                                                        source_groups['other_bacteria'].keys())\n",
    "\n",
    "        else:\n",
    "            # Run analysis for detailed classification\n",
    "            for group, dir_name in DETAILED_BASE.items():\n",
    "\n",
    "                # Known bacteria\n",
    "                known_output_dir = base_dir / DETAILED_BASE['known']\n",
    "                success_known = run_picrust2_pipeline(aligned_file, str(known_output_dir))\n",
    "                if success_known:\n",
    "                    results_known = analyze_functional_profiles(str(known_output_dir),\n",
    "                                                            source_groups['known_bacteria'].keys())\n",
    "\n",
    "                # Pure checked bacteria\n",
    "                checked_output_dir = base_dir /  DETAILED_BASE['pure_checked']\n",
    "                success_checked = run_picrust2_pipeline(aligned_file, str(checked_output_dir))\n",
    "                if success_checked:\n",
    "                    results_checked = analyze_functional_profiles(str(checked_output_dir),\n",
    "                                                            source_groups['pure_checked'].keys())\n",
    "\n",
    "                # Pure core bacteria\n",
    "                core_output_dir = base_dir /DETAILED_BASE['pure_core']\n",
    "                success_core = run_picrust2_pipeline(aligned_file, str(core_output_dir))\n",
    "                if success_core:\n",
    "                    results_core = analyze_functional_profiles(str(core_output_dir),\n",
    "                                                            source_groups['pure_core'].keys())\n",
    "\n",
    "                # Checked-core bacteria\n",
    "                checked_core_output_dir = base_dir /DETAILED_BASE['checked_core']\n",
    "                success_checked_core = run_picrust2_pipeline(aligned_file, str(checked_core_output_dir))\n",
    "                if success_checked_core:\n",
    "                    results_checked_core = analyze_functional_profiles(str(checked_core_output_dir),\n",
    "                                                                    source_groups['checked_core'].keys())\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error running PICRUSt2: {e}\")\n",
    "\n",
    "        return \"Analysis completed successfully\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:08:47.232386Z",
     "iopub.status.busy": "2025-04-19T16:08:47.231959Z",
     "iopub.status.idle": "2025-04-19T16:08:47.257234Z",
     "shell.execute_reply": "2025-04-19T16:08:47.255860Z",
     "shell.execute_reply.started": "2025-04-19T16:08:47.232342Z"
    },
    "id": "3NyEekbBCqST",
    "outputId": "b0a4c6e6-21ca-494a-e540-77bcd1b5e143",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "'''# Run the analysis for both types\n",
    "# Simple source classification\n",
    "simple_results = run_functional_analysis(biom_table, aligned_file, analysis_type='simple') # output_biom\n",
    "\n",
    "# Detailed source classification\n",
    "detailed_results = run_functional_analysis(biom_table, aligned_file, analysis_type='detailed')'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X4rP1kdUCqST"
   },
   "source": [
    "## 6.2 Running picrust full pipeline 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:08:47.259301Z",
     "iopub.status.busy": "2025-04-19T16:08:47.258814Z",
     "iopub.status.idle": "2025-04-19T16:08:47.283837Z",
     "shell.execute_reply": "2025-04-19T16:08:47.282807Z",
     "shell.execute_reply.started": "2025-04-19T16:08:47.259220Z"
    },
    "id": "5A2a1CNLCqSW",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def run_picrust2_pipeline(fasta_file, output_dir, min_align =0.5):\n",
    "    \"\"\"\n",
    "    Run PICRUSt2 pipeline with improved error handling and path management\n",
    "\n",
    "    Args:\n",
    "        fasta_file: Path to aligned sequences fasta file (str or Path)\n",
    "        output_dir: Directory for PICRUSt2 output (str or Path)\n",
    "    \"\"\"\n",
    "    # Convert paths to strings\n",
    "    fasta_file = str(fasta_file)\n",
    "    output_dir = str(output_dir)\n",
    "\n",
    "    try:\n",
    "        # Verify picrust2 is available\n",
    "        picrust_check = subprocess.run(['which', 'picrust2_pipeline.py'],\n",
    "                                     capture_output=True,\n",
    "                                     text=True)\n",
    "        if picrust_check.returncode != 0:\n",
    "            raise RuntimeError(\"picrust2_pipeline.py not found. Please ensure PICRUSt2 is properly installed.\")\n",
    "\n",
    "        # Create output directory\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # Construct command as a single string\n",
    "        cmd = f\"picrust2_pipeline.py -s {fasta_file} -i {fasta_file} -o {output_dir} --processes 1 --verbose\"\n",
    "\n",
    "        # Run pipeline\n",
    "        print(f\"Running command: {cmd}\")\n",
    "        process = subprocess.run(cmd,\n",
    "                               shell=True,  # Use shell to handle command string\n",
    "                               check=True,\n",
    "                               capture_output=True,\n",
    "                               text=True)\n",
    "\n",
    "        print(\"PICRUSt2 Output:\")\n",
    "        print(process.stdout)\n",
    "\n",
    "        if process.stderr:\n",
    "            print(\"Warnings/Errors:\")\n",
    "            print(process.stderr)\n",
    "\n",
    "        # Add descriptions if pathway file exists\n",
    "        pathway_file = os.path.join(output_dir, 'pathways_out/path_abun_unstrat.tsv.gz')\n",
    "        if os.path.exists(pathway_file):\n",
    "            desc_cmd = f\"add_descriptions.py -i {pathway_file} -m PATHWAY -o {os.path.join(output_dir, 'pathways_with_descriptions.tsv')}\"\n",
    "            subprocess.run(desc_cmd, shell=True, check=True)\n",
    "\n",
    "        print(f\"PICRUSt2 pipeline completed successfully for {output_dir}\")\n",
    "        return True\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error running PICRUSt2 command: {e}\")\n",
    "        print(f\"Command output: {e.output}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"Error in pipeline: {str(e)}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:08:47.285143Z",
     "iopub.status.busy": "2025-04-19T16:08:47.284657Z",
     "iopub.status.idle": "2025-04-19T16:08:47.311890Z",
     "shell.execute_reply": "2025-04-19T16:08:47.310679Z",
     "shell.execute_reply.started": "2025-04-19T16:08:47.285101Z"
    },
    "id": "Hbhd5NNkCqSX",
    "outputId": "d034daec-6970-4110-fbb9-d50bfe3553be",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "'''# For original sequences\n",
    "aligned_file = aligned_fasta\n",
    "success = run_picrust2_pipeline(aligned_file, output_large)\n",
    "\n",
    "# For improved sequences\n",
    "optimized_file = output_large / \"picrust_optimized_sequences.fasta\")\n",
    "optimized_output = Path(\"~/MIC/2_Micro/data_picrust/optimized_results\")\n",
    "success_opt = run_picrust2_pipeline(optimized_file, optimized_output)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6EDOctt8uvgP"
   },
   "source": [
    "# 7. Findings and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RXu7ACOAuvgP"
   },
   "source": [
    "The PICRUSt2 pipeline generated a series of interconnected files revealing the functional potential of the microbial community. These files collectively map metabolic pathways, enzymatic functions, and taxonomic relationships, providing a multi-layered view of microbial functional capabilities across samples. Detailed view of the files found in the folder ~data_picrust are located in the manuscript."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Se5YK4UfuvgP"
   },
   "source": [
    "Picrust_Result_SEPP and Picrust_Result_EPA contain the descriptions, pathways and abundance of the full pipeline of picrust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:08:47.313635Z",
     "iopub.status.busy": "2025-04-19T16:08:47.313220Z",
     "iopub.status.idle": "2025-04-19T16:08:47.406771Z",
     "shell.execute_reply": "2025-04-19T16:08:47.405700Z",
     "shell.execute_reply.started": "2025-04-19T16:08:47.313598Z"
    },
    "id": "BZ8njtbRuvgP",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "MetaCyc_EPA_path = input_galaxy / \"Galaxy19_PICRUSt2_Add_descriptions_on_data_8.tabular\"\n",
    "Picrust_Result= pd.read_csv(MetaCyc_EPA_path, sep = \"\\t\")\n",
    "Picrust_Result_EPA= pd.read_csv(MetaCyc_EPA_path, sep = \"\\t\")\n",
    "Picrust_Result_EPA.set_index(\"description\", inplace=True)\n",
    "Picrust_Result_EPA = Picrust_Result_EPA.drop(\"pathway\", axis=1)\n",
    "Picrust_Result_EPA.index.name = \"pathway\"\n",
    "MetaCyc_SEPP_path = input_galaxy / \"Galaxy35_Add_descriptions_SEPP.tabular\"\n",
    "Picrust_Result_SEPP= pd.read_csv(MetaCyc_SEPP_path, sep = \"\\t\")\n",
    "Picrust_Result_SEPP.set_index(\"description\", inplace=True)\n",
    "Picrust_Result_SEPP = Picrust_Result_SEPP.drop(\"pathway\", axis=1)\n",
    "Picrust_Result_SEPP.index.name = \"pathway\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H39IXM7-uvgP"
   },
   "source": [
    "## 7.1. Placement Algorithm EPA vs SEPP\n",
    "nsti_SEPP and nsti_EPA Corresponds to a sample-wide measure of how closely related the microbial taxa in that sample are to known reference genomes with two different placement algoritms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:08:47.408286Z",
     "iopub.status.busy": "2025-04-19T16:08:47.407879Z",
     "iopub.status.idle": "2025-04-19T16:08:47.432196Z",
     "shell.execute_reply": "2025-04-19T16:08:47.430766Z",
     "shell.execute_reply.started": "2025-04-19T16:08:47.408218Z"
    },
    "id": "TW5Je0oouvgP",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "nsti_path_EPA = Path(input_galaxy  / \"Galaxy13_EC_weighted_nsti.tabular\")\n",
    "nsti_EPA= pd.read_csv(nsti_path_EPA, sep = \"\\t\")\n",
    "nsti_path_SEPP = Path(input_galaxy  / \"Galaxy20_EC_weighted_nsti_SEPP.tabular\")\n",
    "nsti_SEPP= pd.read_csv(nsti_path_SEPP, sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:08:47.433944Z",
     "iopub.status.busy": "2025-04-19T16:08:47.433604Z",
     "iopub.status.idle": "2025-04-19T16:08:48.200569Z",
     "shell.execute_reply": "2025-04-19T16:08:48.199447Z",
     "shell.execute_reply.started": "2025-04-19T16:08:47.433914Z"
    },
    "id": "ESTXja6nuvgP",
    "outputId": "f254886b-cc1f-45e6-fcbb-f236435b60bc",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create the scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(nsti_EPA['sample'], nsti_EPA['weighted_NSTI'], alpha=0.5, label= \"EPA\", color=\"blue\")\n",
    "plt.scatter(nsti_SEPP['sample'], nsti_SEPP['weighted_NSTI'], alpha=0.5, label= \"SEPP\", color=\"gray\")\n",
    "\n",
    "# Add the threshold line\n",
    "plt.axhline(y=0.15, color='black', linestyle='--', label='Threshold (0.15)')\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('NSTI Values by Site')\n",
    "plt.xlabel('Site')\n",
    "plt.ylabel('NSTI Value')\n",
    "plt.legend()\n",
    "\n",
    "# Rotate x-axis labels if there are many samples\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Adjust layout and display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7_fb468duvgP"
   },
   "source": [
    "Interestingly, the results are no as expected, it was though that the algorithm for placing the sequences more convenient for the present samples was SEPP because it is design specially for 16sRNA samples and diverse microbios communities, however the samples show another story. I fail to realise that the present data has been validated with the greenes genes database with the purpose of finding more compatibility with the picrust2 database, and therefore the EPA algoritm is performing much better on the all of samples using EPA placement algoritm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dUPl-r2huvgQ"
   },
   "source": [
    "## 7.2. Explore Pathway Patterns\n",
    "The pathway analysis strategy is to do a preliminar exploration before diving into specific hypotheses about organic matter metabolism and corrosion. It was chosen to start with unbiased exploratory data analysis of the PICRUSt pathways. The aim is to let the data reveal natural patterns without preconceptions. That helps to identify unexpected relationships between pathways, providing a baseline understanding of pathway distributions and relationships. This will guide subsequent targeted analyses of corrosion-relevant pathways.\n",
    "The following script takes multiple perspectives in order to visualise the data without bias and let it reveal itself. We do PCA for linear patterns, NMF for modular organization, UMAP for non-linear relationships and take different clustering approaches. The aim being to look for natural Patterns without predefined categories, so that strong strong correlations can be identified regardless of pathway type. It is visualised the distribution of pathway abundances, correlation structure, hierarchical relationships and non-linear patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gfLGAWTXuvgQ"
   },
   "source": [
    "__Category Dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:08:48.202539Z",
     "iopub.status.busy": "2025-04-19T16:08:48.202112Z",
     "iopub.status.idle": "2025-04-19T16:08:48.209832Z",
     "shell.execute_reply": "2025-04-19T16:08:48.208485Z",
     "shell.execute_reply.started": "2025-04-19T16:08:48.202508Z"
    },
    "id": "bSQwysg7uvgQ",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define category dict outside so that all charts can use same dict\n",
    "\n",
    "category_dict = Integrated_T.T.iloc[0, 0:-1].to_dict()\n",
    "\n",
    "# Define colors and categories\n",
    "category_colors = {1: '#008800',  # Dark green\n",
    "                   2: '#FF8C00',  # Dark orange\n",
    "                   3: '#FF0000'}   # Red\n",
    "\n",
    "categories_labels = {1: 'Normal Operation',\n",
    "              2: 'Early Warning',\n",
    "              3: 'System Failure'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:08:48.211319Z",
     "iopub.status.busy": "2025-04-19T16:08:48.210772Z",
     "iopub.status.idle": "2025-04-19T16:08:48.243346Z",
     "shell.execute_reply": "2025-04-19T16:08:48.241827Z",
     "shell.execute_reply.started": "2025-04-19T16:08:48.211243Z"
    },
    "id": "TvoGFdS5uvgQ",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def explore_pathway_patterns(df):\n",
    "    \"\"\"\n",
    "    Explore pathway patterns using multiple analytical approaches\n",
    "    \"\"\"\n",
    "    # Standardize data\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(df)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    def plot_exploration_results(df, results, category_dict, category_colors, categories_labels):\n",
    "        \"\"\"\n",
    "        Create visualizations for the exploratory analysis with consistent category colors\n",
    "        \"\"\"\n",
    "        # 1. Distribution of pathway abundances with category colors - side by side\n",
    "        plt.figure(figsize=(12, 6))\n",
    "\n",
    "        # Create dictionary to store abundances by category\n",
    "        category_abundances = {cat_id: [] for cat_id in categories_labels.keys()}\n",
    "\n",
    "        # Group abundances by category\n",
    "        for site_col in df.columns:\n",
    "            if site_col.startswith('site_'):\n",
    "                site_num = int(site_col.split('_')[1])\n",
    "                category = category_dict.get(f'site_{site_num}', 0)\n",
    "                if category in category_abundances:\n",
    "                    category_abundances[category].extend(df[site_col].values)\n",
    "\n",
    "        # Plot distribution for each category side by side\n",
    "        for category_id in categories_labels.keys():\n",
    "            sns.histplot(data=category_abundances[category_id],\n",
    "                        bins=50,\n",
    "                        color=category_colors[category_id],\n",
    "                        label=categories_labels[category_id],\n",
    "                        alpha=0.6,\n",
    "                        multiple=\"layer\")\n",
    "\n",
    "        plt.title('Distribution of Pathway Abundances by Category')\n",
    "        plt.xlabel('Abundance')\n",
    "        plt.yscale('log')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # 2. PCA Dimensionality Reduction\n",
    "    pca = PCA(n_components=5)\n",
    "    X_pca = pca.fit_transform(scaled_data)\n",
    "    results['pca'] = {\n",
    "        'components': X_pca,\n",
    "        'explained_variance': pca.explained_variance_ratio_,\n",
    "        'loadings': pd.DataFrame(\n",
    "            pca.components_.T,\n",
    "            index=df.columns,\n",
    "            columns=[f'PC{i+1}' for i in range(5)])}\n",
    "\n",
    "    # 3 NMF for pathway modules\n",
    "    nmf = NMF(n_components=5, init='random', random_state=0, max_iter=400)\n",
    "    W = nmf.fit_transform(df.clip(lower=0))\n",
    "    H = nmf.components_\n",
    "    results['nmf'] = {\n",
    "        'W': pd.DataFrame(W, index=df.index, columns=[f'NMF{i+1}' for i in range(5)]), # Pathway contributions\n",
    "        'H': pd.DataFrame(H, columns=df.columns, index=[f'NMF{i+1}' for i in range(5)]), # Sample patterns\n",
    "        'reconstruction_err': nmf.reconstruction_err_ }\n",
    "\n",
    "    # 4 UMAP for non-linear patterns\n",
    "    umap_reducer = umap.UMAP(random_state=0)\n",
    "    umap_result = umap_reducer.fit_transform(scaled_data)\n",
    "    results['umap'] = pd.DataFrame(umap_result, index=df.index, columns=['UMAP1', 'UMAP2'])\n",
    "\n",
    "    # 5. Multiple Clustering Approaches / Hierarchical clustering\n",
    "    linkage_matrix = hierarchy.linkage(scaled_data, method='ward')\n",
    "\n",
    "    # Try different numbers of clusters\n",
    "    cluster_results = {}\n",
    "    for n_clusters in [5, 10, 15]:\n",
    "        # Hierarchical\n",
    "        hc = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "        hc_labels = hc.fit_predict(scaled_data)\n",
    "\n",
    "        # K-means\n",
    "        km = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "        km_labels = km.fit_predict(scaled_data)\n",
    "\n",
    "        cluster_results[n_clusters] = {'hierarchical': pd.Series(hc_labels, index=df.index, name='cluster'),\n",
    "            'kmeans': pd.Series(km_labels, index=df.index, name='cluster')}\n",
    "\n",
    "    results['clustering'] = cluster_results\n",
    "    results['linkage'] = linkage_matrix\n",
    "\n",
    "    # 4. Correlation Analysis/Spearman correlation for non-linear relationships\n",
    "    corr_matrix = spearmanr(df.T)[0]\n",
    "    results['correlation'] = pd.DataFrame(corr_matrix, index=df.index, columns=df.index)\n",
    "\n",
    "    return results, X_pca\n",
    "\n",
    "def plot_exploration_results(df, results, category_dict, category_colors, categories_labels):\n",
    "    \"\"\"\n",
    "    Create visualizations for the exploratory analysis with colored categories_labels\n",
    "    \"\"\"\n",
    "    # Modified PCA visualization with categories\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    # Create subplots\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, 6), results['pca']['explained_variance'], 'bo-')\n",
    "    plt.title('PCA Explained Variance')\n",
    "    plt.xlabel('Component')\n",
    "    plt.ylabel('Explained Variance Ratio')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "\n",
    "    # Get PCA components\n",
    "    pca_data = results['pca']['components']\n",
    "\n",
    "    # Plot each category separately to create the legend\n",
    "    for category_id in categories_labels.keys():\n",
    "        # Get indices for current category\n",
    "        category_mask = [category_dict.get(f'site_{i+1}', 0) == category_id\n",
    "                        for i in range(len(pca_data))]\n",
    "\n",
    "        # Plot points for current category\n",
    "        plt.scatter(pca_data[category_mask, 0],\n",
    "                   pca_data[category_mask, 1],\n",
    "                   c=category_colors[category_id],\n",
    "                   label=categories_labels[category_id],\n",
    "                   alpha=0.6)\n",
    "\n",
    "    plt.title('PCA First Two Components')\n",
    "    plt.xlabel('PC1')\n",
    "    plt.ylabel('PC2')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # 3. UMAP visualization with categories\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    umap_df = results['umap']\n",
    "\n",
    "    for category_id in categories_labels.keys():\n",
    "        category_mask = [category_dict.get(f'site_{i+1}', 0) == category_id\n",
    "                        for i in range(len(umap_df))]\n",
    "        category_data = umap_df[category_mask]\n",
    "\n",
    "        plt.scatter(category_data['UMAP1'],\n",
    "                   category_data['UMAP2'],\n",
    "                   c=category_colors[category_id],\n",
    "                   label=categories_labels[category_id],\n",
    "                   alpha=0.6)\n",
    "\n",
    "    plt.title('UMAP Projection of Pathways by Category')\n",
    "    plt.xlabel('UMAP1')\n",
    "    plt.ylabel('UMAP2')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 4. Hierarchical clustering dendrogram - simplified version\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    dendrogram = hierarchy.dendrogram(\n",
    "        results['linkage'],\n",
    "        labels=df.index,  # Use index , columns instead of index\n",
    "        leaf_rotation=90,\n",
    "        leaf_font_size=8\n",
    "    )\n",
    "    plt.title('Pathway Clustering Dendrogram')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 5. Correlation heatmap\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    mask = np.triu(np.ones_like(results['correlation']))\n",
    "\n",
    "    # Create a custom colormap that uses our category colors\n",
    "    custom_cmap = sns.diverging_palette(220, 20, as_cmap=True)\n",
    "\n",
    "    sns.heatmap(results['correlation'],\n",
    "                mask=mask,\n",
    "                cmap=custom_cmap,\n",
    "                center=0,\n",
    "                vmin=-1,\n",
    "                vmax=1)\n",
    "    plt.title('Pathway Correlation Heatmap')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def identify_key_patterns(df, results):\n",
    "    \"\"\"\n",
    "    Identify and summarize key patterns in the data\n",
    "    \"\"\"\n",
    "    patterns = {}\n",
    "\n",
    "    # Find highly correlated pathway groups\n",
    "    corr = results['correlation']\n",
    "    high_corr = pd.DataFrame(\n",
    "        [(i, j, corr.loc[i,j])\n",
    "         for i in corr.index\n",
    "         for j in corr.index\n",
    "         if i < j and abs(corr.loc[i,j]) > 0.8],\n",
    "        columns=['pathway1', 'pathway2', 'correlation']\n",
    "    ).sort_values('correlation', ascending=False)\n",
    "\n",
    "    # Find pathways with strong PCA loadings\n",
    "    loadings = results['pca']['loadings']\n",
    "    strong_loadings = pd.DataFrame({\n",
    "        'PC1_contribution': abs(loadings['PC1']),\n",
    "        'PC2_contribution': abs(loadings['PC2'])\n",
    "    }).sort_values('PC1_contribution', ascending=False)\n",
    "\n",
    "    patterns['high_correlations'] = high_corr\n",
    "    patterns['strong_loadings'] = strong_loadings\n",
    "\n",
    "    return patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:08:48.245184Z",
     "iopub.status.busy": "2025-04-19T16:08:48.244868Z",
     "iopub.status.idle": "2025-04-19T16:09:00.521560Z",
     "shell.execute_reply": "2025-04-19T16:09:00.520328Z",
     "shell.execute_reply.started": "2025-04-19T16:08:48.245157Z"
    },
    "id": "TrS9Ii_GuvgQ",
    "outputId": "0dd22ecb-4822-4f50-835a-44df13024954",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Calling the function for the pipeline using EPA algoritm\n",
    "results_SEPP, X_pca_SEPP = explore_pathway_patterns(Picrust_Result_SEPP)\n",
    "plot_exploration_results(Picrust_Result_SEPP, results_SEPP, category_dict, category_colors, categories_labels)\n",
    "patterns_SEPP = identify_key_patterns(Picrust_Result_SEPP, results_SEPP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:09:00.523055Z",
     "iopub.status.busy": "2025-04-19T16:09:00.522740Z",
     "iopub.status.idle": "2025-04-19T16:09:10.632812Z",
     "shell.execute_reply": "2025-04-19T16:09:10.631597Z",
     "shell.execute_reply.started": "2025-04-19T16:09:00.523027Z"
    },
    "id": "EDmJPqM7uvgQ",
    "outputId": "289fcbba-df35-44cd-dc81-d11e9e5b6fe1",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Calling the function for the pipeline using EPA algoritm\n",
    "results_EPA, X_pca_EPA = explore_pathway_patterns(Picrust_Result_EPA)\n",
    "plot_exploration_results(Picrust_Result_EPA, results_EPA, category_dict, category_colors, categories_labels)\n",
    "patterns_EPA = identify_key_patterns(Picrust_Result_EPA, results_EPA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R59lKZ60uvgQ"
   },
   "source": [
    "__Discussing first results__\n",
    "\n",
    "The distribution of pathway abundances shows a typical microbial community pattern with few dominant pathways, suggesting key metabolic processes are essential across samples. PCA analysis reveals that only two components explain over 80% of the variance, indicating that metabolism in these systems might be driven by two major functional groups. The UMAP visualization confirms this binary pattern through two distinct clusters, demonstrating the robustness of this separation across different dimensional reduction techniques. The hierarchical clustering dendrogram further validates this division by showing two major branches, which notably align with previously observed physicochemical patterns in our Pourbaix plot analysis. The correlation heatmap exhibits strong relationships between specific pathway groups, suggesting coordinated metabolic activities that require detailed pathway mapping for full biological interpretation. EPA sequence placement shows much better differenciation on the pc plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Picrust_Result_EPA.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RI1NL6_uuvgQ"
   },
   "source": [
    "## 7.3. Distribution of pathway abundances and Heatmap Hierarchies\n",
    "In the following script we map the column pathway on the dataframe Picrust_Result_raw to the actual names provided by the Galaxy website that corresponds to the MetaCyc pathways. We will end up with the original Picrust_Results df with disernible names.After the 20 most abundant pathways will be plotted and the heatmap with the hierarchichal pathways drawn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:09:10.634275Z",
     "iopub.status.busy": "2025-04-19T16:09:10.633941Z",
     "iopub.status.idle": "2025-04-19T16:09:10.640647Z",
     "shell.execute_reply": "2025-04-19T16:09:10.639386Z",
     "shell.execute_reply.started": "2025-04-19T16:09:10.634228Z"
    },
    "id": "Zczh30NruvgQ",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define category dict outside so that all charts can use same dict\n",
    "\n",
    "category_dict = Integrated_T.T.iloc[0, 0:-1].to_dict()\n",
    "\n",
    "# Define colors and categories\n",
    "category_colors = {1: '#008800',  # Dark green\n",
    "                   2: '#FF8C00',  # Dark orange\n",
    "                   3: '#FF0000'}   # Red\n",
    "\n",
    "categories_labels = {1: 'Normal Operation',\n",
    "              2: 'Early Warning',\n",
    "              3: 'System Failure'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:09:10.642370Z",
     "iopub.status.busy": "2025-04-19T16:09:10.641934Z",
     "iopub.status.idle": "2025-04-19T16:09:12.434831Z",
     "shell.execute_reply": "2025-04-19T16:09:12.433590Z",
     "shell.execute_reply.started": "2025-04-19T16:09:10.642329Z"
    },
    "id": "praqb_f6uvgQ",
    "outputId": "3d18d54e-3e97-4cba-ee1f-51f132acc1ee",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def analyze_pathway_patterns(df, mean_abundances, category_dict, top_n=20):\n",
    "    \"\"\"\n",
    "    Create two separate visualizations for pathway analysis:\n",
    "    1. Stacked bar chart of top pathways by system state\n",
    "    2. Correlation heatmap of top pathways\n",
    "\n",
    "    Parameters:\n",
    "    df: DataFrame with pathway data\n",
    "    mean_abundances: Series with pre-calculated mean abundances\n",
    "    category_dict: Dictionary mapping sites to risk categories\n",
    "    top_n: Number of top pathways to display\n",
    "    \"\"\"\n",
    "    # Get top pathways\n",
    "    top_pathways = mean_abundances.nlargest(top_n)\n",
    "\n",
    "    # 1. Stacked Bar Chart\n",
    "    plt.figure(figsize=(15, 8))\n",
    "\n",
    "    # Prepare data for stacking\n",
    "    pathway_data = []\n",
    "    for pathway in top_pathways.index:\n",
    "        cat_means = {}\n",
    "        for cat in [1, 2, 3]:\n",
    "            cat_sites = [site for site, c in category_dict.items() if c == cat]\n",
    "            if cat_sites:\n",
    "                cat_means[cat] = df.loc[pathway, cat_sites].mean()\n",
    "            else:\n",
    "                cat_means[cat] = 0\n",
    "        pathway_data.append((pathway, cat_means))\n",
    "\n",
    "    # Create stacked bars\n",
    "    bottoms = np.zeros(len(top_pathways))\n",
    "    for cat in [1, 2, 3]:\n",
    "        values = [d[1][cat] for d in pathway_data]\n",
    "        plt.bar(range(len(top_pathways)), values, bottom=bottoms,\n",
    "                label=categories_labels[cat], color=category_colors[cat], alpha=0.7)\n",
    "        bottoms += values\n",
    "\n",
    "    plt.title('Top 20 Most Abundant Pathways by System State', fontsize=14, pad=20)\n",
    "    plt.xlabel('Pathway', fontsize=12)\n",
    "    plt.ylabel('Mean Abundance', fontsize=12)\n",
    "    plt.xticks(range(len(top_pathways)), top_pathways.index,\n",
    "               rotation=45, ha='right', fontsize=10)\n",
    "    plt.legend(title='System State', title_fontsize=12, fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 2. Correlation Heatmap (separate figure)\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    top_data = df.loc[top_pathways.index]\n",
    "    corr = top_data.T.corr()\n",
    "\n",
    "    # Create mask for upper triangle\n",
    "    mask = np.triu(np.ones_like(corr), k=1)\n",
    "\n",
    "    # Create heatmap with improved readability\n",
    "    sns.heatmap(corr,\n",
    "                mask=mask,\n",
    "                cmap='coolwarm',\n",
    "                center=0,\n",
    "                annot=True,\n",
    "                fmt='.2f',\n",
    "                square=True,\n",
    "                cbar_kws={'label': 'Correlation Coefficient'},\n",
    "                annot_kws={'size': 8})\n",
    "\n",
    "    plt.title('Pathway Correlation Heatmap\\n(Top 20 Most Abundant)',\n",
    "              fontsize=14,\n",
    "              pad=20)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return corr, top_data\n",
    "\n",
    "#Calculate mean abundances and run analysis\n",
    "mean_abundances_epa = Picrust_Result_EPA.mean(axis=1)\n",
    "corr_epa, top_data = analyze_pathway_patterns(Picrust_Result_EPA, mean_abundances_epa, category_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "meleGILNuvgR"
   },
   "source": [
    "__Discussing the 20 biggest metabolisms and their Hierarchical Heatmap__\n",
    "The metabolic pathway analysis reveals aerobic respiration as the dominant metabolism, showing approximately 75% higher abundance than other pathways across all systems. Correlation analysis highlights strong relationships between aerobic respiration and key metabolic processes, including TCA cycles and amino acid biosynthesis pathways, particularly those involved in biofilm formation. While these patterns provide insights into the overall metabolic landscape, a more detailed analysis separating corroded and non-corroded systems, along with integration of physicochemical variables and risk labels, would be necessary for actionable conclusions about corrosion processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_LmHp51DYel2"
   },
   "source": [
    "## 7.4. Distribution of Reactions abundances and Heatmap Hierarchies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:09:12.436633Z",
     "iopub.status.busy": "2025-04-19T16:09:12.436172Z",
     "iopub.status.idle": "2025-04-19T16:09:12.451481Z",
     "shell.execute_reply": "2025-04-19T16:09:12.450175Z",
     "shell.execute_reply.started": "2025-04-19T16:09:12.436585Z"
    },
    "id": "ISSI1EOxyxkB",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#parsing pathways (PWY) to the reactions (RXN), parce has a single column with 575 rows, that will mean that the patways can be more than once with different reactions\n",
    "parce_path = input_galaxy / \"Galaxy17_parsed_mapfile.tabular\"\n",
    "parce= pd.read_csv(parce_path, sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KSBe_WevhaZy"
   },
   "source": [
    "However attemps to parse the parce df were no suscessful, therefore it was used elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:09:12.453340Z",
     "iopub.status.busy": "2025-04-19T16:09:12.452882Z",
     "iopub.status.idle": "2025-04-19T16:09:12.467085Z",
     "shell.execute_reply": "2025-04-19T16:09:12.465901Z",
     "shell.execute_reply.started": "2025-04-19T16:09:12.453297Z"
    },
    "id": "65XT_sm0R0lC",
    "outputId": "75a36e46-d83e-489e-ab79-42dfb1e10c92",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(parce.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:09:12.468729Z",
     "iopub.status.busy": "2025-04-19T16:09:12.468391Z",
     "iopub.status.idle": "2025-04-19T16:09:12.545209Z",
     "shell.execute_reply": "2025-04-19T16:09:12.544061Z",
     "shell.execute_reply.started": "2025-04-19T16:09:12.468700Z"
    },
    "id": "PPFMKGrgYel2",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# reaction is a regroup file comprises the list of reactions in the index and the sites with abundances, similar to the pathways with abundances master file\n",
    "# whiles pathways has 366 rows (pathway), react has 2956 rows(reactions)\n",
    "react_path = input_galaxy / \"Galaxy18_regrouped_infile.tabular\"\n",
    "react= pd.read_csv(react_path, sep = \"\\t\")\n",
    "react = react.set_index(\"function\")\n",
    "react.index = react.index.astype(str)\n",
    "# Sort columns numerically\n",
    "def sort_sites_numerically(df):\n",
    "    # Extract site numbers\n",
    "    site_numbers = [int(col.replace('site_', '')) for col in df.columns if col.startswith('site_')]\n",
    "\n",
    "    # Create sorted column list\n",
    "    sorted_cols = ['site_' + str(num) for num in sorted(site_numbers)]\n",
    "\n",
    "    # Return reordered dataframe\n",
    "    return df[sorted_cols]\n",
    "react = sort_sites_numerically(react)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kYplXnPPv-j-"
   },
   "source": [
    "The parce dataframe was used for parsing the names of the function on the df reaction but the results were no human readable, so an api call was done to the rea, kegg dbs to get the names, however none of them gave results, a manual retrieval of the top 20 was done through https://gem-aureme.genouest.org/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:09:12.546320Z",
     "iopub.status.busy": "2025-04-19T16:09:12.546009Z",
     "iopub.status.idle": "2025-04-19T16:09:12.558296Z",
     "shell.execute_reply": "2025-04-19T16:09:12.556808Z",
     "shell.execute_reply.started": "2025-04-19T16:09:12.546280Z"
    },
    "id": "QDPL2I7v65sa",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "react['mean_abundances'] = react.mean(axis=1)\n",
    "\n",
    "# Get the top 20 most abundant functions\n",
    "top_functions = react['mean_abundances'].nlargest(20)\n",
    "top_functions.index = top_functions.index.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:09:12.559804Z",
     "iopub.status.busy": "2025-04-19T16:09:12.559508Z",
     "iopub.status.idle": "2025-04-19T16:09:12.575651Z",
     "shell.execute_reply": "2025-04-19T16:09:12.574218Z",
     "shell.execute_reply.started": "2025-04-19T16:09:12.559779Z"
    },
    "id": "ctzF9Q1CPW6o",
    "outputId": "d693f22b-de4e-415c-fa16-4b661fd2312b",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "top_functions.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:09:12.577209Z",
     "iopub.status.busy": "2025-04-19T16:09:12.576876Z",
     "iopub.status.idle": "2025-04-19T16:09:12.599585Z",
     "shell.execute_reply": "2025-04-19T16:09:12.598285Z",
     "shell.execute_reply.started": "2025-04-19T16:09:12.577168Z"
    },
    "id": "_tOz3yil85rb",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "rxn_dict = {'DNA-DIRECTED-DNA-POLYMERASE-RXN': 'DNA-directed DNA Polymerase',\n",
    " 'RXN-11135' : 'DNA helicase (DNA repair), Rad3 type' ,\n",
    " 'NADH-DEHYDROG-A-RXN': 'NADH dehydrogenase' ,\n",
    " '2.7.13.3-RXN': 'histidine kinase' ,\n",
    " 'PEPTIDYLPROLYL-ISOMERASE-RXN': 'peptidylprolyl isomerase ',\n",
    " '3-OXOACYL-ACP-REDUCT-RXN': '3-oxoacyl-(acyl-carrier-protein)',\n",
    " 'RXN-10060': '3-oxocerotoyl-[acp] reductase',\n",
    " 'RXN-10655': '3-oxo-cis-Δ7-tetradecenoyl-[acp] reductase',\n",
    " 'RXN-10659': '3-oxo-cis-Δ9-hexadecenoyl-[acp] reductase',\n",
    " 'RXN-11476': '3-oxo-glutaryl-[acp] methyl ester reductase',\n",
    " 'RXN-11480': '3-oxo-pimeloyl-[acp] methyl ester reductase',\n",
    " 'RXN-13008': '3-oxo-docosapentaenoyl [acp][c] ',\n",
    " 'RXN-16616': '(5Z)-3-oxo-tetradec-5-enoyl-[acyl-carrier-protein] reductase',\n",
    " 'RXN-16622': '(7Z)-3-oxo-hexadec-7-enoyl-[acp] reductase',\n",
    " 'RXN-16626': '(9Z)-3-oxo-octadec-9-enoyl-[acp] reductase',\n",
    " 'RXN-16630': '(11Z)-3-oxo-icos-11-enoyl-[acp] reductase',\n",
    " 'RXN-9514': 'acetoacetyl-[acyl-carrier protein] reductase',\n",
    " 'RXN-9518': '3-hydroxyhexanoyl-[acyl-carrier protein] reductase',\n",
    " 'RXN-9524': '3-oxo-octanoyl-[acyl-carrier protein] reductase',\n",
    " 'RXN-9528': '3-oxo-decanoyl-[acyl-carrier protein] reductase'}\n",
    "\n",
    "clean_rxn_dict = {k.strip(): v for k, v in rxn_dict.items()}\n",
    "\n",
    "react= react.rename(index =clean_rxn_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:09:12.601346Z",
     "iopub.status.busy": "2025-04-19T16:09:12.600981Z",
     "iopub.status.idle": "2025-04-19T16:09:14.430379Z",
     "shell.execute_reply": "2025-04-19T16:09:14.429377Z",
     "shell.execute_reply.started": "2025-04-19T16:09:12.601315Z"
    },
    "id": "A7kaHJFkYel2",
    "outputId": "b6612cae-017d-4054-dc64-303263232441",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def analyze_reaction_patterns(top_functions, mean_abundances, category_dict):\n",
    "    \"\"\"\n",
    "    Analyzes pathway patterns for a DataFrame with 'function' as index and 'Sites' as columns.\n",
    "    This is the FINAL, CORRECTED implementation.\n",
    "    \"\"\"\n",
    "    # 1. Stacked Bar Chart\n",
    "    plt.figure(figsize=(15, 8))\n",
    "\n",
    "    function_data = []\n",
    "    for function in top_functions.index:\n",
    "        cat_means = {}\n",
    "        for cat in [1, 2, 3]:\n",
    "            cat_sites = [site for site, c in category_dict.items() if c == cat]\n",
    "            # Optimized site selection:\n",
    "            relevant_sites = list(df.columns.intersection(cat_sites)) # More efficient intersection\n",
    "            if relevant_sites:\n",
    "                cat_means[cat] = df.loc[function, relevant_sites].mean()\n",
    "            else:\n",
    "                cat_means[cat] = 0\n",
    "        function_data.append((function, cat_means))\n",
    "\n",
    "    bottoms = np.zeros(len(top_functions))\n",
    "    for cat in [1, 2, 3]:\n",
    "        values = [d[1][cat] for d in function_data]\n",
    "        plt.bar(range(len(top_functions)), values, bottom=bottoms,\n",
    "                label=categories_labels[cat], color=category_colors[cat], alpha=0.7)\n",
    "        bottoms += values\n",
    "\n",
    "    plt.title('Top 20 Most Abundant Functions by System State', fontsize=14, pad=20)\n",
    "    plt.xlabel('Function', fontsize=12)\n",
    "    plt.ylabel('Mean Abundance', fontsize=12)\n",
    "    plt.xticks(range(len(top_functions)), top_functions.index, rotation=45, ha='right', fontsize=10)\n",
    "    plt.legend(title='System State', title_fontsize=12, fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 2. Correlation Heatmap\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    top_data = df.loc[top_functions.index]\n",
    "    # Convert all columns of top_data to numeric, coercing errors to NaN\n",
    "    top_data = top_data.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # Drop rows with any NaN values to ensure only numeric data is used for correlation\n",
    "    top_data = top_data.dropna(axis=1, how='all')\n",
    "    # Check if there are any columns left after dropping NaNs\n",
    "    if top_data.empty:\n",
    "        print(\"Warning: DataFrame is empty after dropping NaN columns. Skipping correlation heatmap.\")\n",
    "        return None  # Or return an empty DataFrame or a placeholder\n",
    "\n",
    "    corr = top_data.T.corr()  # Transpose for function correlation\n",
    "\n",
    "    mask = np.triu(np.ones_like(corr), k=1)  # Mask for upper triangle\n",
    "    sns.heatmap(corr, mask=mask, cmap='coolwarm', center=0, annot=True, fmt='.2f',\n",
    "                square=True, cbar_kws={'label': 'Correlation Coefficient'}, annot_kws={'size': 8})\n",
    "\n",
    "    plt.title('Function Correlation Heatmap\\n(Top 20 Most Abundant)', fontsize=14, pad=20)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return corr, top_data\n",
    "\n",
    "# Convert numeric columns to the correct data type\n",
    "for col in react.columns[1:]:  # Exclude 'function' column\n",
    "    try:\n",
    "        react[col] = pd.to_numeric(react[col], errors='coerce') # Skip errors but convert rest\n",
    "    except ValueError:\n",
    "        print(f\"Could not convert column '{col}' to numeric. Check its contents.\")\n",
    "        # Handle the error or investigate the column for non-numeric values\n",
    "\n",
    "# Calculate the mean after type conversion\n",
    "mean_abundances_react = react.mean(axis=1, numeric_only=True) # Specify only numeric in case strings remain\n",
    "corr_react, top_data = analyze_pathway_patterns(react, mean_abundances_react, category_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tYrg4kbRUSll"
   },
   "source": [
    "Reactions chart confirm our label system, the clear progression of abundance across our system states (normal operation → early warning → system failure) is particularly compelling evidence that these reactions are directly involved in the corrosion process rather than just coincidental. Also the top reactions corresponds to core taxa that however are seem to be implicated on the corrosion failure. Additionally the fact that most of the reactions deal with oxo groups on an aromatic ring, further reinfor the study intuition that oxalic and acetic acid can be a good dummy compounds to represent organic matter on the TOC. These small organic acids are likely end products or intermediates of the metabolic pathways involving the 3-oxoacyl compounds so abundant in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U-Xa6aVgUSGf"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w0Sbd6A7uvgR"
   },
   "source": [
    "# 8. Mapping the Pathways back to the Genera\n",
    "\n",
    "The result we obtained from the picrust pipeline contain the following dataframes, here described so it would be possible to parse. Following are the files description with the shape\n",
    "| Picrust_Result | Picrust_Result | Picrust_Result | parce | parce | parce | ECcontri | ECcontri | ECcontri | ECcontri | React | React |\n",
    "|---|---|---|---|---|---|---|---|---|---|---|---|\n",
    "| pathway | description | Sites/abund | pathway | RXN | EC number | EC number | varios abundances | Sites | OTU | Sites/abund | Reactions |\n",
    "|366,72|366,72|366,72|574,1|574,1|574,1|1491288, 9|1491288, 9|1491288, 9|1491288, 9| (2955, 71)|(2955, 71)|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:09:14.431920Z",
     "iopub.status.busy": "2025-04-19T16:09:14.431559Z",
     "iopub.status.idle": "2025-04-19T16:09:17.132483Z",
     "shell.execute_reply": "2025-04-19T16:09:17.131224Z",
     "shell.execute_reply.started": "2025-04-19T16:09:14.431887Z"
    },
    "id": "RGMCpOrRuvgR",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ECcontri and KOcontri files contain sample, function (EC/KO number), taxon (genus/OTU ID), and abundance metrics.\n",
    "ECcontri_path = input_galaxy / \"Galaxy26_contrib.tabular\"\n",
    "\n",
    "#ECcontri_path =  Path(base_dir / \"Galaxy26_contrib.tabular\") # for Kaggle\n",
    "ECcontri= pd.read_csv(ECcontri_path, sep = \"\\t\")\n",
    "#KOcontri_path = Path(large_dir / \"Galaxy30-[KO_pred_metagenome_contrib].tabular\")\n",
    "#KOcontri= pd.read_csv(KOcontri_path, sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:09:17.133945Z",
     "iopub.status.busy": "2025-04-19T16:09:17.133627Z",
     "iopub.status.idle": "2025-04-19T16:09:17.145327Z",
     "shell.execute_reply": "2025-04-19T16:09:17.144033Z",
     "shell.execute_reply.started": "2025-04-19T16:09:17.133918Z"
    },
    "id": "pxTskbBWUg0Q",
    "outputId": "5a1ed1e5-d150-417d-9141-e751d0b20c42",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(ECcontri.head(), ECcontri.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HB57hdTiuvgR"
   },
   "source": [
    "## 8.1. Mapping Genera to Otu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:09:17.146909Z",
     "iopub.status.busy": "2025-04-19T16:09:17.146507Z",
     "iopub.status.idle": "2025-04-19T16:09:17.169317Z",
     "shell.execute_reply": "2025-04-19T16:09:17.168075Z",
     "shell.execute_reply.started": "2025-04-19T16:09:17.146869Z"
    },
    "id": "C_7z90_iuvgR",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Mapping the Genera to Otu for the Taxonomy assigment requeriment\n",
    "def create_otu_mapping(fasta_file_final):\n",
    "    \"\"\"Creates a DataFrame mapping OTUs to genera from a FASTA file\n",
    "    Args: fasta_file (str): Path to FASTA file\n",
    "    Returns: pd.DataFrame: DataFrame with columns ['Genus', 'OTU']\n",
    "    \"\"\"\n",
    "    mapping_data = []\n",
    "\n",
    "    for record in SeqIO.parse(fasta_file_final, \"fasta\"):\n",
    "        # Split description to get genus and OTU\n",
    "        parts = record.description.split()\n",
    "        genus = parts[0]\n",
    "        otu = parts[1]  # Take first OTU number\n",
    "\n",
    "        mapping_data.append({'Genus': genus,'OTU': otu})\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(mapping_data).sort_values('Genus')\n",
    "\n",
    "    return df\n",
    "\n",
    "otu_mapping = create_otu_mapping(fasta_file_final)\n",
    "# Change the name of the Otus since they using taxon\n",
    "otu_mapping = otu_mapping.rename(columns={\"OTU\" : \"taxon\"})\n",
    "\n",
    "otu_path = output_base / \"otu_mapping.tsv\"\n",
    "otu_mapping.to_csv(otu_path, sep='\\t', index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3SWsDEMsuvgR"
   },
   "source": [
    "Data Extraction from the Parce File: The parce file, containing pathway, reaction, and EC number information, is used to extract EC numbers. These EC numbers are intended to link to the corresponding pathways.\n",
    "\n",
    "Initial Mapping Approach: Pathways (from Picrust_Result) were mapped to the parce file to ensure accurate EC–pathway links.\n",
    "Reactions were similarly mapped to maintain correct RXN–pathway links.\n",
    "These mappings were then used to update the ECcontri dataframe, which represents stratified pathway abundance contributions (including KO/EC, taxon, taxon abundance, etc.).\n",
    "Unmapped EC numbers were kept separate for review.\n",
    "Revised Mapping Strategy: Due to incomplete overlap between the EC numbers and pathways in the parce file and ECcontri, the strategy was adjusted. Instead of relying solely on the parce file, the mapping now integrates the 'description' and 'pathways' columns directly from the Picrust_Result file into ECcontri. This integration is performed by matching on the 'Site' column, rather than using the EC number from the function column.\n",
    "\n",
    "Final Integration: The taxonomy assignment file (linking OTUs to genera) is joined with ECcontri via the taxon column. This final combined dataset (Picrust_Result joined with ECcontri) provides complete pathway descriptions and abundance data for subsequent visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kgN_STGJj9Qz"
   },
   "source": [
    "### Pathway Mapping Analysis\n",
    "\n",
    "There were identified a discrepancy between EC predictions and pathway abundances. Found 61 pathways with EC number evidence that were not included in final predictions. Total number of reference pathways: 574 (from MetaCyc), total pathways in final predictions: 366, example missing pathway: PWY-6486 supported by EC:4.2.1.41\n",
    "\n",
    "Implications\n",
    "This finding suggests that the pathway prediction pipeline might be filtering out potentially relevant pathways despite having supporting EC evidence. This could impact the biological interpretation of the functional profiles and warrants further investigation.\n",
    "So in this study we mapped the pathways dataframe directly to the parce file and in doing so, we have also the reaction information, avoiding the discrepancy with the Picrust_Result missing pathways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bCs2cimyuvgS"
   },
   "source": [
    "## 8.2. Map Econtri to pathways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nEkDJg5zuvgS"
   },
   "source": [
    "It is possible now directly map the description and the pathway from Picrust_Result into ECcontri because each site can have several pathways, so we reshaping the Picrust_Result to long format and so that each row corresponds to a pathway for a given site. It is no possible to do this on a go using the whole 1491288 rows on ECcontri, so it would have to be done on agreggated data, as suggested by McKinney, 2010.\n",
    "Source: McKinney, W. (2010). Data Structures for Statistical Computing in Python. Retrieved from https://pandas.pydata.org/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:09:17.170750Z",
     "iopub.status.busy": "2025-04-19T16:09:17.170432Z",
     "iopub.status.idle": "2025-04-19T16:09:17.424340Z",
     "shell.execute_reply": "2025-04-19T16:09:17.423193Z",
     "shell.execute_reply.started": "2025-04-19T16:09:17.170719Z"
    },
    "id": "npxOZlx8uvgS",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Reshape Picrust_Result to long format: each row now corresponds to a pathway for a given site\n",
    "picrust_long = Picrust_Result.melt(id_vars=['pathway', 'description'],\n",
    "                                   var_name='sample',\n",
    "                                   value_name='abundance')\n",
    "\n",
    "# Filter out rows where the abundance is 0\n",
    "picrust_long = picrust_long[picrust_long['abundance'] > 0]\n",
    "\n",
    "# Aggregate pathway info per site\n",
    "mapping = picrust_long.groupby('sample').agg({\n",
    "    'pathway': lambda x: list(x),\n",
    "    'description': lambda x: list(x)\n",
    "}).reset_index()\n",
    "\n",
    "# Merge the aggregated mapping with ECcontri\n",
    "ECcontri_agg_site = pd.merge(ECcontri, mapping, on='sample', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:09:17.425645Z",
     "iopub.status.busy": "2025-04-19T16:09:17.425377Z",
     "iopub.status.idle": "2025-04-19T16:09:20.169421Z",
     "shell.execute_reply": "2025-04-19T16:09:20.168365Z",
     "shell.execute_reply.started": "2025-04-19T16:09:17.425613Z"
    },
    "id": "5EXHu-QeuvgS",
    "trusted": true
   },
   "outputs": [],
   "source": [
    " # Add genus information from otu_mapping\n",
    "ECcontri_agg_site['taxon'] = ECcontri_agg_site['taxon'].astype(str)\n",
    "otu_mapping['taxon'] = otu_mapping['taxon'].astype(str)\n",
    "\n",
    "ECcontri_otu= pd.merge(ECcontri_agg_site, otu_mapping, on='taxon', how='left', validate='m:1')\n",
    "\n",
    "unmapped = ECcontri_otu['Genus'].isna().sum()\n",
    "if unmapped > 0:\n",
    "    print(f\"Warning: {unmapped} rows could not be mapped to genera\")\n",
    "# Rename columns: here \"description\" becomes \"pathway\" and \"pathway\" becomes \"npath\"\n",
    "ECcontri_otu  = ECcontri_otu.rename(columns={\"sample\":\"Sites\", \"function\": \"EC\", \"taxon\": \"OTU\", \"description\":\"pathway\", \"pathway\":\"npath\",\n",
    "                                     \"taxon_abun\": \"abund_raw\", \"taxon_function_abun\": \"abund_contri\", \"taxon_rel_abun\": \"rel_abund_raw\",\n",
    "                                       \"taxon_rel_function_abun\": \"rel_abund_contri\", \"norm_taxon_function_contrib\" :\"norm_abund_contri\", \"genome_function_count\":\"genome_EC_count\"})\n",
    "# Organize columns in logical groups\n",
    "cols_order = ['Sites', 'Genus', 'OTU', 'EC', # Identification columns\n",
    "              'npath', 'pathway', # Pathway information\n",
    "              'abund_raw', 'rel_abund_raw', # Raw abundance metrics\n",
    "              'genome_EC_count', 'abund_contri', 'rel_abund_contri', 'norm_abund_contri'] # Contribution metrics\n",
    "# Reorder columns, takes like 4 minutes on this slow laptop\n",
    "ECcontri_otu = ECcontri_otu[cols_order]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2eC_YFQdY1Yj"
   },
   "source": [
    "## 8.3 Critical comparison of the abundances on ECcontri_otu\n",
    "abund_contri: is the absolute contribution of a specific genus to a specific function   \n",
    "rel_abund_contri is the relative contribution (percentage)   \n",
    "norm_abund_contri is the normalized contribution   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:09:20.171062Z",
     "iopub.status.busy": "2025-04-19T16:09:20.170643Z",
     "iopub.status.idle": "2025-04-19T16:09:29.825082Z",
     "shell.execute_reply": "2025-04-19T16:09:29.824021Z",
     "shell.execute_reply.started": "2025-04-19T16:09:20.171018Z"
    },
    "id": "RaauSfmoP0tb",
    "outputId": "dcd36e4f-4db3-41c9-ff1f-2185cc57cf0b",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot 1: rel_abund_contri vs. abund_contri\n",
    "sns.scatterplot(data=ECcontri_otu, x='abund_contri', y='rel_abund_contri', ax=axes[0])\n",
    "axes[0].set_title(\"Rel vs Absolute Abundance Contribution\")\n",
    "axes[0].set_xlabel(\"Absolute Abundance Contribution\")\n",
    "axes[0].set_ylabel(\"Relative Abundance Contribution\")\n",
    "\n",
    "# Plot 2: norm_abund_contri vs. abund_contri\n",
    "sns.scatterplot(data=ECcontri_otu, x='abund_contri', y='norm_abund_contri', ax=axes[1])\n",
    "axes[1].set_title(\"Norm vs Absolute Abundance Contribution\")\n",
    "axes[1].set_xlabel(\"Absolute Abundance Contribution\")\n",
    "axes[1].set_ylabel(\"Normalized Abundance Contribution\")\n",
    "\n",
    "# Plot 3: rel_abund_contri vs. norm_abund_contri\n",
    "sns.scatterplot(data=ECcontri_otu, x='rel_abund_contri', y='norm_abund_contri', ax=axes[2])\n",
    "axes[2].set_title(\"Rel vs Norm Abundance Contribution\")\n",
    "axes[2].set_xlabel(\"Relative Abundance Contribution\")\n",
    "axes[2].set_ylabel(\"Normalized Abundance Contribution\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O2P3cmYSbuNs"
   },
   "source": [
    "From the three abundances it is seen that the absolute abundance is less granular as the relative and normalised abundances. The absolute abundance tends to lead to higher relative abundance with a strong positive correlation. Seems that it aggreegates some data and there are fewer values. The Normallise Abundance removes some of the direct correlation between absolute and relative abundance and highlights differences between individual bacteria or proteins. This alignes with ther biology that same EC is present in many bacteria with varying expression levels of proteins. Since normalisation removes sample size effects, it reveals true variations across bacteria, since protein expression is highly variable within ECs, the normalise abundance appears to be the most suitable for comparisons moving forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:09:29.826491Z",
     "iopub.status.busy": "2025-04-19T16:09:29.826205Z",
     "iopub.status.idle": "2025-04-19T16:09:30.065186Z",
     "shell.execute_reply": "2025-04-19T16:09:30.063938Z",
     "shell.execute_reply.started": "2025-04-19T16:09:29.826467Z"
    },
    "id": "xQOtjMOhZQzj",
    "outputId": "e646cbde-c920-4f27-f73c-4b2554331da6",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Count unique proteins per EC\n",
    "EC_counts = ECcontri_otu.groupby(\"EC\")[\"OTU\"].nunique()\n",
    "print(f\"Unique proteins per EC\", EC_counts.describe())  # Check expected range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MPaVL4T0uvgS"
   },
   "source": [
    "ECcontri_otu is a comprehensive dataframe that combines site locations, taxonomic information (genera and OTUs), enzyme classifications (ECs), and pathways (code for pathway (npath) and description (pathway)). The associated abundance metrics belong to the original ECcontri. The abundance metrics include:\n",
    "abund_raw: The original count of each organism (OTU) at each site\n",
    "rel_abund_raw: The relative abundance of each organism at each site, expressed as a proportion of total counts\n",
    "genome_function_count represents the predicted number of copies of a particular EC number (enzyme) in an organism's genome. This prediction comes from PICRUSt's hidden-state prediction process, which infers gene family abundances for each organism based on its phylogenetic placement relative to reference genomes\n",
    "abund_contri: The contribution of each organism to a specific enzyme function, calculated by multiplying the raw abundance by the number of copies of that enzyme in the organism's genome\n",
    "rel_abund_contri: The relative contribution of each organism to the enzyme function, accounting for both abundance and genome copy number\n",
    "norm_abund_contri: The normalized contribution metric that allows comparison across different sites and functions\n",
    "\n",
    "## 8.4. Statistical Analysis of the Genome Function Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:09:30.066614Z",
     "iopub.status.busy": "2025-04-19T16:09:30.066334Z",
     "iopub.status.idle": "2025-04-19T16:09:30.662427Z",
     "shell.execute_reply": "2025-04-19T16:09:30.661196Z",
     "shell.execute_reply.started": "2025-04-19T16:09:30.066589Z"
    },
    "id": "xeKQezwyuvgS",
    "outputId": "7aae35d4-551b-492c-9566-375a98703801",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Analyze genome_function_count\n",
    "print(\"Genome function count statistics:\")\n",
    "print(\"\\nOverall statistics:\")\n",
    "print(ECcontri_otu['genome_EC_count'].describe())\n",
    "\n",
    "# Look at distribution by EC number\n",
    "print(\"\\nExample EC numbers and their genome counts:\")\n",
    "ec_counts = ECcontri_otu.groupby('EC')['genome_EC_count'].agg(['unique', 'mean', 'max']).head()\n",
    "print(ec_counts)\n",
    "\n",
    "# Check if genome_function_count is consistent for each OTU-EC pair\n",
    "print(\"\\nCheck if genome_EC_count is consistent for OTU-EC combinations:\")\n",
    "consistency_check = ECcontri_otu.groupby(['OTU', 'EC'])['genome_EC_count'].nunique()\n",
    "inconsistent = consistency_check[consistency_check > 1]\n",
    "if len(inconsistent) > 0:\n",
    "    print(f\"Found {len(inconsistent)} OTU-EC pairs with inconsistent genome counts\")\n",
    "else:\n",
    "    print(\"Genome counts are consistent for all OTU-EC pairs\")\n",
    "\n",
    "# Explain the metrics in the dataframe\n",
    "print(\"\\nDataframe Components:\")\n",
    "print(\"1. Abundance Metrics:\")\n",
    "print(\"   - abund_raw: Raw abundance of each organism in each site\")\n",
    "print(\"   - abund_contri: Organism's abundance contribution to function/pathway\")\n",
    "print(\"   - rel_abund_raw: Original relative abundance\")\n",
    "print(\"   - rel_abund_contri: Relative abundance contribution to pathway\")\n",
    "print(\"   - norm_abund_contri: Normalized abundance contribution\")\n",
    "print(\"\\n2. Genome Function Count:\")\n",
    "print(\"   Number of copies of each EC (enzyme) in organism's genome\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xpjxR3vk-PJK"
   },
   "source": [
    "Genome function count statistics:\n",
    "\n",
    "Overall statistics:\n",
    "count    1.491288e+06\n",
    "mean     1.390277e+00\n",
    "std      1.071974e+00\n",
    "min      1.000000e+00\n",
    "25%      1.000000e+00\n",
    "50%      1.000000e+00\n",
    "75%      1.000000e+00\n",
    "max      1.000000e+01\n",
    "Name: genome_EC_count, dtype: float64\n",
    "\n",
    "Example EC numbers and their genome counts:\n",
    "                                    unique      mean  max\n",
    "EC                                                       \n",
    "EC:1.1.1.1              [3, 2, 1, 5, 4, 8]  2.310375    8\n",
    "EC:1.1.1.100  [8, 5, 2, 3, 4, 9, 10, 6, 1]  4.237317   10\n",
    "EC:1.1.1.102                           [1]  1.000000    1\n",
    "EC:1.1.1.103                           [1]  1.000000    1\n",
    "EC:1.1.1.105                           [1]  1.000000    1\n",
    "\n",
    "Check if genome_EC_count is consistent for OTU-EC combinations:\n",
    "Genome counts are consistent for all OTU-EC pairs\n",
    "\n",
    "Dataframe Components:\n",
    "1. Abundance Metrics:\n",
    "   - abund_raw: Raw abundance of each organism in each site\n",
    "   - abund_contri: Organism's abundance contribution to function/pathway\n",
    "   - rel_abund_raw: Original relative abundance\n",
    "   - rel_abund_contri: Relative abundance contribution to pathway\n",
    "   - norm_abund_contri: Normalized abundance contribution\n",
    "\n",
    "2. Genome Function Count:\n",
    "   Number of copies of each EC (enzyme) in organism's genome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MA-sQJpQuvgS"
   },
   "source": [
    "Analysis of genome_function_count(genome_EC_count) shows that most organisms typically have just one copy of any given enzyme (EC number) in their genome, with 75% of all cases showing a single copy. However, there is notable variation, with some organisms having up to 10 copies of certain enzymes. The average across all cases is 1.4 copies per enzyme per organism.\n",
    "Some enzymes show more variation than others. For example:\n",
    "\n",
    "EC:1.1.1.1 varies from 1 to 8 copies across different organisms\n",
    "EC:1.1.1.100 shows the widest range, from 1 to 10 copies\n",
    "Many enzymes (like EC:1.1.1.102, 103, 105) consistently appear as single copies\n",
    "\n",
    "Importantly, the copy number is consistent for each organism-enzyme combination across all sites, indicating this is a stable genomic characteristic.\n",
    "____________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "voefPo-VuvgS"
   },
   "source": [
    "\n",
    "\n",
    "Now ECcontri_otu has several rows and columns providing information of the EC contribution to the metrics to each enzime aka EC number to the sites, genera combination, however the pathways are from origin link to most of the sites. This is perhaps because the methos infwee dunxriona bAWS ON XOMON sets of reference genomes.  Then, same environment in this case heating and cooling water systems poses similar organisms with similar pathways, the difference being on the abundance. So in order for this data to be usable, it is necesary to parse the EC into human readable information from a external enzyme databases to retrieve functional information about an EC number. Common resources include:\n",
    "\n",
    "UniProt: query UniProt’s REST API to get enzyme details by searching with the EC number.\n",
    "ExPASy Enzyme Database: Provides enzyme information based on EC numbers.\n",
    "BRENDA: A comprehensive enzyme database that can be queried either via its web interface or programmatically (e.g., using the bioservices Python package). Following script creates an EnzymeRetriever class that handles API requests to UniProt, processes unique EC numbers to avoid duplicate requests\n",
    "Adds protein names, functions, and UniProt IDs to ECcontri_otu df and includes rate limiting to avoid API restrictions.\n",
    "The retrieval was done localy using vscode and via colab because the retrieval require a superior ram and cpu, it took batches spread on several days.\n",
    "\n",
    "## 8.5. Retrieval of protein names from Uniprot though Api call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:09:30.663901Z",
     "iopub.status.busy": "2025-04-19T16:09:30.663585Z",
     "iopub.status.idle": "2025-04-19T16:09:30.704160Z",
     "shell.execute_reply": "2025-04-19T16:09:30.702992Z",
     "shell.execute_reply.started": "2025-04-19T16:09:30.663874Z"
    },
    "id": "hTO90O2duvgT",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ColabEnzymeRetriever:\n",
    "    def __init__(self, batch_size=100, save_every=5):\n",
    "        self.uniprot_api = \"https://rest.uniprot.org/uniprotkb/search\"\n",
    "        self.batch_size = batch_size\n",
    "        self.save_every = save_every\n",
    "        self.results_file = Path('uniprot_results.tsv')\n",
    "        self.state_file = Path('retrieval_state.json')\n",
    "        self.processed_pairs: Set[Tuple[str, str]] = set()\n",
    "        self.existing_results = None\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "\n",
    "        if not self.logger.handlers:\n",
    "            handler = logging.StreamHandler()\n",
    "            formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "            handler.setFormatter(formatter)\n",
    "            self.logger.addHandler(handler)\n",
    "\n",
    "    def load_existing_results(self, file_path: Path) -> pd.DataFrame:\n",
    "        \"\"\"Load and validate existing results\"\"\"\n",
    "        if file_path.exists():\n",
    "            try:\n",
    "                self.existing_results = pd.read_csv(file_path, sep='\\t')\n",
    "                self.logger.info(f\"Loaded {len(self.existing_results)} existing results\")\n",
    "\n",
    "                # Build set of processed pairs\n",
    "                self.processed_pairs = set()\n",
    "                for _, row in self.existing_results.iterrows():\n",
    "                    if pd.notna(row['ec_number']) and pd.notna(row['organism']):\n",
    "                        ec_num = str(row['ec_number']).strip()\n",
    "                        org = str(row['organism']).split()[0].strip()\n",
    "                        self.processed_pairs.add((ec_num, org))\n",
    "\n",
    "                return self.existing_results\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error loading results file: {e}\")\n",
    "                self.existing_results = pd.DataFrame(\n",
    "                    columns=['uniprot_id', 'ec_number', 'protein_name', 'organism', 'score']\n",
    "                )\n",
    "                return self.existing_results\n",
    "\n",
    "        self.existing_results = pd.DataFrame(\n",
    "            columns=['uniprot_id', 'ec_number', 'protein_name', 'organism', 'score']\n",
    "        )\n",
    "        return self.existing_results\n",
    "\n",
    "    def get_uniprot_info(self, ec: str, organism: str) -> Optional[dict]:\n",
    "        \"\"\"Get UniProt information for a specific EC-organism pair\"\"\"\n",
    "        if (ec, organism) in self.processed_pairs:\n",
    "            return None\n",
    "\n",
    "        query = f'({ec}) AND (organism_name:\"{organism}*\")'\n",
    "        params = {\n",
    "            'query': query,\n",
    "            'format': 'tsv',\n",
    "            'fields': 'id,ec,protein_name,organism_name',\n",
    "            'size': 10\n",
    "        }\n",
    "\n",
    "        max_retries = 3\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = requests.get(self.uniprot_api, params=params)\n",
    "                response.raise_for_status()\n",
    "                time.sleep(0.5)\n",
    "\n",
    "                lines = response.text.strip().split('\\n')\n",
    "                if len(lines) < 2:\n",
    "                    return None\n",
    "\n",
    "                best_match = None\n",
    "                best_score = -float('inf')\n",
    "\n",
    "                for line in lines[1:]:\n",
    "                    parts = line.split('\\t')\n",
    "                    if len(parts) < 4:\n",
    "                        continue\n",
    "\n",
    "                    uniprot_id, ec_numbers, protein_name, organism_name = parts\n",
    "\n",
    "                score = 0\n",
    "                if organism_name and isinstance(organism_name, str):\n",
    "                    name_parts = organism_name.split()\n",
    "                    genus = name_parts[0] if name_parts else \"\"\n",
    "\n",
    "                    # Exact genus match gets highest score\n",
    "                    if genus.lower() == organism.lower():\n",
    "                        score += 500\n",
    "                        # Prefer entries with just the genus name\n",
    "                        if len(name_parts) == 1:\n",
    "                            score += 300\n",
    "                        # Heavily penalize strain designations or subspecies\n",
    "                        elif len(name_parts) > 2 or any(char.isdigit() for char in organism_name):\n",
    "                            score -= 400\n",
    "\n",
    "                    if score > -float('inf'):\n",
    "                        if ec.replace('EC:', '') in ec_numbers.split('; '):\n",
    "                            score += 150\n",
    "\n",
    "                            if score > best_score:\n",
    "                                best_score = score\n",
    "                                best_match = {\n",
    "                                    'uniprot_id': uniprot_id,\n",
    "                                    'ec_number': ec,\n",
    "                                    'protein_name': protein_name,\n",
    "                                    'organism': organism_name,\n",
    "                                    'score': score\n",
    "                                }\n",
    "\n",
    "                return best_match if best_match else None\n",
    "\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(2 ** attempt)\n",
    "                    continue\n",
    "                self.logger.error(f\"Error fetching data from UniProt: {e}\")\n",
    "                return None\n",
    "\n",
    "    def process_remaining_pairs(self, unique_pairs: pd.DataFrame, start_ec: str) -> pd.DataFrame:\n",
    "        \"\"\"Process remaining pairs with enforced starting point\"\"\"\n",
    "        # Ensure EC format consistency\n",
    "        if not start_ec.startswith('EC:'):\n",
    "            start_ec = f\"EC:{start_ec.replace('EC:', '')}\"\n",
    "\n",
    "        # Sort and filter pairs\n",
    "        unique_pairs = unique_pairs.sort_values(['EC', 'Genus']).reset_index(drop=True)\n",
    "        unique_pairs = unique_pairs[unique_pairs['EC'] >= start_ec].reset_index(drop=True)\n",
    "\n",
    "        if len(unique_pairs) == 0:\n",
    "            self.logger.warning(f\"No EC numbers found after {start_ec}\")\n",
    "            return self.existing_results\n",
    "\n",
    "        self.logger.info(f\"Starting processing from {unique_pairs.iloc[0]['EC']}\")\n",
    "        total_pairs = len(unique_pairs)\n",
    "\n",
    "        results = []\n",
    "        for idx in range(0, total_pairs, self.batch_size):\n",
    "            batch = unique_pairs.iloc[idx:idx + self.batch_size]\n",
    "            batch_results = []\n",
    "\n",
    "            self.logger.info(f\"\\nProcessing batch {idx//self.batch_size + 1} of {total_pairs//self.batch_size + 1}\")\n",
    "            self.logger.info(f\"Progress: {idx}/{total_pairs} pairs ({(idx/total_pairs)*100:.1f}%)\")\n",
    "\n",
    "            current_ec = None\n",
    "            for _, row in batch.iterrows():\n",
    "                if current_ec != row['EC']:\n",
    "                    current_ec = row['EC']\n",
    "                    self.logger.info(f\"\\nProcessing EC number: {current_ec}\")\n",
    "\n",
    "                if (row['EC'], row['Genus']) not in self.processed_pairs:\n",
    "                    result = self.get_uniprot_info(row['EC'], row['Genus'])\n",
    "                    if result:\n",
    "                        batch_results.append(result)\n",
    "                        self.processed_pairs.add((row['EC'], row['Genus']))\n",
    "\n",
    "            if batch_results:\n",
    "                results.extend(batch_results)\n",
    "\n",
    "                # Save progress periodically\n",
    "                if (idx//self.batch_size) % self.save_every == 0:\n",
    "                    combined_results = pd.concat(\n",
    "                        [self.existing_results, pd.DataFrame(results)],\n",
    "                        ignore_index=True\n",
    "                    )\n",
    "                    combined_results.to_csv(self.results_file, sep='\\t', index=False)\n",
    "                    self.logger.info(f\"Saved {len(combined_results)} total results to file\")\n",
    "\n",
    "        # Final save\n",
    "        final_results = pd.concat(\n",
    "            [self.existing_results, pd.DataFrame(results)],\n",
    "            ignore_index=True\n",
    "        )\n",
    "        final_results.to_csv(self.results_file, sep='\\t', index=False)\n",
    "\n",
    "        return final_results\n",
    "\n",
    "def continue_enzyme_retrieval(unique_pairs: pd.DataFrame, existing_results_file: Path, start_ec: str):\n",
    "    \"\"\"Main function to continue enzyme data retrieval\"\"\"\n",
    "    retriever = ColabEnzymeRetriever(batch_size=100)\n",
    "\n",
    "    # Load existing results\n",
    "    retriever.load_existing_results(existing_results_file)\n",
    "\n",
    "    # Ensure input data is properly formatted\n",
    "    if isinstance(unique_pairs, str):\n",
    "        unique_pairs = pd.read_csv(unique_pairs, sep='\\t')\n",
    "    elif isinstance(unique_pairs, pd.DataFrame):\n",
    "        unique_pairs = unique_pairs.copy(deep=False)\n",
    "    else:\n",
    "        raise ValueError(\"Input must be either a file path or a pandas DataFrame\")\n",
    "\n",
    "    # Validate and prepare input data\n",
    "    required_columns = ['EC', 'Genus']\n",
    "    if not all(col in unique_pairs.columns for col in required_columns):\n",
    "        raise ValueError(f\"Input data must contain columns: {required_columns}\")\n",
    "\n",
    "    unique_pairs['EC'] = unique_pairs['EC'].astype(str).apply(lambda x: f\"EC:{x.replace('EC:', '')}\")\n",
    "    unique_pairs['Genus'] = unique_pairs['Genus'].astype(str).str.strip()\n",
    "    unique_pairs = unique_pairs[['EC', 'Genus']].drop_duplicates()\n",
    "\n",
    "    # Process remaining pairs\n",
    "    results_df = retriever.process_remaining_pairs(unique_pairs, start_ec)\n",
    "\n",
    "    # Save final results\n",
    "    final_path = Path('uniprot_results_final.tsv')\n",
    "    results_df.to_csv(final_path, sep='\\t', index=False)\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:09:30.705836Z",
     "iopub.status.busy": "2025-04-19T16:09:30.705462Z",
     "iopub.status.idle": "2025-04-19T16:09:30.727645Z",
     "shell.execute_reply": "2025-04-19T16:09:30.726422Z",
     "shell.execute_reply.started": "2025-04-19T16:09:30.705791Z"
    },
    "id": "mowtkQwJuvgT",
    "outputId": "613621e5-73a7-4c08-a9bd-ea62d140716b",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "'''uniprot_results_path = Path(base_dir '/uniprot_results.tsv')\n",
    "# Usage (after uploading files to Colab), ECcontri_otu was made in colab because it was too big to upload after transformed\n",
    "results = continue_enzyme_retrieval(ECcontri_otu, uniprot_results_path, start_ec=\"x.3.1.12\" )'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nARoxMBKuvgT"
   },
   "source": [
    "## 8.6. Cleaning and Preparing Retrieved Data to integrate to ECContri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:09:30.729224Z",
     "iopub.status.busy": "2025-04-19T16:09:30.728831Z",
     "iopub.status.idle": "2025-04-19T16:09:30.952519Z",
     "shell.execute_reply": "2025-04-19T16:09:30.951420Z",
     "shell.execute_reply.started": "2025-04-19T16:09:30.729184Z"
    },
    "id": "9r_F4kMtuvgU",
    "outputId": "c2edd633-79ce-4c56-802b-bd1ef3946b16",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_1_path = Path(base_dir / \"uniprot_1_4_sorted.tsv\") # First file retrieved on first run 4 am\n",
    "df_2_path = Path(base_dir / \"uniprot_2_1.38_sorted.tsv\") # Same file retrieven when corrupted around 1:38 following day\n",
    "df_3_path = Path(base_dir / \"uniprot_3_sorted.tsv\") # Rerun done trying to get following EC numbers\n",
    "df_4_path = Path(base_dir / \"uniprot_4_missing_sorted.tsv\") # Final run in missing data\n",
    "\n",
    "df_1 = pd.read_csv(df_1_path, sep='\\t')\n",
    "df_2 = pd.read_csv(df_2_path, sep='\\t')\n",
    "df_3 = pd.read_csv(df_3_path, sep='\\t')\n",
    "df_4 = pd.read_csv(df_4_path, sep='\\t')\n",
    "print(len(df_1), len(df_2), len(df_3), len(df_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:09:30.953758Z",
     "iopub.status.busy": "2025-04-19T16:09:30.953464Z",
     "iopub.status.idle": "2025-04-19T16:09:31.235721Z",
     "shell.execute_reply": "2025-04-19T16:09:31.234512Z",
     "shell.execute_reply.started": "2025-04-19T16:09:30.953733Z"
    },
    "id": "9a2RbXR1uvgU",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# extract EC and Genus from the Retrieved files, so I need to join them first\n",
    "retrieved = pd.concat([df_1, df_2, df_3, df_4], axis = 0)\n",
    "# unique pairs on our data\n",
    "unique_pairs = ECcontri_otu[['EC', 'Genus']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DwbF_HFmuvgU"
   },
   "source": [
    "## 8.7 Extracting the Genus from the retrieved_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:09:31.237502Z",
     "iopub.status.busy": "2025-04-19T16:09:31.237156Z",
     "iopub.status.idle": "2025-04-19T16:09:31.402205Z",
     "shell.execute_reply": "2025-04-19T16:09:31.401147Z",
     "shell.execute_reply.started": "2025-04-19T16:09:31.237471Z"
    },
    "id": "jfKMErDWuvgU",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Function to extract the Genus from the organism str\n",
    "def extract_genus(organism_str):\n",
    "    # Assumes Genus is the first word that starts with an uppercase letter.\n",
    "    match = re.search(r'([A-Z][a-z]+)', organism_str)\n",
    "    return match.group(1) if match else None\n",
    "# Creating a Genus column in the retrieved dataframe.\n",
    "retrieved['Genus'] = retrieved['organism'].astype(str).apply(extract_genus)\n",
    "\n",
    "# if there are duplicates, we want the best entry based on score:\n",
    "retrieved_unique = retrieved.sort_values('score', ascending=False)\\\n",
    "                            .drop_duplicates(subset=['ec_number', 'Genus'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:09:31.403494Z",
     "iopub.status.busy": "2025-04-19T16:09:31.403188Z",
     "iopub.status.idle": "2025-04-19T16:09:31.408947Z",
     "shell.execute_reply": "2025-04-19T16:09:31.407710Z",
     "shell.execute_reply.started": "2025-04-19T16:09:31.403468Z"
    },
    "id": "MZiDFgoGgmmJ",
    "outputId": "0006f1ac-9071-4d26-9a13-93a1374c244f",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(f\"unique_pairs Galaxy data:{len(unique_pairs)}, Uniprot retrieved data:{len(retrieved_unique)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:09:31.410298Z",
     "iopub.status.busy": "2025-04-19T16:09:31.409950Z",
     "iopub.status.idle": "2025-04-19T16:09:32.051652Z",
     "shell.execute_reply": "2025-04-19T16:09:32.050204Z",
     "shell.execute_reply.started": "2025-04-19T16:09:31.410242Z"
    },
    "id": "yoQasxuauvgU",
    "outputId": "99eea2f6-ddde-48df-fb20-66098b946369",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Merging using a left join on the two keys (EC_number and Genus). Plus an indicator of missing data.\n",
    "ECcontri_Uniprot  = pd.merge(\n",
    "    ECcontri_otu,\n",
    "    retrieved_unique[['ec_number', 'Genus', 'protein_name', 'score', 'uniprot_id']],\n",
    "    left_on=['EC', 'Genus'],\n",
    "    right_on=['ec_number', 'Genus'],\n",
    "    how='left',\n",
    "    suffixes=('', '_retr')\n",
    ")\n",
    "print(ECcontri_Uniprot.shape) # Very slow 1 minute, can kill the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:09:32.053176Z",
     "iopub.status.busy": "2025-04-19T16:09:32.052870Z",
     "iopub.status.idle": "2025-04-19T16:09:32.209157Z",
     "shell.execute_reply": "2025-04-19T16:09:32.208139Z",
     "shell.execute_reply.started": "2025-04-19T16:09:32.053152Z"
    },
    "id": "elCz1V27O-E3",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ECcontri_Uniprot = ECcontri_Uniprot.drop(columns = [\"OTU\",\t\"ec_number\",\t\"npath\", \"pathway\",\t\"score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qA_nz2qFuvgU"
   },
   "source": [
    "## 8.8. Missing Values\n",
    "ECcontri_uniprot_info is the final df mixed and is keep for reference only purposes. With the missing unique df I will retrive again the rest of the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:09:32.210579Z",
     "iopub.status.busy": "2025-04-19T16:09:32.210281Z",
     "iopub.status.idle": "2025-04-19T16:09:32.312364Z",
     "shell.execute_reply": "2025-04-19T16:09:32.311121Z",
     "shell.execute_reply.started": "2025-04-19T16:09:32.210553Z"
    },
    "id": "q6iKlSuHuvgU",
    "outputId": "4c829bbf-3e68-4134-fece-521fb65a7a8a",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Rows with no match from retrieved_unique will have '_merge' value of 'left_only'\n",
    "merged_unique = pd.merge(\n",
    "    unique_pairs,\n",
    "    retrieved_unique,\n",
    "    left_on=['EC', 'Genus'],\n",
    "    right_on=['ec_number', 'Genus'],\n",
    "    how='left',\n",
    "    indicator=True\n",
    ")\n",
    "\n",
    "# Filter unique pairs missing from retrieved data\n",
    "ECcontri_missing = merged_unique[merged_unique['_merge'] == 'left_only']\n",
    "print(\"Missing unique pairs count:\", ECcontri_missing.shape[0])\n",
    "ECcontri_missing = ECcontri_missing[[\"EC\", \"Genus\"]]\n",
    "file_path = os.path.join(output_base, \"ECcontri_missing.tsv\")\n",
    "ECcontri_missing.to_csv(file_path, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9lMQJ2mM8ZGp"
   },
   "source": [
    "## 8.9 Cleaning Protein Names on ECcontri_Uniprot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:09:32.313672Z",
     "iopub.status.busy": "2025-04-19T16:09:32.313394Z",
     "iopub.status.idle": "2025-04-19T16:09:32.330637Z",
     "shell.execute_reply": "2025-04-19T16:09:32.329282Z",
     "shell.execute_reply.started": "2025-04-19T16:09:32.313638Z"
    },
    "id": "pnWf_c20TPbA",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "def enhanced_clean_protein_name(name):\n",
    "    \"\"\"\n",
    "    Enhanced protein name cleaning that builds on the original function\n",
    "    with additional normalization steps:\n",
    "\n",
    "    1. Handles EC numbers appropriately\n",
    "    2. Removes parenthetical content and brackets\n",
    "    3. Removes duplicated terms and standardizes spacing\n",
    "    4. Handles specific protein families consistently\n",
    "    5. Standardizes capitalizations and hyphenations\n",
    "    6. Removes common suffixes that don't change the protein identity\n",
    "    7. Normalizes common protein name patterns\n",
    "    \"\"\"\n",
    "    if pd.isna(name):\n",
    "        return \"Uncharacterized protein\"\n",
    "\n",
    "    # Save EC numbers for later reattachment if needed\n",
    "    ec_match = re.search(r'(EC\\s*[\\d\\.]+)', name)\n",
    "    ec_number = ec_match.group(1) if ec_match else None\n",
    "\n",
    "    # If the name is just an EC number in any format, return it\n",
    "    if re.match(r'^[\\s]*EC\\s*[\\d\\.]+[\\s]*$', name):\n",
    "        return name.strip()\n",
    "\n",
    "    # Remove EC numbers and content in parentheses\n",
    "    name = re.sub(r'EC\\s∗[\\d\\.]+EC\\s*[\\d\\.]+', '', name)\n",
    "    name = re.sub(r'[)]∗[^)]*', '', name)\n",
    "    name = re.sub(r'[^[^]*\\]', '', name)  # Also remove content in square brackets\n",
    "\n",
    "    # Convert to lowercase for better matching\n",
    "    name = name.lower()\n",
    "\n",
    "    # Standardize spacing around hyphens, slashes\n",
    "    name = re.sub(r'[\\s]*[\\-\\/][\\s]*', '-', name)\n",
    "\n",
    "    # Remove common suffixes that don't change protein identity\n",
    "    name = re.sub(r'\\s+(protein|domain|enzyme|family|subunit|chain|component|type|complex|fragment|precursor)$', '', name)\n",
    "\n",
    "    # Remove specific protein ID suffixes (like FabG)\n",
    "    name = re.sub(r'\\s+[a-z]+\\d+$', '', name)  # e.g., \"reductase FabG\" -> \"reductase\"\n",
    "\n",
    "    # Normalize common protein name patterns\n",
    "    replacements = {\n",
    "        # Standardize dehydrogenases\n",
    "        r'(\\w+)\\s*dehydrogenase': r'\\1-dehydrogenase',\n",
    "\n",
    "        # Standardize reductases\n",
    "        r'(\\w+)\\s*reductase': r'\\1-reductase',\n",
    "        r'\\[acyl-carrier-protein\\]': 'acp',\n",
    "        r'\\[acyl carrier protein\\]': 'acp',\n",
    "        r'3-oxoacyl-acp reductase': '3-oxoacyl-acp-reductase',\n",
    "\n",
    "        # Standardize synthetases\n",
    "        r'(\\w+)\\s*synthase': r'\\1-synthase',\n",
    "        r'(\\w+)\\s*synthetase': r'\\1-synthetase',\n",
    "\n",
    "        # Standardize common protein terms\n",
    "        r'alcohol dehydrogenase': 'alcohol-dehydrogenase',\n",
    "        r'glutathione dehydrogenase': 'glutathione-dehydrogenase',\n",
    "        r's-glutathione dehydrogenase': 'glutathione-dehydrogenase',\n",
    "        r'threonine dehydrogenase': 'threonine-dehydrogenase',\n",
    "        r'l-threonine dehydrogenase': 'threonine-dehydrogenase'\n",
    "    }\n",
    "\n",
    "    for pattern, replacement in replacements.items():\n",
    "        name = re.sub(pattern, replacement, name)\n",
    "\n",
    "    # Split into words and remove duplicates while preserving order\n",
    "    words = name.split()\n",
    "    seen = set()\n",
    "    unique_words = []\n",
    "    for word in words:\n",
    "        if word not in seen:\n",
    "            seen.add(word)\n",
    "            unique_words.append(word)\n",
    "\n",
    "    # Rejoin words\n",
    "    name = ' '.join(unique_words)\n",
    "\n",
    "    # Remove specific redundant patterns\n",
    "    redundant_patterns = [\n",
    "        (r'enzyme\\s+enzyme', 'enzyme'),\n",
    "        (r'synthase\\s+synthase', 'synthase'),\n",
    "        (r'transferase\\s+transferase', 'transferase'),\n",
    "        (r'-glucan\\s+glucan', 'glucan'),\n",
    "        (r'protein\\s+protein', 'protein'),\n",
    "        (r'reductase\\s+reductase', 'reductase'),\n",
    "        (r'dehydrogenase\\s+dehydrogenase', 'dehydrogenase')\n",
    "    ]\n",
    "\n",
    "    for pattern, replacement in redundant_patterns:\n",
    "        name = re.sub(pattern, replacement, name, flags=re.IGNORECASE)\n",
    "\n",
    "    # Reattach EC number if it was the main identifier\n",
    "    if ec_number and len(name.strip()) < 5:  # If the remaining name is very short\n",
    "        name = f\"{name} {ec_number}\" if name.strip() else ec_number\n",
    "\n",
    "    return name.strip()\n",
    "\n",
    "def normalize_dataset(df, name_col='protein_name', sample_size=20):\n",
    "    \"\"\"\n",
    "    Apply enhanced protein name cleaning to a dataset and\n",
    "    show before/after examples\n",
    "    \"\"\"\n",
    "    # Create a copy with normalized names\n",
    "    normalized_df = df.copy()\n",
    "    normalized_df['normalized_protein'] = normalized_df[name_col].apply(enhanced_clean_protein_name)\n",
    "\n",
    "    # Display samples of the normalization\n",
    "    samples = df[[name_col]].drop_duplicates().sample(min(sample_size, df[name_col].nunique()))\n",
    "\n",
    "    print(f\"Original unique protein names: {df[name_col].nunique()}\")\n",
    "    samples['normalized'] = samples[name_col].apply(enhanced_clean_protein_name)\n",
    "\n",
    "    print(\"\\nSample normalization results:\")\n",
    "    for _, row in samples.iterrows():\n",
    "        print(f\"\\nOriginal:   {row[name_col]}\")\n",
    "        print(f\"Normalized: {row['normalized']}\")\n",
    "\n",
    "    # Count reduction in unique names\n",
    "    original_count = df[name_col].nunique()\n",
    "    normalized_count = normalized_df['normalized_protein'].nunique()\n",
    "    reduction = original_count - normalized_count\n",
    "    reduction_pct = 100 * reduction / original_count if original_count > 0 else 0\n",
    "\n",
    "    print(f\"\\nNormalized unique protein names: {normalized_count}\")\n",
    "    print(f\"Reduction: {reduction} ({reduction_pct:.1f}%)\")\n",
    "\n",
    "    # Find similar protein names that are now treated as the same\n",
    "    if normalized_count < original_count:\n",
    "        print(\"\\nExamples of proteins that were normalized to the same name:\")\n",
    "        norm_to_orig = {}\n",
    "\n",
    "        # Build mapping of normalized names to original names\n",
    "        for _, row in samples.iterrows():\n",
    "            norm_name = row['normalized']\n",
    "            orig_name = row[name_col]\n",
    "\n",
    "            if norm_name not in norm_to_orig:\n",
    "                norm_to_orig[norm_name] = []\n",
    "\n",
    "            if orig_name not in norm_to_orig[norm_name]:\n",
    "                norm_to_orig[norm_name].append(orig_name)\n",
    "\n",
    "        # Display examples where multiple original names map to the same normalized name\n",
    "        examples_shown = 0\n",
    "        for norm_name, orig_names in norm_to_orig.items():\n",
    "            if len(orig_names) > 1 and examples_shown < 5:\n",
    "                print(f\"\\nNormalized to: {norm_name}\")\n",
    "                for orig in orig_names:\n",
    "                    print(f\"  - {orig}\")\n",
    "                examples_shown += 1\n",
    "\n",
    "    return normalized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:09:32.332505Z",
     "iopub.status.busy": "2025-04-19T16:09:32.332066Z",
     "iopub.status.idle": "2025-04-19T16:12:45.093204Z",
     "shell.execute_reply": "2025-04-19T16:12:45.091900Z",
     "shell.execute_reply.started": "2025-04-19T16:09:32.332463Z"
    },
    "id": "ZOok4gH71k5L",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Standardize EC format: Extract numbers without the 'EC:' prefix\n",
    "ECcontri_Uniprot['EC_clean'] = ECcontri_Uniprot['EC'].str.replace('EC:', '', regex=False)\n",
    "ECcontri_Uniprot = ECcontri_Uniprot.drop(columns = [\"EC\"])\n",
    "ECcontri_Uniprot = ECcontri_Uniprot.rename(columns={\"EC_clean\": \"EC\"})\n",
    "\n",
    "# Calling the function to normalise the name\n",
    "ECcontri_Uniprot['protein_name'] = ECcontri_Uniprot['protein_name'].apply(enhanced_clean_protein_name)\n",
    "\n",
    "# Saving the data\n",
    "ECcontri_Uniprot_path = output_large / 'ECcontri_Uniprot.parquet'\n",
    "ECcontri_Uniprot.to_parquet(ECcontri_Uniprot_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:12:45.094770Z",
     "iopub.status.busy": "2025-04-19T16:12:45.094444Z",
     "iopub.status.idle": "2025-04-19T16:12:46.277957Z",
     "shell.execute_reply": "2025-04-19T16:12:46.276924Z",
     "shell.execute_reply.started": "2025-04-19T16:12:45.094733Z"
    },
    "id": "u0mYrM2gI8TY",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ECcontri_Uniprot_path = output_large / 'ECcontri_Uniprot.parquet'\n",
    "ECcontri_Uniprot = pd.read_parquet(ECcontri_Uniprot_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pABT9cmAcoqI"
   },
   "source": [
    "## 8.10 Calculate & Visualize Total Protein Count per EC\n",
    "The aim here is to calculate if the protein are in direct relationship with EC numbers, so to be able to understand if those are totally equivalent and how the abundance would relate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ec_protein_counts = ECcontri_Uniprot.groupby('EC')['protein_name'].nunique().reset_index()\n",
    "ec_protein_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:12:46.292928Z",
     "iopub.status.busy": "2025-04-19T16:12:46.292579Z",
     "iopub.status.idle": "2025-04-19T16:12:48.594528Z",
     "shell.execute_reply": "2025-04-19T16:12:48.593208Z",
     "shell.execute_reply.started": "2025-04-19T16:12:46.292894Z"
    },
    "id": "fJNiW0QlVsV7",
    "outputId": "ca1c76b9-8805-4985-eabc-4efdfcdbc781",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Replace inf and -inf with NaN future warning\n",
    "ECcontri_Uniprot = ECcontri_Uniprot.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Grouping by EC and counting unique proteins\n",
    "ec_protein_counts = ECcontri_Uniprot.groupby('EC')['protein_name'].nunique().reset_index()\n",
    "ec_protein_counts.columns = ['EC', 'Protein_Count']\n",
    "\n",
    "# Plot histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(ec_protein_counts['Protein_Count'].to_numpy(), bins=50, kde=True)\n",
    "plt.xlabel(\"Total Protein Count per EC\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Protein Counts per EC\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YZ-VErJjg3zC"
   },
   "source": [
    "It is notice that EC wont scale with protein abundance proportionally as the skewed plot shows, this is notice onthe Uniprot tables where a EC has hundred if no thousands of proteins and hundred of organisms. There might be threshold effects were a minimum protein abundance will be relevant for a microorganism metabolism and hence we could stablish those threshold in order to visualise better those protein that are really relevant to the bacteria in question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XxWk1eJGmp5u"
   },
   "source": [
    "## 8.11 Computing the Knee point for Genus of Protein Significance\n",
    "Here it is computed the knee point where the activity of protein drops for each genus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:12:48.596009Z",
     "iopub.status.busy": "2025-04-19T16:12:48.595722Z",
     "iopub.status.idle": "2025-04-19T16:14:47.760540Z",
     "shell.execute_reply": "2025-04-19T16:14:47.758717Z",
     "shell.execute_reply.started": "2025-04-19T16:12:48.595984Z"
    },
    "id": "vSx0nkBPnC5Y",
    "outputId": "a56d0c89-024d-49d7-89db-36de3159ce82",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def knee_point_analysis(eccontri_df, abundance_col):\n",
    "    # Dictionary to store knee points for each genus\n",
    "\n",
    "    knee_points = {}\n",
    "    # Calculate total number of genera to determine plot grid size\n",
    "    num_genera = eccontri_df[\"Genus\"].nunique()\n",
    "    num_rows = int(np.ceil(np.sqrt(num_genera)))  # Number of rows in subplot grid\n",
    "    num_cols = num_rows  # Same for columns in a square grid\n",
    "    # Adjust figure size based on the number of genera\n",
    "    figsize = (num_cols * 3, num_rows * 3)  # Dynamically adjust figure size\n",
    "\n",
    "    # Plot setup\n",
    "    fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=figsize)  # Adjust for ~85 genera\n",
    "    axes = axes.flatten()\n",
    "    num_subplots = num_rows * num_cols\n",
    "    for i, (genus, group) in enumerate(eccontri_df.groupby(\"Genus\")):\n",
    "        group = group.sort_values(abundance_col, ascending=False).reset_index(drop=True)\n",
    "        x = np.arange(len(group))  # Protein rank\n",
    "        y = group[abundance_col].values  # Abundance values\n",
    "\n",
    "        # Apply Savitzky-Golay smoothing\n",
    "        y_smoothed = savgol_filter(y, window_length=7, polyorder=3)\n",
    "\n",
    "        # Knee detection (concave function, decreasing trend)\n",
    "        kneedle = KneeLocator(x, y_smoothed, curve=\"convex\", direction=\"decreasing\", online=True, S=1.5)\n",
    "\n",
    "        if kneedle.knee is not None:\n",
    "            knee_points[genus] = group.iloc[kneedle.knee][abundance_col]\n",
    "\n",
    "        # Plot for visualization - blue for data\n",
    "        axes[i].plot(x, y, color='blue', label=genus)\n",
    "        axes[i].plot(x, y_smoothed, color='blue', alpha=0.7, label=f\"{genus} (Smoothed)\")\n",
    "        if kneedle.knee is not None:\n",
    "            axes[i].axvline(kneedle.knee, color='red', linestyle=\"--\", label=\"Knee Point\")\n",
    "        axes[i].set_title(genus, fontsize=8)\n",
    "        axes[i].set_ylabel(abundance_col)\n",
    "        axes[i].tick_params(axis='both', which='major', labelsize=6)\n",
    "        legend = axes[i].legend(fontsize=6, handlelength=1)\n",
    "\n",
    "    # Remove empty subplots (after the loop)\n",
    "    for i in range(num_genera, num_subplots):\n",
    "        fig.delaxes(axes[i])  # Remove the extra subplot from the figure\n",
    "\n",
    "    plt.tight_layout(pad=0.5)\n",
    "    plt.show()\n",
    "\n",
    "    # Manual overrides at the end as in your original code\n",
    "    manual_overrides = {\n",
    "        \"Methylocystis\": 4000, \"Smithella\": 4000, \"Thermincola\": 750, \"Acidovorax\": 10500,\n",
    "        \"Anoxybacillus\": 3000, \"Pseudorhodoferax\": 7500, \"Pseudoxanthomonas\": 5000,\n",
    "        \"Phreatobacter\": 3000, \"Propionibacterium\": 1500, \"Novosphingobium\": 1500,\n",
    "        \"Herbaspirillum\": 2000, \"Azospira\": 10000\n",
    "    }\n",
    "\n",
    "    # Convert knee points dictionary to DataFrame for analysis\n",
    "    knee_df = pd.DataFrame(knee_points.items(), columns=[\"Genus\", \"Knee_Abundance\"])\n",
    "\n",
    "    # Apply manual overrides\n",
    "    for genus, override_value in manual_overrides.items():\n",
    "        # If genus exists in knee_df, update it; otherwise, add it\n",
    "        if genus in knee_df[\"Genus\"].values:\n",
    "            knee_df.loc[knee_df[\"Genus\"] == genus, \"Knee_Abundance\"] = override_value\n",
    "        else:\n",
    "            knee_df = pd.concat([knee_df, pd.DataFrame({\"Genus\": [genus], \"Knee_Abundance\": [override_value]})],\n",
    "                              ignore_index=True)\n",
    "    # Create a dictionary mapping each genus to its threshold value\n",
    "    genus_to_threshold = knee_df.set_index('Genus')['Knee_Abundance'].to_dict()\n",
    "\n",
    "    return knee_df, genus_to_threshold\n",
    "\n",
    "# Call the function and store the result\n",
    "knee_df, genus_to_threshold = knee_point_analysis(ECcontri_Uniprot, \"norm_abund_contri\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:14:47.762599Z",
     "iopub.status.busy": "2025-04-19T16:14:47.762191Z",
     "iopub.status.idle": "2025-04-19T16:14:47.770560Z",
     "shell.execute_reply": "2025-04-19T16:14:47.769309Z",
     "shell.execute_reply.started": "2025-04-19T16:14:47.762556Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "knee_path = output_base / \"genus_to_threshold.csv\"\n",
    "with open(knee_path, 'w', newline='') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=genus_to_threshold.keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerow(genus_to_threshold)  # Note: writerow (singular) not writerows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KTHudmFdvjs9"
   },
   "source": [
    "#!#[image.png](attachment:image.png)\n",
    "The abundance on the activity of the protein on the microorganism decrease until a point that is no more relevant, this cutt point was capture. The knee points in red are consistent with protein drop in protein abundance between each genera. Some genera have a steeper drop, making the knee point clear, while others have a gradual decline, which might make the knee less biologically meaningful. Some of the knee points were manually override in order to make the selection more homogeneus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hokee4LzA090"
   },
   "source": [
    "### Identifying the Uniquenes datapoints of the Data\n",
    "All 1.5 millon rows are unique?, would a EC-Genus-Site be unique or is it protein_name-Genus-Site combination?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:14:47.772128Z",
     "iopub.status.busy": "2025-04-19T16:14:47.771733Z",
     "iopub.status.idle": "2025-04-19T16:14:52.315582Z",
     "shell.execute_reply": "2025-04-19T16:14:52.314364Z",
     "shell.execute_reply.started": "2025-04-19T16:14:47.772085Z"
    },
    "id": "JWwX3SWj-KMO",
    "outputId": "764c7f2e-239e-476a-875d-8853cd33d7ce",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def identify_uniqueness_factors(eccontri_df):\n",
    "    \"\"\"\n",
    "    Analyzes what column combinations create uniqueness in the original dataframe.\n",
    "    \"\"\"\n",
    "    # Original columns\n",
    "    columns = ['Sites', 'Genus', 'abund_raw', 'rel_abund_raw', 'genome_EC_count',\n",
    "              'abund_contri', 'rel_abund_contri', 'norm_abund_contri', 'protein_name',\n",
    "              'uniprot_id', 'EC']\n",
    "\n",
    "    # Test various column combinations for uniqueness\n",
    "    combinations = [\n",
    "        ['Sites', 'Genus', 'EC'],\n",
    "        ['Sites', 'Genus', 'protein_name'],\n",
    "        ['Sites', 'Genus', 'EC', 'protein_name'],\n",
    "        ['Sites', 'Genus', 'EC', 'abund_contri'],\n",
    "        ['Sites', 'Genus', 'EC', 'abund_raw'],\n",
    "        ['Sites', 'Genus', 'EC', 'protein_name', 'abund_contri'],\n",
    "        # Test if all columns together make rows unique\n",
    "        columns\n",
    "    ]\n",
    "\n",
    "    results = {}\n",
    "    for combo in combinations:\n",
    "        # Count how many duplicates exist with this combination\n",
    "        duplicate_count = eccontri_df.duplicated(combo, keep=False).sum()\n",
    "        unique_count = len(eccontri_df) - duplicate_count\n",
    "        percent_unique = (unique_count / len(eccontri_df)) * 100\n",
    "\n",
    "        results[tuple(combo)] = {\n",
    "            'total_rows': len(eccontri_df),\n",
    "            'unique_rows': unique_count,\n",
    "            'duplicate_rows': duplicate_count,\n",
    "            'percent_unique': percent_unique\n",
    "        }\n",
    "\n",
    "        # If we found a perfectly unique combination, print it prominently\n",
    "        if duplicate_count == 0:\n",
    "            print(f\"PERFECT MATCH: {combo} creates unique rows\")\n",
    "\n",
    "    # Print all results\n",
    "    for combo, stats in results.items():\n",
    "        print(f\"Columns {combo}:\")\n",
    "        print(f\"  Unique rows: {stats['unique_rows']} ({stats['percent_unique']:.2f}%)\")\n",
    "        print(f\"  Duplicates: {stats['duplicate_rows']}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "uniquenes= identify_uniqueness_factors(ECcontri_Uniprot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ymxNjJ4BMZs"
   },
   "source": [
    "The same EC number (enzyme function) can be performed by different proteins and the same protein can have multiple EC numbers (multiple enzymatic functions), that mens the reltion EC-Genus-Sites is unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oVnGj7D_-PJL"
   },
   "source": [
    "### Cleaning anc collecting garbage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:14:52.317223Z",
     "iopub.status.busy": "2025-04-19T16:14:52.316875Z",
     "iopub.status.idle": "2025-04-19T16:14:52.383083Z",
     "shell.execute_reply": "2025-04-19T16:14:52.381602Z",
     "shell.execute_reply.started": "2025-04-19T16:14:52.317191Z"
    },
    "id": "2rdKHvKj-PJM",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "del picrust_long\n",
    "del retrieved\n",
    "del retrieved_unique\n",
    "del unique_pairs\n",
    "del df_1\n",
    "del df_2\n",
    "del df_3\n",
    "del df_4\n",
    "del ECcontri\n",
    "del ECcontri_agg_site\n",
    "#del ECcontri_otu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:16:08.295693Z",
     "iopub.status.busy": "2025-04-19T16:16:08.295201Z",
     "iopub.status.idle": "2025-04-19T16:16:08.960296Z",
     "shell.execute_reply": "2025-04-19T16:16:08.958881Z",
     "shell.execute_reply.started": "2025-04-19T16:16:08.295656Z"
    },
    "id": "AwyO_mme-PJM",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9d2KfaHxuvgU"
   },
   "source": [
    "### Data Retrieval Completion Note\n",
    "After multiple retrieval attempts, 12,656 pairs remain unmapped out of approximately 1,500,000 total rows (0.84%). Given this small percentage and the diminishing returns from further retrieval attempts, we concluded that this level of completeness is acceptable for analysis.\n",
    "______________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5lnzHgflRGFV"
   },
   "source": [
    "# 9. Building a Dictionary from Databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eNr6ECYly6LJ"
   },
   "source": [
    "Data Normalization and Mapping\n",
    "It was ensured that all protein/EC data (including EC numbers, KO numbers, and reaction IDs) were parsed and cleaned. This identifiers were mapped to their corresponding metabolic pathways using databases such as KEGG, MetaCyc, and BioCyc.\n",
    "\n",
    "Identifying Metal-Related Proteins:\n",
    "cross-reference proteins with metal-related databases (BRENDA, MetalPDB, TransportDB) where crossreferenced to flag those with direct metal-binding or metal-transport roles. Then consolidate similar metal terms (e.g., “iron”, “Fe”, “ferric”) into a unified field to improve consistency in later analyses.\n",
    "\n",
    "\n",
    "\n",
    "Final Assembly:\n",
    "I compile the data into a final dictionary/table that includes all relevant columns (Protein, EC/KO, Metabolism, Pathway, Metal Interaction, MIC Function). This allows me to search programmatically for the functional roles of proteins that are influential in corrosion studies.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-FEargpNkucP"
   },
   "source": [
    "## 9.1 Setting up Paths and Parsing the Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-21T09:50:49.894079Z",
     "iopub.status.idle": "2025-04-21T09:50:49.894696Z",
     "shell.execute_reply": "2025-04-21T09:50:49.894489Z"
    },
    "id": "0efKvfwuHc0f",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def setup_paths():\n",
    "    \"\"\"Set up paths for database access\"\"\"\n",
    "\n",
    "    # Database paths\n",
    "    db_paths = {\n",
    "        'enzyme': db_dir / 'enzyme',\n",
    "        'enzyme_class': db_dir / 'enzclass.txt',\n",
    "        'enzyme_brenda' : db_dir/ 'brenda_2024.txt',\n",
    "        'ko': db_dir / 'ko',\n",
    "        'ko_hierarchy': db_dir / 'ko_hierarchy.txt',\n",
    "        'pathway': db_dir / 'pathway',\n",
    "        'module': db_dir / 'module',\n",
    "        'reaction': db_dir / 'reaction',\n",
    "        'compound': db_dir / 'compound',\n",
    "        'metalpdb': db_dir / 'flat_db_file.xml',\n",
    "        'ko_pathway': db_dir / 'ec_pathway.list'\n",
    "    }\n",
    "\n",
    "    return db_paths\n",
    "\n",
    "#  Calling the paths\n",
    "'''if __name__ == \"__main__\":\n",
    "    paths = setup_paths()\n",
    "    # Print paths to verify\n",
    "    for db_name, path in paths.items():\n",
    "        print(f\"{db_name}: {path}\")\n",
    "        print(f\"Exists: {path.exists()}\")'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 Importing and Parsing Authoritative Databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7P2TUJLNWX3B"
   },
   "source": [
    "### Brenda Enzyme Parse Brenda\n",
    "\n",
    "https://www.brenda-enzymes.org/download.php\n",
    "\n",
    "Chang A., Jeske L., Ulbrich S., Hofmann J., Koblitz J., Schomburg I., Neumann-Schaal M., Jahn D., Schomburg D.\n",
    "BRENDA, the ELIXIR core data resource in 2021: new developments and updates. (2021), Nucleic Acids Res., 49:D498-D508.\n",
    "DOI: 10.1093/nar/gkaa1025 PubMed: 33211880"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-19T16:14:53.639775Z",
     "iopub.status.idle": "2025-04-19T16:14:53.640344Z",
     "shell.execute_reply": "2025-04-19T16:14:53.640109Z"
    },
    "id": "mkW7GPMhvlL3",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def parse_brenda_file():\n",
    "    \"\"\"Parse BRENDA database file for detailed enzyme information\"\"\"\n",
    "    paths = setup_paths()\n",
    "    enzyme_brenda_path = paths['enzyme_brenda']\n",
    "\n",
    "    ec_detailed_info = {}\n",
    "    current_ec = None\n",
    "    in_enzyme_entry = False\n",
    "\n",
    "    try:\n",
    "        with open(enzyme_brenda_path, 'r') as f:\n",
    "          for line in f:\n",
    "              line = line.strip()\n",
    "\n",
    "              # Skip empty lines\n",
    "              if not line:\n",
    "                  continue\n",
    "\n",
    "              # Check for the end of an entry\n",
    "              if line == \"///\":\n",
    "                  current_ec = None\n",
    "                  in_enzyme_entry = False\n",
    "                  continue\n",
    "\n",
    "              # Process ID line - identify enzyme entries\n",
    "              if line.startswith('ID\\t'):\n",
    "                  current_ec = line.split('\\t')[1]\n",
    "\n",
    "                  # Skip \"spontaneous\" and other non-EC entries\n",
    "                  if not any(c.isdigit() for c in current_ec):\n",
    "                      current_ec = None\n",
    "                      in_enzyme_entry = False\n",
    "                      continue\n",
    "\n",
    "                  # Initialize proper EC entry\n",
    "                  ec_detailed_info[current_ec] = {\n",
    "                      'metals': [],\n",
    "                      'cofactors': [],\n",
    "                      'reactions': [],\n",
    "                      'substrates': [],\n",
    "                      'inhibitors': []\n",
    "                  }\n",
    "                  in_enzyme_entry = True\n",
    "\n",
    "              # Only process other lines if we're in a valid enzyme entry\n",
    "              elif in_enzyme_entry and current_ec:\n",
    "                  if line.startswith('ME\\t'):\n",
    "                      # Extract metal information\n",
    "                      metal_info = line.split('\\t')[1]\n",
    "                      ec_detailed_info[current_ec]['metals'].append(metal_info)\n",
    "\n",
    "                  elif line.startswith('CF\\t'):\n",
    "                      # Extract cofactor information\n",
    "                      cofactor_info = line.split('\\t')[1]\n",
    "                      ec_detailed_info[current_ec]['cofactors'].append(cofactor_info)\n",
    "\n",
    "                  elif line.startswith('RE\\t'):\n",
    "                      # Extract detailed reaction information\n",
    "                      reaction_info = line.split('\\t')[1]\n",
    "                      ec_detailed_info[current_ec]['reactions'].append(reaction_info)\n",
    "\n",
    "                  elif line.startswith('SP\\t') or line.startswith('NSP\\t'):\n",
    "                      # Extract substrate information\n",
    "                      substrate_info = line.split('\\t')[1]\n",
    "                      ec_detailed_info[current_ec]['substrates'].append(substrate_info)\n",
    "\n",
    "                  elif line.startswith('IN\\t'):\n",
    "                      # Extract inhibitor information\n",
    "                      inhibitor_info = line.split('\\t')[1]\n",
    "                      ec_detailed_info[current_ec]['inhibitors'].append(inhibitor_info)\n",
    "\n",
    "        # Verify we have valid EC numbers\n",
    "        ec_detailed_info = {ec: info for ec, info in ec_detailed_info.items()\n",
    "                            if ec.count('.') == 3 and all(part.isdigit() for part in ec.split('.'))}\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error parsing BRENDA file: %s\", e)\n",
    "        return {}\n",
    "    return ec_detailed_info\n",
    "\n",
    "#brenda_data = parse_brenda_file() # Only file of the dictionary that has to be call before the main function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-19T16:14:53.641670Z",
     "iopub.status.idle": "2025-04-19T16:14:53.642438Z",
     "shell.execute_reply": "2025-04-19T16:14:53.642178Z"
    },
    "id": "d2S3JREa6rg7",
    "outputId": "0399b8d6-dc2a-44f3-a4ca-99d67a601325",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def process_brenda_data(brenda_data):\n",
    "    \"\"\"Process BRENDA data to extract clean metal information while keeping other data intact\"\"\"\n",
    "    import sys\n",
    "    #sys.path.append('/kaggle/input/corrosion-scoring') \n",
    "    sys.path.append('/home/beatriz/MIC/2_Micro/corrosion_scoring')\n",
    "    from corrosion_scoring.global_terms import (\n",
    "    #from global_terms import (\n",
    "        metal_terms, \n",
    "        corrosion_mechanisms, \n",
    "        pathway_categories, \n",
    "        organic_categories,\n",
    "        corrosion_synergies, \n",
    "        functional_categories, \n",
    "        corrosion_keyword_groups, \n",
    "        metal_mapping\n",
    "    )\n",
    "    import corrosion_scoring.scoring_system as score_sys\n",
    "    \n",
    "    processed_data = {}\n",
    "\n",
    "    for ec_number, data in brenda_data.items():\n",
    "        processed_data[ec_number] = {\n",
    "            'cofactors': data.get('cofactors', []),\n",
    "            'reactions': data.get('reactions', []),\n",
    "            'substrates': data.get('substrates', []),\n",
    "            'inhibitors': data.get('inhibitors', []),\n",
    "            'raw_metals': data.get('metals', []),\n",
    "            'clean_metals': [],\n",
    "            'corrosion_mechanisms': [],\n",
    "            'pathway_categories': {},\n",
    "            'organic_processes': {},\n",
    "            'corrosion_synergies': []\n",
    "        }\n",
    "\n",
    "        # Create a single text string for analysis\n",
    "        all_text = ' '.join([\n",
    "            ' '.join(data.get('reactions', [])),\n",
    "            ' '.join(data.get('substrates', [])),\n",
    "            ' '.join(data.get('cofactors', [])), \n",
    "            ' '.join(data.get('inhibitors', []))\n",
    "        ]).lower()\n",
    "\n",
    "        # Extract clean metal names first for both paths\n",
    "        for entry in data.get('metals', []):\n",
    "            entry_lower = entry.lower()\n",
    "            for metal in metal_terms:\n",
    "                metal_lower = metal.lower()\n",
    "                if metal_lower in entry_lower:\n",
    "                    if metal not in processed_data[ec_number]['clean_metals']:\n",
    "                        processed_data[ec_number]['clean_metals'].append(metal)\n",
    "        using_imported_modules = True\n",
    "        if using_imported_modules:\n",
    "            # Use the scoring system module for comprehensive scoring\n",
    "            score_results = score_sys.calculate_overall_scores(\n",
    "                all_text, \n",
    "                brenda_metals=processed_data[ec_number]['clean_metals']\n",
    "            )\n",
    "            \n",
    "            # Update the processed data with scores from the module\n",
    "            processed_data[ec_number]['corrosion_mechanisms'] = score_results['corrosion_mechanisms']\n",
    "            processed_data[ec_number]['organic_processes'] = score_results['organic_processes']\n",
    "            processed_data[ec_number]['corrosion_synergies'] = score_results['corrosion_synergies']\n",
    "            \n",
    "            # Add pathway categories - transform from scoring format to expected format\n",
    "            for category_info in score_results.get('functional_categories', []):\n",
    "                category = category_info.get('category')\n",
    "                if category:\n",
    "                    processed_data[ec_number]['pathway_categories'][category] = True\n",
    "            \n",
    "            # Add metal information\n",
    "            processed_data[ec_number]['metals_involved'] = score_results['metals_involved']\n",
    "            \n",
    "            # Define corrosion-relevant metals\n",
    "            corrosion_relevant_metals = ['iron', 'manganese', 'copper', 'nickel', 'cobalt', 'sulfide', \n",
    "                                        'sulfate', 'chloride', 'Al3+', 'Cr3+']\n",
    "            \n",
    "            processed_data[ec_number]['corrosion_metals_from_brenda'] = [\n",
    "                metal for metal in processed_data[ec_number]['clean_metals']\n",
    "                if metal in corrosion_relevant_metals\n",
    "            ]\n",
    "            \n",
    "            # Calculate pathway scores\n",
    "            pathways = []  # No pathways directly from BRENDA, but include for API consistency\n",
    "            enzyme_names = []  # No names from BRENDA, but include for API consistency\n",
    "            enzyme_class = \"\"  # No class from BRENDA, but include for API consistency\n",
    "            \n",
    "            pathway_score, pathway_category_scores = score_sys.calculate_pathway_score(\n",
    "                pathways, enzyme_names, enzyme_class\n",
    "            )\n",
    "            \n",
    "            # Calculate synergy score\n",
    "            synergy_score = score_sys.check_metal_organic_synergy(\n",
    "                processed_data[ec_number]['clean_metals'], \n",
    "                []  # No enzyme names in BRENDA\n",
    "            )\n",
    "            \n",
    "            # Calculate final corrosion relevance score using the module\n",
    "            corrosion_relevance_score, corrosion_relevance = score_sys.calculate_corrosion_relevance_score(\n",
    "                score_results.get('overall_metal_score', 0),\n",
    "                score_results.get('overall_corrosion_score', 0),\n",
    "                pathway_score,\n",
    "                score_results.get('overall_organic_process_score', 0),\n",
    "                score_results.get('overall_keyword_score', 0),\n",
    "                synergy_score,\n",
    "                score_results.get('overall_functional_score', 0)\n",
    "            )\n",
    "            \n",
    "            processed_data[ec_number]['corrosion_relevance_score'] = corrosion_relevance_score\n",
    "            processed_data[ec_number]['corrosion_relevance'] = corrosion_relevance\n",
    "            \n",
    "        else:\n",
    "            # Use the original scoring approach if module import failed\n",
    "            # Check for corrosion mechanisms\n",
    "            for mechanism, terms in corrosion_mechanisms.items():\n",
    "                if any(term.lower() in all_text for term in terms):\n",
    "                    processed_data[ec_number]['corrosion_mechanisms'].append(mechanism)\n",
    "\n",
    "            # Check for pathway categories\n",
    "            for category, terms in pathway_categories.items():\n",
    "                if any(term.lower() in all_text for term in terms):\n",
    "                    processed_data[ec_number]['pathway_categories'][category] = True\n",
    "\n",
    "            # Check for organic matter processes\n",
    "            for category, terms in organic_categories.items():\n",
    "                if any(term.lower() in all_text for term in terms):\n",
    "                    processed_data[ec_number]['organic_processes'][category] = True\n",
    "\n",
    "            # Check for corrosion synergies\n",
    "            for synergy, terms in corrosion_synergies.items():\n",
    "                if any(term.lower() in all_text for term in terms):\n",
    "                    processed_data[ec_number]['corrosion_synergies'].append(synergy)\n",
    "\n",
    "            # Add corrosion relevance information\n",
    "            corrosion_relevant_metals = ['iron', 'manganese', 'copper', 'nickel', 'cobalt', 'sulfide', \n",
    "                                        'sulfate', 'chloride', 'Al3+', 'Cr3+']\n",
    "            \n",
    "            processed_data[ec_number]['corrosion_metals_from_brenda'] = [\n",
    "                metal for metal in processed_data[ec_number]['clean_metals']\n",
    "                if metal in corrosion_relevant_metals\n",
    "            ]\n",
    "\n",
    "            # Calculate comprehensive corrosion relevance score\n",
    "            corrosion_score = 0\n",
    "            \n",
    "            # Score based on metal involvement\n",
    "            corrosion_score += len(processed_data[ec_number]['corrosion_metals_from_brenda']) * 1.5\n",
    "            \n",
    "            # Score based on corrosion mechanisms\n",
    "            corrosion_score += len(processed_data[ec_number]['corrosion_mechanisms']) * 2.0\n",
    "            \n",
    "            # Score based on pathway categories related to corrosion\n",
    "            corrosion_related_pathways = ['organic_acid_metabolism', 'metal_organic_interaction', \n",
    "                                        'biofilm_formation', 'sulfur_metabolism', 'nitrogen_metabolism',\n",
    "                                        'manganese_processes', 'electron_transfer', 'iron_sulfur_redox', \n",
    "                                        'ocre_formation', 'hydrogen_metabolism']\n",
    "            \n",
    "            for pathway in corrosion_related_pathways:\n",
    "                if pathway in processed_data[ec_number]['pathway_categories']:\n",
    "                    corrosion_score += 1.0\n",
    "                    \n",
    "            # Score based on organic processes relevant to corrosion\n",
    "            corrosion_related_processes = ['degradation', 'oxidation','reduction']\n",
    "            for process in corrosion_related_processes:\n",
    "                if process in processed_data[ec_number]['organic_processes']:\n",
    "                    corrosion_score += 0.5\n",
    "                            \n",
    "            # Score based on corrosion synergies\n",
    "            corrosion_score += len(processed_data[ec_number]['corrosion_synergies']) * 2.0\n",
    "            \n",
    "            # Set corrosion relevance category\n",
    "            if corrosion_score >= 5:\n",
    "                processed_data[ec_number]['corrosion_relevance'] = 'high'\n",
    "            elif corrosion_score >= 2:\n",
    "                processed_data[ec_number]['corrosion_relevance'] = 'medium'\n",
    "            else:\n",
    "                processed_data[ec_number]['corrosion_relevance'] = 'low'\n",
    "            \n",
    "            # Store the numerical score\n",
    "            processed_data[ec_number]['corrosion_relevance_score'] = corrosion_score\n",
    "\n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EA-pSl0dvd0W"
   },
   "source": [
    "### Enzyme names\n",
    "The database containing enzyme names and EC numbers\n",
    "\n",
    "wget https://www.enzyme-database.org/downloads/enzyme-database.sql.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-19T16:14:53.643454Z",
     "iopub.status.idle": "2025-04-19T16:14:53.643934Z",
     "shell.execute_reply": "2025-04-19T16:14:53.643730Z"
    },
    "id": "L6FSLRy-E1_a",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def read_enzyme_names():\n",
    "    \"\"\"Read and parse enzyme file to get EC numbers and their names\"\"\"\n",
    "    paths = setup_paths()\n",
    "    enzyme_path = paths['enzyme']\n",
    "   \n",
    "    ec_to_names = {}  # More descriptive name\n",
    "    with open(enzyme_path, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) >= 2:\n",
    "                ec_number = parts[0]\n",
    "                names = parts[1].split('; ')\n",
    "                ec_to_names[ec_number] = names\n",
    "\n",
    "    for ec_number, names_list in ec_to_names.items():\n",
    "\n",
    "        cleaned_names = [enhanced_clean_protein_name(name) if isinstance(name, str) else str(name) for name in names_list]\n",
    "        ec_to_names[ec_number] = cleaned_names\n",
    "\n",
    "    return ec_to_names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aUUP7OAF9iaM"
   },
   "source": [
    "### Enzyme Class\n",
    "The enzyme classification system (text-based hierarchy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-19T16:14:53.644988Z",
     "iopub.status.idle": "2025-04-19T16:14:53.645485Z",
     "shell.execute_reply": "2025-04-19T16:14:53.645281Z"
    },
    "id": "6FTH4E8gZfdu",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def read_enzyme_class():\n",
    "    paths = setup_paths()\n",
    "    ec_file_path = paths['enzyme_class']\n",
    "\n",
    "    enzyme_class = {}\n",
    "\n",
    "    with open(ec_file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            # Format is like \"1. 1. 1.-    With NAD(+) or NADP(+) as acceptor.\"\n",
    "            if line.strip() and any(line.startswith(str(i)) for i in range(1, 7)):\n",
    "                parts = line.strip().split('  ')\n",
    "                if len(parts) >= 2:\n",
    "                    ec_id = parts[0].replace(' ', '')\n",
    "                    desc = parts[1].strip()\n",
    "                    enzyme_class[ec_id] = desc\n",
    "    return enzyme_class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GBIfRLP65Jzc"
   },
   "source": [
    "Ko list\n",
    "\n",
    "wget -O ec_pathway.list \"https://rest.kegg.jp/link/pathway/ec\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-19T16:14:53.646379Z",
     "iopub.status.idle": "2025-04-19T16:14:53.646913Z",
     "shell.execute_reply": "2025-04-19T16:14:53.646702Z"
    },
    "id": "EEH4ACcm558K",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def read_ec_pathway_mapping():\n",
    "    \"\"\"Read EC to pathway mapping file downloaded from KEGG\"\"\"\n",
    "\n",
    "    paths = setup_paths()\n",
    "    ko_pathway_path = paths['ko_pathway']\n",
    "\n",
    "    ec_to_pathway = {}\n",
    "\n",
    "    try:\n",
    "        with open(ko_pathway_path , 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split('\\t')\n",
    "                if len(parts) == 2:\n",
    "                    ec_id = parts[0].replace('ec:', '')\n",
    "                    pathway_id = parts[1].replace('path:', '')\n",
    "\n",
    "                    if ec_id not in ec_to_pathway:\n",
    "                        ec_to_pathway[ec_id] = []\n",
    "                    ec_to_pathway[ec_id].append(pathway_id)\n",
    "\n",
    "        print(f\"Loaded pathway mappings for {len(ec_to_pathway)} EC numbers\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading EC-pathway mapping: {e}\")\n",
    "        return {}\n",
    "\n",
    "    return ec_to_pathway\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CBtayQh4eQ82"
   },
   "source": [
    "### Ko Database\n",
    " A new variable mapping  KO numbers to EC numbers from KEGG KO file.\n",
    "rsync -avz rsync://rest.kegg.jp/kegg/pathway/ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-19T16:14:53.648612Z",
     "iopub.status.idle": "2025-04-19T16:14:53.649110Z",
     "shell.execute_reply": "2025-04-19T16:14:53.648907Z"
    },
    "id": "Nz9K72kaKM52",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def read_ko_data():\n",
    "    \"\"\"Read and parse KEGG KO file\"\"\"\n",
    "    paths = setup_paths()\n",
    "    ko_file_path = paths['ko']\n",
    "\n",
    "    ko_info = {}\n",
    "    with open(ko_file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.startswith('K'):\n",
    "                parts = line.strip().split('\\t')\n",
    "                if len(parts) > 1:\n",
    "                    ko_info[parts[0]] = {\n",
    "                        'definition': parts[1],\n",
    "                        'pathway': parts[2] if len(parts) > 2 else ''\n",
    "                    }\n",
    "    return ko_info\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iBwbp_UkeTI6"
   },
   "source": [
    "Ko Hierarchi Database\n",
    "Hierarchy of KO numbers (helps in pathway mapping).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-19T16:14:53.650365Z",
     "iopub.status.idle": "2025-04-19T16:14:53.650849Z",
     "shell.execute_reply": "2025-04-19T16:14:53.650639Z"
    },
    "id": "l-PjltuxE1_b",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def read_ko_hierarchy():\n",
    "    paths = setup_paths()\n",
    "    ko_path = paths['ko_hierarchy'] \n",
    "\n",
    "    hierarchy = {\n",
    "        'A': {},  # Top level\n",
    "        'B': {},  # ko Category\n",
    "        'C': {},  # Pathway\n",
    "        'D': {}   # KO/Enzyme\n",
    "    }\n",
    "\n",
    "    current = {'A': None, 'B': None, 'C': None}\n",
    "\n",
    "    with open(ko_path, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.startswith('A'):\n",
    "                parts = line.strip().split()\n",
    "                id = parts[1]\n",
    "                name = ' '.join(parts[2:])\n",
    "                hierarchy['A'][id] = name\n",
    "                current['A'] = id\n",
    "\n",
    "            elif line.startswith('B'):\n",
    "                parts = line.strip().split()\n",
    "                id = parts[1]\n",
    "                name = ' '.join(parts[2:])\n",
    "                hierarchy['B'][id] = {'name': name, 'parent': current['A']}\n",
    "                current['B'] = id\n",
    "\n",
    "            elif line.startswith('C'):\n",
    "                parts = line.strip().split()\n",
    "                id = parts[1]\n",
    "                name = ' '.join(parts[2:])\n",
    "                if '[PATH:' in name:\n",
    "                    path_parts = name.split('[PATH:')\n",
    "                    name = path_parts[0].strip()\n",
    "                    path_id = path_parts[1].split(']')[0]\n",
    "                else:\n",
    "                    path_id = None\n",
    "\n",
    "                hierarchy['C'][id] = {\n",
    "                    'name': name,\n",
    "                    'parent': current['B'],\n",
    "                    'path_id': path_id\n",
    "                }\n",
    "                current['C'] = id\n",
    "\n",
    "            elif line.startswith('D'):\n",
    "                parts = line.strip().split()\n",
    "                ko_id = parts[1]\n",
    "                name = ' '.join(parts[2:])\n",
    "\n",
    "                # Extract EC numbers if present\n",
    "                ec_numbers = []\n",
    "                if '[EC:' in name:\n",
    "                    ec_part = name.split('[EC:')[1].split(']')[0]\n",
    "                    ec_numbers = ec_part.split()\n",
    "                    name = name.split('[EC:')[0].strip()\n",
    "\n",
    "                hierarchy['D'][ko_id] = {\n",
    "                    'name': name,\n",
    "                    'parent': current['C'],\n",
    "                    'ec_numbers': ec_numbers\n",
    "                }\n",
    "\n",
    "    return hierarchy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4-kOhcGCcn-i"
   },
   "source": [
    "Reaction Data\n",
    " Reaction-level information.\n",
    "\n",
    "!wget -c \"ftp://ftp.genome.jp/pub/kegg/reaction/reaction.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-19T16:14:53.651909Z",
     "iopub.status.idle": "2025-04-19T16:14:53.652402Z",
     "shell.execute_reply": "2025-04-19T16:14:53.652177Z"
    },
    "id": "s5Pd6vUTLR4M",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def read_reaction_data():\n",
    "    paths = setup_paths()\n",
    "    reaction_file_path = paths['reaction']\n",
    "\n",
    "    reaction_info = {}\n",
    "\n",
    "    with open(reaction_file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            parts = line.split(None, 1)  # Split on first whitespace\n",
    "            if len(parts) >= 2:\n",
    "                rxn_id = parts[0]\n",
    "                desc_parts = parts[1].split(';')\n",
    "\n",
    "                # First part is reaction name\n",
    "                name = desc_parts[0].strip()\n",
    "\n",
    "                # Rest might contain equation\n",
    "                equation = desc_parts[1].strip() if len(desc_parts) > 1 else \"\"\n",
    "\n",
    "                reaction_info[rxn_id] = {\n",
    "                    'name': name,\n",
    "                    'equation': equation\n",
    "                }\n",
    "\n",
    "    return reaction_info\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LIyE7Tx5chQ6"
   },
   "source": [
    "### Pathway Database\n",
    "hemical compounds database.wget https://biocyc.org/download.shtml. wget https://www.brenda-enzymes.org/download.php\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-19T16:14:53.653434Z",
     "iopub.status.idle": "2025-04-19T16:14:53.653910Z",
     "shell.execute_reply": "2025-04-19T16:14:53.653700Z"
    },
    "id": "xu5sqGIPc0op",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def read_pathway_data():\n",
    "    paths = setup_paths()\n",
    "    pathway_path = paths['pathway']\n",
    "    \n",
    "    pathway_info = {}\n",
    "    with open(pathway_path, 'r') as f:\n",
    "          for line in f:\n",
    "              parts = line.strip().split('\\t')\n",
    "              if len(parts) >= 2:\n",
    "                  pathway_id = parts[0]\n",
    "                  pathway_name = parts[1]\n",
    "                  pathway_info[pathway_id] = pathway_name\n",
    "    return pathway_info\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zxYkP8l0dCXo"
   },
   "source": [
    "### Module Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-19T16:14:53.655120Z",
     "iopub.status.idle": "2025-04-19T16:14:53.655599Z",
     "shell.execute_reply": "2025-04-19T16:14:53.655400Z"
    },
    "id": "0Ue-7KZpdEfe",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def read_module_data():\n",
    "    paths = setup_paths()\n",
    "    module_path = paths['module']\n",
    "\n",
    "    module_info = {}\n",
    "    with open(module_path, 'r') as f:\n",
    "      for line in f:\n",
    "          parts = line.strip().split('\\t')\n",
    "          if len(parts) >= 2:\n",
    "              module_id = parts[0]\n",
    "              module_desc = parts[1]\n",
    "              module_info[module_id] = module_desc\n",
    "    return module_info\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W2GClcq8dmrP"
   },
   "source": [
    "### Compound Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-19T16:14:53.656575Z",
     "iopub.status.idle": "2025-04-19T16:14:53.657035Z",
     "shell.execute_reply": "2025-04-19T16:14:53.656836Z"
    },
    "id": "K9riHOnadqSm",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def read_compound_data():\n",
    "    paths = setup_paths()\n",
    "    compound_path = paths['compound']\n",
    "\n",
    "    compound_info = {}\n",
    "    with open(compound_path, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split('\\t')\n",
    "                if len(parts) >= 2:\n",
    "                    compound_id = parts[0]\n",
    "                    compound_names = parts[1].split('; ')\n",
    "                    compound_info[compound_id] = {\n",
    "                        'name': compound_names[0],\n",
    "                        'synonyms': compound_names[1:] if len(compound_names) > 1 else []\n",
    "                    }\n",
    "    return compound_info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hi9MNHn55BT3"
   },
   "source": [
    "### Metal pdb\n",
    "MetalPDB in 2018: a database of metal sites in biological macromolecular structures.\n",
    "Putignano V., Rosato A., Banci L., Andreini C.\n",
    "Nucleic Acids Res. 2018 Jan;46(D1):D459-D464. [PMID: 29077942]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-19T16:14:53.658500Z",
     "iopub.status.idle": "2025-04-19T16:14:53.658974Z",
     "shell.execute_reply": "2025-04-19T16:14:53.658766Z"
    },
    "id": "MhoKLqYQihT8",
    "outputId": "0cf6f34c-0fef-490f-948f-6905d676cdec",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def parse_metalpdb_xml():\n",
    "    \"\"\"Parse MetalPDB XML file to extract metal-binding information\"\"\"\n",
    "    paths = setup_paths()\n",
    "    metalpdb_path = paths['metalpdb']\n",
    "\n",
    "    metal_binding_data = {}\n",
    "\n",
    "    try:\n",
    "        # Use a more tolerant parser\n",
    "        parser = etree.XMLParser(recover=True)\n",
    "        tree = etree.parse(metalpdb_path, parser)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        # Process each site\n",
    "        for site in root.findall('.//site'):\n",
    "            # Extract site information\n",
    "            site_name = site.findtext('site_name')\n",
    "            pdb_code = site.findtext('pdb_code')\n",
    "            site_nuclearity = site.findtext('site_nuclearity')\n",
    "\n",
    "            # Process each metal in the site\n",
    "            for metal in site.findall('.//metal'):\n",
    "                metal_symbol = metal.findtext('periodic_symbol')\n",
    "                metal_name = metal.findtext('periodic_name')\n",
    "                coordination_number = metal.findtext('coordination_number')\n",
    "                geometry = metal.findtext('geometry')\n",
    "\n",
    "                # Process ligands\n",
    "                ligands = []\n",
    "                for ligand in metal.findall('.//ligand'):\n",
    "                    residue_name = ligand.findtext('residue_name')\n",
    "                    residue_num = ligand.findtext('residue_pdb_number')\n",
    "                    chain = ligand.findtext('chain_letter')\n",
    "                    binding_type = ligand.findtext('endo_exo')\n",
    "\n",
    "                    # Process donor atoms\n",
    "                    donors = []\n",
    "                    for donor in ligand.findall('.//donor'):\n",
    "                        distance = donor.findtext('distance')\n",
    "                        atom_name = donor.findtext('atom_pdb_name')\n",
    "                        atom_symbol = donor.findtext('atom_symbol')\n",
    "                        interaction_type = donor.findtext('interaction_type')\n",
    "\n",
    "                        donors.append({\n",
    "                            'distance': distance,\n",
    "                            'atom_name': atom_name,\n",
    "                            'atom_symbol': atom_symbol,\n",
    "                            'interaction_type': interaction_type\n",
    "                        })\n",
    "\n",
    "                    ligands.append({\n",
    "                        'residue_name': residue_name,\n",
    "                        'residue_number': residue_num,\n",
    "                        'chain': chain,\n",
    "                        'binding_type': binding_type,\n",
    "                        'donors': donors\n",
    "                    })\n",
    "\n",
    "                # Get the protein/molecule information\n",
    "                site_chains = []\n",
    "                for chain in site.findall('.//site_chain'):\n",
    "                    molecule_name = chain.findtext('molecule_name')\n",
    "                    molecule_type = chain.findtext('molecule_type')\n",
    "                    chain_letter = chain.findtext('letter')\n",
    "\n",
    "                    site_chains.append({\n",
    "                        'molecule_name': molecule_name,\n",
    "                        'molecule_type': molecule_type,\n",
    "                        'chain_letter': chain_letter\n",
    "                    })\n",
    "\n",
    "                # Create a unique key for this metal site\n",
    "                metal_site_key = f\"{pdb_code}_{site_name}_{metal_symbol}\"\n",
    "\n",
    "                # Store the data\n",
    "                metal_binding_data[metal_site_key] = {\n",
    "                    'pdb_code': pdb_code,\n",
    "                    'site_name': site_name,\n",
    "                    'site_nuclearity': site_nuclearity,\n",
    "                    'metal': {\n",
    "                        'symbol': metal_symbol,\n",
    "                        'name': metal_name,\n",
    "                        'coordination_number': coordination_number,\n",
    "                        'geometry': geometry\n",
    "                    },\n",
    "                    'ligands': ligands,\n",
    "                    'site_chains': site_chains\n",
    "                }\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error parsing MetalPDB XML: %s\", e)\n",
    "        return {}\n",
    "\n",
    "    return metal_binding_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ScqMYjZ3dIM"
   },
   "source": [
    "### Extracting metal binding patterns from metal binding data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-19T16:14:53.659857Z",
     "iopub.status.idle": "2025-04-19T16:14:53.660342Z",
     "shell.execute_reply": "2025-04-19T16:14:53.660117Z"
    },
    "id": "2SwP7fDN3so5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def extract_metal_coordination_patterns(metal_binding_data):\n",
    "    \"\"\"Extract metal coordination patterns from MetalPDB data\"\"\"\n",
    "\n",
    "    # Track metal coordination patterns\n",
    "    metal_coordination = {}\n",
    "    metal_residue_binding = {}\n",
    "    # Add a counter to track processing\n",
    "    processed_sites = 0\n",
    "\n",
    "    for site_key, site_data in metal_binding_data.items():\n",
    "        try:\n",
    "            metal_symbol = site_data.get('metal', {}).get('symbol')\n",
    "            if not metal_symbol:\n",
    "                continue\n",
    "    \n",
    "            # Track coordination environments\n",
    "            coord_num = site_data.get('metal', {}).get('coordination_number')\n",
    "            geometry = site_data.get('metal', {}).get('geometry')\n",
    "\n",
    "            if coord_num and geometry:\n",
    "                coord_key = f\"{metal_symbol}_{coord_num}_{geometry}\"\n",
    "                metal_coordination[coord_key] = metal_coordination.get(coord_key, 0) + 1\n",
    "        \n",
    "              # Track metal-residue binding\n",
    "            if metal_symbol not in metal_residue_binding:\n",
    "                metal_residue_binding[metal_symbol] = {}\n",
    "                \n",
    "            for ligand in site_data.get('ligands', []):\n",
    "                residue = ligand.get('residue_name')\n",
    "                if residue:\n",
    "                    metal_residue_binding[metal_symbol][residue] = metal_residue_binding[metal_symbol].get(residue, 0) + 1\n",
    "            \n",
    "            processed_sites += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing site {site_key}: {e}\")\n",
    "    \n",
    "    print(f\"Processed {processed_sites} sites out of {len(metal_binding_data)}\")\n",
    "    \n",
    "    if not metal_coordination:\n",
    "        print(\"Warning: No coordination patterns were extracted\")\n",
    "    if not metal_residue_binding:\n",
    "        print(\"Warning: No residue binding patterns were extracted\")\n",
    "        \n",
    "    return {\n",
    "        'coordination': metal_coordination,\n",
    "        'residue_binding': metal_residue_binding\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jdl9jF7TikIR"
   },
   "source": [
    "### EC to reaction Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-19T16:14:53.661225Z",
     "iopub.status.idle": "2025-04-19T16:14:53.661690Z",
     "shell.execute_reply": "2025-04-19T16:14:53.661497Z"
    },
    "id": "FH9jMXKKSJM7",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_ec_to_reaction_mapping():  \n",
    "    ec_to_names = read_enzyme_names()\n",
    "    reaction_info = read_reaction_data()\n",
    "    \n",
    "    # Build keyword-to-ECs reverse index\n",
    "    keyword_to_ecs = {}\n",
    "    for ec, names in ec_to_names.items():\n",
    "        words = set()\n",
    "        for name in names:\n",
    "            for word in str(name).lower().split():\n",
    "                if len(word) > 4:\n",
    "                    words.add(word)\n",
    "        for keyword in words:\n",
    "            if keyword not in keyword_to_ecs:\n",
    "                keyword_to_ecs[keyword] = set()\n",
    "            keyword_to_ecs[keyword].add(ec)\n",
    "    \n",
    "    # Preprocess reaction names\n",
    "    rxn_name_lower = {rxn_id: info['name'].lower() for rxn_id, info in reaction_info.items()}\n",
    "    \n",
    "    # Match reactions to ECs\n",
    "    ec_to_reaction = {}\n",
    "    for rxn_id, rxn_name in rxn_name_lower.items():\n",
    "        matched_ecs = set()\n",
    "        for keyword in keyword_to_ecs:\n",
    "            if keyword in rxn_name: \n",
    "                matched_ecs.update(keyword_to_ecs[keyword])\n",
    "        for ec in matched_ecs:\n",
    "            if ec not in ec_to_reaction:\n",
    "                ec_to_reaction[ec] = set()\n",
    "            ec_to_reaction[ec].add(rxn_id)\n",
    "    \n",
    "    return {ec: list(rxns) for ec, rxns in ec_to_reaction.items()} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XBwBw-ufrWsh"
   },
   "source": [
    "### Consolidation Metals from Different Origins Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-19T16:14:53.662942Z",
     "iopub.status.idle": "2025-04-19T16:14:53.663426Z",
     "shell.execute_reply": "2025-04-19T16:14:53.663198Z"
    },
    "id": "HXsb57_SraU0",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Standard mapping: lower-case keys for matching, with standard symbols as values\n",
    "import corrosion_scoring.global_terms as gt\n",
    "def consolidate_metal_terms(brenda_metals, text_detected_metals):\n",
    "    \"\"\"\n",
    "    Consolidates metal names from BRENDA and text mining into standardized symbols.\n",
    "\n",
    "    Parameters:\n",
    "        brenda_metals (list of str): Metals obtained from BRENDA data.\n",
    "        text_detected_metals (list of str): Metals detected from text mining.\n",
    "\n",
    "    Returns:\n",
    "        list: Consolidated list of unique, standardized metal symbols.\n",
    "    \"\"\"\n",
    "    # Import metal_mapping from global terms module # for kaggle\n",
    "    from global_terms_py import metal_mapping\n",
    "    \n",
    "    consolidated = set()\n",
    "    all_metals = (brenda_metals or []) + (text_detected_metals or [])\n",
    "    \n",
    "    for metal in all_metals:\n",
    "        metal_norm = metal.strip().lower()\n",
    "        # Check if the normalized term matches any key in the standard mapping\n",
    "        for key, symbol in metal_mapping.items():\n",
    "            if key in metal_norm:\n",
    "                consolidated.add(symbol)\n",
    "                break\n",
    "        else:\n",
    "            # If no mapping is found, add the original\n",
    "            consolidated.add(metal.strip())\n",
    "    return list(consolidated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8BNjB4FHE1_e"
   },
   "source": [
    "## 9.3. Main Function of Creating a Consolidated DataBase\n",
    "\n",
    "Brenda Enzyme Database. (n.d.). In BRENDA. Retrieved from https://www.brenda-enzymes.org (APA citation format)\n",
    "MetalPDB: a database of metal sites in biological macromolecular structures. (n.d.). Retrieved from http://metalpdb.cerm.unifi.it (APA citation format)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-19T16:14:53.666220Z",
     "iopub.status.idle": "2025-04-19T16:14:53.666697Z",
     "shell.execute_reply": "2025-04-19T16:14:53.666499Z"
    },
    "id": "Smqh2fy8-PJP",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_metabolism_database():\n",
    "    \"\"\"\n",
    "    Build a list of dictionaries, each representing a single EC record.\n",
    "    \n",
    "    This function builds a comprehensive database of enzyme records with detailed information\n",
    "    about their potential relevance to corrosion processes. It incorporates data from various\n",
    "    sources, including BRENDA, MetalPDB, KEGG, and others.\n",
    "    \n",
    "    The function can use either:\n",
    "    1. The external scoring_system_py module (preferred for consistency and maintainability)\n",
    "    2. The original inline scoring logic (as a fallback if the module import fails)\n",
    "    \n",
    "    Args:   sample_size (int, optional): If specified, random sample of EC numbers to process\n",
    "        \n",
    "    Returns:  list: List of dictionaries, each containing information about an enzyme\n",
    "    \"\"\"\n",
    "    global brenda_data\n",
    "    import sys\n",
    "    #sys.path.append('/kaggle/input/corrosion-scoring') # for kaggle\n",
    "    sys.path.append('/home/beatriz/MIC/2_Micro/corrosion_scoring')\n",
    "    from corrosion_scoring.global_terms import (\n",
    "    #from global_terms import ( # for kaggle\n",
    "        metal_terms, \n",
    "        corrosion_mechanisms, \n",
    "        pathway_categories, \n",
    "        organic_categories,\n",
    "        corrosion_synergies, \n",
    "        functional_categories, \n",
    "        corrosion_keyword_groups, \n",
    "        metal_mapping\n",
    "    )\n",
    "    import corrosion_scoring.scoring_system as score_sys\n",
    "    \n",
    "    try:\n",
    "        # Read all necessary files\n",
    "        print(\"Loading data sources...\")\n",
    "        ec_to_names = read_enzyme_names() or {}\n",
    "        enzyme_class = read_enzyme_class() or {}\n",
    "        reaction_equation = read_reaction_data() or {}\n",
    "        ko_ec = read_ko_data() or {}\n",
    "        ko_hierarchy = read_ko_hierarchy() or {}\n",
    "        pathway_data = read_pathway_data() or {}\n",
    "        module_info = read_module_data() or {}\n",
    "        compound_info = read_compound_data() or {}\n",
    "        brenda_en = process_brenda_data(brenda_data) or {}\n",
    "        metal_binding_data = parse_metalpdb_xml() or {}\n",
    "        metal_patterns = extract_metal_coordination_patterns(metal_binding_data)\n",
    "        ec_pathway_mapping = read_ec_pathway_mapping() or {}\n",
    "\n",
    "        print(f\"Loaded: {len(ec_to_names)} enzymes, {len(pathway_data)} pathways, {len(brenda_en)} BRENDA entries\")\n",
    "\n",
    "        # Get EC to reaction mapping \n",
    "        print(\"Creating EC to reaction mapping...\")\n",
    "        ec_to_rxn = create_ec_to_reaction_mapping()\n",
    "        \n",
    "        # extract protein information\n",
    "        print(\"Preparing protein database...\")\n",
    "        protein_database = []\n",
    "        # From BRENDA\n",
    "        if brenda_en:\n",
    "            for ec_number, data in brenda_en.items():\n",
    "                # Use EC number to find enzyme names from ec_to_names\n",
    "                if ec_number in ec_to_names:\n",
    "                    protein_name = ec_to_names[ec_number][0] if ec_to_names[ec_number] else None\n",
    "                    if protein_name:\n",
    "                        protein_database.append({\n",
    "                            'source': 'BRENDA',\n",
    "                            'ec_number': ec_number,\n",
    "                            'protein_name': protein_name,\n",
    "                            'brenda_data': data\n",
    "                        })\n",
    "        # From MetalPDB\n",
    "        if metal_binding_data:\n",
    "            for site_key, site_data in metal_binding_data.items():\n",
    "                for chain in site_data.get('site_chains', []):\n",
    "                    if 'molecule_name' in chain and chain['molecule_type'] == 'protein':\n",
    "                        protein_database.append({\n",
    "                            'source': 'MetalPDB',\n",
    "                            'pdb_code': site_data.get('pdb_code'),\n",
    "                            'protein_name': chain['molecule_name'],\n",
    "                            'metal_binding': site_data.get('metal', {})\n",
    "                        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data sources: {e}\")\n",
    "    \n",
    "        return []\n",
    "    # Track statistics for validation\n",
    "    stats = {\n",
    "        'total_enzymes': 0,\n",
    "        'with_brenda_data': 0,\n",
    "        'with_reactions': 0,\n",
    "        'with_pathways': 0,\n",
    "        'with_ko': 0,\n",
    "        'with_metal_involvement': 0,\n",
    "        'with_corrosion_mechanisms': 0\n",
    "    }\n",
    "    \n",
    "    start_time_record = time.time()\n",
    "    # Prepare a list to store all records\n",
    "    ec_records = []\n",
    "    stats['total_enzymes'] = len(ec_to_names)\n",
    "\n",
    "    print(\"Pre-computing enzyme class lookups...\")\n",
    "    ec_class_lookup = {}\n",
    "    for ec_number in ec_to_names.keys():\n",
    "        try:\n",
    "            ec_prefix = '.'.join(ec_number.split('.')[:2])\n",
    "            # Try exact match first\n",
    "            if ec_prefix in enzyme_class:\n",
    "                ec_class_lookup[ec_number] = enzyme_class[ec_prefix].strip()\n",
    "            # Then try pattern match\n",
    "            else:\n",
    "                pattern_key = f\"{ec_prefix}.-.-\"\n",
    "                if pattern_key in enzyme_class:\n",
    "                    ec_class_lookup[ec_number] = enzyme_class[pattern_key]\n",
    "        except Exception:\n",
    "            pass  # Skip if error occurs\n",
    "    \n",
    "    # Pre-compute KO lookups - do this once\n",
    "    print(\"Pre-computing KO lookups...\")\n",
    "    ko_lookup = {}\n",
    "    for ko, data in ko_ec.items():\n",
    "        if isinstance(data, dict) and 'definition' in data:\n",
    "            for ec_number in ec_to_names.keys():\n",
    "                if f\"[EC:{ec_number}]\" in data['definition']:\n",
    "                    if ec_number not in ko_lookup:\n",
    "                        ko_lookup[ec_number] = []\n",
    "                    ko_lookup[ec_number].append(ko)\n",
    "    \n",
    "    # Populate from ec_to_names for basic enzyme names\n",
    "    print(\"Building enzyme records...\")\n",
    "    for ec_number, names in ec_to_names.items():\n",
    "        # Data validation for EC number format\n",
    "        if not (ec_number.count('.') == 3 and all(part.isdigit() for part in ec_number.split('.'))):\n",
    "            print(f\"Warning: Invalid EC number format: {ec_number}\")\n",
    "            continue\n",
    "\n",
    "        record ={\n",
    "            'ec_number': ec_number,\n",
    "            'enzyme_names': [str(name).strip() for name in (names if isinstance(names, list) else [names]) if name], # Ensure names are strings and stripped\n",
    "            'protein_name': str(names[0]).strip() if isinstance(names, list) and names else str(names).strip() if names else None, # Strip protein name\n",
    "            'enzyme_class': ec_class_lookup.get(ec_number),\n",
    "            'pathways': [],\n",
    "            'hierarchy': [],\n",
    "            'ko': ko_lookup.get(ec_number, []),\n",
    "            'reactions': [],\n",
    "            'compounds': [],\n",
    "            'modules': [],\n",
    "            'metals_from_brenda': [],\n",
    "            'corrosion_metals_from_brenda': []\n",
    "        }\n",
    "        # Add pathways from EC-pathway mapping\n",
    "        if ec_number in ec_pathway_mapping:\n",
    "            for pathway_id in ec_pathway_mapping[ec_number]:\n",
    "                # Standardize to map prefix\n",
    "                std_id = pathway_id\n",
    "                if pathway_id.startswith('ec'):\n",
    "                    std_id = 'map' + pathway_id[2:]\n",
    "\n",
    "                # Look up the pathway name\n",
    "                if std_id in pathway_data:\n",
    "                    pathway_name = pathway_data[std_id]\n",
    "                    if pathway_name not in record['pathways']:\n",
    "                        record['pathways'].append(pathway_name)\n",
    "\n",
    "        # Add pathways from KO data\n",
    "        if ec_number in ko_ec and isinstance(ko_ec[ec_number], list):\n",
    "            for path in ko_ec[ec_number]:\n",
    "                if path not in record['pathways']:\n",
    "                    record['pathways'].append(path)\n",
    "        elif ec_number in ko_ec and isinstance(ko_ec[ec_number], dict) and 'pathway' in ko_ec[ec_number]:\n",
    "            path = ko_ec[ec_number]['pathway']\n",
    "            if path not in record['pathways']:\n",
    "                record['pathways'].append(path)\n",
    "        # Add KO IDs\n",
    "        ko_ids = []\n",
    "        for ko, data in ko_ec.items():\n",
    "            if isinstance(data, dict) and 'definition' in data and f\"[EC:{ec_number}]\" in data['definition']:\n",
    "                ko_ids.append(ko)\n",
    "        record['ko'] = ko_ids\n",
    "\n",
    "        if ko_ids:\n",
    "            stats['with_ko'] += 1\n",
    "\n",
    "        # Build reaction list\n",
    "        rxns = ec_to_rxn.get(ec_number, [])\n",
    "        for rxn_id in rxns:\n",
    "            if rxn_id in reaction_equation:\n",
    "                eqn = reaction_equation.get(rxn_id, {}).get('equation', 'Unknown')\n",
    "                record['reactions'].append({'id': rxn_id, 'equation': eqn})\n",
    "\n",
    "                # Add compounds involved in this reaction\n",
    "                for compound_id in reaction_equation.get(rxn_id, {}).get('compounds', []):\n",
    "                    if compound_id in compound_info:\n",
    "                        compound_data = compound_info[compound_id]\n",
    "                        if compound_data not in record['compounds']:\n",
    "                            record['compounds'].append(compound_data)\n",
    "\n",
    "        if rxns:\n",
    "            stats['with_reactions'] += 1\n",
    "\n",
    "        # Add module information\n",
    "        for module_id, module_desc in module_info.items():\n",
    "            if f\"[EC:{ec_number}]\" in module_desc:\n",
    "                record['modules'].append({'id': module_id, 'description': module_desc})\n",
    "\n",
    "        # Reconcile metals from BRENDA with text mining\n",
    "        record['metals_from_brenda'] = []\n",
    "        record['corrosion_metals_from_brenda'] = []\n",
    "\n",
    "        # Add BRENDA metal information\n",
    "        if brenda_en and ec_number in brenda_en:\n",
    "            record['metals_from_brenda'] = brenda_en[ec_number].get('clean_metals', [])\n",
    "            record['corrosion_metals_from_brenda'] = brenda_en[ec_number].get('corrosion_metals_from_brenda', [])\n",
    "            stats['with_brenda_data'] += 1\n",
    "\n",
    "        # Check if EC number is valid\n",
    "        ec_number = record['ec_number']\n",
    "        has_valid_ec = ec_number.count('.') == 3 and all(part.isdigit() for part in ec_number.split('.'))\n",
    "        # Initialize protein_name to None as a default\n",
    "        protein_name = None\n",
    "        # If no valid EC number or it failed any checks\n",
    "        if not has_valid_ec and 'protein_name' in record and record['protein_name']:\n",
    "            protein_name = record['protein_name'].lower()    \n",
    "\n",
    "        ec_records.append(record)\n",
    "        \n",
    "    elapsed_time_record = time.time() - start_time_record\n",
    "    print(f\"Processing took {elapsed_time_record:.2f} seconds\")\n",
    "\n",
    "     # Process records in batches to avoid memory issues\n",
    "    print(\"Processing protein names and calculating scores...\")\n",
    "    batch_size = 1000  # Adjust based on available memory\n",
    "    for i in range(0, len(ec_records), batch_size):\n",
    "        batch = ec_records[i:i+batch_size]\n",
    "        \n",
    "        # Process protein names in batch\n",
    "        for rec in batch:\n",
    "            # Process protein name matches\n",
    "            try:\n",
    "                if rec.get('protein_name') is None and rec.get('enzyme_names'):\n",
    "                    rec['protein_name'] = rec['enzyme_names'][0]\n",
    "                # Use indexing/hashing to speed up matching    \n",
    "                protein_name = rec.get('protein_name', '')\n",
    "                if not protein_name or len(protein_name) <= 2:\n",
    "                    continue  \n",
    "                \n",
    "                protein_name_lower = protein_name.lower()\n",
    "                for db_rec in protein_database:\n",
    "                    db_protein_name = db_rec.get('protein_name', '')\n",
    "                    if not db_protein_name:\n",
    "                        continue\n",
    "                        \n",
    "                    db_protein_lower = db_protein_name.lower()\n",
    "                    if (db_protein_lower == protein_name_lower or \n",
    "                        db_protein_lower in protein_name_lower or\n",
    "                        protein_name_lower in db_protein_lower):\n",
    "                            # Found a matching protein, merge data\n",
    "                            if 'brenda_data' in db_rec:\n",
    "                                # Process BRENDA data\n",
    "                                brenda_data = db_rec['brenda_data']\n",
    "                                if 'metals' in brenda_data and 'metals' not in rec:\n",
    "                                    rec['metals'] = brenda_data['metals']\n",
    "                                    \n",
    "                            if 'metal_binding' in db_rec:\n",
    "                                # Process MetalPDB data\n",
    "                                if 'metal_binding_info' not in rec:\n",
    "                                    rec['metal_binding_info'] = {}\n",
    "                                    metal = db_rec['metal_binding'].get('symbol')\n",
    "                                    if metal:\n",
    "                                        rec['metal_binding_info'][metal] = db_rec['metal_binding']\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing protein name {rec.get('protein_name')}: {e}\")\n",
    "\n",
    "        # Calculate scores for this batch\n",
    "        for rec in batch:\n",
    "            # Build text for scoring once\n",
    "            enzyme_names = rec.get('enzyme_names', []) or []\n",
    "            class_text = rec.get('enzyme_class', '') or ''\n",
    "            pathways = rec.get('pathways', []) or []\n",
    "            names_text = ' '.join(enzyme_names) + ' ' + class_text + ' ' + ' '.join(pathways)\n",
    "            \n",
    "            # Add reaction text for more context\n",
    "            reaction_text = \"\"\n",
    "            for r in rec.get('reactions', []):\n",
    "                if isinstance(r, dict) and 'equation' in r:\n",
    "                    reaction_text += \" \" + r['equation']\n",
    "\n",
    "            # Combined text for all scoring\n",
    "            all_text = f\"{names_text} {class_text} {' '.join(pathways)} {reaction_text}\".lower()\n",
    "            using_imported_modules = True\n",
    "            # Score metals, mechanisms, etc. all at once\n",
    "            try:\n",
    "                if using_imported_modules:\n",
    "                    # Use the scoring system module if available\n",
    "                    score_results = score_sys.calculate_overall_scores(\n",
    "                        all_text, \n",
    "                        brenda_metals=rec.get('metals_from_brenda', [])\n",
    "                    )\n",
    "                    \n",
    "                    # Merge the score results into the record\n",
    "                    rec.update(score_results)\n",
    "                    \n",
    "                    # Consolidate metals using the module function\n",
    "                    rec['metals_consolidated'] = score_sys.consolidate_metal_terms(\n",
    "                        rec.get('metals_from_brenda', []),\n",
    "                        rec.get('metals_involved', [])\n",
    "                    )\n",
    "                else:\n",
    "                    # Fall back to original inline scoring logic\n",
    "                    # Score metals\n",
    "                    metal_score, metal_matches = score_keyword_matches(all_text, metal_terms)\n",
    "                    for metal in rec.get('metals_from_brenda', []):\n",
    "                        if metal not in metal_matches:\n",
    "                            metal_matches[metal] = 1.0\n",
    "                    \n",
    "                    rec['metals_involved'] = list(metal_matches.keys())\n",
    "                    rec['metal_scores'] = metal_matches\n",
    "                    rec['overall_metal_score'] = float(metal_score)\n",
    "                    \n",
    "                    # Score corrosion mechanisms  \n",
    "                    corrosion_score, corrosion_matches = score_keyword_matches(all_text, corrosion_mechanisms)\n",
    "                    rec['corrosion_mechanisms'] = list(corrosion_matches.keys())\n",
    "                    rec['corrosion_mechanism_scores'] = corrosion_matches\n",
    "                    rec['overall_corrosion_score'] = float(corrosion_score)\n",
    "                    \n",
    "                    # Score synergies\n",
    "                    synergy_score, synergy_matches = score_keyword_matches(all_text, corrosion_synergies)\n",
    "                    rec['corrosion_synergies'] = list(synergy_matches.keys())\n",
    "                    rec['corrosion_synergy_scores'] = synergy_matches\n",
    "                    rec['overall_synergy_score'] = float(synergy_score)\n",
    "                    \n",
    "                    # Score functional categories\n",
    "                    functional_terms = {cat: details['terms'] for cat, details in functional_categories.items()}\n",
    "                    func_score, func_matches = score_keyword_matches(all_text, functional_terms)\n",
    "                    weighted_func_matches = {}\n",
    "                    for cat, match_score in func_matches.items():\n",
    "                        original_weight = functional_categories[cat]['score']\n",
    "                        weighted_func_matches[cat] = match_score * original_weight\n",
    "\n",
    "                    rec['functional_categories'] = [{\"category\": cat, \"score\": score} \n",
    "                                                 for cat, score in weighted_func_matches.items()]\n",
    "                    rec['overall_functional_score'] = float(sum(weighted_func_matches.values()))\n",
    "                    \n",
    "                    # Score organic processes\n",
    "                    organic_score, organic_matches = score_keyword_matches(all_text, organic_categories)\n",
    "                    rec['organic_processes'] = list(organic_matches.keys())\n",
    "                    rec['organic_process_scores'] = organic_matches\n",
    "                    rec['overall_organic_process_score'] = float(organic_score)\n",
    "                    \n",
    "                    # Score keyword groups\n",
    "                    keyword_score, keyword_matches = score_keyword_matches(all_text, corrosion_keyword_groups)\n",
    "                    rec['corrosion_keyword_groups'] = list(keyword_matches.keys())\n",
    "                    rec['corrosion_keyword_scores'] = keyword_matches\n",
    "                    rec['overall_keyword_score'] = float(keyword_score)\n",
    "                    \n",
    "                    # Consolidate metals\n",
    "                    rec['metals_consolidated'] = consolidate_metal_terms(\n",
    "                        rec.get('metals_from_brenda', []),\n",
    "                        rec.get('metals_involved', [])\n",
    "                    )\n",
    "                \n",
    "                # Update statistics (same for both paths)\n",
    "                if rec['metals_consolidated']:\n",
    "                    stats['with_metal_involvement'] += 1\n",
    "            \n",
    "                if rec['corrosion_mechanisms']:\n",
    "                    stats['with_corrosion_mechanisms'] += 1\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error scoring data for {rec.get('ec_number')}: {e}\")\n",
    "                \n",
    "    # Process metal binding in a separate pass\n",
    "    print(\"Processing metal binding information...\")\n",
    "    try:\n",
    "        # Add metal binding information to records\n",
    "        for rec in ec_records:\n",
    "            rec['metal_binding_info'] = {}\n",
    "\n",
    "            # Check metals from BRENDA or detected in text\n",
    "            all_metals = set(rec.get('metals_consolidated', []))\n",
    "\n",
    "            for metal in all_metals:\n",
    "                # Try to map to standard symbol\n",
    "                for metal_name, symbol in metal_mapping.items():\n",
    "                    if metal_name in metal.lower() or symbol.lower() in metal.lower():\n",
    "                        # Check if we have binding data for this metal\n",
    "                        if symbol in metal_patterns.get('residue_binding', {}):\n",
    "                            # Get top binding residues\n",
    "                            residue_counts = metal_patterns['residue_binding'][symbol]\n",
    "                            top_residues = sorted(residue_counts.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "\n",
    "                            rec['metal_binding_info'][symbol] = {'common_residues': [res for res, count in top_residues],  'binding_count': sum(residue_counts.values())}\n",
    "                                                                  \n",
    "                # Check for common coordination geometries\n",
    "                geometries = [key.split('_')[2:] for key, count in metal_patterns['coordination'].items() if key.startswith(f\"{symbol}_\")]\n",
    "                if geometries:\n",
    "                    rec['metal_binding_info'][symbol]['common_geometries'] = geometries[:3]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing metal binding data: {e}\")\n",
    "\n",
    "    # Process KO hierarchy information\n",
    "    print(\"Processing KO hierarchy...\")\n",
    "    if 'D' in ko_hierarchy:\n",
    "        for ko, info in ko_hierarchy.get('D', {}).items():\n",
    "            for ec in info.get('ec_numbers', []):\n",
    "                # find matching records\n",
    "                for rec in ec_records:\n",
    "                    if rec['ec_number'] == ec:\n",
    "                        try:\n",
    "                            parent_c = info.get('parent')\n",
    "                            if parent_c and 'C' in ko_hierarchy and parent_c in ko_hierarchy['C']:\n",
    "                                path_info = ko_hierarchy['C'][parent_c]\n",
    "                                parent_b = path_info.get('parent')\n",
    "                                if parent_b and 'B' in ko_hierarchy and parent_b in ko_hierarchy['B']:\n",
    "                                    hi_category = ko_hierarchy['B'][parent_b].get('name', '')\n",
    "                                    pathway = path_info.get('name', '')\n",
    "\n",
    "                                    # Use sets to efficiently track unique values\n",
    "                                    if pathway and pathway not in rec['pathways']:\n",
    "                                        rec['pathways'].append(pathway)\n",
    "\n",
    "                                    hierarchy = f\"{hi_category} > {pathway}\"\n",
    "                                    if hierarchy and hierarchy not in rec['hierarchy']:\n",
    "                                        rec['hierarchy'].append(hierarchy)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error processing KO hierarchy for {rec.get('ec_number')}: {e}\")\n",
    "    \n",
    "    # Calculate pathway statistics\n",
    "    print(\"Calculating pathway statistics...\")\n",
    "    for rec in ec_records:\n",
    "        if rec['pathways']:\n",
    "            stats['with_pathways'] += 1\n",
    "    \n",
    "    # Calculate final corrosion relevance scores using the scoring module\n",
    "    print(\"Calculating corrosion relevance scores...\")\n",
    "    for rec in ec_records:\n",
    "        try:\n",
    "            enzyme_names = rec.get('enzyme_names', []) or []\n",
    "            enzyme_class_text = rec.get('enzyme_class', '') or ''\n",
    "            pathways = rec.get('pathways', []) or []\n",
    "            \n",
    "            if using_imported_modules:\n",
    "                # Calculate pathway scores using the module function\n",
    "                pathway_score, pathway_category_scores = score_sys.calculate_pathway_score(\n",
    "                    pathways, enzyme_names, enzyme_class_text\n",
    "                )\n",
    "                \n",
    "                # Check for metal-organic synergy\n",
    "                synergy_score = score_sys.check_metal_organic_synergy(\n",
    "                    rec.get('metals_consolidated', []), \n",
    "                    enzyme_names\n",
    "                )\n",
    "                rec['metal_organic_synergy_score'] = synergy_score\n",
    "                \n",
    "                # Calculate final corrosion relevance score using the module function\n",
    "                corrosion_relevance_score, corrosion_relevance = score_sys.calculate_corrosion_relevance_score(\n",
    "                    rec.get('overall_metal_score', 0),\n",
    "                    rec.get('overall_corrosion_score', 0),\n",
    "                    pathway_score,\n",
    "                    rec.get('overall_organic_process_score', 0),\n",
    "                    rec.get('overall_keyword_score', 0),\n",
    "                    synergy_score,\n",
    "                    rec.get('overall_functional_score', 0)\n",
    "                )\n",
    "            else:\n",
    "                # Replicate original pathway scoring logic but using global terms directly\n",
    "                pathway_score = 0\n",
    "                pathway_category_scores = {}\n",
    "                \n",
    "                # Add pathway categories from BRENDA data or calculate them\n",
    "                ec_number = rec.get('ec_number')\n",
    "                if ec_number in brenda_en and 'pathway_categories' in brenda_en[ec_number]:\n",
    "                    # Get pathway categories from BRENDA\n",
    "                    rec['pathway_categories'] = brenda_en[ec_number]['pathway_categories']\n",
    "                    # Score each pathway category\n",
    "                    for category in rec['pathway_categories']:\n",
    "                        pathway_category_scores[category] = 1.0  # Base score of 1.0 for each category\n",
    "                        pathway_score += 1.0\n",
    "                else:\n",
    "                    # If not available from BRENDA, check against pathway terms\n",
    "                    rec['pathway_categories'] = {}\n",
    "                    all_text = ' '.join(rec.get('enzyme_names', '') or []) + ' ' + (rec.get('enzyme_class', '') or '') + ' ' + ' '.join(rec.get('pathways', []))\n",
    "                    all_text = all_text.lower()\n",
    "            \n",
    "                    for category, terms in pathway_categories.items():\n",
    "                        if any(term.lower() in all_text for term in terms):\n",
    "                            rec['pathway_categories'][category] = True\n",
    "                            pathway_category_scores[category] = 1.0\n",
    "                            pathway_score += 1.0\n",
    "\n",
    "                # Define corrosion-relevant categories to check\n",
    "                corrosion_relevant_categories = [\n",
    "                    'iron_sulfur_redox', 'ocre', 'acid_production', \n",
    "                    'electron_transfer', 'biofilm', 'sulfide'\n",
    "                ]\n",
    "                \n",
    "                # Extract terms from relevant categories in corrosion_keyword_groups\n",
    "                corrosion_pathway_terms = []\n",
    "                for category in corrosion_relevant_categories:\n",
    "                    if category in corrosion_keyword_groups:\n",
    "                        corrosion_pathway_terms.extend(corrosion_keyword_groups[category])\n",
    "                \n",
    "                # Check pathway names against these terms\n",
    "                for pathway in rec.get('pathways', []):\n",
    "                    pathway_lower = pathway.lower()\n",
    "                    if any(term.lower() in pathway_lower for term in corrosion_pathway_terms):\n",
    "                        pathway_score += 1\n",
    "                \n",
    "                # Use the organic_acid_metabolism category directly for organic acid terms\n",
    "                organic_acid_terms = pathway_categories.get('organic_acid_metabolism', [])\n",
    "                \n",
    "                name_text = ' '.join(rec.get('enzyme_names', []))\n",
    "                if any(term.lower() in name_text.lower() for term in organic_acid_terms):\n",
    "                    pathway_score += 0.5  # Smaller weight for organic terms alone\n",
    "\n",
    "                # Add synergy bonus for metal-organic combination using terms from global collections\n",
    "                synergy_score = 0\n",
    "                relevant_metals = ['Fe', 'Mn', 'Cu']  # Standard symbols from metal_mapping\n",
    "                \n",
    "                metal_found = False\n",
    "                for metal in rec.get('metals_consolidated', []):\n",
    "                    # Check if the metal is one of our relevant metals\n",
    "                    metal_lower = metal.lower()\n",
    "                    for key, symbol in metal_mapping.items():\n",
    "                        if (key == metal_lower or symbol.lower() == metal_lower) and symbol in relevant_metals:\n",
    "                            metal_found = True\n",
    "                            break\n",
    "                    if metal_found:\n",
    "                        break\n",
    "                \n",
    "                if metal_found and any(term.lower() in name_text.lower() for term in organic_acid_terms):\n",
    "                    synergy_score = 1.0  # Higher weight for metal-organic combination\n",
    "                \n",
    "                rec['metal_organic_synergy_score'] = synergy_score\n",
    "  \n",
    "                # Calculate final score using original logic but with scoring weights from the module\n",
    "                # if available, otherwise use the original hardcoded weights\n",
    "                metal_score = rec.get('overall_metal_score', 0) * 1.5  # METAL_SCORE_WEIGHT\n",
    "                mech_score = rec.get('overall_corrosion_score', 0) * 2.0  # CORROSION_MECHANISM_WEIGHT\n",
    "                process_score = rec.get('overall_organic_process_score', 0) * 1.0  # ORGANIC_PROCESS_WEIGHT\n",
    "                keyword_score = rec.get('overall_keyword_score', 0) * 0.5  # KEYWORD_SCORE_WEIGHT\n",
    "                func_score = rec.get('overall_functional_score', 0) * 0.7  # FUNCTIONAL_SCORE_WEIGHT\n",
    "                weighted_synergy = synergy_score * 0.6  # SYNERGY_SCORE_WEIGHT\n",
    "                \n",
    "                corrosion_relevance_score = float(\n",
    "                    metal_score + \n",
    "                    mech_score + \n",
    "                    pathway_score + \n",
    "                    process_score + \n",
    "                    keyword_score + \n",
    "                    weighted_synergy + \n",
    "                    func_score\n",
    "                )\n",
    "                \n",
    "                # Categorize using original thresholds\n",
    "                if corrosion_relevance_score >= 5:  # HIGH_RELEVANCE_THRESHOLD\n",
    "                    corrosion_relevance = 'high'\n",
    "                elif corrosion_relevance_score >= 2:  # MEDIUM_RELEVANCE_THRESHOLD\n",
    "                    corrosion_relevance = 'medium'\n",
    "                else:\n",
    "                    corrosion_relevance = 'low'\n",
    "            \n",
    "            # Add pathway scores to record (same for both paths)\n",
    "            rec['pathway_category_scores'] = pathway_category_scores\n",
    "            rec['overall_pathway_category_score'] = sum(pathway_category_scores.values())\n",
    "            rec['pathway_score'] = float(pathway_score)\n",
    "            \n",
    "            # Store final scores in record (same for both paths)\n",
    "            rec['corrosion_relevance_score'] = corrosion_relevance_score\n",
    "            rec['corrosion_relevance'] = corrosion_relevance\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating corrosion score for {rec.get('ec_number')}: {e}\")\n",
    "            rec['corrosion_relevance_score'] = 0\n",
    "            rec['corrosion_relevance'] = 'unknown'        \n",
    "\n",
    "    # Filter records without content\n",
    "    print(\"Filtering records...\")\n",
    "    filtered_ec_records = []\n",
    "    for rec in ec_records:\n",
    "        protein_name = (rec.get('protein_name') or \"\").lower()\n",
    "        enzyme_names = rec.get('enzyme_names', [])\n",
    "        enzyme_names = [] if enzyme_names is None else enzyme_names\n",
    "        ec_number = rec.get('ec_number', \"\")\n",
    "\n",
    "        # Condition 1: At least one valid identifier must be present\n",
    "        has_valid_protein = \"uncharacterized\" not in protein_name and len(protein_name) > 2\n",
    "        has_valid_enzyme = any(len(name) > 2 for name in enzyme_names)\n",
    "        has_valid_ec = ec_number.count('.') == 3 and all(part.isdigit() for part in ec_number.split('.'))\n",
    "      \n",
    "        # Condition 2: Check for valuable data that should be preserved\n",
    "        has_mechanisms = len(rec.get('corrosion_mechanisms', [])) > 0\n",
    "        has_pathways = len(rec.get('pathways', [])) > 0\n",
    "        has_metals_consolidated = len(rec.get('metals_consolidated', [])) > 0\n",
    "\n",
    "        # Include record if it meets either condition\n",
    "        if (has_valid_protein or has_valid_enzyme or has_valid_ec) or (has_mechanisms or has_pathways or has_metals_consolidated):\n",
    "            filtered_ec_records.append(rec)\n",
    "\n",
    "    # Replace original list with filtered version\n",
    "    ec_records = filtered_ec_records\n",
    "\n",
    "    # Print summary statistics\n",
    "    print(\"\\nMetabolism Database Summary:\")\n",
    "    print(f\"Total enzyme records: {stats['total_enzymes']}\")\n",
    "    print(f\"Records with BRENDA data: {stats['with_brenda_data']} ({stats['with_brenda_data']/stats['total_enzymes']*100:.1f}%)\")\n",
    "    print(f\"Records with reactions: {stats['with_reactions']} ({stats['with_reactions']/stats['total_enzymes']*100:.1f}%)\")\n",
    "    print(f\"Records with pathways: {stats['with_pathways']} ({stats['with_pathways']/stats['total_enzymes']*100:.1f}%)\")\n",
    "    print(f\"Records with KO terms: {stats['with_ko']} ({stats['with_ko']/stats['total_enzymes']*100:.1f}%)\")\n",
    "    print(f\"Records with corrosion mechanisms: {stats['with_corrosion_mechanisms']} ({stats['with_corrosion_mechanisms']/stats['total_enzymes']*100:.1f}%)\")\n",
    "\n",
    "    # Validate the data - check for missing essential fields\n",
    "    validation_issues = []\n",
    "    for i, rec in enumerate(ec_records):\n",
    "        if not rec.get('ec_number'):\n",
    "            validation_issues.append(f\"Record {i} missing EC number\")\n",
    "        if not rec.get('enzyme_names'):\n",
    "            validation_issues.append(f\"EC {rec.get('ec_number')} missing enzyme names\")\n",
    "\n",
    "    if validation_issues:\n",
    "        print(\"\\nValidation Issues:\")\n",
    "        for issue in validation_issues[:10]:  # Show first 10 issues\n",
    "            print(f\"- {issue}\")\n",
    "        if len(validation_issues) > 10:\n",
    "            print(f\"...and {len(validation_issues) - 10} more issues\")\n",
    "    else:\n",
    "        print(\"\\nValidation: All records have essential fields.\")\n",
    "\n",
    "    return ec_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-19T16:14:53.667694Z",
     "iopub.status.idle": "2025-04-19T16:14:53.668146Z",
     "shell.execute_reply": "2025-04-19T16:14:53.667953Z"
    },
    "id": "G_pX1Jqj4KNZ",
    "outputId": "d252dcc3-018e-4980-f5ee-4d898b4a6c03",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Takes 30 - 240 min\n",
    "#ec_records = create_metabolism_database(sample_size=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metabolism Database Summary:    \n",
    "779.2s\t114\tTotal enzyme records: 8235   \n",
    "779.2s\t115\tRecords with BRENDA data: 6710 (81.5%)   \n",
    "779.2s\t116\tRecords with reactions: 5514 (67.0%)    \n",
    "779.2s\t117\tRecords with pathways: 5964 (72.4%)   \n",
    "779.2s\t118\tRecords with KO terms: 4873 (59.2%)   \n",
    "779.2s\t119\tRecords with corrosion mechanisms: 5818 (70.6%)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-19T16:14:53.669273Z",
     "iopub.status.idle": "2025-04-19T16:14:53.669724Z",
     "shell.execute_reply": "2025-04-19T16:14:53.669527Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "'''# Save to JSON with timing\n",
    "json_path = output_large / \"ec_records.json\"\n",
    "print(f\"Starting JSON save to {json_path}...\")\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(ec_records, f)\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed = end_time - start_time\n",
    "    size_mb = os.path.getsize(json_path) / 1024 / 1024\n",
    "\n",
    "    print(f\"Successfully saved to {json_path} in {elapsed:.2f} seconds ({size_mb:.2f} MB)\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving to JSON: {e}\")'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dictionary done elsewhere due to capacity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-19T16:14:53.670696Z",
     "iopub.status.idle": "2025-04-19T16:14:53.671154Z",
     "shell.execute_reply": "2025-04-19T16:14:53.670957Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#json_path = Path(\"/kaggle/input/ec-records/ec_records.json\")\n",
    "json_path = output_large /\"ec_records.json\"\n",
    "print(f\"Starting JSON load from {json_path}...\")\n",
    "start_time = time.time()\n",
    "\n",
    "with open(json_path, 'r') as f:\n",
    "    ec_records = json.load(f)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed = end_time - start_time\n",
    "print(f\"Loaded {len(ec_records)} records from JSON in {elapsed:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ec_meta = pd.DataFrame(ec_records)\n",
    "ec_meta['pathways']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.4 Flattening Database List of Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-19T16:14:53.672149Z",
     "iopub.status.idle": "2025-04-19T16:14:53.672612Z",
     "shell.execute_reply": "2025-04-19T16:14:53.672417Z"
    },
    "id": "HgVjVgyVjib0",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "\n",
    "def targeted_flatten_ec_records(ec_records):\n",
    "    \"\"\"\n",
    "    Specifically targeted flattening to handle problematic fields in EC records.\n",
    "\n",
    "    Parameters:\n",
    "        ec_records : list of dictionaries containing EC record data\n",
    "\n",
    "    Returns:\n",
    "        list of dictionaries with properly flattened structure\n",
    "    \"\"\"\n",
    "    fixed_records = []\n",
    "\n",
    "    for record in ec_records:\n",
    "        fixed_record = record.copy()  # Start with a copy of the original record\n",
    "\n",
    "        # Fix pathways - convert from list to semicolon-separated string\n",
    "        if 'pathways' in record and isinstance(record['pathways'], list):\n",
    "            fixed_record['pathways'] = '; '.join(str(p) for p in record['pathways'] if p)\n",
    "\n",
    "        # Fix hierarchy - convert from list to semicolon-separated string\n",
    "        if 'hierarchy' in record and isinstance(record['hierarchy'], list):\n",
    "            fixed_record['hierarchy'] = '; '.join(str(h) for h in record['hierarchy'] if h)\n",
    "\n",
    "        # Fix ko - convert from list to semicolon-separated string\n",
    "        if 'ko' in record and isinstance(record['ko'], list):\n",
    "            fixed_record['ko'] = '; '.join(str(k) for k in record['ko'] if k)\n",
    "\n",
    "        # Fix corrosion_keyword_groups - convert from list to semicolon-separated string\n",
    "        if 'corrosion_keyword_groups' in record and isinstance(record['corrosion_keyword_groups'], list):\n",
    "            fixed_record['corrosion_keyword_groups'] = '; '.join(str(g) for g in record['corrosion_keyword_groups'] if g)\n",
    "\n",
    "        # Fix metals_consolidated - convert from list to semicolon-separated string\n",
    "        if 'metals_consolidated' in record and isinstance(record['metals_consolidated'], list):\n",
    "            fixed_record['metals_consolidated'] = '; '.join(str(m) for m in record['metals_consolidated'] if m)\n",
    "\n",
    "        # Fix corrosion_keyword_scores - convert from dict to string representation\n",
    "        if 'corrosion_keyword_scores' in record and isinstance(record['corrosion_keyword_scores'], dict):\n",
    "            # Store the dictionary as a JSON string\n",
    "            fixed_record['corrosion_keyword_scores'] = json.dumps(record['corrosion_keyword_scores'])\n",
    "\n",
    "            # Create individual score columns for each keyword\n",
    "            for keyword, score in record['corrosion_keyword_scores'].items():\n",
    "                safe_name = re.sub(r'[^a-zA-Z0-9_]', '_', keyword.lower())\n",
    "                column_name = f'corrosion_keyword_{safe_name}_score'\n",
    "                fixed_record[column_name] = float(score) if score is not None else None\n",
    "\n",
    "        # Fix pathway_category_scores - convert from dict to string representation\n",
    "        if 'pathway_category_scores' in record and isinstance(record['pathway_category_scores'], dict):\n",
    "            # Store the dictionary as a JSON string\n",
    "            fixed_record['pathway_category_scores'] = json.dumps(record['pathway_category_scores'])\n",
    "\n",
    "            # Create individual score columns for each category\n",
    "            for category, score in record['pathway_category_scores'].items():\n",
    "                safe_name = re.sub(r'[^a-zA-Z0-9_]', '_', category.lower())\n",
    "                column_name = f'pathway_category_{safe_name}_score'\n",
    "                fixed_record[column_name] = float(score) if score is not None else None\n",
    "\n",
    "        # Handle metal_scores if present\n",
    "        if 'metal_scores' in record and isinstance(record['metal_scores'], dict):\n",
    "            # Store the dictionary as a JSON string\n",
    "            fixed_record['metal_scores'] = json.dumps(record['metal_scores'])\n",
    "\n",
    "            # Create individual score columns for each metal\n",
    "            for metal, score in record['metal_scores'].items():\n",
    "                safe_name = re.sub(r'[^a-zA-Z0-9_]', '_', str(metal).lower())\n",
    "                column_name = f'metal_{safe_name}_score'\n",
    "                fixed_record[column_name] = float(score) if score is not None else None\n",
    "\n",
    "        # Handle corrosion_mechanism_scores if present\n",
    "        if 'corrosion_mechanism_scores' in record and isinstance(record['corrosion_mechanism_scores'], dict):\n",
    "            # Store the dictionary as a JSON string\n",
    "            fixed_record['corrosion_mechanism_scores'] = json.dumps(record['corrosion_mechanism_scores'])\n",
    "\n",
    "            # Create individual score columns for each mechanism\n",
    "            for mechanism, score in record['corrosion_mechanism_scores'].items():\n",
    "                safe_name = re.sub(r'[^a-zA-Z0-9_]', '_', str(mechanism).lower())\n",
    "                column_name = f'mechanism_{safe_name}_score'\n",
    "                fixed_record[column_name] = float(score) if score is not None else None\n",
    "\n",
    "        # Handle reactions - could be complex structure\n",
    "        if 'reactions' in record and isinstance(record['reactions'], list):\n",
    "            # For reactions, we'll just store as JSON since it contains complex nested structure\n",
    "            fixed_record['reactions'] = json.dumps(record['reactions'])\n",
    "\n",
    "        # Handle functional_categories specially if it contains dictionaries\n",
    "        if 'functional_categories' in record:\n",
    "            if isinstance(record['functional_categories'], list):\n",
    "                # Check if it's a list of dictionaries with 'category' and 'score' keys\n",
    "                categories = []\n",
    "                for item in record['functional_categories']:\n",
    "                    if isinstance(item, dict) and 'category' in item:\n",
    "                        categories.append(item['category'])\n",
    "\n",
    "                        # Add score field if available\n",
    "                        if 'score' in item:\n",
    "                            safe_name = re.sub(r'[^a-zA-Z0-9_]', '_', str(item['category']).lower())\n",
    "                            column_name = f'func_{safe_name}_score'\n",
    "                            fixed_record[column_name] = float(item['score']) if item['score'] is not None else None\n",
    "\n",
    "                # Join categories for display\n",
    "                fixed_record['functional_categories'] = '; '.join(categories)\n",
    "            elif isinstance(record['functional_categories'], str):\n",
    "                # Already a string, keep as is\n",
    "                pass\n",
    "            else:\n",
    "                # Convert to string representation\n",
    "                fixed_record['functional_categories'] = str(record['functional_categories'])\n",
    "\n",
    "        # Handle corrosion_synergies\n",
    "        if 'corrosion_synergies' in record:\n",
    "            if isinstance(record['corrosion_synergies'], list):\n",
    "                fixed_record['corrosion_synergies'] = '; '.join(str(s) for s in record['corrosion_synergies'] if s)\n",
    "            else:\n",
    "                fixed_record['corrosion_synergies'] = str(record['corrosion_synergies'])\n",
    "\n",
    "        # Ensure overall scores are properly handled\n",
    "        for score_field in ['overall_metal_score', 'overall_corrosion_score',\n",
    "                          'overall_functional_score', 'overall_keyword_score',\n",
    "                          'overall_organic_process_score', 'overall_synergy_score',\n",
    "                          'overall_pathway_category_score', 'corrosion_relevance_score']:\n",
    "            if score_field in record and record[score_field] is not None:\n",
    "                try:\n",
    "                    fixed_record[score_field] = float(record[score_field])\n",
    "                except (ValueError, TypeError):\n",
    "                    fixed_record[score_field] = record[score_field]\n",
    "\n",
    "        fixed_records.append(fixed_record)\n",
    "\n",
    "    return fixed_records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-19T16:14:53.674573Z",
     "iopub.status.idle": "2025-04-19T16:14:53.674938Z",
     "shell.execute_reply": "2025-04-19T16:14:53.674791Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ec_records_flat =targeted_flatten_ec_records(ec_records)\n",
    "\n",
    "ec_metadata = pd.DataFrame(ec_records_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-19T16:14:53.678516Z",
     "iopub.status.idle": "2025-04-19T16:14:53.678857Z",
     "shell.execute_reply": "2025-04-19T16:14:53.678717Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save to JSON with timing\n",
    "json_path = output_large / \"ec_records_flat.json\"\n",
    "print(f\"Starting JSON save to {json_path}...\")\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(ec_records_flat, f)\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed = end_time - start_time\n",
    "    size_mb = os.path.getsize(json_path) / 1024 / 1024\n",
    "\n",
    "    print(f\"Successfully saved to {json_path} in {elapsed:.2f} seconds ({size_mb:.2f} MB)\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving to JSON: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T10:28:39.075778Z",
     "iopub.status.busy": "2025-04-21T10:28:39.075331Z",
     "iopub.status.idle": "2025-04-21T10:28:40.322773Z",
     "shell.execute_reply": "2025-04-21T10:28:40.321414Z",
     "shell.execute_reply.started": "2025-04-21T10:28:39.075746Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load from JSON with timing\n",
    "json_path = output_large/ \"ec_records_flat.json\"\n",
    "print(f\"Starting JSON load from {json_path}...\")\n",
    "start_time = time.time()\n",
    "\n",
    "with open(json_path, 'r') as f:\n",
    "    ec_records_flat = json.load(f)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed = end_time - start_time\n",
    "print(f\"Loaded {len(ec_records_flat)} records from JSON in {elapsed:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metabolism Database Summary:\n",
    "779.2s\t114\tTotal enzyme records: 8235\n",
    "779.2s\t115\tRecords with BRENDA data: 6710 (81.5%)\n",
    "779.2s\t116\tRecords with reactions: 5514 (67.0%)\n",
    "779.2s\t117\tRecords with pathways: 5964 (72.4%)\n",
    "779.2s\t118\tRecords with KO terms: 4873 (59.2%)\n",
    "779.2s\t119\tRecords with corrosion mechanisms: 5818 (70.6%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2TO-o4FC3933"
   },
   "source": [
    "Loaded pathway mappings for 3889 EC numbers   \n",
    "Loaded: 8235 enzymes, 578 pathways, 6710 BRENDA entries   \n",
    "Added pathway information to 3888 records   \n",
    "Extracted coordination patterns for 1586 metal-coordination environments   \n",
    "Extracted residue binding patterns for 65 metals   \n",
    "\n",
    "Metabolism Database Summary:   \n",
    "Total enzyme records: 8235   \n",
    "Records with BRENDA data: 6710 (81.5%)  \n",
    "Records with reactions: 6405 (77.8%)  \n",
    "Records with pathways: 3888 (47.2%)  \n",
    "Records with KO terms: 4873 (59.2%)  \n",
    "Records with metal involvement: 7873 (95.6%)  \n",
    "Records with corrosion mechanisms: 6131 (74.5%)  \n",
    "\n",
    "Validation: All records have essential fields..  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3n7sW03WDVOP"
   },
   "source": [
    "__Sample of ec_records db list of dictionaries__\n",
    "ec_records: [{'ec_number': '1.1.1.1',  \n",
    "  'enzyme_names': ['alcohol dehydrogenase',  \n",
    "   'aldehyde reductase',  \n",
    "   'ADH',  \n",
    "   'alcohol dehydrogenase (NAD)',  \n",
    "   'aliphatic alcohol dehydrogenase',  \n",
    "   'ethanol dehydrogenase',  \n",
    "   'NAD-dependent alcohol dehydrogenase',  \n",
    "   'NAD-specific aromatic alcohol dehydrogenase',  \n",
    "   'NADH-alcohol dehydrogenase',  \n",
    "   'NADH-aldehyde dehydrogenase',  \n",
    "   'primary alcohol dehydrogenase',  \n",
    "   'yeast alcohol dehydrogenase'],  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ec_records[0].keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T10:27:32.418195Z",
     "iopub.status.busy": "2025-04-21T10:27:32.417838Z",
     "iopub.status.idle": "2025-04-21T10:27:33.434234Z",
     "shell.execute_reply": "2025-04-21T10:27:33.433037Z",
     "shell.execute_reply.started": "2025-04-21T10:27:32.418168Z"
    },
    "id": "spPAz5h286uL",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ECcontri_Uniprot_path = output_large / 'ECcontri_Uniprot.parquet'\n",
    "ECcontri_Uniprot = pd.read_parquet(ECcontri_Uniprot_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.5 Enrich ECcontri_Uniprot Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6zqpU2g8Xpkv"
   },
   "outputs": [],
   "source": [
    "## 9.6.  Building Enriched Dataframe of ECcontri\n",
    "\n",
    "def enrich_eccontri_data(eccontri_df, ec_records_flat):\n",
    "    \"\"\"\n",
    "    Enrich the ECcontri_Uniprot dataframe with complete information from ec_records dictionary\n",
    "    and apply scoring to rows not already scored from ec_records.\n",
    "\n",
    "    It searches protein names, enzyme names and EC numbers for metadata to enrich the original eccontri.\n",
    "    When it finds an Uncharacterised protein, it replaces this name with the enzyme names corresponding to the EC number.\n",
    "    After enrichment with ec_records data, it uses imported scoring modules to calculate scores for rows\n",
    "    that didn't receive scoring data during the enrichment process.\n",
    "\n",
    "    Parameters:    eccontri_df : pandas DataFrame original ECcontri_Uniprot data with EC numbers in format EC:x.x.x.x\n",
    "                ec_records : list of Dictionary where keys are EC numbers (without 'EC:' prefix) and values are metadata dictionaries\n",
    "\n",
    "    Returns:       enriched_df : pandas DataFrame with additional metadata columns and scoring applied\n",
    "    \"\"\"\n",
    "    # Try to import scoring modules - adding support for both environments\n",
    "    try:\n",
    "        # First try the kaggle path\n",
    "        sys.path.append('/kaggle/input/corrosion-scoring')\n",
    "        from global_terms import (\n",
    "            metal_terms, \n",
    "            corrosion_mechanisms, \n",
    "            pathway_categories, \n",
    "            organic_categories,\n",
    "            corrosion_synergies, \n",
    "            functional_categories, \n",
    "            corrosion_keyword_groups, \n",
    "            metal_mapping\n",
    "        )\n",
    "        import scoring_system as score_sys\n",
    "        using_imported_modules = True\n",
    "        print(\"Successfully imported scoring modules from Kaggle path\")\n",
    "    except ImportError:\n",
    "        try:\n",
    "            # Then try the local path\n",
    "            sys.path.append('/home/beatriz/MIC/2_Micro/corrosion_scoring')\n",
    "            from corrosion_scoring.global_terms import (\n",
    "                metal_terms, \n",
    "                corrosion_mechanisms, \n",
    "                pathway_categories, \n",
    "                organic_categories,\n",
    "                corrosion_synergies, \n",
    "                functional_categories, \n",
    "                corrosion_keyword_groups, \n",
    "                metal_mapping\n",
    "            )\n",
    "            import corrosion_scoring.scoring_system as score_sys\n",
    "            using_imported_modules = True\n",
    "            print(\"Successfully imported scoring modules from local path\")\n",
    "        except ImportError as e:\n",
    "            print(f\"Warning: Could not import scoring modules: {e}\")\n",
    "            using_imported_modules = False\n",
    "\n",
    "    # Make a copy to avoid modifying the original\n",
    "    enriched_df = eccontri_df.copy()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Create dictionaries for faster lookups\n",
    "    print(\"Creating lookup dictionaries...\")\n",
    "\n",
    "    # EC number dictionary flaten\n",
    "    ec_dict = {record['ec_number']: record for record in ec_records_flat if 'ec_number' in record}\n",
    "    print(f\"Created EC dictionary with {len(ec_dict)} entries\")\n",
    "\n",
    "    # Protein name dictionary\n",
    "    protein_name_dict = {}\n",
    "    for record in ec_records_flat:\n",
    "        enzyme_names_str = record.get('enzyme_names', '')\n",
    "        if enzyme_names_str:\n",
    "            # Handle both string and list formats\n",
    "            if isinstance(enzyme_names_str, list):\n",
    "                names = enzyme_names_str\n",
    "            else:\n",
    "                # Flattened records will have enzyme_names as semicolon-separated string\n",
    "                names = [name.strip() for name in enzyme_names_str.split(';')]\n",
    "            \n",
    "            for name in names:\n",
    "                if name:  # Skip empty names\n",
    "                    protein_name_dict[name.lower()] = record\n",
    "    print(f\"Created protein name dictionary with {len(protein_name_dict)} entries\")\n",
    "\n",
    "    # Create a mapping dictionary to store all matches\n",
    "    idx_to_metadata = {}\n",
    "\n",
    "    # Add all metadata columns\n",
    "    metadata_columns = ['enzyme_names', 'enzyme_class', 'pathways', 'hierarchy', 'metals_consolidated', 'corrosion_mechanisms', 'corrosion_relevance_score', \n",
    "                        'corrosion_relevance', 'metal_scores', 'overall_metal_score',  'corrosion_mechanism_scores', 'overall_corrosion_score', \n",
    "                        'functional_categories', 'overall_functional_score', 'corrosion_keyword_groups',  \n",
    "                        'corrosion_keyword_scores', 'overall_keyword_score', 'corrosion_synergies',\n",
    "                        'organic_processes', 'overall_organic_process_score']\n",
    "    for col in metadata_columns:\n",
    "        enriched_df[col] = None\n",
    "\n",
    "    # a boolean 'has_metal' column\n",
    "    enriched_df['has_metal'] = False\n",
    "\n",
    "    # Define progress reporting\n",
    "    total_rows = len(enriched_df)\n",
    "    log_interval = max(1, min(10000, total_rows // 20))  # Log at most 20 times, minimum every 10000 rows\n",
    "\n",
    "    print(f\"Processing {total_rows} rows with logging every {log_interval} rows\")\n",
    "\n",
    "    # try protein name matches\n",
    "    print(\"Performing protein name matches...\")\n",
    "    # Get rows without EC+Genus matches\n",
    "    remaining_indices = set(enriched_df.index) - set(idx_to_metadata.keys())\n",
    "    mask_remaining = enriched_df.index.isin(remaining_indices)\n",
    "    mask_valid_protein = enriched_df['protein_name'].notna() & (enriched_df['protein_name'] != \"Uncharacterized protein\")\n",
    "    mask_protein_match = mask_remaining & mask_valid_protein\n",
    "\n",
    "    # This part still needs row-by-row processing for fuzzy matching\n",
    "    protein_matches = 0\n",
    "    for idx in enriched_df.index[mask_protein_match]:\n",
    "        if idx % log_interval == 0:\n",
    "            print(f\"Processing protein matches: row {idx}/{total_rows} ({idx/total_rows*100:.1f}%)\")\n",
    "            \n",
    "        protein_name = enriched_df.loc[idx, 'protein_name'].lower()\n",
    "\n",
    "        # Direct lookup in protein name dictionary\n",
    "        if protein_name in protein_name_dict:\n",
    "            idx_to_metadata[idx] = protein_name_dict[protein_name]\n",
    "            protein_matches += 1\n",
    "        else:\n",
    "            # Try partial matches\n",
    "            for name, record in protein_name_dict.items():\n",
    "                if name in protein_name or protein_name in name:\n",
    "                    idx_to_metadata[idx] = record\n",
    "                    protein_matches += 1\n",
    "                    break\n",
    "\n",
    "    print(f\"Found {protein_matches} protein name matches\")\n",
    "\n",
    "    # For any remaining rows, try EC-only matching\n",
    "    print(\"Performing EC-only matches...\")\n",
    "    # Get rows without matches so far\n",
    "    remaining_indices = set(enriched_df.index) - set(idx_to_metadata.keys())\n",
    "    mask_remaining = enriched_df.index.isin(remaining_indices)\n",
    "    mask_valid_ec = enriched_df['EC'].notna()\n",
    "    mask_ec_match = mask_remaining & mask_valid_ec\n",
    "\n",
    "    ec_only_matches = 0\n",
    "    for idx in enriched_df.index[mask_ec_match]:\n",
    "        if idx % log_interval == 0:\n",
    "            print(f\"Processing EC matches: row {idx}/{total_rows} ({idx/total_rows*100:.1f}%)\")\n",
    "            \n",
    "        ec_num = enriched_df.loc[idx, 'EC']\n",
    "        if ec_num in ec_dict:\n",
    "            idx_to_metadata[idx] = ec_dict[ec_num]\n",
    "            ec_only_matches += 1\n",
    "\n",
    "    print(f\"Found {ec_only_matches} EC-only matches\")\n",
    "\n",
    "    # Apply all metadata in one go based on the matches we found\n",
    "    print(\"Applying metadata to matched rows...\")\n",
    "    for idx, metadata in idx_to_metadata.items():\n",
    "        if idx % log_interval == 0:\n",
    "            print(f\"Applying metadata: row {idx}/{total_rows} ({idx/total_rows*100:.1f}%)\")\n",
    "\n",
    "        # Only proceed if we have metadata (either from EC or from protein/enzyme name)\n",
    "        if metadata is not None:\n",
    "\n",
    "            # If protein_name is missing or uncharacterized, try to fill it with enzyme_names\n",
    "            if pd.isna(enriched_df.at[idx, 'protein_name']) or enriched_df.at[idx, 'protein_name'].lower() == \"uncharacterized protein\":\n",
    "                if 'enzyme_names' in metadata and metadata['enzyme_names']:\n",
    "                    # Handle both flattened (string) and non-flattened (list) records\n",
    "                    if isinstance(metadata['enzyme_names'], list):\n",
    "                        enriched_df.at[idx, 'protein_name'] = '; '.join(metadata['enzyme_names'])\n",
    "                    else:\n",
    "                        enriched_df.at[idx, 'protein_name'] = str(metadata['enzyme_names'])\n",
    "\n",
    "            # Add basic metadata\n",
    "            for field in ['enzyme_names', 'enzyme_class', 'pathways', 'hierarchy', 'corrosion_mechanisms',\n",
    "                          'functional_categories',  'corrosion_keyword_groups', 'corrosion_synergies', 'organic_processes']:\n",
    "                if field in metadata and metadata[field]:\n",
    "                    if isinstance(metadata[field], list):\n",
    "                        enriched_df.at[idx, field] = '; '.join(str(v) for v in metadata[field])\n",
    "                    else:\n",
    "                        enriched_df.at[idx, field] = str(metadata[field])\n",
    "            \n",
    "            # add corrosion_relevance category:\n",
    "            if 'corrosion_relevance' in metadata:\n",
    "                enriched_df.at[idx, 'corrosion_relevance'] = metadata['corrosion_relevance']\n",
    "\n",
    "            # add the consolidated metals field:\n",
    "            if 'metals_consolidated' in metadata and metadata['metals_consolidated']:\n",
    "                if isinstance(metadata['metals_consolidated'], list):\n",
    "                    enriched_df.at[idx, 'metals_consolidated'] = '; '.join(metadata['metals_consolidated'])\n",
    "                    # Set has_metal flag based on whether metals exist\n",
    "                    enriched_df.at[idx, 'has_metal'] = len(metadata['metals_consolidated']) > 0\n",
    "                else:\n",
    "                    enriched_df.at[idx, 'metals_consolidated'] = str(metadata['metals_consolidated'])\n",
    "                    # Set has_metal flag for string case too\n",
    "                    enriched_df.at[idx, 'has_metal'] = bool(metadata['metals_consolidated'])\n",
    "\n",
    "            # Adding the scores for each component, simple score fields, handle in a loop\n",
    "            score_fields = ['overall_metal_score', 'overall_corrosion_score', 'overall_functional_score', \n",
    "                            'overall_keyword_score', 'overall_organic_process_score', 'overall_synergy_score',\n",
    "                            'corrosion_relevance_score']\n",
    "            \n",
    "            for field in score_fields:\n",
    "                if field in metadata:\n",
    "                    try:\n",
    "                        enriched_df.at[idx, field] = float(metadata.get(field, 0))\n",
    "                    except (ValueError, TypeError):\n",
    "                        print(f\"Row {idx}: Could not convert {field} to float\")\n",
    "                        enriched_df.at[idx, field] = None\n",
    "            \n",
    "            # For dictionary score fields, handle in a loop too\n",
    "            dict_fields = ['metal_scores', 'corrosion_mechanism_scores', 'corrosion_keyword_scores', \n",
    "                           'organic_process_scores', 'corrosion_synergy_scores']\n",
    "            \n",
    "            for field in dict_fields:\n",
    "                if field in metadata:\n",
    "                    enriched_df.at[idx, field] = json.dumps(metadata.get(field, {}))\n",
    "                            \n",
    "    # Now perform scoring for all rows, regardless of whether they got metadata from EC records\n",
    "    if using_imported_modules:\n",
    "        print(\"Calculating scores for rows with missing score data...\")\n",
    "        scores_calculated = 0\n",
    "        scores_already_present = 0\n",
    "        \n",
    "        for idx in enriched_df.index:\n",
    "            if idx % log_interval == 0:\n",
    "                print(f\"Processing scoring: row {idx}/{total_rows} ({idx/total_rows*100:.1f}%)\")\n",
    "            \n",
    "            # Check if this row already has scoring data\n",
    "            if (enriched_df.at[idx, 'overall_metal_score'] is not None and \n",
    "                enriched_df.at[idx, 'overall_corrosion_score'] is not None and\n",
    "                enriched_df.at[idx, 'corrosion_relevance_score'] is not None):\n",
    "                scores_already_present += 1\n",
    "                continue  # Skip rows that already have scores\n",
    "                \n",
    "            # Only calculate scores if we have some text to analyze\n",
    "            protein_name = enriched_df.at[idx, 'protein_name']\n",
    "            if protein_name is not None and not pd.isna(protein_name):\n",
    "                try:\n",
    "                    # Generate text to analyze\n",
    "                    analysis_text = protein_name\n",
    "                    \n",
    "                    # Add enzyme class if available\n",
    "                    enzyme_class = enriched_df.at[idx, 'enzyme_class'] \n",
    "                    if enzyme_class is not None and not pd.isna(enzyme_class):\n",
    "                        analysis_text += ' ' + enzyme_class\n",
    "                    \n",
    "                    # Add pathways if available\n",
    "                    pathways = enriched_df.at[idx, 'pathways']\n",
    "                    if pathways is not None and not pd.isna(pathways):\n",
    "                        analysis_text += ' ' + pathways\n",
    "                        \n",
    "                    # Calculate scores using the scoring system\n",
    "                    score_results = score_sys.calculate_overall_scores(analysis_text)\n",
    "                    \n",
    "                    # Update scores in the dataframe\n",
    "                    for key, value in score_results.items():\n",
    "                        if key in enriched_df.columns:\n",
    "                            if isinstance(value, list):\n",
    "                                enriched_df.at[idx, key] = '; '.join(map(str, value))\n",
    "                            elif isinstance(value, dict):\n",
    "                                enriched_df.at[idx, key] = json.dumps(value)\n",
    "                            else:\n",
    "                                enriched_df.at[idx, key] = value\n",
    "                    \n",
    "                    # Calculate corrosion relevance explicitly\n",
    "                    corrosion_relevance_score, corrosion_relevance = score_sys.calculate_corrosion_relevance_score(\n",
    "                        score_results.get('overall_metal_score', 0),\n",
    "                        score_results.get('overall_corrosion_score', 0),\n",
    "                        score_results.get('overall_organic_process_score', 0),\n",
    "                        score_results.get('overall_keyword_score', 0),\n",
    "                        score_results.get('overall_synergy_score', 0),\n",
    "                        score_results.get('overall_functional_score', 0)\n",
    "                    )\n",
    "                    enriched_df.at[idx, 'corrosion_relevance_score'] = corrosion_relevance_score\n",
    "                    enriched_df.at[idx, 'corrosion_relevance'] = corrosion_relevance\n",
    "                    \n",
    "                    scores_calculated += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error calculating scores for row {idx}: {e}\")\n",
    "        \n",
    "        print(f\"Scoring complete: {scores_calculated} rows scored, {scores_already_present} rows already had scores\")\n",
    "                      \n",
    "    # Final report\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    print(f\"Completed enrichment in {total_time:.2f} seconds\")\n",
    "    print(f\"Processed {total_rows} rows at {total_rows/total_time:.1f} rows/second\")\n",
    "\n",
    "    # Count non-null values in the metadata columns to see success rate\n",
    "    metadata_counts = {col: enriched_df[col].notnull().sum() for col in metadata_columns}\n",
    "    print(\"\\nMetadata population statistics:\")\n",
    "    for col, count in metadata_counts.items():\n",
    "        print(f\"  {col}: {count} rows ({count/total_rows*100:.1f}%)\")\n",
    "\n",
    "    return enriched_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_ECcontri_Uniprot_enriched= enrich_eccontri_data(ECcontri_Uniprot, ec_records_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-19T16:14:53.691145Z",
     "iopub.status.idle": "2025-04-19T16:14:53.691643Z",
     "shell.execute_reply": "2025-04-19T16:14:53.691438Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pre_path = output_large / \"pre_ECcontri_Uniprot_enriched.parquet\"\n",
    "pre_ECcontri_Uniprot_enriched.to_parquet(pre_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-19T16:14:53.694224Z",
     "iopub.status.idle": "2025-04-19T16:14:53.694704Z",
     "shell.execute_reply": "2025-04-19T16:14:53.694505Z"
    },
    "id": "zOraHS9I8Adf",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Count occurrences of each value in both columns\n",
    "enzyme_counts = pre_ECcontri_Uniprot_enriched['enzyme_names'].value_counts()\n",
    "protein_counts = pre_ECcontri_Uniprot_enriched['protein_name'].value_counts()\n",
    "\n",
    "# Get the number of unique values in each column\n",
    "enzyme_unique_count = pre_ECcontri_Uniprot_enriched['enzyme_names'].nunique()\n",
    "protein_unique_count = pre_ECcontri_Uniprot_enriched['protein_name'].nunique()\n",
    "\n",
    "# Print the results\n",
    "print(\"Enzyme Names - Unique Values:\", enzyme_unique_count)\n",
    "print(\"Enzyme Names - Top 5 Occurrences:\\n\", enzyme_counts.head())\n",
    "print(\"\\nProtein Names - Unique Values:\", protein_unique_count)\n",
    "print(\"Protein Names - Top 5 Occurrences:\\n\", protein_counts.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G4hqE4YG4vvU"
   },
   "source": [
    "## 9.6 Preparing the Enriched DF for entering the Pipeline\n",
    "Before the data enters the pipeline it was noticed that around 18% of the protein-names were Uncharacterized proteins that is because the retrieval from uniprot was no totally suscessful, however in the process of enriching the data, the enzyme name was added with many other entities and so, the already enriched data will have those entries known as Uncharacterized, replaced by the enzyme name that was retrieved from the many other db and compiled on ec_records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJcp5J3rGES3"
   },
   "source": [
    "### Assign unique identifier\n",
    "- Each unique combination of Site, Genus, and Protein name is assigned a unique integer ID\n",
    "- IDs are generated using pandas' groupby().ngroup() function, which creates sequential integers\n",
    "- A separate mapping file (protein_genus_site_id_mapping.csv) preserves the connection between IDs and the original values for traceability\n",
    "- Add a flag column to know which of the last protein was name replaced\n",
    "### Replacing uncharacterized_proteins names with Enzyme names\n",
    "Since the amount of Uncharacterized_proteins is around 270.000 out of 1.5 millon those are around 20%, that are large difference and the strategy here used is to  replace uncharacterized proteins with enzyme names. This is razonable since enzyme names often better reflect the protein's function and from an analysis perspective view the enzyme names as proteins are more meaningful for analysis than \"Uncharacterized protein. Ultimately this approach is widely used in bioinformatics when working with draft genomes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_path = output_large / \"pre_ECcontri_Uniprot_enriched.parquet\" \n",
    "pre_ECcontri_Uniprot_enriched = pd.read_parquet(pre_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-19T16:14:53.697223Z",
     "iopub.status.idle": "2025-04-19T16:14:53.697690Z",
     "shell.execute_reply": "2025-04-19T16:14:53.697518Z"
    },
    "id": "uTiNDJ2RF59p",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ No duplicates found with Sites-Genus-EC criteria\n",
      "Original proteins have IDs 0-1491287\n",
      "Uncharacterized proteins have IDs 1491288+\n",
      "\n",
      "=== ID Range Verification ===\n",
      "First 5 IDs: [0, 1, 2, 3, 4]\n",
      "Last 5 IDs: [1491283, 1491284, 1491285, 1491286, 1491287]\n",
      "Last protein names:\n",
      "1491283    cobaltochelatase; hydrogenobyrinic acid a,c-di...\n",
      "1491284           cobaltochelatase subunit cobt (ec 6.6.1.2)\n",
      "1491285    cobaltochelatase; hydrogenobyrinic acid a,c-di...\n",
      "1491286           cobaltochelatase subunit cobn (ec 6.6.1.2)\n",
      "1491287           cobaltochelatase subunit cobs (ec 6.6.1.2)\n",
      "Name: protein_name, dtype: object\n",
      "\n",
      "=== Starting Protein Correction ===\n",
      "Input shape: (1491288, 38)\n",
      "=== Replacing uncharacterized ===\n",
      "✓ All rows remain unique based on Sites-Genus-EC\n"
     ]
    }
   ],
   "source": [
    "# Define category dict outside\n",
    "category_dict = Integrated_T.T.iloc[0, 0:-1].to_dict()\n",
    "\n",
    "# Define colors and categories\n",
    "category_colors = {1: '#008800',  # Dark green\n",
    "                   2: '#FF8C00',  # Dark orange\n",
    "                   3: '#FF0000'}   # Red\n",
    "\n",
    "categories_labels = {1: 'Normal Operation',\n",
    "              2: 'Early Warning',\n",
    "              3: 'System Failure'}\n",
    "# Add Category on eccontry\n",
    "cat_ECcontri_Uniprot_enriched = pre_ECcontri_Uniprot_enriched.reset_index()\n",
    "cat_ECcontri_Uniprot_enriched['Category'] = cat_ECcontri_Uniprot_enriched['Sites'].map(category_dict)\n",
    "\n",
    "\n",
    "def assign_unique_identifier(df):\n",
    "    \"\"\"\n",
    "    Assigns a unique identifier to all protein-genus pairs using Site, Category, Genus,\n",
    "    and protein_name (including replaced enzyme names).\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Flag uncharacterized proteins\n",
    "    df['was_uncharacterized'] = df['protein_name'] == 'Uncharacterized protein'\n",
    "\n",
    "    # First check for true duplicates using the correct unique combination\n",
    "    exact_dups = df.duplicated(['Sites', 'Genus', 'EC']).sum()\n",
    "    if exact_dups > 0:\n",
    "        print(f\"Warning: Found {exact_dups} duplicate rows based on Sites-Genus-EC\")\n",
    "    else:\n",
    "        print(\"✓ No duplicates found with Sites-Genus-EC criteria\")\n",
    "\n",
    "    # Split the dataframe into two parts\n",
    "    characterized = df[~df['was_uncharacterized']].copy()\n",
    "    uncharacterized = df[df['was_uncharacterized']].copy()\n",
    "\n",
    "    # Assign sequential IDs\n",
    "    characterized['idx'] = range(len(characterized))\n",
    "    uncharacterized['idx'] = range(len(characterized), len(characterized) + len(uncharacterized))\n",
    "\n",
    "    # Record the cutoff point\n",
    "    unchar_id_start = len(characterized)\n",
    "    print(f\"Original proteins have IDs 0-{unchar_id_start-1}\")\n",
    "    print(f\"Uncharacterized proteins have IDs {unchar_id_start}+\")\n",
    "\n",
    "    # Recombine the dataframes (uncharacterized at the end)\n",
    "    sorted_df = pd.concat([characterized, uncharacterized], ignore_index=True)\n",
    "    # After recombination, verify ordering\n",
    "    print(\"\\n=== ID Range Verification ===\")\n",
    "    print(f\"First 5 IDs: {sorted_df.index[:5].tolist()}\")\n",
    "    print(f\"Last 5 IDs: {sorted_df.index[-5:].tolist()}\")\n",
    "    print(f\"Last protein names:\\n{sorted_df.tail(5)['protein_name']}\")\n",
    "    # Create mapping dataframe for reference\n",
    "    id_mapping = sorted_df[['idx', 'Sites', 'Category', 'Genus', 'protein_name', 'EC', 'abund_contri', 'was_uncharacterized']].drop_duplicates()\n",
    "\n",
    "    # Set index and sort by ID (to ensure order is preserved)\n",
    "    sorted_df = sorted_df.set_index('idx').sort_index()\n",
    "\n",
    "    return sorted_df, id_mapping\n",
    "id_ECcontri_Uniprot_enriched, id_mapping = assign_unique_identifier(cat_ECcontri_Uniprot_enriched)\n",
    "\n",
    "def correct_uncharacterized_proteins(df):\n",
    "    \"\"\"\n",
    "    Corrects 'Uncharacterized protein' entries and handles abundance-aware duplicates.\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Starting Protein Correction ===\")\n",
    "    print(f\"Input shape: {df.shape}\")\n",
    "\n",
    "    # Create a copy\n",
    "    corrected_df = df.copy(deep=False)\n",
    "\n",
    "    # Replacement\n",
    "    print(\"=== Replacing uncharacterized ===\")\n",
    "    unchar_mask = corrected_df['protein_name'] == 'Uncharacterized protein'\n",
    "    valid_mask = corrected_df['enzyme_names'].notna() & (corrected_df['enzyme_names'] != '')\n",
    "    corrected_df.loc[unchar_mask & valid_mask, 'protein_name'] = corrected_df.loc[unchar_mask & valid_mask, 'enzyme_names']\n",
    "\n",
    "    # Check for duplicates with accurate criteria\n",
    "    duplicates = corrected_df.duplicated(['Sites', 'Genus', 'EC'], keep=False)\n",
    "    if duplicates.any():\n",
    "        print(f\"Warning: Found {duplicates.sum()} duplicates after correction\")\n",
    "    else:\n",
    "        print(\"✓ All rows remain unique based on Sites-Genus-EC\")\n",
    "    \n",
    "    corrected_df.sort_values('idx', inplace=True)\n",
    "    \n",
    "    return corrected_df\n",
    "\n",
    "# Apply correction\n",
    "ECcontri_Uniprot_enriched = correct_uncharacterized_proteins(id_ECcontri_Uniprot_enriched)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The replacement of the Uncharacterised protein names has been suscessful and now it has been replaced by the enzyme names. The problem with the duplicates has been resolved. The uncharacterized proteins were 18% now there are none. The id were left as plain numbers so to make the computational load smaller since we working with small space more over the id is mean to trace done the combinations and it is no really involved in the calculations, it is more for tracing and mapping at the end.\n",
    "\n",
    "l-Histidine is one of the 20 standard proteinogenic amino acids present in proteins of all living organisms. Histidine biosynthesis seems to be conserved in all organisms including archaea (Lee et al., 2008),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-19T16:14:53.706545Z",
     "iopub.status.idle": "2025-04-19T16:14:53.706878Z",
     "shell.execute_reply": "2025-04-19T16:14:53.706750Z"
    },
    "id": "pf0TmrNPwCuM",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31970"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del cat_ECcontri_Uniprot_enriched\n",
    "del id_ECcontri_Uniprot_enriched\n",
    "#del pre_ECcontri_Uniprot_enriched\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.7 Symplifying Protein_name values\n",
    "\n",
    "The full pipeline was displayed with too many repeated names on the protein_name side, that is the reason why it was decided to simplify the protein_name keeping the biological meaning without removing too much. We remove words such as \"probable\", \"putative\", \"possible\", \"uncharacterized\", \"hypothetical\", etc.\n",
    "→ These terms indicate annotation uncertainty but are not part of the function name. Additionaly EC was remove which are already available on a different column, if required. And since the original protein_name values contain multiple names that are suppose to be synonim and annotated according to the enviroment, it was necesary to take a decision to be able to leverage this information without overhelming the plot because of the long name. The selected name out of the whole variety is the first as widerly accepted or cannonical name. The spaces extra were cleaned whitespace at start/end and between words and it was lowercased. Several runs to the whole pipeline including the notebook 7 were necesary to really detect the names and the removals necesary. The first core name up to the first parenthesis or bracket only if what follows is clearly additional information (e.g., enzyme classification, isoforms) and keep all text if the parentheses are truly part of the main name.\n",
    "Lastly the depest names were remove as redundant. These are selected examples 2,3 of the protein_name before\n",
    "|idx| protein_name|\n",
    "|--|--|\n",
    "|2|   alcohol-dehydrogenase; aldehyde-reductase; adh; alcohol-dehydrogenase (nad); aliphatic alcohol-dehydrogenase; ethanol-dehydrogenase; nad-dependent alcohol-dehydrogenase; nad-specific aromatic alcohol-dehydrogenase; nadh-alcohol-dehydrogenase; nadh-aldehyde-dehydrogenase; primary alcohol-dehydrogenase; yeast alcohol-dehydrogenase|\n",
    "|3 |  alcohol-dehydrogenase; aldehyde-reductase; adh; alcohol-dehydrogenase (nad); aliphatic alcohol-dehydrogenase; ethanol-dehydrogenase; nad-dependent alcohol-dehydrogenase; nad-specific aromatic alcohol-dehydrogenase; nadh-alcohol-dehydrogenase; nadh-aldehyde-dehydrogenase; primary alcohol-dehydrogenase; yeast alcohol-dehydrogenase|\n",
    "|1491283|   cobaltochelatase; hydrogenobyrinic acid a,c-diamide cobaltochelatase; cobnst; cobncobst; hydrogenobyrinic-acid-a,c-diamide:cobalt cobalt-ligase (adp-forming)|\n",
    "|1491285 |    cobaltochelatase; hydrogenobyrinic acid a,c-diamide cobaltochelatase; cobnst; cobncobst; hydrogenobyrinic-acid-a,c-diamide:cobalt cobalt-ligase dp-forming)|\n",
    "\n",
    "Name: protein_name, Length: 1491288, dtype: object\n",
    "\n",
    "And these are the simplified versions with which we continue:\n",
    "| idx | protein_name|\n",
    "|--|--|\n",
    "|2  |alcohol-dehydrogenase|\n",
    "|3    |alcohol-dehydrogenase|\n",
    "|1491283|  cobaltochelatase|\n",
    "|1491285| cobaltochelatase|\n",
    "\n",
    "protein_name simplification as \n",
    "UniProt, NCBI RefSeq standards (The UniProt Consortium, 2021)\n",
    "Text cleaning standard in biomedical data preparation \n",
    "(Müller et al., 2016, Introduction to Biomedical Text Mining).\n",
    "Logical matching best practice in name cleaning (APA: UniProt Consortium, 2021).\n",
    "\n",
    "References : \n",
    "\n",
    "Müller, H. M., Kenny, E. E., & Sternberg, P. W. (2016). Textpresso: An ontology-based information retrieval and extraction system for biological literature. PLoS Biology, 2(11), e309. https://doi.org/10.1371/journal.pbio.0020309\n",
    "\n",
    "Source (for regex best practices):\n",
    "Friedl, J. E. F. (2006). Mastering Regular Expressions (3rd ed.). O'Reilly Media.\n",
    "\n",
    "McKinney, W. (2012). Python for Data Analysis. O'Reilly Media. (p.77-78)\n",
    "\n",
    "Pruitt, K. D., Tatusova, T., & Maglott, D. R. (2007). NCBI reference sequences (RefSeq): a curated non-redundant sequence database of genomes, transcripts and proteins. Nucleic Acids Research, 35(Database issue), D61–D65. https://doi.org/10.1093/nar/gkl842\n",
    "\n",
    "The UniProt Consortium. (2021). UniProt: the universal protein knowledgebase in 2021. Nucleic Acids Research, 49(D1), D480–D489. https://doi.org/10.1093/nar/gkaa1100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_protein_name(name: str) -> str:\n",
    "    \"Simplify the names of the protein without losing the biological meaning as literature recomends\"\n",
    "    # Step 1: Remove EC numbers like (EC 1.1.1.1)\n",
    "    name = re.sub(r'\\(ec\\s*\\d+\\.\\d+\\.\\d+\\.\\d+\\)', '', name, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Step 2: Remove uncertainty terms\n",
    "    uncertainty_terms = ['probable', 'putative', 'possible', 'uncharacterized', 'hypothetical']\n",
    "    pattern_uncertainty = r'^(?:' + '|'.join(uncertainty_terms) + r')\\s+'\n",
    "    name = re.sub(pattern_uncertainty, '', name, flags=re.IGNORECASE)\n",
    "\n",
    "    # Step 3: Take only the first name if separated by ';'\n",
    "    name = name.split(';')[0]\n",
    "\n",
    "    # Step 4: Lowercase and clean spaces\n",
    "    name = name.lower().strip()\n",
    "    name = re.sub(r'\\s+', ' ', name)  # Replace multiple spaces by single space\n",
    "\n",
    "    name = re.sub(r'[\\[\\(].*?[\\]\\)]$', '', name)\n",
    "    \n",
    "    return name\n",
    "# Appliying the cleaning on the same colum without changing the others\n",
    "ECcontri_Uniprot_enriched[\"protein_name\"] = ECcontri_Uniprot_enriched[\"protein_name\"].apply(clean_protein_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________________\n",
    "## 9.8 Cleaning the pathways for further processing\n",
    "note that in protein_name the rows are compose of the same protein name writen in different ways or with explanations, in the contrary pathways column has diffent pathways that are separated by a semicolon, hence the logic is different. The following function applies the correct pandas series logic Series.apply behavior and regular expression design (Pandas documentation, 2024).\n",
    "For regex practice\n",
    "Friedl, J. E. F. (2006). Mastering Regular Expressions (3rd ed.). O'Reilly Media."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_pathway_strings(pathway_series: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Clean pathway strings by removing common headers and database references that don't represent specific pathways.\n",
    "    Parameters:pathway_series : pandas.Series containing pathway strings with pathways separated by semicolons\n",
    "    Returns:  pandas.Series with cleaned pathway strings\n",
    "    \"\"\"\n",
    "    # Terms to remove - add or remove based on your specific needs\n",
    "    terms_to_remove = [\n",
    "        'Enzymes with EC numbers', # anotations mistake\n",
    "        'Metabolic pathways', # to broad term\n",
    "        'Other Metabolic Processes',\n",
    "        'Biosynthesis of secondary metabolites', # to broad term\n",
    "        'Microbial metabolism in diverse environments','Microbial metabolism in diverse environments', 'Metabolic pathways', # to broad term\n",
    "        'Exosome \\[BR:ko04147\\]', # mostly eucariotic\n",
    "        'photosynthesis', 'Photosynthesis', 'Photosynthesis  proteins [BR:ko00194]', # mostly plants\n",
    "        'Photosynthesis proteins', # mostly plants\n",
    "    ]\n",
    "    \n",
    "    # Compile regex pattern\n",
    "    pattern = re.compile(\n",
    "        '|'.join([re.escape(term) for term in terms_to_remove]),\n",
    "        flags=re.IGNORECASE\n",
    "    )\n",
    "    \n",
    "    def clean_single_entry(entry):\n",
    "        if pd.isna(entry) or not isinstance(entry, str) or entry == '[]':\n",
    "            return ''\n",
    "        \n",
    "        # Remove matched unwanted terms\n",
    "        cleaned = pattern.sub('', entry)\n",
    "        \n",
    "        # Clean up formatting issues\n",
    "        cleaned = re.sub(r';\\s*;', ';', cleaned)\n",
    "        cleaned = re.sub(r'^[;\\s]+|[;\\s]+$', '', cleaned)\n",
    "        \n",
    "        return cleaned\n",
    "    \n",
    "    return pathway_series.apply(clean_single_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the pathways\n",
    "ECcontri_Uniprot_enriched[\"pathways\"] = clean_pathway_strings(ECcontri_Uniprot_enriched[\"pathways\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ No unwanted terms found.\n"
     ]
    }
   ],
   "source": [
    "# List of terms you want to verify are not present\n",
    "terms_to_check = [\n",
    "    'Enzymes with EC numbers',\n",
    "    'Metabolic pathways',\n",
    "    'Other Metabolic Processes',\n",
    "    'Biosynthesis of secondary metabolites',\n",
    "    'Microbial metabolism in diverse environments',\n",
    "    'Exosome [BR:ko04147]',\n",
    "    'photosynthesis',\n",
    "    'Photosynthesis',\n",
    "    'Photosynthesis proteins',\n",
    "]\n",
    "\n",
    "# Function to search for any term in the cleaned pathways\n",
    "def find_unwanted_terms(series, terms):\n",
    "    found_terms = {}\n",
    "    for term in terms:\n",
    "        mask = series.str.contains(term, case=False, na=False)\n",
    "        if mask.any():\n",
    "            found_terms[term] = series[mask]\n",
    "    return found_terms\n",
    "\n",
    "# Run the verification\n",
    "found = find_unwanted_terms(ECcontri_Uniprot_enriched[\"pathways\"],  terms_to_check)\n",
    "\n",
    "# Print the result\n",
    "if found:\n",
    "    for term, entries in found.items():\n",
    "        print(f\"Term still present: '{term}'\")\n",
    "        print(entries)\n",
    "else:\n",
    "    print(\"✅ No unwanted terms found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enzyme Names - Unique Values: 1713\n",
      "Enzyme Names - Top 5 Occurrences:\n",
      " enzyme_names\n",
      "mrna guanylyltransferase; rngtt (gene name); ceg1 (gene name); mrna capping; messenger rna guanylyltransferase; protein                                                                                                                                        122914\n",
      "ethanolamine-phosphate cytidylyltransferase; phosphorylethanolamine transferase; et; ctp-phosphoethanolamine cytidylyltransferase; phosphoethanolamine cytidylyltransferase; ethanolamine phosphate cytidylyltransferase                                        67761\n",
      "nitrite-reductase (no-forming); cd-cytochrome nitrite-reductase; [; cytochrome c-551:o2, no2+ oxido-reductase; cytochrome cd; cytochrome; hydroxylamine (acceptor) reductase; methyl viologen-nitrite-reductase; nitrite-reductase (cytochrome; no-forming)     46802\n",
      "rifampicin monooxygenase; rif-o; rox; rifmo; rifampicin:nad(p)h:oxygen oxido-reductase (2'-n-hydroxyrifampicin-forming) (incorrect)                                                                                                                             46553\n",
      "d-nopaline-dehydrogenase; d-nopaline-synthase; nopaline-dehydrogenase; nopaline-synthase; nos; 2-n-(d-1,3-dicarboxypropyl)-l-arginine:nadp+ oxido-reductase (l-arginine-forming)                                                                                44470\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Protein Names - Unique Values: 5232\n",
      "Protein Names - Top 5 Occurrences:\n",
      " protein_name\n",
      "trna                                                                                                                                                                   7280\n",
      "multifunctional fusion protein [                                                                                                                                       5732\n",
      "dna ligase                                                                                                                                                             5639\n",
      "bifunctional protein fold [                                                                                                                                            4617\n",
      "coenzyme a biosynthesis bifunctional protein coabc (dna-pantothenate metabolism flavoprotein) (phosphopantothenoylcysteine-synthetase-decarboxylase) (ppcs-ppcdc) [    4326\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count occurrences of each value in both columns\n",
    "enzyme_counts = ECcontri_Uniprot_enriched['enzyme_names'].value_counts()\n",
    "protein_counts = ECcontri_Uniprot_enriched['protein_name'].value_counts()\n",
    "\n",
    "# Get the number of unique values in each column\n",
    "enzyme_unique_count = ECcontri_Uniprot_enriched['enzyme_names'].nunique()\n",
    "protein_unique_count = ECcontri_Uniprot_enriched['protein_name'].nunique()\n",
    "\n",
    "# Print the results\n",
    "print(\"Enzyme Names - Unique Values:\", enzyme_unique_count)\n",
    "print(\"Enzyme Names - Top 5 Occurrences:\\n\", enzyme_counts.head())\n",
    "print(\"\\nProtein Names - Unique Values:\", protein_unique_count)\n",
    "print(\"Protein Names - Top 5 Occurrences:\\n\", protein_counts.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "ECcontri_Uniprot_enriched['version'] = 'v9_proteins_pathways_fold'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alcohol-dehydrogenase' 's-' 'fe-containing alcohol-dehydrogenase' ...\n",
      " '4-hydroxybutyrate---coa ligase '\n",
      " 'biotin--(acetyl-coa-carboxylase) ligase'\n",
      " \"rna 3'-terminal-phosphate cyclase \"]\n"
     ]
    }
   ],
   "source": [
    "# Expand column width display\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Now display your dataframe\n",
    "print(ECcontri_Uniprot_enriched['protein_name'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the new corrected dataframe\n",
    "ECcontri_Uniprot_enriched.to_parquet(output_large / 'ECcontri_Uniprot_enriched.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0PVjx1_TrqhG"
   },
   "source": [
    "Making the category dictionary in case it starts from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-19T16:14:53.711704Z",
     "iopub.status.idle": "2025-04-19T16:14:53.712186Z",
     "shell.execute_reply": "2025-04-19T16:14:53.711985Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Recomenzar\n",
    "eccontri_path = output_large / 'ECcontri_Uniprot_enriched.parquet'\n",
    "ECcontri_Uniprot_enriched = pd.read_parquet(eccontri_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-19T16:14:53.713210Z",
     "iopub.status.idle": "2025-04-19T16:14:53.713720Z",
     "shell.execute_reply": "2025-04-19T16:14:53.713503Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "knee_path = output_base / \"genus_to_threshold.csv\"\n",
    "with open(knee_path, 'r') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    genus_to_threshold = next(reader) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-19T16:14:53.714894Z",
     "iopub.status.idle": "2025-04-19T16:14:53.715443Z",
     "shell.execute_reply": "2025-04-19T16:14:53.715197Z"
    },
    "id": "8J-xTtj2rusm",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define category dict outside\n",
    "category_dict = Integrated_T.T.iloc[0, 0:-1].to_dict()\n",
    "\n",
    "# Define colors and categories\n",
    "category_colors = {1: '#008800',  # Dark green\n",
    "                   2: '#FF8C00',  # Dark orange\n",
    "                   3: '#FF0000'}   # Red\n",
    "\n",
    "categories_labels = {1: 'Normal Operation',\n",
    "              2: 'Early Warning',\n",
    "              3: 'System Failure'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dq8C9vSywZjZ"
   },
   "source": [
    "# 10. Filtering pairs Bacteria-Protein by significance to the risk category\n",
    "\n",
    "The analyze_corrosion_proteins function establishes a systematic framework for identifying protein-genus pairs associated with corrosion across different risk categories. The function prepares the data by converting enzyme records to a searchable dictionary and mapping sites to risk categories, then tracks which sites contain each protein-genus combination to maintain traceability throughout the analysis.\n",
    "After initial preparation, the function performs pattern recognition using prevalence, significance, and frequency metrics through the classify_abundance_patterns function. It then integrates biological metadata into the results, classifies proteins into housekeeping and niche-specific categories, and separates increasing patterns from others. Only increasing patterns are used to prioritize markers based on combined pattern and biological significance. These results are further refined using the knee abundance threshold (calculated in section 8.11) to select proteins of biological relevance. Finally, it organizes results into specialized groups for interpretation.\n",
    "The analysis uses absolute contribution to abundance (abund_contri) as this metric is comparable across samples and reflects biological influence without relativization. Inverse patterns are preserved separately as they potentially represent bacteria with protective capacities in the ecosystem or indicate bystander organisms whose protein content could provide insights about their ecological role and why they decrease as systems deteriorate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-19T16:14:53.716480Z",
     "iopub.status.idle": "2025-04-19T16:14:53.716978Z",
     "shell.execute_reply": "2025-04-19T16:14:53.716764Z"
    },
    "id": "aoMYUGIDoMc-",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def analyze_corrosion_proteins(eccontri_df, alpha=0.05, per_genus_count=10, genus_to_threshold = genus_to_threshold, knee_abundance_threshold =None):\n",
    "    \"\"\"\n",
    "    Comprehensive analysis of protein-genus pairs for corrosion relevance\n",
    "\n",
    "    Parameters:\n",
    "    eccontri_df : pandas DataFrame The enriched ECcontri_Uniprot dataframe with EC, protein_name, Genus, abundance, etc.\n",
    "    ec_records : List of dictionaries with EC, protein_name metadata with corrosion relevance information\n",
    "    alpha : float,  Significance level for statistical tests (default: 0.05)\n",
    "    balance_genera : bool, Whether to balance representation across genera (default: False)\n",
    "    per_genus_count : intNumber of markers to include per genus if balancing (default: 10)\n",
    "    genus_to_threshold : dict, Optional mapping of genus to Knee_abundance thresholds for filtering (default: None)\n",
    "    pattern_result : pd.DataFrame, precomputed pattern result DataFrame to bypass compute-intensive steps.\n",
    "\n",
    "    Returns:\n",
    "     results :  Dictionary containing various analysis results\n",
    "    \"\"\"\n",
    "    print(\"Starting corrosion protein analysis...\")\n",
    "    eccontri_df = eccontri_df.copy().reset_index()\n",
    "\n",
    "    print(f\"Analyzing {len(eccontri_df)} data points across {len(eccontri_df['Sites'].unique())} Sites...\")\n",
    "    if 'index' in eccontri_df.columns:\n",
    "        eccontri_df= eccontri_df.drop(columns=\"index\")\n",
    "\n",
    "    eccontri_df[\"idx\"] = eccontri_df[\"idx\"].astype(int)\n",
    "    eccontri_df = eccontri_df.reset_index(drop=False)\n",
    "\n",
    "    # Fist insert the Frequency in the original df so that it be granular with transform\n",
    "    eccontri_df['Frequency'] = eccontri_df.groupby(['Sites', 'Genus', 'protein_name', 'Category'])['idx'].transform('size')\n",
    "    print(\"eccontri_df.columns immediately after creating column Frequency:\", eccontri_df.columns.tolist())\n",
    "\n",
    "    pattern_data = classify_abundance_patterns(eccontri_df)\n",
    "    print(\"Pattern data created\")\n",
    "\n",
    "    # Merging pattern data with ecrecords columns\n",
    "    integrated_results = merging_pattern_record(pattern_data, eccontri_df)\n",
    "    print(\"Integrated results created\")\n",
    "\n",
    "    # Clasify ubiquitous, niche genus-protein names by mechanism, pathways\n",
    "    print(\"Classifying pathways by specificity...\")\n",
    "    classified_results = classify_pathways_by_specificity(integrated_results)\n",
    "\n",
    "    # Separate positive and inverse patterns\n",
    "    print(\"Separating positive and inverse patterns by patterns\")\n",
    "    increasing_results, inverse_results, constant_df = separate_by_pattern(classified_results)\n",
    "\n",
    "    # Prioritize positive results only\n",
    "    print(\"Prioritizing markers based on statistical and biological relevance...\")\n",
    "    prioritized_markers = prioritize_markers(increasing_results)\n",
    "\n",
    "    # Create marker groups based on knee inflexion\n",
    "    print(f\"Balancing genus representation (top {per_genus_count} per genus)...\")\n",
    "    balanced_markers = balance_genus_representation(prioritized_markers, eccontri_df, genus_to_threshold =genus_to_threshold,\n",
    "                                per_genus_count=per_genus_count,  knee_abundance_threshold=knee_abundance_threshold)\n",
    "    print(f\"Created balanced dataset with {len(balanced_markers)} markers from {len(prioritized_markers['Genus'].unique())} genera\")\n",
    "\n",
    "    # Create marker groups from balanced markers\n",
    "    print(\"Creating specialized marker groups from balanced markers...\")\n",
    "    marker_groups = create_marker_groups(balanced_markers)\n",
    "\n",
    "    print(\"Analysis complete!\")\n",
    "\n",
    "    results_dict = {\n",
    "        'pattern_data': pattern_data,\n",
    "        'integrated_results': integrated_results,\n",
    "        'classified_results': classified_results,\n",
    "        'increasing_markers': increasing_results,\n",
    "        'prioritized_markers': prioritized_markers,\n",
    "        'balanced_markers': balanced_markers,\n",
    "        'marker_groups': marker_groups,\n",
    "        'inverse_markers': inverse_results\n",
    "    }\n",
    "\n",
    "    return results_dict, marker_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XDxX599EwamS"
   },
   "source": [
    "## 10.1 Determining Abundance Patters\n",
    "The classify_abundance_patterns function implements a sophisticated classification system for protein-genus abundance profiles across risk categories, prioritizing patterns with potential corrosion relevance.\n",
    "This classification focuses on corrosion-relevant trends by first calculating mean abundance per category for each genus-protein pair. The function begins by capturing the frequency metrics from the original dataframe, where frequency represents the number of occurrences of each unique genus-protein pair across sites and categories.\n",
    "The algorithm tracks which categories (1, 2, and 3) have non-zero means for each protein-genus pair and applies detailed pattern recognition logic based on the relationships between these means. These detailed patterns are then grouped into broader categories (\"increasing\", \"decreasing\", \"constant\", or \"other\") to facilitate downstream analysis focusing on proteins that correlate positively with corrosion risk.\n",
    "Beyond pattern identification, the function calculates several quantitative metrics to prioritize patterns:\n",
    "\n",
    "Fold changes between categories (with pseudocounts to avoid division by zero)\n",
    "Pattern specificity (how unique a protein is to certain categories)\n",
    "Prevalence (proportion of categories where a protein appears)\n",
    "\n",
    "The frequency metric plays an important role in the final significance score calculation, where it's logarithmically scaled to give weight to commonly occurring protein-genus pairs without overly penalizing rare but potentially important cases. This approach balances the importance of frequently observed patterns (which may represent core functions) with unique patterns that might indicate specialized adaptive responses to corrosion conditions.The algorithm identifies distinct patterns\n",
    "with 2 transitions between 3 categories, we have 3² = 9 possible patterns:\n",
    "\n",
    "\"steadily_increasing\" (cat1 < cat2 < cat3) - Clear positive correlation with risk  \n",
    "\"early_plateau\" (cat1 < cat2 = cat3) - Initial response that stabilizes  \n",
    "\"peak_at_transition\" (cat1 < cat2 > cat3) - Specific to transitional environments  \n",
    "\"late_response\" (cat1 = cat2 < cat3) - Only activated in high-risk environments  \n",
    "\"risk_independent\" (cat1 = cat2 = cat3) - Housekeeping/core function  \n",
    "\"late_decline\" (cat1 = cat2 > cat3) - Inhibited in severe conditions  \n",
    "\"stress_recovery\" (cat1 > cat2 < cat3) - Recovery pattern after initial stress  \n",
    "\"adaptation_plateau\" (cat1 > cat2 = cat3) - Adjustment to new baseline  \n",
    "\"steadily_decreasing\" (cat1 > cat2 > cat3) - Negative correlation with risk  \n",
    "\n",
    "epsilon = 1e-5 → used only to judge whether two means are considered \"equal\" (floating point tolerance).  (are two means practically equal?)\n",
    "pseudocount = 1e-5 → used only to avoid division by zero in fold change calculations.  \n",
    "clip(-10, 10) → used only to cap extreme values of log2fc, not to define significance.  \n",
    "\n",
    "Fold changes refer to how much the normalized abundance of a protein increases when changing the risk category from a lower to a higher category.\n",
    "The threshold follows common biological conventions for fold change interpretation:\n",
    "    Fold Change > 1.5 → Moderate increase.\n",
    "    Fold Change > 2.0 → Strong increase.\n",
    "presence_threshold is separately used for presence detection, meaning whether the abundance is considered meaningfully greater than a small value (e.g., to ignore near-zero noise)\n",
    "This convention appears for example in proteomics and transcriptomics studies (Smyth, 2004; Ritchie et al., 2015 [limma package]).\n",
    "Reference:\n",
    "\n",
    "Ritchie, M. E., Phipson, B., Wu, D., Hu, Y., Law, C. W., Shi, W., & Smyth, G. K. (2015). limma powers differential expression analyses for RNA-sequencing and microarray studies. Nucleic Acids Research, 43(7), e47. https://doi.org/10.1093/nar/gkv007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-19T16:14:53.718567Z",
     "iopub.status.idle": "2025-04-19T16:14:53.719081Z",
     "shell.execute_reply": "2025-04-19T16:14:53.718860Z"
    },
    "id": "taPX1UTZtK5i",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def classify_abundance_patterns(eccontri_df):\n",
    "    \"\"\"\n",
    "    Comprehensive classification of abundance patterns with biological descriptors,\n",
    "    fold changes, and other key metrics for prioritization.\n",
    "\n",
    "    Parameters:\n",
    "    df: DataFrame with columns for Genus, protein_name, Category, norm_abund_contri, Frequency\n",
    "\n",
    "    Returns:\n",
    "    DataFrame with pattern classifications and related metrics\n",
    "    \"\"\"\n",
    "    # Make a copy to avoid modifying the original\n",
    "    result_df = eccontri_df.copy()\n",
    "    # Define thresholds\n",
    "    epsilon = 1e-10  # For comparing means (pattern detection)\n",
    "    presence_threshold = 1e-10  # For defining presence/absence # Minimum abundance to consider as 'present'\n",
    "\n",
    "\n",
    "    freq_mapping = result_df.groupby(['Genus', 'protein_name'])[['Frequency', 'idx']].first().reset_index()\n",
    "\n",
    "    # Calculate mean abundance per category for each genus-protein pair\n",
    "    mean_by_cat = result_df.groupby(['Genus', 'protein_name', 'Category'])['norm_abund_contri'].mean().reset_index()\n",
    "    # Pivot to get means by category\n",
    "    pattern_data = mean_by_cat.pivot_table(\n",
    "        index=['Genus', 'protein_name'],\n",
    "        columns='Category',\n",
    "        values='norm_abund_contri'\n",
    "    ).reset_index()\n",
    "\n",
    "    # Rename the columns for clarity\n",
    "    pattern_data.columns.name = None\n",
    "    if 1 in pattern_data.columns:\n",
    "        pattern_data.rename(columns={1: 'mean_cat1'}, inplace=True)\n",
    "    else:\n",
    "        pattern_data['mean_cat1'] = 0\n",
    "\n",
    "    if 2 in pattern_data.columns:\n",
    "        pattern_data.rename(columns={2: 'mean_cat2'}, inplace=True)\n",
    "    else:\n",
    "        pattern_data['mean_cat2'] = 0\n",
    "\n",
    "    if 3 in pattern_data.columns:\n",
    "        pattern_data.rename(columns={3: 'mean_cat3'}, inplace=True)\n",
    "    else:\n",
    "        pattern_data['mean_cat3'] = 0\n",
    "\n",
    "    # Fill NaN with 0 and ensure all categories exist\n",
    "    pattern_data = pattern_data.fillna(0)\n",
    "    for cat in [1, 2, 3]:\n",
    "        if cat not in pattern_data.columns:\n",
    "            pattern_data[cat] = 0\n",
    "\n",
    "    # Merge pattern data with frequency data\n",
    "    pattern_data = pd.merge(pattern_data, freq_mapping, on=['Genus', 'protein_name'], how='left')\n",
    "    # Reindex pattern_data columns after merge\n",
    "    pattern_data = pattern_data[['idx', 'Genus', 'protein_name', 'mean_cat1', 'mean_cat2', 'mean_cat3', 'Frequency']]\n",
    "\n",
    "    # Define function to determine which categories are present\n",
    "    def get_category_string(row):\n",
    "        categories = []\n",
    "        if row['mean_cat1'] > presence_threshold:\n",
    "            categories.append('1')\n",
    "        if row['mean_cat2']  > presence_threshold:\n",
    "            categories.append('2')\n",
    "        if row['mean_cat3']  > presence_threshold:\n",
    "            categories.append('3')\n",
    "        return ''.join(categories)\n",
    "\n",
    "    pattern_data['category_str'] = pattern_data.apply(get_category_string, axis=1)\n",
    "\n",
    "    # Tolerance for floating point comparisons\n",
    "    epsilon = 1e-5\n",
    "\n",
    "    # Calculate pattern using descriptive naming\n",
    "    def determine_pattern(row):\n",
    "        category_str = row['category_str']\n",
    "        mean1 = row['mean_cat1']\n",
    "        mean2 = row['mean_cat2']\n",
    "        mean3 = row['mean_cat3']\n",
    "\n",
    "        pattern = \"other_pattern\"\n",
    "\n",
    "        if category_str == '123':\n",
    "            # When present in all categories\n",
    "            if mean1 < mean2 and mean2 < mean3:\n",
    "                pattern = \"steadily_increasing\"\n",
    "            elif mean1 > mean2 and mean2 > mean3:\n",
    "                pattern = \"steadily_decreasing\"\n",
    "            elif mean1 < mean2 and mean2 > mean3:\n",
    "                if mean1 < mean3:\n",
    "                    pattern = \"peak_at_transition_high\"  # Peak at cat2 but ends higher\n",
    "                elif abs(mean1 - mean3) < epsilon:\n",
    "                    pattern = \"peak_at_transition_equal\"  # Perfect peak at cat2\n",
    "                else:\n",
    "                    pattern = \"peak_at_transition_low\"  # Peak at cat2 but ends lower\n",
    "            elif mean1 > mean2 and mean2 < mean3:\n",
    "                if mean1 > mean3:\n",
    "                    pattern = \"stress_recovery_partial\"  # Valley at cat2 but ends lower\n",
    "                elif abs(mean1 - mean3) < epsilon:\n",
    "                    pattern = \"stress_recovery_equal\"  # Perfect valley at cat2\n",
    "                else:\n",
    "                    pattern = \"stress_recovery_full\"  # Valley at cat2 but ends higher\n",
    "            elif abs(mean1 - mean2) < epsilon and mean2 < mean3:\n",
    "                pattern = \"late_response\"  # Plateaus then increases\n",
    "            elif abs(mean1 - mean2) < epsilon and abs(mean2 - mean3) < epsilon:\n",
    "                pattern = \"risk_independent\"  # Consistent across all\n",
    "            elif abs(mean1 - mean2) < epsilon and mean2 > mean3:\n",
    "                pattern = \"late_decline\"  # Plateaus then decreases\n",
    "\n",
    "        elif category_str == '12':\n",
    "            # When only categories 1 and 2 are present\n",
    "            if mean1 < mean2:\n",
    "                pattern = \"early_increase_only\"\n",
    "            elif mean1 > mean2:\n",
    "                pattern = \"early_decrease_only\"\n",
    "            else:\n",
    "                pattern = \"early_constant\"\n",
    "\n",
    "        elif category_str == '13':\n",
    "            # When only categories 1 and 3 are present\n",
    "            if mean1 < mean3:\n",
    "                pattern = \"extreme_response\"  # Responds to normal and severe but not transition\n",
    "            elif mean1 > mean3:\n",
    "                pattern = \"extreme_inhibition\"\n",
    "            else:\n",
    "                pattern = \"extreme_constant\"\n",
    "\n",
    "        elif category_str == '23':\n",
    "            # When only categories 2 and 3 are present\n",
    "            if mean2 < mean3:\n",
    "                pattern = \"late_emergence_increasing\"  # Absent in normal, increases with risk\n",
    "            elif mean2 > mean3:\n",
    "                pattern = \"late_emergence_decreasing\"\n",
    "            else:\n",
    "                pattern = \"late_emergence_constant\"\n",
    "\n",
    "        elif category_str == '1':\n",
    "            pattern = \"normal_exclusive\"  # Only in normal condition\n",
    "        elif category_str == '2':\n",
    "            pattern = \"transition_exclusive\"  # Only in transition condition\n",
    "        elif category_str == '3':\n",
    "            pattern = \"severe_exclusive\"  # Only in severe condition\n",
    "\n",
    "        return pattern\n",
    "\n",
    "    pattern_data['descriptive_pattern'] = pattern_data.apply(determine_pattern, axis=1)\n",
    "\n",
    "    # Add broader pattern categories for filtering\n",
    "    pattern_categories = {\n",
    "        # Increasing patterns\n",
    "        'steadily_increasing': 'increasing',\n",
    "        'peak_at_transition_high': 'increasing',\n",
    "        'peak_at_transition_equal': 'increasing',\n",
    "        'late_response': 'increasing',\n",
    "        'stress_recovery_full': 'increasing',\n",
    "        'early_increase_only': 'increasing',\n",
    "        'extreme_response': 'increasing',\n",
    "        'late_emergence_increasing': 'increasing',\n",
    "        'transition_exclusive': 'increasing',\n",
    "        'severe_exclusive': 'increasing',\n",
    "\n",
    "        # Decreasing patterns\n",
    "        'steadily_decreasing': 'decreasing',\n",
    "        'peak_at_transition_low': 'decreasing',\n",
    "        'stress_recovery_partial': 'decreasing',\n",
    "        'stress_recovery_equal': 'decreasing',\n",
    "        'late_decline': 'decreasing',\n",
    "        'early_decrease_only': 'decreasing',\n",
    "        'extreme_inhibition': 'decreasing',\n",
    "        'late_emergence_decreasing': 'decreasing',\n",
    "        'normal_exclusive': 'decreasing',\n",
    "\n",
    "        # Other patterns\n",
    "        'risk_independent': 'constant',\n",
    "        'early_constant': 'constant',\n",
    "        'extreme_constant': 'constant',\n",
    "        'late_emergence_constant': 'constant',\n",
    "        'other_pattern': 'other'\n",
    "    }\n",
    "\n",
    "    pattern_data['pattern_category'] = pattern_data['descriptive_pattern'].map(pattern_categories)\n",
    "\n",
    "    # Calculate fold changes with pseudocount to avoid division by zero\n",
    "    pseudocount = 1e-5\n",
    "\n",
    "    # Cat2 vs Cat1 fold change\n",
    "    pattern_data['fold_change_2vs1'] = (pattern_data['mean_cat2'] + pseudocount) / (pattern_data['mean_cat1'] + pseudocount)\n",
    "    pattern_data['log2fc_2vs1'] = np.log2(pattern_data['fold_change_2vs1'])\n",
    "\n",
    "    # Cat3 vs Cat2 fold change\n",
    "    pattern_data['fold_change_3vs2'] = (pattern_data['mean_cat3'] + pseudocount) / (pattern_data['mean_cat2'] + pseudocount)\n",
    "    pattern_data['log2fc_3vs2'] = np.log2(pattern_data['fold_change_3vs2'])\n",
    "\n",
    "    # Cat3 vs Cat1 fold change\n",
    "    pattern_data['fold_change_3vs1'] = (pattern_data['mean_cat3'] + pseudocount) / (pattern_data['mean_cat1'] + pseudocount)\n",
    "    pattern_data['log2fc_3vs1'] = np.log2(pattern_data['fold_change_3vs1'])\n",
    "\n",
    "    # Cap extreme log2fc values\n",
    "    for col in ['log2fc_2vs1', 'log2fc_3vs2', 'log2fc_3vs1']:\n",
    "        pattern_data[col] = pattern_data[col].clip(-10, 10)\n",
    "\n",
    "    # Calculate specificity (how specific pattern is to particular categories)\n",
    "    def calculate_specificity(row):\n",
    "        vals = [row['mean_cat1'], row['mean_cat2'], row['mean_cat3']]\n",
    "        total = sum(vals)\n",
    "        if total == 0:\n",
    "            return 0\n",
    "\n",
    "        # Calculate proportions for each category\n",
    "        props = [v/total for v in vals if v > 0]\n",
    "\n",
    "        if len(props) == 0:\n",
    "            return 0\n",
    "        elif len(props) == 1:\n",
    "            return 1.0  # Completely specific to one category\n",
    "        else:\n",
    "            # Calculate a Gini-like coefficient of specificity\n",
    "            # 0 = evenly distributed, 1 = completely in one category\n",
    "            mean_prop = sum(props) / len(props)\n",
    "            max_prop = max(props)\n",
    "            return 1 - (mean_prop / max_prop)\n",
    "\n",
    "    pattern_data['specificity'] = pattern_data.apply(calculate_specificity, axis=1)\n",
    "\n",
    "    # Calculate prevalence score\n",
    "    def calculate_prevalence(row):\n",
    "        # Count categories with significant abundance (relative to the maximum)\n",
    "        max_val = max([row['mean_cat1'], row['mean_cat2'], row['mean_cat3']])\n",
    "        if max_val == 0:\n",
    "            return 0\n",
    "\n",
    "        threshold = max_val * 0.1  # 10% of maximum as significance threshold\n",
    "        present_cats = sum(1 for v in [row['mean_cat1'], row['mean_cat2'], row['mean_cat3']] if v > threshold)\n",
    "        return present_cats / 3  # Scale to 0-1 range\n",
    "\n",
    "    pattern_data['prevalence'] = pattern_data.apply(calculate_prevalence, axis=1)\n",
    "\n",
    "    # Calculate maximum log2 fold change (absolute value)\n",
    "    pattern_data['max_abs_log2fc'] = pattern_data[['log2fc_2vs1', 'log2fc_3vs2', 'log2fc_3vs1']].abs().max(axis=1)\n",
    "\n",
    "    # Count distinct patterns\n",
    "    pattern_counts = pattern_data['descriptive_pattern'].value_counts()\n",
    "    print(\"Detailed pattern distribution:\")\n",
    "    print(pattern_counts)\n",
    "\n",
    "    # Count by category\n",
    "    category_counts = pattern_data['pattern_category'].value_counts()\n",
    "    print(\"\\nPattern category distribution:\")\n",
    "    print(category_counts)\n",
    "\n",
    "    # Calculate a combined significance score\n",
    "    def calculate_significance(row):\n",
    "        # Base score from pattern category\n",
    "        if row['pattern_category'] == 'increasing':\n",
    "            pattern_score = 3.0\n",
    "        elif row['pattern_category'] == 'decreasing':\n",
    "            pattern_score = 1.0\n",
    "        elif row['pattern_category'] == 'constant':\n",
    "            pattern_score = 0.5\n",
    "        else:\n",
    "            pattern_score = 0.0\n",
    "        # Special bonus for unique patterns\n",
    "        if row['descriptive_pattern'] == 'severe_exclusive':\n",
    "            unique_bonus = 2.0  # Highest bonus for Cat3 exclusive\n",
    "        elif row['descriptive_pattern'] == 'transition_exclusive':\n",
    "            unique_bonus = 1.0  # Medium bonus for Cat2 exclusive\n",
    "        elif row['descriptive_pattern'] == 'normal_exclusive':\n",
    "            unique_bonus = 0.0  # No bonus for Cat1 exclusive\n",
    "        else:\n",
    "            unique_bonus = 0.0\n",
    "\n",
    "        # Fold change component (capped at 3.0)\n",
    "        fc_score = min(abs(row['log2fc_3vs1']), 3.0)\n",
    "\n",
    "        # Specificity component\n",
    "        spec_score = row['specificity'] * 2.0  # Scale to 0-2 range\n",
    "\n",
    "        # Frequency component - logarithmic scaling to give weight to frequency\n",
    "        # without overly penalizing rare but important cases\n",
    "        freq_score = np.log2(row['Frequency'] + 1)  # +1 to handle freq=1 case\n",
    "\n",
    "        # Final weighted score (adjust weights as needed)\n",
    "        return (pattern_score * 0.25) + (unique_bonus * 0.3) + (fc_score * 0.2) + (spec_score * 0.2) + (freq_score * 0.25)\n",
    "\n",
    "    pattern_data['significance_score'] = pattern_data.apply(calculate_significance, axis=1)\n",
    "    # Return the pattern data\n",
    "    return pattern_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HqN_0dSKdU1q"
   },
   "source": [
    "## 10.2. Integrating pattern data with ec_records columns\n",
    "The merging_pattern_record function serves as a critical integration step that combines statistical pattern analysis with rich biological metadata. This function takes the pattern analysis results (containing information about abundance trends across risk categories) and merges them with extensive protein metadata from the original enriched dataset.\n",
    "The function first ensures data consistency by sorting both dataframes by index, then performs a left join that preserves all pattern data while incorporating valuable biological context from the original dataset. This includes enzyme names and classes, metabolic pathways, functional hierarchies, metal interactions, corrosion mechanisms, and relevance scores.\n",
    "This integration step is essential as it bridges statistical observations with biological meaning, allowing subsequent analyses to interpret abundance patterns in the context of protein function, metal interactions, and corrosion relevance. The comprehensive metadata incorporated here was derived from multiple authoritative databases processed in section 9, providing a rich foundation for biological interpretation of the statistical patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-19T16:14:53.722236Z",
     "iopub.status.idle": "2025-04-19T16:14:53.722755Z",
     "shell.execute_reply": "2025-04-19T16:14:53.722530Z"
    },
    "id": "i6mL5vcD23EZ",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def merging_pattern_record(pattern_df, eccontri_df):\n",
    "    \"\"\"\n",
    "    Merges pattern results with eccontri_df while preserving the two-step merging\n",
    "    process for important domain-specific reasons. Removes redundant individual\n",
    "    prevalence columns, keeping only the unified prevalence.\n",
    "\n",
    "    Parameters:    pattern_result : pandas DataFrame result from perform_abundance_analysis\n",
    "    eccontri_df : pandas DataFrame original dataframe with protein metadata\n",
    "\n",
    "    Returns:  integrated_results : pandas DataFrame, clean merged results with pattern data and metadata\n",
    "    \"\"\"\n",
    "    # Sort both df\n",
    "    eccontri_df = eccontri_df.copy()\n",
    "    eccontri_df = eccontri_df.sort_values(by=[\"idx\"])\n",
    "    pattern_df = pattern_df.copy()\n",
    "    pattern_df = pattern_df.sort_values(by=[\"idx\"])\n",
    "\n",
    "    # merge for metadata\n",
    "    integrated_results = pd.merge(pattern_df, eccontri_df[['idx', 'Category', 'Genus', 'protein_name', 'Frequency', 'enzyme_names', \n",
    "                                'enzyme_class', 'pathways', 'hierarchy','has_metal', 'metals_consolidated', 'metal_scores',  'overall_metal_score',  'corrosion_relevance_score', 'corrosion_relevance', \n",
    "                                 'functional_categories', 'overall_functional_score',  'corrosion_keyword_groups', 'overall_keyword_score', 'corrosion_synergies', \n",
    "                                'overall_synergy_score', 'organic_processes', 'overall_organic_process_score', 'corrosion_mechanisms', 'corrosion_mechanism_scores', 'overall_corrosion_score', 'version'         \n",
    "        ]],    \n",
    "        on=['idx', 'Genus', 'protein_name', 'Frequency'],\n",
    "        how='left'\n",
    "    )\n",
    "    integrated_results = integrated_results.drop(columns= ['has_metal', 'enzyme_names'], axis = 1)\n",
    "    integrated_results = integrated_results.sort_values(by=[\"idx\"])\n",
    "    integrated_results = integrated_results.reset_index(drop=True)\n",
    "\n",
    "    return integrated_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mR_DPU31yG_U"
   },
   "source": [
    "## 10.3. Classifying Housekeeping, Niche and Mixed Protein depending on pathways, mechanisms and hierarchy\n",
    "The classify_pathways_by_specificity function implements a systematic approach to categorize protein-associated pathways based on their ecological and metabolic roles. The function distinguishes between universal pathways (found across most microorganisms), niche-specific pathways (specialized for particular environments), and mixed pathways (containing elements of both).\n",
    "The classification relies on a comprehensive dictionary of universal pathways organized into six major functional categories: energy production, carbon storage, DNA/RNA/protein processes, membrane transport, stress response, and biomolecule synthesis. Each category contains specific pathways that represent core metabolic functions essential for microbial life across diverse environments.For each protein entry, the function analyzes pathway text using regex pattern matching to identify universal pathway components. Then extracts remaining text as potential niche-specific components, assigns a classification based on the presence of universal and/or niche-specific elements\n",
    "Records specific universal pathways detected and remaining niche-specific content\n",
    "\n",
    "The function also generates summary statistics showing the distribution of pathway classifications across the dataset and identifies the most common universal pathways. This classification is crucial for downstream analysis as it helps distinguish proteins involved in basic cellular maintenance from those potentially specialized for corrosion environments.This classification step provides important context for understanding which proteins represent housekeeping functions and which might be more directly involved in corrosion-specific adaptations, allowing for more targeted analysis of potentially relevant biomarkers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-19T16:14:53.725796Z",
     "iopub.status.idle": "2025-04-19T16:14:53.726313Z",
     "shell.execute_reply": "2025-04-19T16:14:53.726080Z"
    },
    "id": "W_Ovd0LHyReP",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def classify_pathways_by_specificity(integrated_results):\n",
    "    \"\"\"\n",
    "    Classify pathways as 'universal', 'niche-specific', or 'mixed' based on matching to universal pathways list\n",
    "    Parameters: integrated_results : DataFrame with pathway information\n",
    "    Returns:    DataFrame with additional columns for pathway classification\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    results = integrated_results.copy(deep=False)\n",
    "\n",
    "    # Define the universal pathways based on the document\n",
    "    universal_pathways = {\n",
    "        \"metabolism\": \"General Metabolism\",\n",
    "        \"central metabolism\": \"1. Energy Production\",\n",
    "        \"energy production\": \"1. Energy Production\",\n",
    "        # Energy Production\n",
    "        \"glycolysis\": \"1.1. Glycolysis\",\n",
    "        \"gluconeogenesis\": \"1.2. Gluconeogenesis\",\n",
    "        \"pentose phosphate\": \"1.3. Pentose Phosphate Pathway\",\n",
    "        \"tca cycle\": \"1.4. Krebs/TCA Cycle\",\n",
    "        \"krebs cycle\": \"1.4. Krebs/TCA Cycle\",\n",
    "        \"citric acid cycle\": \"1.4. Krebs/TCA Cycle\",\n",
    "        \"carbon cycle\": \"1.4. Krebs/TCA Cycle\",\n",
    "        \"electron transport chain\": \"1.5. Electron Transport Chain\",\n",
    "        \"etc\": \"1.5. Electron Transport Chain\",\n",
    "        \"respiration\": \"1.5. Electron Transport Chain\",\n",
    "        \"fermentation\": \"1.6. Fermentation\",\n",
    "        \"atp synthase\": \"1.7. ATP Synthase\",\n",
    "        \"atp synthesis\": \"1.7. ATP Synthase\",\n",
    "        # Add these to your universal_pathways dictionary\n",
    "\n",
    "        # Carbon Storage\n",
    "        \"fatty acid synthesis\": \"2.1. Fatty Acid Synthesis\",\n",
    "        \"fatty acid oxidation\": \"2.2. Fatty Acid Oxidation\",\n",
    "        \"beta-oxidation\": \"2.2. Fatty Acid Oxidation\",\n",
    "        \"amino acid degradation\": \"2.3. Amino Acid Degradation\",\n",
    "        \"glycogenesis\": \"2.4. Carbohydrate Storage\",\n",
    "        \"glycogenolysis\": \"2.4. Carbohydrate Storage\",\n",
    "        \"triacylglycerol synthesis\": \"2.5. Triacylglycerol Synthesis\",\n",
    "\n",
    "        # DNA/RNA/Protein\n",
    "        \"dna replication\": \"3.1. DNA Replication\",\n",
    "        \"dna polymerase\": \"3.1. DNA Replication\",\n",
    "        \"helicase\": \"3.1. DNA Replication\",\n",
    "        \"transcription\": \"3.2. Transcription\",\n",
    "        \"rna polymerase\": \"3.2. Transcription\",\n",
    "        \"translation\": \"3.3. Translation\",\n",
    "        \"ribosomal\": \"3.3. Translation\",\n",
    "        \"ribosome\": \"3.3. Translation\",\n",
    "        \"ribosomes\": \"3.3. Translation\",\n",
    "        \"protein synthesis\": \"3.3. Translation\",\n",
    "        \"protein folding\": \"3.4. Protein Folding and Chaperones\",\n",
    "        \"chaperone\": \"3.4. Protein Folding and Chaperones\",\n",
    "        \"proteasome\": \"3.5. Proteasome System\",\n",
    "        \"protease\": \"3.5. Proteasome System\",\n",
    "        \"amino acid biosynthesis\": \"3.6. Amino Acid Biosynthesis\",\n",
    "\n",
    "        # Membrane Transport\n",
    "        \"abc transporter\": \"4.1. ABC Transporters\",\n",
    "        \"pilus\": \"4.2. Pilus and Flagella Formation\",\n",
    "        \"flagella\": \"4.2. Pilus and Flagella Formation\",\n",
    "        \"peptidoglycan\": \"4.3. Cell Wall Maintenance\",\n",
    "        \"s-layer\": \"4.3. Cell Wall Maintenance\",\n",
    "        \"lipid membrane\": \"4.4. Lipid Membrane Synthesis\",\n",
    "        \"glycerophospholipid\": \"4.4. Lipid Membrane Synthesis\",\n",
    "\n",
    "        # Stress Response\n",
    "        \"oxidative stress\": \"5.1. Oxidative Stress Response\",\n",
    "        \"superoxide dismutase\": \"5.1. Oxidative Stress Response\",\n",
    "        \"catalase\": \"5.1. Oxidative Stress Response\",\n",
    "        \"peroxidase\": \"5.1. Oxidative Stress Response\",\n",
    "        \"heat shock\": \"5.2. Heat Shock Proteins\",\n",
    "        \"cold shock\": \"5.3. Cold Shock Proteins\",\n",
    "        # Biomolecule Synthesis\n",
    "        \"nucleotide biosynthesis\": \"6.2. Nucleotide Biosynthesis\",\n",
    "    }\n",
    "    # Add key pathway identifiers that should be prioritized as universals\n",
    "    core_universals = {\n",
    "            \"glycolysis\": \"1.1. Glycolysis\",\n",
    "            \"Pyruvate metabolism\": \"1.1. Glycolysis\",\n",
    "            \"gluconeogenesis\": \"1.2. Gluconeogenesis\",\n",
    "            \"pentose phosphate\": \"1.3. Pentose Phosphate Pathway\",\n",
    "            \"tca\": \"1.4. Krebs/TCA Cycle\",\n",
    "            \"krebs\": \"1.4. Krebs/TCA Cycle\",\n",
    "            \"citric acid\": \"1.4. Krebs/TCA Cycle\",\n",
    "            \"electron transport\": \"1.5. Electron Transport Chain\",\n",
    "            \"oxidative phosphorylation\": \"1.5. Electron Transport Chain\",\n",
    "            \"dna replication\": \"3.1. DNA Replication\",\n",
    "            \"Chromosome and associated proteins\": \"3.1. DNA Replication\",\n",
    "            \"transcription\": \"3.2. Transcription\", \n",
    "            \"Messenger RNA biogenesis\": \"3.2. Transcription\",\n",
    "            \"Transfer RNA biogenesis\": \"3.2. Transcription\", \n",
    "            \"translation\": \"3.3. Translation\"\n",
    "        }\n",
    "    # Define mappings from KEGG KO IDs to universal pathway categories\n",
    "    kegg_universal_map = {\n",
    "        # Central Carbon Metabolism\n",
    "        \"ko00010\": \"1.1. Glycolysis\",                # Glycolysis / Gluconeogenesis\n",
    "        \"ko00030\": \"1.3. Pentose Phosphate Pathway\", # Pentose phosphate pathway\n",
    "        \"ko00020\": \"1.4. Krebs/TCA Cycle\",           # Citrate cycle (TCA cycle)\n",
    "        \"ko00190\": \"1.5. Electron Transport Chain\",  # Oxidative phosphorylation\n",
    "        \n",
    "        # Amino Acid Metabolism\n",
    "        \"ko00250\": \"6.1. Amino Acid Biosynthesis\",   # Alanine, aspartate and glutamate metabolism\n",
    "        \"ko00260\": \"6.1. Amino Acid Biosynthesis\",   # Glycine, serine and threonine metabolism\n",
    "        \"ko00270\": \"6.1. Amino Acid Biosynthesis\",   # Cysteine and methionine metabolism\n",
    "        \"ko00280\": \"6.1. Amino Acid Biosynthesis\",   # Valine, leucine and isoleucine degradation\n",
    "        \"ko00290\": \"6.1. Amino Acid Biosynthesis\",   # Valine, leucine and isoleucine biosynthesis\n",
    "        \"ko00520\": \"4.3. Cell Wall Maintenance\",  # Amino sugar and nucleotide sugar metabolism\n",
    "        \n",
    "        # Lipid Metabolism\n",
    "        \"ko00061\": \"2.1. Fatty Acid Synthesis\",      # Fatty acid biosynthesis\n",
    "        \"ko00071\": \"2.2. Fatty Acid Oxidation\",      # Fatty acid degradation\n",
    "        \n",
    "        # Nucleotide Metabolism\n",
    "        \"ko00230\": \"6.2. Nucleotide Biosynthesis\",   # Purine metabolism\n",
    "        \"ko00240\": \"6.2. Nucleotide Biosynthesis\",   # Pyrimidine metabolism\n",
    "        \n",
    "        # Replication and Repair\n",
    "        \"ko03030\": \"3.1. DNA Replication\",           # DNA replicatio       \n",
    "        # Transcription/Translation\n",
    "        \"ko03010\": \"3.3. Translation\",               # Ribosome\n",
    "        \"ko00970\": \"3.3. Translation\",\n",
    "        \"ko03020\": \"3.2. Transcription\",             # RNA polymerase\n",
    "        \"ko00970\": \"3.2. Transcription\",\n",
    "        \"ko03060\": \"3.4. Protein Folding and Chaperones\", # Protein export\n",
    "    }\n",
    "    # Add specific amino acid biosynthesis pathways\n",
    "    amino_acids = [\"isoleucine\", \"valine\", \"leucine\", \"alanine\", \"arginine\",  \"asparagine\", \"aspartate\", \"cysteine\", \"glutamate\", \"glutamine\",\n",
    "                  \"glycine\", \"histidine\", \"lysine\", \"methionine\", \"phenylalanine\",  \"proline\", \"serine\", \"threonine\", \"tryptophan\", \"tyrosine\"]\n",
    "     \n",
    "    # Define terms that should always be considered niche-specific even if they contain universal keywords\n",
    "    niche_specific_terms = {\n",
    "        \"sos dna repair\", \"dna repair\", \"dna repair and recombination\",  \"flavonoid biosynthesis\", \"antibiotic biosynthesis\", \"antimicrobial resistance\",\n",
    "        \"viral proteins\"}\n",
    "\n",
    "    for aa in amino_acids:\n",
    "        universal_pathways[f\"{aa} biosynthesis\"] = \"6.1. Amino Acid Biosynthesis\"\n",
    "\n",
    "    # Convert universal pathways to lowercase\n",
    "    universal_pathways = {k.lower(): v for k, v in universal_pathways.items()}\n",
    "    core_universals = {k.lower(): v for k, v in core_universals.items()}\n",
    "    niche_specific_terms = {term.lower() for term in niche_specific_terms}\n",
    "\n",
    "    # Then merge for the full matching\n",
    "    all_universals = universal_pathways.copy()\n",
    "    all_universals.update(core_universals)\n",
    "\n",
    "    #===============\n",
    "\n",
    "    # Initialize classification columns\n",
    "    if 'idx' in results.columns:\n",
    "        results[\"idx\"] = results[\"idx\"].astype(int)\n",
    "    if 'Category' in results.columns:\n",
    "        results[\"Category\"] = results[\"Category\"].astype(int)\n",
    "    results[\"pathway_classification\"] = \"niche-specific\"  # Default\n",
    "    results[\"universal_pathways\"] = \"\"\n",
    "    results[\"niche_specific_pathways\"] = \"\"\n",
    "\n",
    "    # Function to classify a pathway\n",
    "    def classify_pathway(pathway_text):\n",
    "        if pd.isna(pathway_text) or pathway_text == \"\":\n",
    "            return \"unknown\", \"\", \"\"\n",
    "            \n",
    "        # Handle both string and list inputs for pathway_text\n",
    "        if isinstance(pathway_text, list):\n",
    "            pathway_text = \"; \".join(pathway_text)\n",
    "\n",
    "        pathway_text_lower = pathway_text.lower()\n",
    "        \n",
    "        # Check if any niche-specific terms are present - these override universal classification\n",
    "        for term in niche_specific_terms:\n",
    "            if term in pathway_text_lower:\n",
    "                return \"niche-specific\", \"\", pathway_text\n",
    "        \n",
    "        matched_universals = set()\n",
    "        matched_core = False\n",
    "        \n",
    "        # Break pathway text into components (semicolon-separated)\n",
    "        path_components = [p.strip() for p in pathway_text_lower.split(';')]\n",
    "        \n",
    "        # 1. Handle KEGG IDs first with strong priority\n",
    "        kegg_ids = re.findall(r'\\[br:ko(\\d+)\\]', pathway_text_lower)\n",
    "        for ko_id in kegg_ids:\n",
    "            ko_key = f\"ko{ko_id}\"\n",
    "            if ko_key in kegg_universal_map:\n",
    "                matched_universals.add(kegg_universal_map[ko_key])\n",
    "                matched_core = True  # KEGG matches in your map are considered core\n",
    "        \n",
    "        # 2. Check for exact or phrase matches with core universals\n",
    "        for key, value in core_universals.items():\n",
    "            if re.search(r'\\b' + re.escape(key) + r'\\b', pathway_text_lower):\n",
    "                matched_universals.add(value)\n",
    "                matched_core = True\n",
    "        \n",
    "        # 3. If no core match, check for broader universal matches\n",
    "        if not matched_core:\n",
    "            for key, value in all_universals.items():\n",
    "                if re.search(r'\\b' + re.escape(key) + r'\\b', pathway_text_lower):\n",
    "                    matched_universals.add(value)\n",
    "        \n",
    "        # 4. Identify niche-specific content by removing universal terms\n",
    "        niche_text = pathway_text_lower\n",
    "        for uni_term in [k for k, v in all_universals.items() if v in matched_universals]:\n",
    "            niche_text = re.sub(r'\\b' + re.escape(uni_term) + r'\\b', '', niche_text)\n",
    "        \n",
    "        # Clean up niche text\n",
    "        niche_text = re.sub(r'\\[br:ko\\d+\\]', '', niche_text)  # Remove KEGG IDs\n",
    "        niche_text = re.sub(r'enzymes with ec numbers', '', niche_text)  # Remove problematic phrase\n",
    "        niche_text = re.sub(r'\\s+', ' ', niche_text).strip()\n",
    "        niche_text = re.sub(r'[,;]\\s*[,;]', ',', niche_text)\n",
    "        niche_text = re.sub(r'^[,;]\\s*|\\s*[,;]$', '', niche_text)\n",
    "        \n",
    "        # Split into terms for more accurate classification\n",
    "        niche_terms = [term for term in niche_text.split() if len(term) > 3]\n",
    "        \n",
    "        # 5. Classify based on matched patterns\n",
    "        if matched_universals:\n",
    "            if matched_core or len(niche_terms) < 5:  # Core match or few niche terms\n",
    "                classification = \"universal\"\n",
    "            else:\n",
    "                classification = \"mixed\"\n",
    "        elif niche_text.strip():\n",
    "            classification = \"niche-specific\"\n",
    "        else:\n",
    "            classification = \"unknown\"\n",
    "        \n",
    "        return classification, \", \".join(sorted(list(matched_universals))), niche_text.strip()\n",
    "    \n",
    "    # Apply classification to pathways column if it exists\n",
    "    if 'pathways' in results.columns:\n",
    "        # Ensure pathways are strings\n",
    "        results['pathways'] = results['pathways'].astype(str).fillna('')\n",
    "        \n",
    "        classifications = results['pathways'].apply(classify_pathway)\n",
    "        results[\"pathway_classification\"] = classifications.apply(lambda x: x[0])\n",
    "        results[\"universal_pathways\"] = classifications.apply(lambda x: x[1])\n",
    "        results[\"niche_specific_pathways\"] = classifications.apply(lambda x: x[2])\n",
    "\n",
    "        # Count each classification\n",
    "        class_counts = results['pathway_classification'].value_counts()\n",
    "        print(\"Pathway classification results:\")\n",
    "        for cls, count in class_counts.items():\n",
    "            print(f\"  - {cls}: {count} ({count/len(results):.1%})\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def generate_specificity_report(classified_results, output_file=None):\n",
    "    \"\"\"\n",
    "    Generate a report of pathway classifications with examples\n",
    "\n",
    "    Parameters:  classified_results : DataFrame with pathway classification\n",
    "                 output_file : str, optional path to save Excel report\n",
    "    Returns:     Dict with summary statistics\n",
    "    \"\"\"\n",
    "    #def generate_specificity_report(classified_results, output_file=None):\n",
    "    \"\"\"\n",
    "    Generate a summary report of pathway classifications with useful statistics\n",
    "    without creating a separate DataFrame that caused the array length error.\n",
    "\n",
    "    Parameters:\n",
    "        classified_results : DataFrame with pathway classification\n",
    "        output_file : str, optional path to save summary (not used in this simplified version)\n",
    "\n",
    "    Returns:\n",
    "        classified_results : The same DataFrame with summary printed to screen\n",
    "    \"\"\"\n",
    "    # Calculate basic statistics\n",
    "    total_pathways = len(classified_results)\n",
    "\n",
    "    # Count classifications\n",
    "    classification_counts = classified_results['pathway_classification'].value_counts().to_dict()\n",
    "\n",
    "    # Calculate percentages for each classification\n",
    "    percentages = {\n",
    "        cls: (count / total_pathways * 100)\n",
    "        for cls, count in classification_counts.items()\n",
    "    }\n",
    "\n",
    "    # Get universal pathway counts if available\n",
    "    universal_pathway_counts = {}\n",
    "    if 'universal_pathways' in classified_results.columns:\n",
    "        all_universal = []\n",
    "        for pathways in classified_results['universal_pathways'].dropna():\n",
    "            if pathways:\n",
    "                all_universal.extend([p.strip() for p in pathways.split(',')])\n",
    "\n",
    "        from collections import Counter\n",
    "        universal_pathway_counts = dict(Counter(all_universal))\n",
    "\n",
    "    # Get examples of niche-specific pathways\n",
    "    niche_examples = []\n",
    "    if 'niche_specific_pathways' in classified_results.columns:\n",
    "        niche_examples = classified_results.loc[\n",
    "            classified_results['pathway_classification'] == 'niche-specific',\n",
    "            'niche_specific_pathways'\n",
    "        ].dropna().unique()[:20]  # Top 20 examples\n",
    "\n",
    "    # Print comprehensive summary to screen\n",
    "    print(\"\\n=== PATHWAY SPECIFICITY ANALYSIS ===\")\n",
    "    print(f\"Total pathways analyzed: {total_pathways}\")\n",
    "\n",
    "    print(\"\\nClassification distribution:\")\n",
    "    for cls, count in classification_counts.items():\n",
    "        print(f\"  - {cls}: {count} ({percentages[cls]:.1f}%)\")\n",
    "\n",
    "    # Get all universal pathway entries\n",
    "    universal_entries = classified_results[classified_results['pathway_classification'].isin(['universal', 'mixed'])]['universal_pathways'].dropna()\n",
    "    \n",
    "    # Split multi-pathway entries and count individual pathways\n",
    "    all_pathways = []\n",
    "    for entry in universal_entries:\n",
    "        pathways = [p.strip() for p in entry.split(',')]\n",
    "        all_pathways.extend(pathways)\n",
    "    \n",
    "    # Count occurrences of each unique pathway\n",
    "    from collections import Counter\n",
    "    pathway_counts = Counter(all_pathways)\n",
    "    \n",
    "    # Print sorted results\n",
    "    print(\"Universal pathway distribution (without duplication):\")\n",
    "    for pathway, count in sorted(pathway_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "        if pathway:  # Skip empty entries\n",
    "            print(f\"  - {pathway}: {count} occurrences\")\n",
    "\n",
    "    # Return summary statistics as a dictionary as documented\n",
    "    summary = {\n",
    "        'total_pathways': total_pathways,\n",
    "        'classification_counts': classification_counts,\n",
    "        'percentages': percentages,\n",
    "        'universal_pathway_counts': universal_pathway_counts,\n",
    "        'niche_examples': list(niche_examples)\n",
    "    }\n",
    "\n",
    "    # Return the original DataFrame \n",
    "    return classified_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VUcwwFReAAON"
   },
   "source": [
    "## 10.4. Patterns Separtation\n",
    "The separate_by_pattern function organizes protein-genus pairs into three categories based on their abundance patterns across risk levels:\n",
    "\n",
    "Increasing patterns - Proteins showing higher abundance with increasing corrosion risk, suggesting involvement in corrosion processes.\n",
    "Inverse patterns - Proteins decreasing as corrosion risk increases. These may represent protective mechanisms being overwhelmed or protective bacteria that actively inhibit corrosion in healthy systems (Category 1) but decline as systems deteriorate, allowing corrosion-promoting organisms to flourish.\n",
    "Constant/Mixed patterns - Proteins showing consistent abundance or complex patterns without clear directional trends.\n",
    "\n",
    "The function handles any unaccounted patterns, sorts by significance score, and provides summary statistics on distribution and significance across categories, enabling focused analysis on patterns most relevant to corrosion mechanisms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-19T16:14:53.728878Z",
     "iopub.status.idle": "2025-04-19T16:14:53.729366Z",
     "shell.execute_reply": "2025-04-19T16:14:53.729148Z"
    },
    "id": "fHfwD3dkAIG0",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def separate_by_pattern(classified_results):\n",
    "    \"\"\"\n",
    "    Separates classified_results into groups based on their pattern\n",
    "    of presence across risk categories, and sorts by significance.\n",
    "\n",
    "    Returns:\n",
    "      increasing_df : DataFrame with patterns suggesting increasing risk\n",
    "      inverse_df : DataFrame with patterns suggesting decreasing risk\n",
    "      constant_df : DataFrame with other patterns\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid rename warning\n",
    "    results = classified_results.copy()\n",
    "\n",
    "    # Rename to be able to use the name pattern\n",
    "    if 'pattern_category' in results.columns and 'pattern' not in results.columns:\n",
    "        results = results.rename(columns={'pattern_category': 'pattern'})\n",
    "\n",
    "    # Define patterns associated with increasing risk\n",
    "    increasing_patterns = ['increasing']\n",
    "\n",
    "    # Define patterns associated with decreasing risk\n",
    "    inverse_patterns = ['decreasing']\n",
    "\n",
    "    # Define patterns that are ambiguous or mixed\n",
    "    mixed_patterns = ['constant', 'other']\n",
    "\n",
    "    # Separate based on patterns\n",
    "    increasing_df = results[results['pattern'].isin(increasing_patterns)].copy()\n",
    "    inverse_df = results[results['pattern'].isin(inverse_patterns)].copy()\n",
    "    constant_df = results[results['pattern'].isin(mixed_patterns)].copy()\n",
    "\n",
    "    # Check if there are any patterns not accounted for\n",
    "    unaccounted_patterns = set(results['pattern'].unique()) - set(increasing_patterns + inverse_patterns + mixed_patterns)\n",
    "    if unaccounted_patterns:\n",
    "        print(f\"Warning: Found {len(unaccounted_patterns)} unaccounted patterns: {unaccounted_patterns}\")\n",
    "        # Add rows with unaccounted patterns to constant_df\n",
    "        unaccounted_df = results[results['pattern'].isin(unaccounted_patterns)]\n",
    "        constant_df = pd.concat([constant_df, unaccounted_df])\n",
    "\n",
    "    # Sort each dataframe by significance_score if it exists\n",
    "    if 'significance_score' in results.columns:\n",
    "        increasing_df = increasing_df.sort_values('significance_score', ascending=False)\n",
    "        inverse_df = inverse_df.sort_values('significance_score', ascending=False)\n",
    "        constant_df = constant_df.sort_values('significance_score', ascending=False)\n",
    "\n",
    "    print(f\"Separated {len(increasing_df)} increasing patterns, {len(inverse_df)} inverse patterns, and {len(constant_df)} mixed/other patterns.\")\n",
    "\n",
    "    # Calculate average significance score for each group if available\n",
    "    if 'significance_score' in results.columns:\n",
    "        inc_avg = increasing_df['significance_score'].mean() if len(increasing_df) > 0 else 0\n",
    "        inv_avg = inverse_df['significance_score'].mean() if len(inverse_df) > 0 else 0\n",
    "        con_avg = constant_df['significance_score'].mean() if len(constant_df) > 0 else 0\n",
    "        print(f\"Average significance scores: Increasing={inc_avg:.2f}, Inverse={inv_avg:.2f}, Constant={con_avg:.2f}\")\n",
    "\n",
    "    return increasing_df, inverse_df, constant_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YCHfeZHEtB9M"
   },
   "source": [
    "## 10.5. Priorise Markers\n",
    "The prioritize_markers function implements a sophisticated scoring system to rank protein-genus pairs based on their potential relevance to corrosion processes. The function combines statistical pattern significance with rich biological context, using a multi-dimensional approach that evaluates candidates from multiple perspectives. The scoring system incorporates several key components:\n",
    "\n",
    "Pattern significance (up to 3 points) - Leverages previously calculated significance scores that consider specificity and frequency of proteins across risk categories. Functional categories (up to 4.5 points) - Evaluates proteins against comprehensive functional categories relevant to corrosion, including iron/sulfur redox, acid production, biofilm formation, and other mechanisms, with additional points for synergistic combinations.Pathways and hierarchy relevance (up to 3 points each) - Uses weighted keyword groups to identify corrosion-relevant pathways and functional hierarchies while avoiding redundant scoring. Metal interaction synergies (up to 2 points) - Identifies key metal combinations known to accelerate corrosion, such as iron-sulfur interactions, metal complexation, and Fe-S clusters. Pathway classification tier (up to 2 points) - Assigns higher scores to niche-specific pathways likely specialized for corrosion environments. Effect size (up to 2 points) - Considers the magnitude of abundance changes across risk categories. Corrosion relevance (up to 3 points) - Incorporates pre-existing relevance scores from metadata.\n",
    "\n",
    "The function produces a combined score capped at 10 points, creating a balanced ranking that identifies proteins most likely to play meaningful roles in corrosion processes. Additionally, it generates explanatory columns documenting which factors contributed to each protein's score, enhancing interpretability and providing biological context for the statistical rankings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prioritize_markers(increasing_results):\n",
    "    \"\"\"\n",
    "    Prioritize markers based on statistical and biological significance,\n",
    "    incorporating Frequency and keeping only specific patterns.\n",
    "    \"\"\"\n",
    "    # Try to import scoring modules - adding support for both environments\n",
    "    try:\n",
    "        # First try the kaggle path\n",
    "        sys.path.append('/kaggle/input/corrosion-scoring')\n",
    "        from global_terms import (\n",
    "            metal_terms,\n",
    "            corrosion_mechanisms,\n",
    "            pathway_categories,\n",
    "            organic_categories,\n",
    "            corrosion_synergies,\n",
    "            functional_categories,\n",
    "            corrosion_keyword_groups,\n",
    "            metal_mapping\n",
    "        )\n",
    "        import scoring_system as score_sys\n",
    "        print(\"Successfully imported scoring modules from Kaggle path\")\n",
    "    except ImportError:\n",
    "        try:\n",
    "            # Then try the local path\n",
    "            sys.path.append('/home/beatriz/MIC/2_Micro/corrosion_scoring')\n",
    "            from corrosion_scoring.global_terms import (\n",
    "                metal_terms,\n",
    "                corrosion_mechanisms,\n",
    "                pathway_categories,\n",
    "                organic_categories,\n",
    "                corrosion_synergies,\n",
    "                functional_categories,\n",
    "                corrosion_keyword_groups,\n",
    "                metal_mapping\n",
    "            )\n",
    "            import corrosion_scoring.scoring_system as score_sys\n",
    "            print(\"Successfully imported scoring modules from local path\")\n",
    "        except ImportError as e:\n",
    "            print(f\"Warning: Could not import scoring modules: {e}\")\n",
    "            return increasing_results  # Return original if modules can't be loaded\n",
    "\n",
    "    # Copy to avoid modifying original\n",
    "    results = increasing_results.copy(deep=False)\n",
    "    \n",
    "    # Initialize combined score\n",
    "    results['combined_score'] = 0.0\n",
    "    \n",
    "    # 1. Pattern significance COMPONENT\n",
    "    if 'significance_score' in results.columns:\n",
    "        results['combined_score'] = results['significance_score'] * 3\n",
    "    else:\n",
    "        # Add basic scoring components if significance_score is not available\n",
    "        if 'specificity' in results.columns:\n",
    "            results['specificity_score'] = results['specificity'] * 2\n",
    "            results['combined_score'] += results['specificity_score']\n",
    "            \n",
    "        if 'Frequency' in results.columns:\n",
    "            # Frequency normalization: scale to 0-2.0 range\n",
    "            freq_max = results['Frequency'].max()\n",
    "            freq_min = results['Frequency'].min()\n",
    "            if freq_max != freq_min:\n",
    "                results['frequency_score'] = (results['Frequency'] - freq_min) / (freq_max - freq_min) * 2.0\n",
    "            else:\n",
    "                results['frequency_score'] = 0.0\n",
    "            results['combined_score'] += results['frequency_score']\n",
    "    \n",
    "    # 2. DATABASE DERIVED SCORES using scoring_system module\n",
    "    def calculate_database_scores(row):\n",
    "        \"\"\"Use the scoring_system module to calculate database-derived scores\"\"\"\n",
    "        # Extract fields needed for scoring\n",
    "        enzyme_names = row.get('enzyme_names', []) if isinstance(row.get('enzyme_names', []), list) else []\n",
    "        enzyme_class = row.get('enzyme_class', '') if isinstance(row.get('enzyme_class', ''), str) else ''\n",
    "        pathways = row.get('pathways', []) if isinstance(row.get('pathways', []), list) else []\n",
    "        if isinstance(pathways, str):\n",
    "            pathways = [pathways]\n",
    "        \n",
    "        metals_consolidated = row.get('metals_consolidated', [])\n",
    "        if isinstance(metals_consolidated, str):\n",
    "            # Convert string representation to list\n",
    "            metals_consolidated = [m.strip() for m in metals_consolidated.split(',')]\n",
    "        \n",
    "        # Build text for overall scoring\n",
    "        combined_text = ' '.join(enzyme_names) + ' ' + enzyme_class + ' ' + ' '.join(pathways)\n",
    "        \n",
    "        # Call existing scoring_system functions\n",
    "        overall_scores = score_sys.calculate_overall_scores(combined_text, brenda_metals=metals_consolidated)\n",
    "        pathway_score, pathway_categories = score_sys.calculate_pathway_score(pathways, enzyme_names, enzyme_class)\n",
    "        synergy_score = score_sys.check_metal_organic_synergy(metals_consolidated, enzyme_names, pathways)\n",
    "        \n",
    "        # Calculate final corrosion relevance score\n",
    "        corrosion_score, corrosion_category = score_sys.calculate_corrosion_relevance_score(\n",
    "            overall_scores.get('overall_metal_score', 0),\n",
    "            overall_scores.get('overall_corrosion_score', 0),\n",
    "            pathway_score,\n",
    "            overall_scores.get('overall_organic_process_score', 0),\n",
    "            overall_scores.get('overall_keyword_score', 0),\n",
    "            synergy_score,\n",
    "            overall_scores.get('overall_functional_score', 0)\n",
    "        )\n",
    "        \n",
    "        # Return all scores as a Series\n",
    "        return pd.Series({\n",
    "            'overall_metal_score': overall_scores.get('overall_metal_score', 0),\n",
    "            'metals_involved': ', '.join(overall_scores.get('metals_involved', [])),\n",
    "            'overall_corrosion_score': overall_scores.get('overall_corrosion_score', 0),\n",
    "            'corrosion_mechanisms': ', '.join(overall_scores.get('corrosion_mechanisms', [])),\n",
    "            'overall_functional_score': overall_scores.get('overall_functional_score', 0),\n",
    "            'overall_synergy_score': overall_scores.get('overall_synergy_score', 0),\n",
    "            'overall_organic_process_score': overall_scores.get('overall_organic_process_score', 0),\n",
    "            'overall_keyword_score': overall_scores.get('overall_keyword_score', 0),\n",
    "            'pathway_score': pathway_score,\n",
    "            'synergy_score': synergy_score,\n",
    "            'corrosion_relevance_score': corrosion_score,\n",
    "            'corrosion_relevance': corrosion_category\n",
    "        })\n",
    "    \n",
    "    # Apply database scoring to each row\n",
    "    try:\n",
    "        db_scores = results.apply(calculate_database_scores, axis=1)\n",
    "        \n",
    "        # Add score columns to results\n",
    "        for col in db_scores.columns:\n",
    "            results[col] = db_scores[col]\n",
    "        \n",
    "        # Calculate database combined score with weights\n",
    "        database_score = (\n",
    "            results['overall_metal_score'] * 0.5 +\n",
    "            results['overall_corrosion_score'] * 0.7 +\n",
    "            results['overall_functional_score'] * 0.8 +\n",
    "            results['overall_synergy_score'] * 0.6 +\n",
    "            results['overall_organic_process_score'] * 0.5 +\n",
    "            results['overall_keyword_score'] * 0.4\n",
    "        )\n",
    "        results['database_combined_score'] = database_score.clip(upper=4.0)\n",
    "        results['combined_score'] += results['database_combined_score']\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Error in database scoring: {e}\")\n",
    "    \n",
    "    # 3. PATHWAYS AND HIERARCHY COMPONENT (up to 3 points)\n",
    "    # Assign weights to keyword groups\n",
    "    group_weights = {\n",
    "        'iron_sulfur_redox': 0.5,\n",
    "        'ocre': 0.5,\n",
    "        'acid_production': 0.5,\n",
    "        'electron_transfer': 0.4,\n",
    "        'biofilm': 0.4,\n",
    "        'sulfide': 0.4,\n",
    "        'metal_binding': 0.3,\n",
    "        'nitrogen': 0.3,\n",
    "        'manganese': 0.3,\n",
    "        'methanogenesis': 0.2,\n",
    "        'fumarate': 0.2,\n",
    "        'hydrogen': 0.2,\n",
    "        'oxygen': 0.2,\n",
    "        'corrosion_general': 0.5\n",
    "    }\n",
    "    \n",
    "    # Helper function for scoring pathways and hierarchy\n",
    "    def score_weighted_keyword_groups(text, keyword_dict, weight_dict, max_score=3.0):\n",
    "        if not isinstance(text, str):\n",
    "            return 0.0, \"\"\n",
    "        \n",
    "        score = 0.0\n",
    "        text = text.lower()\n",
    "        matched_groups = []\n",
    "        \n",
    "        for group_name, keywords in keyword_dict.items():\n",
    "            if any(term.lower() in text for term in keywords):\n",
    "                score += weight_dict.get(group_name, 0)\n",
    "                matched_groups.append(group_name)\n",
    "                \n",
    "        return min(score, max_score), \"; \".join(matched_groups)\n",
    "    \n",
    "    # Score pathways\n",
    "    if 'pathways' in results.columns:\n",
    "        pathway_results = results['pathways'].apply(\n",
    "            lambda x: score_weighted_keyword_groups(x, corrosion_keyword_groups, group_weights, 3.0)\n",
    "        )\n",
    "        results['pathways_score'] = pathway_results.apply(lambda x: x[0])\n",
    "        results['pathway_matches'] = pathway_results.apply(lambda x: x[1])\n",
    "        results['combined_score'] += results['pathways_score']\n",
    "        \n",
    "    # Apply tier scores based on pathway classification\n",
    "    tier_scores = {\n",
    "        'niche-specific': 2.0,\n",
    "        'mixed': 1.0,\n",
    "        'universal': 0.5,\n",
    "    }\n",
    "    \n",
    "    if 'pathway_classification' in results.columns:\n",
    "        results['tier_score'] = results['pathway_classification'].map(tier_scores).fillna(0.5)\n",
    "        results['combined_score'] += results['tier_score']\n",
    "    \n",
    "    # Score hierarchy\n",
    "    if 'hierarchy' in results.columns:\n",
    "        hierarchy_results = results['hierarchy'].apply(\n",
    "            lambda x: score_weighted_keyword_groups(x, corrosion_keyword_groups, group_weights, 3.0)\n",
    "        )\n",
    "        results['hierarchy_score'] = hierarchy_results.apply(lambda x: x[0])\n",
    "        results['hierarchy_matches'] = hierarchy_results.apply(lambda x: x[1])\n",
    "        results['combined_score'] += results['hierarchy_score']\n",
    "    \n",
    "    # 4. FUNCTIONAL CATEGORY GROUPING (up to 4.5 points)\n",
    "    def score_functional_categories(mech_str):\n",
    "        if not isinstance(mech_str, str) or not mech_str:\n",
    "            return 0.0, \"\"\n",
    "        \n",
    "        present_categories = []\n",
    "        total_score = 0.0\n",
    "        \n",
    "        for category, details in functional_categories.items():\n",
    "            if any(term in mech_str.lower() for term in details['terms']):\n",
    "                total_score += details['score']\n",
    "                present_categories.append(category)\n",
    "        \n",
    "        # Add synergy bonus for diversity\n",
    "        if len(present_categories) >= 3:\n",
    "            total_score += 1.0  # strong synergistic interaction\n",
    "        elif len(present_categories) == 2:\n",
    "            total_score += 0.5  # moderate synergy\n",
    "            \n",
    "        return min(total_score, 4.5), \"; \".join(present_categories)\n",
    "    \n",
    "    if 'corrosion_mechanisms' in results.columns:\n",
    "        scores_and_cats = results['corrosion_mechanisms'].apply(score_functional_categories)\n",
    "        results['functional_mechanisms_score'] = scores_and_cats.apply(lambda x: x[0])\n",
    "        results['functional_categories_present'] = scores_and_cats.apply(lambda x: x[1])\n",
    "        results['combined_score'] += results['functional_mechanisms_score']\n",
    "    \n",
    "    # 5. EFFECT SIZE COMPONENT (up to 2 points)\n",
    "    if 'max_log2fc' in results.columns:\n",
    "        # Convert to absolute values for magnitude\n",
    "        fc_magnitude = results['max_log2fc'].abs()\n",
    "        \n",
    "        # Score based on fold change magnitude\n",
    "        results['fc_score'] = 0.0\n",
    "        results.loc[fc_magnitude >= 4, 'fc_score'] = 2.0  # >16x change\n",
    "        results.loc[(fc_magnitude >= 2) & (fc_magnitude < 4), 'fc_score'] = 1.5  # 4-16x change\n",
    "        results.loc[(fc_magnitude >= 1) & (fc_magnitude < 2), 'fc_score'] = 1.0  # 2-4x change\n",
    "        results.loc[(fc_magnitude > 0) & (fc_magnitude < 1), 'fc_score'] = 0.5  # <2x change\n",
    "        \n",
    "        results['combined_score'] += results['fc_score']\n",
    "    \n",
    "    # 6. CORROSION RELEVANCE COMPONENT (up to 3 points)\n",
    "    # Use either numerical or categorical scoring, whichever gives higher score\n",
    "    if 'corrosion_relevance_score' in results.columns:\n",
    "        # Convert to numeric first, coercing errors to NaN\n",
    "        results['corrosion_relevance_score'] = pd.to_numeric(results['corrosion_relevance_score'], errors='coerce')\n",
    "        results['corrosion_score_scaled'] = results['corrosion_relevance_score'] / 5.0 * 3.0  # Scale to max 3 points\n",
    "    \n",
    "    # Add categorical scoring if available\n",
    "    if 'corrosion_relevance' in results.columns:\n",
    "        corrosion_cat_scores = {'high': 3.0, 'medium': 2.0, 'low': 1.0}\n",
    "        results['corrosion_cat_score'] = results['corrosion_relevance'].map(corrosion_cat_scores).fillna(0.0)\n",
    "        \n",
    "        # Use the higher score between numerical and categorical if both are available\n",
    "        if 'corrosion_score_scaled' in results.columns:\n",
    "            results['corrosion_final_score'] = results[['corrosion_score_scaled', 'corrosion_cat_score']].max(axis=1)\n",
    "        else:\n",
    "            results['corrosion_final_score'] = results['corrosion_cat_score']\n",
    "            \n",
    "        results['combined_score'] += results['corrosion_final_score']\n",
    "    elif 'corrosion_score_scaled' in results.columns:\n",
    "        results['combined_score'] += results['corrosion_score_scaled']\n",
    "    \n",
    "    # 7. Create explanation for combined score\n",
    "    def create_explanation(row):\n",
    "        # Get top scoring components\n",
    "        score_components = {\n",
    "            'Functional mechanisms': row.get('functional_mechanisms_score', 0),\n",
    "            'Pathways': row.get('pathways_score', 0),\n",
    "            'Metal interactions': row.get('overall_metal_score', 0) + row.get('synergy_score', 0),\n",
    "            'Effect size': row.get('fc_score', 0),\n",
    "            'Corrosion relevance': row.get('corrosion_final_score', 0),\n",
    "            'Tier': row.get('tier_score', 0)\n",
    "        }\n",
    "        \n",
    "        # Sort by score and get top 3\n",
    "        top_components = sorted(score_components.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "        \n",
    "        # Only include components with non-zero scores\n",
    "        explanation = [f\"{name} ({score:.1f})\" for name, score in top_components if score > 0]\n",
    "        \n",
    "        # Add functional categories if available\n",
    "        if 'functional_categories_present' in row and row['functional_categories_present']:\n",
    "            explanation.append(f\"Categories: {row['functional_categories_present']}\")\n",
    "            \n",
    "        return \" | \".join(explanation)\n",
    "    \n",
    "    results['explanation'] = results.apply(create_explanation, axis=1)\n",
    "    \n",
    "    print(f\"[{datetime.now().strftime('%H:%M:%S')}] Prioritization complete\")\n",
    "    print(f\"Score distribution: Mean={results['combined_score'].mean():.2f}, \"\n",
    "          f\"Median={results['combined_score'].median():.2f}, \"\n",
    "          f\"Min={results['combined_score'].min():.2f}, \"\n",
    "          f\"Max={results['combined_score'].max():.2f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PCZ-qTrpel1y"
   },
   "source": [
    "## 10.6. Balancing Genus Representation\n",
    "\n",
    "The balance_genus_representation function addresses the challenge of over-representation by dominant genera in the protein dataset. Without appropriate balancing, a few abundant genera can dominate the results, obscuring potentially important contributions from less abundant but functionally significant microorganisms.The function implements a two-tier filtering approach:\n",
    "\n",
    "* Abundance Threshold Filtering - Uses genus-specific knee-point thresholds to filter out low-abundance proteins that are likely not metabolically significant. These thresholds identify natural breakpoints in abundance distributions for each genus, ensuring that only proteins present at functionally relevant levels are considered.\n",
    "* Per-Genus Selection - After abundance filtering, selects the top proteins per genus based on their combined score, maintaining a consistent representation limit across all genera.By selecting top protein per genus, the aim is to prevent bias toward dominant genera by enforcing representation limits whiles maintaining biological diversity by ensuring all genera contribute to the final dataset. \n",
    "\n",
    "The function also incorporates a fallback mechanism: if the abundance threshold filtering removes all proteins for a particular genus, it defaults to selecting the top proteins by combined score, ensuring all genera remain represented in the final dataset.\n",
    "By balancing representation across genera while maintaining a focus on functionally significant proteins, this approach creates a more diverse and biologically meaningful set of protein-genus pairs for downstream analysis and visualizatio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-19T16:14:53.732343Z",
     "iopub.status.idle": "2025-04-19T16:14:53.732812Z",
     "shell.execute_reply": "2025-04-19T16:14:53.732609Z"
    },
    "id": "FSMXsGOBepkJ",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def balance_genus_representation(prioritized_markers, eccontri_df, genus_to_threshold, per_genus_count=10, knee_abundance_threshold=None):\n",
    "    \"\"\"\n",
    "    Balance representation by selecting top proteins per genus based on combined score\n",
    "    and applying abundance thresholds from knee point analysis.\n",
    "    parameters: prioritized_markers are filtered data markers pairs and it is the core data\n",
    "                eccontri_df original data need to source the column \"norm_abund_contri\" for comparison\n",
    "                genus_to_threshold is a dictionary that holds the knee abundance calculate on section 8.11\n",
    "    \"\"\"\n",
    "    # Initialize empty DataFrame for balanced markers\n",
    "    balanced_markers = pd.DataFrame()\n",
    "\n",
    "    prioritized_markers['idx'] = prioritized_markers['idx'].astype(int)\n",
    "    eccontri_df['idx'] = eccontri_df['idx'].astype(int)\n",
    "\n",
    "    # Filter eccontri_df to include only rows present in prioritized_markers\n",
    "    relevant_idxs = prioritized_markers['idx'].unique()\n",
    "    filtered_eccontri = eccontri_df[eccontri_df['idx'].isin(relevant_idxs)]\n",
    "\n",
    "    # Enrich prioritized_markers with abundance values from eccontri_df\n",
    "    abundance_data = filtered_eccontri[['idx', 'norm_abund_contri']].drop_duplicates()\n",
    "    # Merge to add abundance data to prioritized_markers\n",
    "    enriched_markers = pd.merge(\n",
    "        prioritized_markers,\n",
    "        abundance_data,\n",
    "        on='idx',\n",
    "        how='left'\n",
    "    )\n",
    "    # Add Knee Abundance thresholds using genus_to_threshold dictionary\n",
    "    enriched_markers['Knee_Abundance'] = enriched_markers['Genus'].map(genus_to_threshold)\n",
    "    #enriched_markers['Knee_Abundance'] = enriched_markers['Knee_Abundance'].astype(float)\n",
    "\n",
    "\n",
    "    # Ensure no missing values in Knee_Abundance; raise error if any are found\n",
    "    if enriched_markers['Knee_Abundance'].isnull().any():\n",
    "        missing_genera = enriched_markers[enriched_markers['Knee_Abundance'].isnull()]['Genus'].unique()\n",
    "        raise ValueError(f\"The following genera are missing from the threshold dictionary: {missing_genera}\")\n",
    "    # Process each genus individually\n",
    "    for genus, group in enriched_markers.groupby('Genus'):\n",
    "\n",
    "        # Filter by Knee Abundance threshold\n",
    "        filtered_group = group[group['norm_abund_contri'].astype(float) >= group['Knee_Abundance'].astype(float)]\n",
    "\n",
    "        # If filtering removes everything, fall back to top N entries by combined score\n",
    "        if len(filtered_group) == 0:\n",
    "            top_group = group.nlargest(per_genus_count, 'combined_score')\n",
    "        else:\n",
    "            top_group = filtered_group.nlargest(per_genus_count, 'combined_score')\n",
    "\n",
    "        # Append the selected entries to balanced_markers\n",
    "        balanced_markers = pd.concat([balanced_markers, top_group])\n",
    "    # Keep relevant columns\n",
    "    balanced_markers  = balanced_markers.drop(columns=['category_str', 'Frequency', 'descriptive_pattern', 'pattern',  'Knee_Abundance'])    \n",
    "\n",
    "    # Return the final balanced markers sorted by combined score\n",
    "    return balanced_markers.sort_values('combined_score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W7TWqLOHtSyR"
   },
   "source": [
    "## 10.7 Creating Marker Groups\n",
    "\n",
    "The `create_marker_groups` function organizes filtered protein-genus pairs into specialized subsets that facilitate targeted analysis of different aspects of corrosion processes. Starting with the balanced marker dataset, this function generates multiple biologically meaningful groupings based on statistical patterns, functional characteristics, and corrosion relevance, they are:\n",
    "\n",
    "- **high_confidence/top_markers**: High-scoring markers (combined score ≥ 7.0) limited to the top 500, providing a manageable subset of the most corrosion-relevant proteins\n",
    "- **significant_markers**: Proteins with statistically significant abundance patterns across risk categories\n",
    "- **very_high_specificity**: Proteins highly specific to particular risk categories (specificity ≥ 0.9)\n",
    "- **mechanism-specific groups**: Proteins associated with particular corrosion mechanisms (acid production, biofilm formation, etc.)\n",
    "- **metal-related groups**: Proteins interacting with specific metals relevant to corrosion processes\n",
    "- **tier_niche-specific**: Proteins with specialized ecological roles, potentially representing niche adaptations to corrosive environments\n",
    "\n",
    "The function determines group membership using appropriate thresholds (either fixed values or percentile-based) for each criterion, and returns a dictionary containing all generated groups. This organizational approach allows for focused analysis of specific biological aspects while maintaining the context of the overall dataset.\n",
    "\n",
    "The balanced_markers and high_confidence/top_markers groups are closely related, with the latter being a higher-quality subset of the former, filtered by confidence threshold and limited in size to enhance analytical focus.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_marker_groups(balanced_markers, top_count=500, threshold_percentile=0.75):\n",
    "    \"\"\"\n",
    "    Create specialized marker groups based on various criteria.\n",
    "    Includes only the specifically requested groups and uses imported terms.\n",
    "\n",
    "    Parameters:\n",
    "        balanced_markers: DataFrame with markers (prioritized or balanced)\n",
    "        top_count: Number of top markers to include (default: 500)\n",
    "        threshold_percentile: Percentile for determining high-relevance markers (default: 0.75)\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of marker groups by different criteria\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    groups = {}\n",
    "\n",
    "    # Import corrosion terms from existing modules\n",
    "    try:\n",
    "        import sys\n",
    "        try:\n",
    "            # First try kaggle path\n",
    "            sys.path.append('/kaggle/input/corrosion-scoring')\n",
    "            from global_terms import (\n",
    "                corrosion_keyword_groups,\n",
    "                functional_categories,\n",
    "                organic_categories,\n",
    "            )\n",
    "        except ImportError:\n",
    "            # Then try local path\n",
    "            sys.path.append('/home/beatriz/MIC/2_Micro/corrosion_scoring')\n",
    "            from corrosion_scoring.global_terms import (\n",
    "                corrosion_keyword_groups,\n",
    "                functional_categories,\n",
    "                organic_categories,\n",
    "            )\n",
    "        print(\"Successfully imported terms for marker grouping\")\n",
    "    except ImportError as e:\n",
    "        print(f\"Warning: Could not import corrosion terms: {e}\")\n",
    "        return {}  # Return empty groups if we can't import necessary terms\n",
    "\n",
    "    # Check if combined_score exists, otherwise calculate it if needed\n",
    "    if 'combined_score' not in balanced_markers.columns:\n",
    "        print(\"Warning: combined_score column not found. Creating basic score.\")\n",
    "        # Create a basic combined score from available columns\n",
    "        if 'significance_score' in balanced_markers.columns:\n",
    "            balanced_markers['combined_score'] = balanced_markers['significance_score'] * 3\n",
    "        else:\n",
    "            balanced_markers['combined_score'] = 5.0  # Default value\n",
    "\n",
    "    # 1. TOP MARKERS - Best overview of highest-scoring markers\n",
    "    top_threshold = balanced_markers['combined_score'].quantile(threshold_percentile)\n",
    "    groups['top_markers'] = balanced_markers[balanced_markers['combined_score'] >= top_threshold].head(top_count)\n",
    "\n",
    "    # 2. SIGNIFICANCE GROUPS - Statistically significant groups\n",
    "    if 'significance_score' in balanced_markers.columns:\n",
    "        sig_threshold = balanced_markers['significance_score'].quantile(threshold_percentile)\n",
    "        groups['significant_markers'] = balanced_markers[balanced_markers['significance_score'] >= sig_threshold]\n",
    "    \n",
    "    # 3. PREVALENCE GROUPS\n",
    "    if 'prevalence' in balanced_markers.columns:\n",
    "        high_threshold = 0.7\n",
    "        groups['high_prevalence'] = balanced_markers[balanced_markers['prevalence'] >= high_threshold]\n",
    "        \n",
    "    # 4. SPECIFICITY very high specificity\n",
    "    if 'specificity' in balanced_markers.columns:\n",
    "        specificity_threshold = 0.9  # Highly specific to one category\n",
    "        groups['very_high_specificity'] = balanced_markers[balanced_markers['specificity'] >= specificity_threshold]\n",
    "    \n",
    "    # 5. FREQUENCY\n",
    "    if 'Frequency' in balanced_markers.columns:\n",
    "        freq_threshold = balanced_markers['Frequency'].quantile(threshold_percentile)\n",
    "        groups['high_frequency'] = balanced_markers[balanced_markers['Frequency'] >= freq_threshold]\n",
    "\n",
    "    # 6. COMPONENT SCORE GROUPS\n",
    "    component_score_fields = {\n",
    "        'overall_metal_score': 'high_metals_relevance',\n",
    "        'overall_corrosion_score': 'high_mechanism_relevance',\n",
    "        'pathways_score': 'high_pathway_relevance',\n",
    "        'tier_score': 'high_tier_relevance',\n",
    "        'corrosion_relevance_score': 'high_corrosion_relevance'\n",
    "    }\n",
    "\n",
    "    for score_field, group_name in component_score_fields.items():\n",
    "        if score_field in balanced_markers.columns and not balanced_markers[score_field].isna().all():\n",
    "            threshold = balanced_markers[score_field].quantile(threshold_percentile)\n",
    "            if threshold > 0:  # Only create group if threshold is meaningful\n",
    "                groups[group_name] = balanced_markers[balanced_markers[score_field] >= threshold]\n",
    "\n",
    "    # 7. MECHANISMS - One consolidated group for all mechanisms\n",
    "    if 'corrosion_mechanisms' in balanced_markers.columns:\n",
    "        # Create one group for all mechanisms\n",
    "        mechanism_markers = balanced_markers[balanced_markers['corrosion_mechanisms'].notna() & \n",
    "                                           (balanced_markers['corrosion_mechanisms'] != '')]\n",
    "        if len(mechanism_markers) > 0:\n",
    "            groups['mechanism_all'] = mechanism_markers.sort_values('combined_score', ascending=False)\n",
    "\n",
    "    # 8. METAL CONSOLIDATED - One group for all metals\n",
    "    if 'metals_consolidated' in balanced_markers.columns:\n",
    "        # One consolidated group for all metal-containing markers\n",
    "        metal_markers = balanced_markers[balanced_markers['metals_consolidated'].notna() & \n",
    "                                       (balanced_markers['metals_consolidated'] != '')]\n",
    "        if len(metal_markers) > 0:\n",
    "            groups['metals_consolidated'] = metal_markers.sort_values('combined_score', ascending=False)\n",
    "        \n",
    "        # Special Fe-S group (important for corrosion)\n",
    "        fe_s_markers = balanced_markers[\n",
    "            balanced_markers['metals_consolidated'].fillna('').str.contains('Fe|iron', case=False) &\n",
    "            balanced_markers['metals_consolidated'].fillna('').str.contains('S|sulfur', case=False)\n",
    "        ]\n",
    "        if len(fe_s_markers) > 0:\n",
    "            groups['metal_iron_sulfur'] = fe_s_markers.sort_values('combined_score', ascending=False)\n",
    "\n",
    "    # 9. PATHWAYS GROUP\n",
    "    if 'pathways' in balanced_markers.columns:\n",
    "        pathway_markers = balanced_markers[balanced_markers['pathways'].notna() & \n",
    "                                         (balanced_markers['pathways'] != '')]\n",
    "        if len(pathway_markers) > 0:\n",
    "            groups['pathways_all'] = pathway_markers.sort_values('combined_score', ascending=False)\n",
    "\n",
    "    # 10. FUNCTIONAL CATEGORIES\n",
    "    # Create a consolidated functional categories group\n",
    "    if 'functional_categories_present' in balanced_markers.columns:\n",
    "        func_markers = balanced_markers[balanced_markers['functional_categories_present'].notna() & \n",
    "                                      (balanced_markers['functional_categories_present'] != '')]\n",
    "        if len(func_markers) > 0:\n",
    "            groups['functional_categories'] = func_markers.sort_values('combined_score', ascending=False)\n",
    "\n",
    "    # 11. HIGH SYNERGY GROUP \n",
    "    if 'functional_categories_present' in balanced_markers.columns:\n",
    "        # Count the number of key categories each marker has\n",
    "        def count_key_categories(categories_str):\n",
    "            if not isinstance(categories_str, str):\n",
    "                return 0\n",
    "            key_cats = ['iron/sulfur_redox', 'ocre', 'acid_production', \n",
    "                      'electron transfer & redox', 'biofilm_formation']\n",
    "            return sum(1 for cat in key_cats if cat.lower() in categories_str.lower())\n",
    "        \n",
    "        balanced_markers['key_category_count'] = balanced_markers['functional_categories_present'].apply(count_key_categories)\n",
    "        \n",
    "        # Markers with 3+ key categories have strong synergistic potential\n",
    "        synergy_markers = balanced_markers[balanced_markers['key_category_count'] >= 3]\n",
    "        if len(synergy_markers) > 0:\n",
    "            groups['high_synergy_markers'] = synergy_markers.sort_values('combined_score', ascending=False)\n",
    "\n",
    "    # 12. ORGANIC-METAL SYNERGY\n",
    "    # Create a group for markers showing synergy between organic processes and metals\n",
    "    if 'organic_processes' in balanced_markers.columns and 'metals_consolidated' in balanced_markers.columns:\n",
    "        organic_metal_markers = balanced_markers[\n",
    "            balanced_markers['organic_processes'].notna() & \n",
    "            (balanced_markers['organic_processes'] != '') &\n",
    "            balanced_markers['metals_consolidated'].notna() & \n",
    "            (balanced_markers['metals_consolidated'] != '')\n",
    "        ]\n",
    "        if len(organic_metal_markers) > 0:\n",
    "            groups['organic_metal_synergy'] = organic_metal_markers.sort_values('combined_score', ascending=False)\n",
    "\n",
    "    # 13. BIOLOGICAL RELEVANCE - Interesting combined group\n",
    "    try:\n",
    "        # Create a combined biological relevance score\n",
    "        bio_score = np.zeros(len(balanced_markers))\n",
    "        bio_cols = ['overall_corrosion_score', 'pathways_score', 'overall_functional_score', \n",
    "                    'overall_keyword_score']\n",
    "        \n",
    "        # Add weights for each biological component\n",
    "        for col in bio_cols:\n",
    "            if col in balanced_markers.columns:\n",
    "                # Safely convert to numeric, replacing non-numeric with 0\n",
    "                bio_score += pd.to_numeric(balanced_markers[col], errors='coerce').fillna(0)\n",
    "        \n",
    "        # Only create group if we have meaningful biological scores\n",
    "        if bio_score.sum() > 0:\n",
    "            bio_threshold = np.percentile(bio_score[bio_score > 0], threshold_percentile * 100)\n",
    "            groups['high_biological_relevance'] = balanced_markers[bio_score >= bio_threshold]\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not create biological relevance group: {e}\")\n",
    "\n",
    "    # 14. CRITICAL MARKERS FOR CORROSION\n",
    "    try:\n",
    "        corrosion_critical = balanced_markers[\n",
    "            (balanced_markers['combined_score'] >= balanced_markers['combined_score'].quantile(0.9))\n",
    "        ]\n",
    "        \n",
    "        # Add metal filter if column exists\n",
    "        if 'metals_consolidated' in balanced_markers.columns:\n",
    "            corrosion_critical = corrosion_critical[\n",
    "                corrosion_critical['metals_consolidated'].fillna('').str.contains('Fe|iron|Mn|manganese|S|sulfur', case=False)\n",
    "            ]\n",
    "        \n",
    "        # Add functional categories filter if column exists\n",
    "        if 'functional_categories_present' in balanced_markers.columns:\n",
    "            corrosion_critical = corrosion_critical[\n",
    "                corrosion_critical['functional_categories_present'].fillna('').str.contains(\n",
    "                    'iron/sulfur_redox|ocre|acid_production|electron transfer|biofilm', case=False)\n",
    "            ]\n",
    "        # Otherwise try corrosion_mechanisms if available\n",
    "        elif 'corrosion_mechanisms' in balanced_markers.columns:\n",
    "            corrosion_critical = corrosion_critical[\n",
    "                corrosion_critical['corrosion_mechanisms'].fillna('').str.contains(\n",
    "                    'iron|sulfur|acid|electron|biofilm', case=False)\n",
    "            ]\n",
    "            \n",
    "        if len(corrosion_critical) > 0:\n",
    "            groups['corrosion_critical'] = corrosion_critical.sort_values('combined_score', ascending=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not create corrosion_critical group: {e}\")\n",
    "\n",
    "    # Add information about group sizes\n",
    "    group_sizes = {name: len(df) for name, df in groups.items()}\n",
    "    print(\"Marker groups created:\")\n",
    "    for name, size in sorted(group_sizes.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"  - {name}: {size} markers\")\n",
    "\n",
    "    # Define the score columns to be dropped from each group\n",
    "    score_columns_to_drop = [\n",
    "        'overall_metal_score', 'corrosion_relevance_score', 'overall_functional_score',\n",
    "        'overall_keyword_score', 'overall_synergy_score', 'overall_organic_process_score',\n",
    "        'corrosion_mechanism_scores', 'overall_corrosion_score',\n",
    "        'pathways_score', 'tier_score', 'hierarchy_score', 'functional_mechanisms_score',\n",
    "        'corrosion_score_scaled', 'corrosion_cat_score', 'corrosion_final_score',\n",
    "        'key_category_count', 'metal_scores', 'significance_score'\n",
    "    ]\n",
    "    \n",
    "    # Drop score columns from each group\n",
    "    for group_name, group_df in groups.items():\n",
    "        # Only drop columns that exist in the DataFrame\n",
    "        columns_to_drop = [col for col in score_columns_to_drop if col in group_df.columns]\n",
    "        if columns_to_drop:\n",
    "            groups[group_name] = group_df.drop(columns=columns_to_drop)\n",
    "    \n",
    "\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4FSuWh3-rZqp"
   },
   "source": [
    "## 10.8. Generating Marker Report\n",
    "The generate_streamlined_report function transforms complex analytical results into an accessible, multi-sheet report that highlights the most relevant protein-genus pairs associated with corrosion processes. The function extracts prioritized markers (using balanced markers if available) and organizes them into four specialized sheets:\n",
    "\n",
    "Top Corrosion Markers: A focused presentation of the highest-scoring markers with core information including genus, protein name, enzyme details, corrosion mechanisms, metal involvement, and pattern characteristics.\n",
    "Visualization Data: A streamlined dataset optimized for creating abundance pattern visualizations across risk categories.\n",
    "Detailed Scores: A comprehensive breakdown of scoring components (prevalence, specificity, frequency, pathway relevance, etc.) to provide transparency into the prioritization process.\n",
    "Mechanisms Focus: An expanded view that separates multi-mechanism proteins into individual entries, facilitating mechanism-specific analysis.\n",
    "\n",
    "The function also generates a summary sheet with metadata about the analysis, including total markers analyzed, balancing status, and pattern distribution. When provided with an output path, the report is saved as an Excel file with appropriately formatted column names for improved readability. This structured reporting approach enables researchers to quickly identify the most promising corrosion biomarkers while maintaining access to the underlying analytical details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-19T16:14:53.735950Z",
     "iopub.status.idle": "2025-04-19T16:14:53.736452Z",
     "shell.execute_reply": "2025-04-19T16:14:53.736220Z"
    },
    "id": "43MU0cZEtkLw",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def consolidate_analysis_results(analysis_results, marker_groups):\n",
    "    \"\"\"\n",
    "    Consolidate all analysis results and marker groups into a single dictionary\n",
    "    for easy parquet storage.\n",
    "\n",
    "    Parameters:   analysis_results: Dictionary containing analysis dataframes\n",
    "                  marker_groups: Dictionary containing marker groups\n",
    "    Returns:      consolidated_results: Dictionary with all dataframes in one place\n",
    "    \"\"\"\n",
    "    # Create a new consolidated dictionary\n",
    "    consolidated_results = {}\n",
    "\n",
    "    # Add main analysis dataframes\n",
    "    key_dataframes = [\n",
    "        'pattern_data',\n",
    "        'integrated_results',\n",
    "        'classified_results',\n",
    "        'increasing_markers',\n",
    "        'inverse_markers',\n",
    "        'prioritized_markers',\n",
    "        'balanced_markers'\n",
    "    ]\n",
    "\n",
    "    for key in key_dataframes:\n",
    "        if key in analysis_results and analysis_results[key] is not None:\n",
    "            consolidated_results[key] = analysis_results[key]\n",
    "            print(f\"Added {key}: {len(analysis_results[key])} rows\")\n",
    "\n",
    "    # Add marker groups\n",
    "    for group_name, group_df in marker_groups.items():\n",
    "        consolidated_results[f'group_{group_name}'] = group_df\n",
    "        print(f\"Added group_{group_name}: {len(group_df)} rows\")\n",
    "\n",
    "    print(f\"Successfully consolidated {len(consolidated_results)} dataframes\")\n",
    "    return consolidated_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-19T16:14:53.737660Z",
     "iopub.status.idle": "2025-04-19T16:14:53.738140Z",
     "shell.execute_reply": "2025-04-19T16:14:53.737936Z"
    },
    "id": "LL_64bSwkKLy",
    "outputId": "39e29a64-e26d-4b9a-cfb1-32e2b8a9e14d",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting corrosion protein analysis...\n",
      "Analyzing 1491288 data points across 70 Sites...\n",
      "eccontri_df.columns immediately after creating column Frequency: ['index', 'idx', 'Sites', 'Genus', 'abund_raw', 'rel_abund_raw', 'genome_EC_count', 'abund_contri', 'rel_abund_contri', 'norm_abund_contri', 'protein_name', 'uniprot_id', 'EC', 'enzyme_names', 'enzyme_class', 'pathways', 'hierarchy', 'metals_consolidated', 'corrosion_mechanisms', 'corrosion_relevance_score', 'corrosion_relevance', 'metal_scores', 'overall_metal_score', 'corrosion_mechanism_scores', 'overall_corrosion_score', 'functional_categories', 'overall_functional_score', 'corrosion_keyword_groups', 'corrosion_keyword_scores', 'overall_keyword_score', 'corrosion_synergies', 'organic_processes', 'overall_organic_process_score', 'has_metal', 'overall_synergy_score', 'organic_process_scores', 'corrosion_synergy_scores', 'Category', 'was_uncharacterized', 'version', 'Frequency']\n",
      "Detailed pattern distribution:\n",
      "descriptive_pattern\n",
      "steadily_increasing          9474\n",
      "peak_at_transition_low       9289\n",
      "peak_at_transition_high      8908\n",
      "stress_recovery_full         6872\n",
      "steadily_decreasing          5824\n",
      "stress_recovery_partial      4034\n",
      "severe_exclusive             1269\n",
      "late_emergence_increasing     938\n",
      "early_decrease_only           677\n",
      "transition_exclusive          582\n",
      "early_increase_only           551\n",
      "normal_exclusive              499\n",
      "risk_independent              199\n",
      "other_pattern                  24\n",
      "peak_at_transition_equal       22\n",
      "stress_recovery_equal          17\n",
      "late_decline                   11\n",
      "early_constant                 10\n",
      "late_emergence_decreasing       9\n",
      "late_emergence_constant         4\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Pattern category distribution:\n",
      "pattern_category\n",
      "increasing    28616\n",
      "decreasing    20360\n",
      "constant        213\n",
      "other            24\n",
      "Name: count, dtype: int64\n",
      "Pattern data created\n",
      "Integrated results created\n",
      "Classifying pathways by specificity...\n",
      "Pathway classification results:\n",
      "  - universal: 20600 (41.9%)\n",
      "  - niche-specific: 16046 (32.6%)\n",
      "  - mixed: 8087 (16.4%)\n",
      "  - unknown: 4480 (9.1%)\n",
      "Separating positive and inverse patterns by patterns\n",
      "Separated 28616 increasing patterns, 20360 inverse patterns, and 237 mixed/other patterns.\n",
      "Average significance scores: Increasing=1.57, Inverse=0.95, Constant=0.46\n",
      "Prioritizing markers based on statistical and biological relevance...\n",
      "Successfully imported scoring modules from Kaggle path\n",
      "Warning: Error in database scoring: argument of type 'NoneType' is not iterable\n",
      "[15:46:21] Prioritization complete\n",
      "Score distribution: Mean=27.44, Median=26.86, Min=4.62, Max=54.98\n",
      "Balancing genus representation (top 10 per genus)...\n",
      "Created balanced dataset with 647 markers from 81 genera\n",
      "Creating specialized marker groups from balanced markers...\n",
      "Successfully imported terms for marker grouping\n",
      "Marker groups created:\n",
      "  - metals_consolidated: 647 markers\n",
      "  - mechanism_all: 637 markers\n",
      "  - functional_categories: 637 markers\n",
      "  - pathways_all: 611 markers\n",
      "  - organic_metal_synergy: 599 markers\n",
      "  - high_synergy_markers: 516 markers\n",
      "  - high_prevalence: 491 markers\n",
      "  - metal_iron_sulfur: 491 markers\n",
      "  - high_pathway_relevance: 280 markers\n",
      "  - high_tier_relevance: 213 markers\n",
      "  - high_mechanism_relevance: 174 markers\n",
      "  - high_metals_relevance: 171 markers\n",
      "  - high_biological_relevance: 165 markers\n",
      "  - top_markers: 162 markers\n",
      "  - significant_markers: 162 markers\n",
      "  - high_corrosion_relevance: 162 markers\n",
      "  - corrosion_critical: 65 markers\n",
      "  - very_high_specificity: 30 markers\n",
      "Analysis complete!\n",
      "Added pattern_data: 49213 rows\n",
      "Added integrated_results: 49213 rows\n",
      "Added classified_results: 49213 rows\n",
      "Added increasing_markers: 28616 rows\n",
      "Added inverse_markers: 20360 rows\n",
      "Added prioritized_markers: 28616 rows\n",
      "Added balanced_markers: 647 rows\n",
      "Added group_top_markers: 162 rows\n",
      "Added group_significant_markers: 162 rows\n",
      "Added group_high_prevalence: 491 rows\n",
      "Added group_very_high_specificity: 30 rows\n",
      "Added group_high_metals_relevance: 171 rows\n",
      "Added group_high_mechanism_relevance: 174 rows\n",
      "Added group_high_pathway_relevance: 280 rows\n",
      "Added group_high_tier_relevance: 213 rows\n",
      "Added group_high_corrosion_relevance: 162 rows\n",
      "Added group_mechanism_all: 637 rows\n",
      "Added group_metals_consolidated: 647 rows\n",
      "Added group_metal_iron_sulfur: 491 rows\n",
      "Added group_pathways_all: 611 rows\n",
      "Added group_functional_categories: 637 rows\n",
      "Added group_high_synergy_markers: 516 rows\n",
      "Added group_organic_metal_synergy: 599 rows\n",
      "Added group_high_biological_relevance: 165 rows\n",
      "Added group_corrosion_critical: 65 rows\n",
      "Successfully consolidated 25 dataframes\n"
     ]
    }
   ],
   "source": [
    "## calling it with balancing the generacomplete_results def balance_genus_representation(prioritized_markers, eccontri_df, genus_to_threshold, per_genus_count=10, knee_abundance_threshold=None)\n",
    "analysis_results, marker_groups = analyze_corrosion_proteins(ECcontri_Uniprot_enriched, alpha=0.05, genus_to_threshold=genus_to_threshold, per_genus_count=10,  knee_abundance_threshold =None)\n",
    "\n",
    "# Making the report\n",
    "corrosion_report = consolidate_analysis_results(analysis_results, marker_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FZApOPOZj7QW"
   },
   "source": [
    "### Saving the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-19T16:14:53.743859Z",
     "iopub.status.idle": "2025-04-19T16:14:53.744341Z",
     "shell.execute_reply": "2025-04-19T16:14:53.744119Z"
    },
    "id": "3YVqllHasECE",
    "outputId": "258e42af-dc74-4256-b527-92b112bd2d0c",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved pattern_data.parquet - Size: 5787199 bytes\n",
      "Saved integrated_results.parquet - Size: 6551801 bytes\n",
      "Saved classified_results.parquet - Size: 6642447 bytes\n",
      "Saved increasing_markers.parquet - Size: 4013918 bytes\n",
      "Saved inverse_markers.parquet - Size: 2986312 bytes\n",
      "Saved prioritized_markers.parquet - Size: 4532026 bytes\n",
      "Saved balanced_markers.parquet - Size: 182271 bytes\n",
      "Saved group_top_markers.parquet - Size: 54244 bytes\n",
      "Saved group_significant_markers.parquet - Size: 60482 bytes\n",
      "Saved group_high_prevalence.parquet - Size: 115343 bytes\n",
      "Saved group_very_high_specificity.parquet - Size: 32355 bytes\n",
      "Saved group_high_metals_relevance.parquet - Size: 53059 bytes\n",
      "Saved group_high_mechanism_relevance.parquet - Size: 54029 bytes\n",
      "Saved group_high_pathway_relevance.parquet - Size: 77524 bytes\n",
      "Saved group_high_tier_relevance.parquet - Size: 68449 bytes\n",
      "Saved group_high_corrosion_relevance.parquet - Size: 53246 bytes\n",
      "Saved group_mechanism_all.parquet - Size: 138338 bytes\n",
      "Saved group_metals_consolidated.parquet - Size: 139411 bytes\n",
      "Saved group_metal_iron_sulfur.parquet - Size: 113013 bytes\n",
      "Saved group_pathways_all.parquet - Size: 134346 bytes\n",
      "Saved group_functional_categories.parquet - Size: 138338 bytes\n",
      "Saved group_high_synergy_markers.parquet - Size: 115504 bytes\n",
      "Saved group_organic_metal_synergy.parquet - Size: 131338 bytes\n",
      "Saved group_high_biological_relevance.parquet - Size: 54094 bytes\n",
      "Saved group_corrosion_critical.parquet - Size: 39839 bytes\n"
     ]
    }
   ],
   "source": [
    "# Create directory for parquet files\n",
    "parquet_dir = os.path.join(output_large, \"corrosion_markers.parquet\")\n",
    "os.makedirs(parquet_dir, exist_ok=True)\n",
    "\n",
    "# Save each dataframe as a separate parquet file\n",
    "for name, df in corrosion_report.items():\n",
    "    parquet_path = os.path.join(parquet_dir, f\"{name}.parquet\")\n",
    "    df.to_parquet(parquet_path, index=False)\n",
    "    print(f\"Saved {name}.parquet - Size: {os.path.getsize(parquet_path)} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-19T16:14:53.745688Z",
     "iopub.status.idle": "2025-04-19T16:14:53.746170Z",
     "shell.execute_reply": "2025-04-19T16:14:53.745971Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group_mechanism_all: (637, 36)\n",
      "group_top_markers: (162, 36)\n",
      "integrated_results: (49213, 41)\n",
      "group_metal_iron_sulfur: (491, 36)\n",
      "group_high_synergy_markers: (516, 36)\n",
      "group_significant_markers: (162, 36)\n",
      "classified_results: (49213, 44)\n",
      "prioritized_markers: (28616, 56)\n",
      "inverse_markers: (20360, 44)\n",
      "group_high_prevalence: (491, 36)\n",
      "group_metals_consolidated: (647, 36)\n",
      "balanced_markers: (647, 54)\n",
      "group_high_metals_relevance: (171, 36)\n",
      "group_high_mechanism_relevance: (174, 36)\n",
      "group_high_biological_relevance: (165, 36)\n",
      "group_pathways_all: (611, 36)\n",
      "group_high_pathway_relevance: (280, 36)\n",
      "group_corrosion_critical: (65, 36)\n",
      "increasing_markers: (28616, 44)\n",
      "group_functional_categories: (637, 36)\n",
      "group_high_corrosion_relevance: (162, 36)\n",
      "group_organic_metal_synergy: (599, 36)\n",
      "group_high_tier_relevance: (213, 36)\n",
      "group_very_high_specificity: (30, 36)\n",
      "pattern_data: (49213, 20)\n"
     ]
    }
   ],
   "source": [
    "parquet_dir = Path(os.path.join(output_large, \"corrosion_markers.parquet\"))\n",
    "# List all Parquet files in directory\n",
    "parquet_files = list(parquet_dir.glob(\"*.parquet\"))\n",
    "\n",
    "# Load all files into a dictionary of DataFrames\n",
    "corrosion_report= {file.stem: pd.read_parquet(file) for file in parquet_files}\n",
    "\n",
    "# Check available DataFrames\n",
    "for name, df in corrosion_report.items():\n",
    "    print(f\"{name}: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main analysis dataframes (core pipeline)\n",
    "pattern_data = corrosion_report[\"pattern_data\"] \n",
    "integrated_results = corrosion_report[\"integrated_results\"]\n",
    "classified_results = corrosion_report[\"classified_results\"]\n",
    "increasing_markers = corrosion_report[\"increasing_markers\"]\n",
    "prioritized_markers = corrosion_report[\"prioritized_markers\"]\n",
    "balanced_markers = corrosion_report[\"balanced_markers\"]\n",
    "inverse_markers = corrosion_report[\"inverse_markers\"]\n",
    "\n",
    "# Top marker groups (overall rankings)\n",
    "top_markers = corrosion_report[\"group_top_markers\"]\n",
    "significant_markers = corrosion_report[\"group_significant_markers\"]\n",
    "high_prevalence = corrosion_report[\"group_high_prevalence\"]\n",
    "very_high_specificity = corrosion_report[\"group_very_high_specificity\"]\n",
    "# high_frequency = corrosion_report[\"group_high_frequency\"]  # This key doesn't exist - commented out\n",
    "\n",
    "# Relevance-based groups\n",
    "high_metals_relevance = corrosion_report[\"group_high_metals_relevance\"]\n",
    "high_mechanism_relevance = corrosion_report[\"group_high_mechanism_relevance\"]\n",
    "high_pathway_relevance = corrosion_report[\"group_high_pathway_relevance\"]\n",
    "high_tier_relevance = corrosion_report[\"group_high_tier_relevance\"]\n",
    "high_corrosion_relevance = corrosion_report[\"group_high_corrosion_relevance\"]\n",
    "high_biological_relevance = corrosion_report[\"group_high_biological_relevance\"]\n",
    "\n",
    "# Consolidated groups\n",
    "mechanism_all = corrosion_report[\"group_mechanism_all\"]\n",
    "metals_consolidated = corrosion_report[\"group_metals_consolidated\"]\n",
    "pathways_all = corrosion_report[\"group_pathways_all\"]\n",
    "functional_categories = corrosion_report[\"group_functional_categories\"]\n",
    "\n",
    "# Special groups\n",
    "metal_iron_sulfur = corrosion_report[\"group_metal_iron_sulfur\"]\n",
    "high_synergy_markers = corrosion_report[\"group_high_synergy_markers\"]\n",
    "organic_metal_synergy = corrosion_report[\"group_organic_metal_synergy\"]\n",
    "corrosion_critical = corrosion_report[\"group_corrosion_critical\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.9 Classification Summary on Prioritized Markers\n",
    "\n",
    "This is a summary of the pathways classification done in section 10.3 Classifying Housekeeping, Niche and Mixed Protein depending on pathways, mechanisms and hierarchy. The pathways by specificity resulted on :\n",
    "  - niche-specific: 1097851 (73.6%)\n",
    "  - mixed: 330939 (22.2%)\n",
    "  - unknown: 62495 (4.2%)\n",
    "\n",
    "Note that the genera here study was preselected to be the 85 most influencial genera out 887 total organisms types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-19T16:14:53.748542Z",
     "iopub.status.idle": "2025-04-19T16:14:53.749198Z",
     "shell.execute_reply": "2025-04-19T16:14:53.748974Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PATHWAY SPECIFICITY ANALYSIS ===\n",
      "Total pathways analyzed: 49213\n",
      "\n",
      "Classification distribution:\n",
      "  - universal: 20600 (41.9%)\n",
      "  - niche-specific: 16046 (32.6%)\n",
      "  - mixed: 8087 (16.4%)\n",
      "  - unknown: 4480 (9.1%)\n",
      "Universal pathway distribution (without duplication):\n",
      "  - General Metabolism: 21662 occurrences\n",
      "  - 4.4. Lipid Membrane Synthesis: 3496 occurrences\n",
      "  - 1.1. Glycolysis: 2135 occurrences\n",
      "  - 6.1. Amino Acid Biosynthesis: 1635 occurrences\n",
      "  - 3.2. Transcription: 1246 occurrences\n",
      "  - 1.2. Gluconeogenesis: 1219 occurrences\n",
      "  - 3.1. DNA Replication: 1123 occurrences\n",
      "  - 1.4. Krebs/TCA Cycle: 770 occurrences\n",
      "  - 1.3. Pentose Phosphate Pathway: 732 occurrences\n",
      "  - 3.3. Translation: 600 occurrences\n",
      "  - 4.3. Cell Wall Maintenance: 369 occurrences\n",
      "  - 1.5. Electron Transport Chain: 202 occurrences\n",
      "  - 3.5. Proteasome System: 2 occurrences\n",
      "           idx             Genus                        protein_name  \\\n",
      "0            0       Pseudomonas               alcohol-dehydrogenase   \n",
      "1            1     Simplicispira               alcohol-dehydrogenase   \n",
      "2            2        Acidovorax               alcohol-dehydrogenase   \n",
      "3            3     Dechloromonas               alcohol-dehydrogenase   \n",
      "4            4     Desulfobulbus               alcohol-dehydrogenase   \n",
      "...        ...               ...                                 ...   \n",
      "49208  1197821  Desulfobacterium           propionyl-coa carboxylase   \n",
      "49209  1197847  Desulfobacterium     methylcrotonoyl-coa carboxylase   \n",
      "49210  1197915  Desulfobacterium                         dna ligase    \n",
      "49211  1197933  Desulfobacterium                         rna ligase    \n",
      "49212  1197938  Desulfobacterium  rna 3'-terminal-phosphate cyclase    \n",
      "\n",
      "       mean_cat1  mean_cat2  mean_cat3  Frequency category_str  \\\n",
      "0       0.039909   0.096480   0.149625          1          123   \n",
      "1       0.012745   0.020928   0.010763          1          123   \n",
      "2       0.013853   0.029974   0.011046          1          123   \n",
      "3       0.025846   0.038623   0.022181          1          123   \n",
      "4       0.000276   0.002059   0.000421          1          123   \n",
      "...          ...        ...        ...        ...          ...   \n",
      "49208   0.000000   0.000000   0.000609          1            3   \n",
      "49209   0.000000   0.000000   0.000421          1            3   \n",
      "49210   0.000000   0.000000   0.000481          1            3   \n",
      "49211   0.000000   0.000000   0.019302          1            3   \n",
      "49212   0.000000   0.000000   0.216842          1            3   \n",
      "\n",
      "           descriptive_pattern pattern_category  ...  overall_synergy_score  \\\n",
      "0          steadily_increasing       increasing  ...               0.000000   \n",
      "1       peak_at_transition_low       decreasing  ...               0.000000   \n",
      "2       peak_at_transition_low       decreasing  ...               0.000000   \n",
      "3       peak_at_transition_low       decreasing  ...               0.000000   \n",
      "4      peak_at_transition_high       increasing  ...               0.000000   \n",
      "...                        ...              ...  ...                    ...   \n",
      "49208         severe_exclusive       increasing  ...               0.000000   \n",
      "49209         severe_exclusive       increasing  ...               0.000000   \n",
      "49210         severe_exclusive       increasing  ...               1.894427   \n",
      "49211         severe_exclusive       increasing  ...               0.000000   \n",
      "49212         severe_exclusive       increasing  ...               0.000000   \n",
      "\n",
      "                                               organic_processes  \\\n",
      "0      degradation; synthesis; respiration; oxidation; reduction   \n",
      "1      degradation; synthesis; respiration; oxidation; reduction   \n",
      "2      degradation; synthesis; respiration; oxidation; reduction   \n",
      "3      degradation; synthesis; respiration; oxidation; reduction   \n",
      "4      degradation; synthesis; respiration; oxidation; reduction   \n",
      "...                                                          ...   \n",
      "49208                                     degradation; synthesis   \n",
      "49209                                                degradation   \n",
      "49210                          respiration; oxidation; reduction   \n",
      "49211                                     degradation; synthesis   \n",
      "49212                                                       None   \n",
      "\n",
      "       overall_organic_process_score  \\\n",
      "0                           4.832244   \n",
      "1                           4.832244   \n",
      "2                           4.832244   \n",
      "3                           4.832244   \n",
      "4                           4.832244   \n",
      "...                              ...   \n",
      "49208                       1.707107   \n",
      "49209                       0.707107   \n",
      "49210                       3.941634   \n",
      "49211                       1.707107   \n",
      "49212                       0.000000   \n",
      "\n",
      "                                                                                                  corrosion_mechanisms  \\\n",
      "0                                    direct_eet; acid_production; h2_consumption; sulfur_metabolism; carbon_metabolism   \n",
      "1                                    direct_eet; acid_production; h2_consumption; sulfur_metabolism; carbon_metabolism   \n",
      "2                                    direct_eet; acid_production; h2_consumption; sulfur_metabolism; carbon_metabolism   \n",
      "3                                    direct_eet; acid_production; h2_consumption; sulfur_metabolism; carbon_metabolism   \n",
      "4                                    direct_eet; acid_production; h2_consumption; sulfur_metabolism; carbon_metabolism   \n",
      "...                                                                                                                ...   \n",
      "49208                                   direct_eet; indirect_eet; h2_consumption; sulfur_metabolism; carbon_metabolism   \n",
      "49209                                                      direct_eet; indirect_eet; h2_consumption; sulfur_metabolism   \n",
      "49210  direct_eet; indirect_eet; acid_production; h2_consumption; o2_consumption; biofilm_formation; sulfur_metabolism   \n",
      "49211                                                                                  acid_production; h2_consumption   \n",
      "49212                                                                                                   h2_consumption   \n",
      "\n",
      "                                                                                                                                                                                                                                                                        corrosion_mechanism_scores  \\\n",
      "0                                                                                                   \"{\\\"direct_eet\\\": 1.0, \\\"acid_production\\\": 0.5547001962252291, \\\"h2_consumption\\\": 1.0690449676496976, \\\"sulfur_metabolism\\\": 0.6666666666666666, \\\"carbon_metabolism\\\": 0.7559289460184544}\"   \n",
      "1                                                                                                   \"{\\\"direct_eet\\\": 1.0, \\\"acid_production\\\": 0.5547001962252291, \\\"h2_consumption\\\": 1.0690449676496976, \\\"sulfur_metabolism\\\": 0.6666666666666666, \\\"carbon_metabolism\\\": 0.7559289460184544}\"   \n",
      "2                                                                                                   \"{\\\"direct_eet\\\": 1.0, \\\"acid_production\\\": 0.5547001962252291, \\\"h2_consumption\\\": 1.0690449676496976, \\\"sulfur_metabolism\\\": 0.6666666666666666, \\\"carbon_metabolism\\\": 0.7559289460184544}\"   \n",
      "3                                                                                                   \"{\\\"direct_eet\\\": 1.0, \\\"acid_production\\\": 0.5547001962252291, \\\"h2_consumption\\\": 1.0690449676496976, \\\"sulfur_metabolism\\\": 0.6666666666666666, \\\"carbon_metabolism\\\": 0.7559289460184544}\"   \n",
      "4                                                                                                   \"{\\\"direct_eet\\\": 1.0, \\\"acid_production\\\": 0.5547001962252291, \\\"h2_consumption\\\": 1.0690449676496976, \\\"sulfur_metabolism\\\": 0.6666666666666666, \\\"carbon_metabolism\\\": 0.7559289460184544}\"   \n",
      "...                                                                                                                                                                                                                                                                                            ...   \n",
      "49208                                                                                   \"{\\\"direct_eet\\\": 0.5773502691896257, \\\"indirect_eet\\\": 0.7559289460184544, \\\"h2_consumption\\\": 0.7559289460184544, \\\"sulfur_metabolism\\\": 0.9428090415820634, \\\"carbon_metabolism\\\": 0.7559289460184544}\"   \n",
      "49209                                                                                                                              \"{\\\"direct_eet\\\": 0.5773502691896257, \\\"indirect_eet\\\": 0.7559289460184544, \\\"h2_consumption\\\": 0.7559289460184544, \\\"sulfur_metabolism\\\": 0.9428090415820634}\"   \n",
      "49210  \"{\\\"direct_eet\\\": 1.2909944487358056, \\\"indirect_eet\\\": 1.0690449676496976, \\\"acid_production\\\": 0.7844645405527362, \\\"h2_consumption\\\": 1.0690449676496976, \\\"o2_consumption\\\": 0.7071067811865476, \\\"biofilm_formation\\\": 0.5345224838248488, \\\"sulfur_metabolism\\\": 1.1547005383792515}\"   \n",
      "49211                                                                                                                                                                                                          \"{\\\"acid_production\\\": 0.7844645405527362, \\\"h2_consumption\\\": 0.7559289460184544}\"   \n",
      "49212                                                                                                                                                                                                                                                   \"{\\\"h2_consumption\\\": 0.7559289460184544}\"   \n",
      "\n",
      "       overall_corrosion_score                    version  \\\n",
      "0                     4.046341  v9_proteins_pathways_fold   \n",
      "1                     4.046341  v9_proteins_pathways_fold   \n",
      "2                     4.046341  v9_proteins_pathways_fold   \n",
      "3                     4.046341  v9_proteins_pathways_fold   \n",
      "4                     4.046341  v9_proteins_pathways_fold   \n",
      "...                        ...                        ...   \n",
      "49208                 3.787946  v9_proteins_pathways_fold   \n",
      "49209                 3.032017  v9_proteins_pathways_fold   \n",
      "49210                 6.609879  v9_proteins_pathways_fold   \n",
      "49211                 1.540393  v9_proteins_pathways_fold   \n",
      "49212                 0.755929  v9_proteins_pathways_fold   \n",
      "\n",
      "       pathway_classification                     universal_pathways  \\\n",
      "0                   universal  1.1. Glycolysis, 1.2. Gluconeogenesis   \n",
      "1                   universal  1.1. Glycolysis, 1.2. Gluconeogenesis   \n",
      "2                   universal  1.1. Glycolysis, 1.2. Gluconeogenesis   \n",
      "3                   universal  1.1. Glycolysis, 1.2. Gluconeogenesis   \n",
      "4                   universal  1.1. Glycolysis, 1.2. Gluconeogenesis   \n",
      "...                       ...                                    ...   \n",
      "49208                   mixed                     General Metabolism   \n",
      "49209          niche-specific                                          \n",
      "49210               universal                     General Metabolism   \n",
      "49211          niche-specific                                          \n",
      "49212                 unknown                                          \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                   niche_specific_pathways  \n",
      "0      / ; fatty acid degradation; glycine, serine and threonine metabolism; tyrosine metabolism; alpha-linolenic acid metabolism, chloroalkane and chloroalkene degradation; naphthalene degradation; one carbon pool by folate; retinol metabolism; metabolism of xenobiotics by cytochrome p450; drug metabolism - cytochrome p450, alcoholic liver disease; amoebiasis  \n",
      "1      / ; fatty acid degradation; glycine, serine and threonine metabolism; tyrosine metabolism; alpha-linolenic acid metabolism, chloroalkane and chloroalkene degradation; naphthalene degradation; one carbon pool by folate; retinol metabolism; metabolism of xenobiotics by cytochrome p450; drug metabolism - cytochrome p450, alcoholic liver disease; amoebiasis  \n",
      "2      / ; fatty acid degradation; glycine, serine and threonine metabolism; tyrosine metabolism; alpha-linolenic acid metabolism, chloroalkane and chloroalkene degradation; naphthalene degradation; one carbon pool by folate; retinol metabolism; metabolism of xenobiotics by cytochrome p450; drug metabolism - cytochrome p450, alcoholic liver disease; amoebiasis  \n",
      "3      / ; fatty acid degradation; glycine, serine and threonine metabolism; tyrosine metabolism; alpha-linolenic acid metabolism, chloroalkane and chloroalkene degradation; naphthalene degradation; one carbon pool by folate; retinol metabolism; metabolism of xenobiotics by cytochrome p450; drug metabolism - cytochrome p450, alcoholic liver disease; amoebiasis  \n",
      "4      / ; fatty acid degradation; glycine, serine and threonine metabolism; tyrosine metabolism; alpha-linolenic acid metabolism, chloroalkane and chloroalkene degradation; naphthalene degradation; one carbon pool by folate; retinol metabolism; metabolism of xenobiotics by cytochrome p450; drug metabolism - cytochrome p450, alcoholic liver disease; amoebiasis  \n",
      "...                                                                                                                                                                                                                                                                                                                                                                    ...  \n",
      "49208                                                                                                                                                                                                                                               valine, leucine and isoleucine degradation; glyoxylate and dicarboxylate ; propanoate ; other carbon fixation pathways  \n",
      "49209                                                                                                                                                                                                                                                                                                                           valine, leucine and isoleucine degradation  \n",
      "49210                                                                                                                                                                                                                                                                                                                                            nitrogen , nitrogen cycle  \n",
      "49211                                                                                                                                                                                                                                                                                                    Viral proteins [BR:ko03200]; Transfer RNA biogenesis [BR:ko03016]  \n",
      "49212                                                                                                                                                                                                                                                                                                                                                                       \n",
      "\n",
      "[49213 rows x 44 columns]\n"
     ]
    }
   ],
   "source": [
    "specificity_report = generate_specificity_report(classified_results, output_file=None)\n",
    "print(specificity_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The predominance of niche-specific pathways (73.6% or 603,118 pathways) compared to mixed (22.2% or 183,743) and unknown (4.2% or 33,759) suggests that corrosion environments select for specialized metabolic functions rather than just general housekeeping processes. This makes biological sense given the unique challenges of metal-rich, potentially oxygen-limited environments where corrosion occurs.\n",
    "The universal pathways identified represent core cellular functions essential for any microorganism's survival (translation, glycolysis, TCA cycle, etc.), which explains their presence across the dataset, lets remember section number 7.3 and 7.4 where the distribution of pathway and reaction abundance showed on a general level that abundance varied according to the risk level, this is no like that on the granular level which is at the moment look at.\n",
    "Many of the niche-specific examples directly relate to processes that could influence corrosion, such as:\n",
    "\n",
    "Methane metabolism (potentially creating anaerobic conditions)\n",
    "Nitrogen metabolism (can affect local pH and redox conditions)\n",
    "Metal-related metabolic pathways (directly interacting with metal surfaces)\n",
    "Various secondary metabolite biosynthesis pathways (which may produce compounds that interact with metals)\n",
    "\n",
    "In fact it corroborate the pre-selected 85 most influential genera out of 887 total organism types. It helps focus on the most relevant players in corrosion processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sqAXwsa32F0r"
   },
   "source": [
    "## 11. Summary of the Results  \n",
    "This analysis pipeline implements a comprehensive approach to identify and prioritize protein-genus pairs associated with microbial-influenced corrosion across varying risk environments. By integrating statistical pattern recognition with biological metadata, the pipeline systematically progresses from identifying abundance patterns to classifying pathways, prioritizing markers, and organizing results into specialized groups. The methodology prioritizes biological relevance through metrics like prevalence, specificity, and frequency, moving beyond traditional statistical tests to better capture the complex relationships between microbial proteins and corrosion processes. Each output dataset serves a specific purpose in refining our understanding of the microbial mechanisms underlying corrosion, ultimately producing a curated set of high-confidence biomarkers with demonstrated relevance to corrosion severity. The following sections detail each component of the results and their contributions to the overall analysis framework.\n",
    "Analysis Files Description  \n",
    "* pattern_data: Previously included traditional statistical hypothesis testing with Kruskal-Wallis H statistics and p-values, however the data showed that this test were no adequated to the data. Therefore, the methodology has been refined to focus on more direct category-specific metrics: prevalence (proportion of samples in a category where a protein appears), specificity (how unique a protein is to certain categories), and frequency (how often a protein appears). This change provides more direct biological interpretability and relevance to corrosion environments.The pattern analysis has been refined to focus specifically on three key patterns: 'increasing_abundance' (proteins that increase with corrosion severity), 'only_cat3' (proteins exclusive to high corrosion environments), and 'only_cat2' (proteins exclusive to medium corrosion environments). This focused approach allows for more targeted identification of corrosion-relevant markers.\n",
    "\n",
    "* integrated_results combines pattner data with functional metadata from the eccontri_df. This dataset enriches statistical findings with biological context including enzyme names, EC numbers, metabolic pathways, functional hierarchies, metal interactions, and corrosion-related mechanisms. It serves as the foundation for subsequent analysis by linking statistical significance to biological function.\n",
    "\n",
    "* classified_results Annotates integrated results with pathway classifications using the comprehensive taxonomy of universal vs. niche-specific pathways. Each entry is labeled as \"universal\" (common across all bacteria/archaea), \"niche-specific\" (specialized for particular environments), or \"mixed\" (containing elements of both). This classification helps distinguish between housekeeping functions and potentially corrosion-specific adaptations. classification_summary is a quantitative breakdown of pathway classifications with counts and percentages for each category, providing insight into the distribution of universal vs. specialized pathways in corrosion environments. Universal_frequency is a detailed frequency analysis of specific universal pathways, showing which core metabolic functions are most prevalent in the dataset.\n",
    "\n",
    "* Correlation Separated Results has tree types of results increasing_results which is a subset of integrated results showing positive correlation with corrosion risk.inverse_results which is a subset showing inverse correlation with corrosion risk, These are preserved for analysis of potentially protective or competitive proteins that might inhibit corrosion and analyse on a different section. Lastly constant_results a subset showing no significant correlation with risk categories.  \n",
    "\n",
    "* prioritized_markers comprising markers ranked according to a comprehensive scoring system that integrates: statistical pattern significance, biological relevance (mechanisms, pathways, hierarchies), prevalence, specificity, frequency, and corrosion-specific factors (metal interactions, synergistic mechanisms). The scoring components include metals_score, functional_mechanisms_score, pathways_score, hierarchy_score, tier_score, and corrosion_relevance.\n",
    "\n",
    "* balanced_markers is an optional filtered subset of prioritized markers that ensures balanced representation across genera, preventing over-representation of abundant genera while maintaining significant proteins.\n",
    "* marker_groups are categorized groups from the top 100 markers at 75% confidence threshold, organized by biological function and relevance.  \n",
    "\n",
    "* Marker_groups: The marker groups organize proteins based on multiple biological and statistical criteria, providing different perspectives on corrosion-relevant functions.  \n",
    "\n",
    "Main analysis dataframes: Core pipeline data from initial pattern detection through final balanced selection\n",
    "Top marker groups: Highest-scoring proteins and statistically significant patterns\n",
    "Relevance groups: Proteins with high scores in specific biological domains\n",
    "Mechanism-specific groups: Proteins associated with particular corrosion mechanisms\n",
    "Metal-related groups: Proteins interacting with specific metals relevant to corrosion\n",
    "Pathway classification groups: Proteins categorized by ecological specialization\n",
    "Effect size groups: Proteins showing dramatic abundance changes between corrosion states\n",
    "\n",
    "This pipeline provides a comprehensive analysis framework that progresses from statistical identification to biological classification and prioritization, creating both comprehensive and focused views of corrosion-relevant microbial proteins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.8 High Confidence Corrosion Microorganisms\n",
    "### Retrieving Selected Significant Groups\n",
    "\n",
    "From notebook 3_Feature_selection the file finalist.xlsx contain the groups worked and that were statistically significant in relation to the risk label. This groups posses interest since the relationship to the label could show better understanding in contrast with the different groups of known bacteria, core taxa, checked bacteria and the mixed groups.\n",
    "The idea is to understand if the core taxa which make up a large influence on the comunities on the water and cooling systems are also influencing corrosion. The known_bacteria_list corresponds to the selected group found on notebook 4, where we conducted an exhaustive api literature review and get the most referenced bacteria to be in this group. In section ## 3.1. Classifying Bacteria by their Source DataFrame, we extracted the list for each of these groups: \"known_bacteria\", \"pure_checked\", \"pure_core\" and \"checked_core\".\n",
    "\n",
    "We can verify the corrosion label \"Category\" by correlating the known_bacteria_list with the physicochemical parameters and the agreement if the known_bacteria_list have major abundance or correlate on someway with no only the label but the physicochemical conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read to Excel with one sheet per category \n",
    "bacteria_clas_path = output_base / \"bacteria_classification.xlsx\"\n",
    "usual_bacteria = pd.read_excel(bacteria_clas_path, sheet_name='usual_bacteria', engine ='openpyxl')\n",
    "components_bacteria = pd.read_excel(bacteria_clas_path, sheet_name='components', engine ='openpyxl')\n",
    "# Extracting the genus lists for each group:\n",
    "usual_list = usual_bacteria[\"Genus\"].tolist()\n",
    "components_list = components_bacteria[\"Genus\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify which genus are on my original data belogning to the known bacteria list, but genus are pair to protein_names\n",
    "usual_eccontri = ECcontri_Uniprot_enriched[ECcontri_Uniprot_enriched['Genus'].astype(str).isin(usual_list)]\n",
    "# Filter for components list\n",
    "component_eccontri = ECcontri_Uniprot_enriched[ECcontri_Uniprot_enriched['Genus'].astype(str).isin(components_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting corrosion protein analysis...\n",
      "Analyzing 371264 data points across 68 Sites...\n",
      "eccontri_df.columns immediately after creating column Frequency: ['index', 'idx', 'Sites', 'Genus', 'abund_raw', 'rel_abund_raw', 'genome_EC_count', 'abund_contri', 'rel_abund_contri', 'norm_abund_contri', 'protein_name', 'uniprot_id', 'EC', 'enzyme_names', 'enzyme_class', 'pathways', 'hierarchy', 'metals_consolidated', 'corrosion_mechanisms', 'corrosion_relevance_score', 'corrosion_relevance', 'metal_scores', 'overall_metal_score', 'corrosion_mechanism_scores', 'overall_corrosion_score', 'functional_categories', 'overall_functional_score', 'corrosion_keyword_groups', 'corrosion_keyword_scores', 'overall_keyword_score', 'corrosion_synergies', 'organic_processes', 'overall_organic_process_score', 'has_metal', 'overall_synergy_score', 'organic_process_scores', 'corrosion_synergy_scores', 'Category', 'was_uncharacterized', 'version', 'Frequency']\n",
      "Detailed pattern distribution:\n",
      "descriptive_pattern\n",
      "steadily_increasing         2598\n",
      "peak_at_transition_high     2037\n",
      "stress_recovery_partial     1496\n",
      "peak_at_transition_low      1003\n",
      "stress_recovery_full         971\n",
      "steadily_decreasing          752\n",
      "severe_exclusive             598\n",
      "risk_independent              52\n",
      "peak_at_transition_equal       4\n",
      "late_decline                   3\n",
      "stress_recovery_equal          2\n",
      "other_pattern                  1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Pattern category distribution:\n",
      "pattern_category\n",
      "increasing    6208\n",
      "decreasing    3256\n",
      "constant        52\n",
      "other            1\n",
      "Name: count, dtype: int64\n",
      "Pattern data created\n",
      "Integrated results created\n",
      "Classifying pathways by specificity...\n",
      "Pathway classification results:\n",
      "  - universal: 3844 (40.4%)\n",
      "  - niche-specific: 3276 (34.4%)\n",
      "  - mixed: 1553 (16.3%)\n",
      "  - unknown: 844 (8.9%)\n",
      "Separating positive and inverse patterns by patterns\n",
      "Separated 6208 increasing patterns, 3256 inverse patterns, and 53 mixed/other patterns.\n",
      "Average significance scores: Increasing=1.57, Inverse=0.85, Constant=0.38\n",
      "Prioritizing markers based on statistical and biological relevance...\n",
      "Successfully imported scoring modules from Kaggle path\n",
      "Warning: Error in database scoring: argument of type 'NoneType' is not iterable\n",
      "[15:46:56] Prioritization complete\n",
      "Score distribution: Mean=27.87, Median=27.53, Min=4.69, Max=54.98\n",
      "Balancing genus representation (top 10 per genus)...\n",
      "Created balanced dataset with 144 markers from 16 genera\n",
      "Creating specialized marker groups from balanced markers...\n",
      "Successfully imported terms for marker grouping\n",
      "Marker groups created:\n",
      "  - metals_consolidated: 144 markers\n",
      "  - mechanism_all: 142 markers\n",
      "  - functional_categories: 142 markers\n",
      "  - pathways_all: 139 markers\n",
      "  - organic_metal_synergy: 138 markers\n",
      "  - high_synergy_markers: 123 markers\n",
      "  - metal_iron_sulfur: 118 markers\n",
      "  - high_prevalence: 115 markers\n",
      "  - high_pathway_relevance: 76 markers\n",
      "  - high_mechanism_relevance: 57 markers\n",
      "  - high_corrosion_relevance: 52 markers\n",
      "  - high_biological_relevance: 51 markers\n",
      "  - high_tier_relevance: 40 markers\n",
      "  - high_metals_relevance: 39 markers\n",
      "  - top_markers: 36 markers\n",
      "  - significant_markers: 36 markers\n",
      "  - corrosion_critical: 15 markers\n",
      "  - very_high_specificity: 10 markers\n",
      "Analysis complete!\n",
      "Added pattern_data: 9517 rows\n",
      "Added integrated_results: 9517 rows\n",
      "Added classified_results: 9517 rows\n",
      "Added increasing_markers: 6208 rows\n",
      "Added inverse_markers: 3256 rows\n",
      "Added prioritized_markers: 6208 rows\n",
      "Added balanced_markers: 144 rows\n",
      "Added group_top_markers: 36 rows\n",
      "Added group_significant_markers: 36 rows\n",
      "Added group_high_prevalence: 115 rows\n",
      "Added group_very_high_specificity: 10 rows\n",
      "Added group_high_metals_relevance: 39 rows\n",
      "Added group_high_mechanism_relevance: 57 rows\n",
      "Added group_high_pathway_relevance: 76 rows\n",
      "Added group_high_tier_relevance: 40 rows\n",
      "Added group_high_corrosion_relevance: 52 rows\n",
      "Added group_mechanism_all: 142 rows\n",
      "Added group_metals_consolidated: 144 rows\n",
      "Added group_metal_iron_sulfur: 118 rows\n",
      "Added group_pathways_all: 139 rows\n",
      "Added group_functional_categories: 142 rows\n",
      "Added group_high_synergy_markers: 123 rows\n",
      "Added group_organic_metal_synergy: 138 rows\n",
      "Added group_high_biological_relevance: 51 rows\n",
      "Added group_corrosion_critical: 15 rows\n",
      "Successfully consolidated 25 dataframes\n"
     ]
    }
   ],
   "source": [
    "## calling it with balancing the generacomplete_results def balance_genus_representation(prioritized_markers, eccontri_df, genus_to_threshold, per_genus_count=10, knee_abundance_threshold=None)\n",
    "analysis_results_us, marker_groups_us = analyze_corrosion_proteins(usual_eccontri, alpha=0.05, genus_to_threshold=genus_to_threshold, per_genus_count=10,  knee_abundance_threshold =None)\n",
    "\n",
    "# Making the report\n",
    "corrosion_report_us = consolidate_analysis_results(analysis_results_us, marker_groups_us)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting corrosion protein analysis...\n",
      "Analyzing 620143 data points across 69 Sites...\n",
      "eccontri_df.columns immediately after creating column Frequency: ['index', 'idx', 'Sites', 'Genus', 'abund_raw', 'rel_abund_raw', 'genome_EC_count', 'abund_contri', 'rel_abund_contri', 'norm_abund_contri', 'protein_name', 'uniprot_id', 'EC', 'enzyme_names', 'enzyme_class', 'pathways', 'hierarchy', 'metals_consolidated', 'corrosion_mechanisms', 'corrosion_relevance_score', 'corrosion_relevance', 'metal_scores', 'overall_metal_score', 'corrosion_mechanism_scores', 'overall_corrosion_score', 'functional_categories', 'overall_functional_score', 'corrosion_keyword_groups', 'corrosion_keyword_scores', 'overall_keyword_score', 'corrosion_synergies', 'organic_processes', 'overall_organic_process_score', 'has_metal', 'overall_synergy_score', 'organic_process_scores', 'corrosion_synergy_scores', 'Category', 'was_uncharacterized', 'version', 'Frequency']\n",
      "Detailed pattern distribution:\n",
      "descriptive_pattern\n",
      "steadily_increasing          6204\n",
      "stress_recovery_full         3664\n",
      "peak_at_transition_high      2726\n",
      "steadily_decreasing          2485\n",
      "peak_at_transition_low       1594\n",
      "stress_recovery_partial      1076\n",
      "late_emergence_increasing     938\n",
      "severe_exclusive              671\n",
      "risk_independent               92\n",
      "late_emergence_decreasing       9\n",
      "peak_at_transition_equal        8\n",
      "late_decline                    5\n",
      "late_emergence_constant         4\n",
      "stress_recovery_equal           3\n",
      "other_pattern                   2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Pattern category distribution:\n",
      "pattern_category\n",
      "increasing    14211\n",
      "decreasing     5172\n",
      "constant         96\n",
      "other             2\n",
      "Name: count, dtype: int64\n",
      "Pattern data created\n",
      "Integrated results created\n",
      "Classifying pathways by specificity...\n",
      "Pathway classification results:\n",
      "  - universal: 8021 (41.2%)\n",
      "  - niche-specific: 6519 (33.5%)\n",
      "  - mixed: 3208 (16.5%)\n",
      "  - unknown: 1733 (8.9%)\n",
      "Separating positive and inverse patterns by patterns\n",
      "Separated 14211 increasing patterns, 5172 inverse patterns, and 98 mixed/other patterns.\n",
      "Average significance scores: Increasing=1.58, Inverse=0.98, Constant=0.40\n",
      "Prioritizing markers based on statistical and biological relevance...\n",
      "Successfully imported scoring modules from Kaggle path\n",
      "Warning: Error in database scoring: argument of type 'NoneType' is not iterable\n",
      "[15:47:36] Prioritization complete\n",
      "Score distribution: Mean=27.88, Median=27.40, Min=4.69, Max=54.08\n",
      "Balancing genus representation (top 10 per genus)...\n",
      "Created balanced dataset with 274 markers from 32 genera\n",
      "Creating specialized marker groups from balanced markers...\n",
      "Successfully imported terms for marker grouping\n",
      "Marker groups created:\n",
      "  - metals_consolidated: 274 markers\n",
      "  - mechanism_all: 271 markers\n",
      "  - functional_categories: 271 markers\n",
      "  - organic_metal_synergy: 262 markers\n",
      "  - pathways_all: 260 markers\n",
      "  - high_synergy_markers: 217 markers\n",
      "  - high_prevalence: 212 markers\n",
      "  - metal_iron_sulfur: 208 markers\n",
      "  - high_pathway_relevance: 113 markers\n",
      "  - high_tier_relevance: 90 markers\n",
      "  - high_biological_relevance: 76 markers\n",
      "  - high_corrosion_relevance: 75 markers\n",
      "  - high_metals_relevance: 74 markers\n",
      "  - high_mechanism_relevance: 74 markers\n",
      "  - top_markers: 69 markers\n",
      "  - significant_markers: 69 markers\n",
      "  - corrosion_critical: 28 markers\n",
      "  - very_high_specificity: 10 markers\n",
      "Analysis complete!\n",
      "Added pattern_data: 19481 rows\n",
      "Added integrated_results: 19481 rows\n",
      "Added classified_results: 19481 rows\n",
      "Added increasing_markers: 14211 rows\n",
      "Added inverse_markers: 5172 rows\n",
      "Added prioritized_markers: 14211 rows\n",
      "Added balanced_markers: 274 rows\n",
      "Added group_top_markers: 69 rows\n",
      "Added group_significant_markers: 69 rows\n",
      "Added group_high_prevalence: 212 rows\n",
      "Added group_very_high_specificity: 10 rows\n",
      "Added group_high_metals_relevance: 74 rows\n",
      "Added group_high_mechanism_relevance: 74 rows\n",
      "Added group_high_pathway_relevance: 113 rows\n",
      "Added group_high_tier_relevance: 90 rows\n",
      "Added group_high_corrosion_relevance: 75 rows\n",
      "Added group_mechanism_all: 271 rows\n",
      "Added group_metals_consolidated: 274 rows\n",
      "Added group_metal_iron_sulfur: 208 rows\n",
      "Added group_pathways_all: 260 rows\n",
      "Added group_functional_categories: 271 rows\n",
      "Added group_high_synergy_markers: 217 rows\n",
      "Added group_organic_metal_synergy: 262 rows\n",
      "Added group_high_biological_relevance: 76 rows\n",
      "Added group_corrosion_critical: 28 rows\n",
      "Successfully consolidated 25 dataframes\n"
     ]
    }
   ],
   "source": [
    "## calling it with balancing the generacomplete_results def balance_genus_representation(prioritized_markers, eccontri_df, genus_to_threshold, per_genus_count=10, knee_abundance_threshold=None)\n",
    "analysis_results_component, marker_groups_component = analyze_corrosion_proteins(component_eccontri, alpha=0.05, genus_to_threshold=genus_to_threshold, per_genus_count=10,  knee_abundance_threshold =None)\n",
    "\n",
    "# Making the report\n",
    "corrosion_report_component = consolidate_analysis_results(analysis_results_component, marker_groups_component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           index    Sites        Genus  abund_raw  rel_abund_raw  \\\n",
      "idx                                                                \n",
      "284414    284414  site_13  Pseudomonas    28186.0       3.816816   \n",
      "1384130  1384130  site_63  Pseudomonas    36755.0       5.422421   \n",
      "1418278  1418278  site_64  Pseudomonas    28500.0       3.226537   \n",
      "1414684  1414684  site_64  Pseudomonas    28500.0       3.226537   \n",
      "469066    469066  site_22  Pseudomonas   450721.0      62.850223   \n",
      "\n",
      "         genome_EC_count  abund_contri  rel_abund_contri  norm_abund_contri  \\\n",
      "idx                                                                           \n",
      "284414                 1       28186.0          3.816816           0.040841   \n",
      "1384130                1       36755.0          5.422421           0.053048   \n",
      "1418278                1       28500.0          3.226537           0.083163   \n",
      "1414684                2       57000.0          6.453074           0.062336   \n",
      "469066                 1      450721.0         62.850223           0.813143   \n",
      "\n",
      "                                                             protein_name  \\\n",
      "idx                                                                         \n",
      "284414   nucleoside diphosphate kinase (ndk) (ndp kinase) (nucleoside-2-p   \n",
      "1384130                            riboflavin biosynthesis protein ribd [   \n",
      "1418278                                       glutamate--cysteine ligase    \n",
      "1414684                                                homoserine kinase    \n",
      "469066                          indolepyruvate ferredoxin oxido-reductase   \n",
      "\n",
      "         ... corrosion_synergies                  organic_processes  \\\n",
      "idx      ...                                                          \n",
      "284414   ...                None               oxidation; reduction   \n",
      "1384130  ...                Fe-S  respiration; oxidation; reduction   \n",
      "1418278  ...                None                               None   \n",
      "1414684  ...                None               synthesis; reduction   \n",
      "469066   ...                None               oxidation; reduction   \n",
      "\n",
      "        overall_organic_process_score has_metal overall_synergy_score  \\\n",
      "idx                                                                     \n",
      "284414                       1.632993      True              0.000000   \n",
      "1384130                      2.527420      True              0.894427   \n",
      "1418278                      0.000000      True              0.000000   \n",
      "1414684                      1.816497      True              0.000000   \n",
      "469066                       1.971197      True              0.000000   \n",
      "\n",
      "                                                                                      organic_process_scores  \\\n",
      "idx                                                                                                            \n",
      "284414                                      {\"oxidation\": 0.816496580927726, \"reduction\": 0.816496580927726}   \n",
      "1384130  {\"respiration\": 0.8944271909999159, \"oxidation\": 0.816496580927726, \"reduction\": 0.816496580927726}   \n",
      "1418278                                                                                                   {}   \n",
      "1414684                                                   {\"synthesis\": 1.0, \"reduction\": 0.816496580927726}   \n",
      "469066                                     {\"oxidation\": 1.1547005383792515, \"reduction\": 0.816496580927726}   \n",
      "\n",
      "             corrosion_synergy_scores Category  was_uncharacterized  \\\n",
      "idx                                                                   \n",
      "284414                             {}        3                False   \n",
      "1384130  {\"Fe-S\": 0.8944271909999159}        2                False   \n",
      "1418278                            {}        2                False   \n",
      "1414684                            {}        2                False   \n",
      "469066                             {}        3                False   \n",
      "\n",
      "                           version  \n",
      "idx                                 \n",
      "284414   v9_proteins_pathways_fold  \n",
      "1384130  v9_proteins_pathways_fold  \n",
      "1418278  v9_proteins_pathways_fold  \n",
      "1414684  v9_proteins_pathways_fold  \n",
      "469066   v9_proteins_pathways_fold  \n",
      "\n",
      "[5 rows x 39 columns]\n",
      "Genus\n",
      "Pseudomonas          100\n",
      "Propionibacterium    100\n",
      "Clostridium          100\n",
      "Gallionella          100\n",
      "Corynebacterium      100\n",
      "Thiobacillus         100\n",
      "Staphylococcus       100\n",
      "Novosphingobium      100\n",
      "Streptococcus        100\n",
      "Desulfobulbus        100\n",
      "Desulfotomaculum     100\n",
      "Micrococcus          100\n",
      "Shewanella           100\n",
      "Acetobacterium       100\n",
      "Desulfovibrio        100\n",
      "Bacillus             100\n",
      "Desulfobacterium     100\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Determine the unique genera in your known data\n",
    "unique_genera = usual_eccontri['Genus'].unique()\n",
    "\n",
    "sampled_df = pd.DataFrame()\n",
    "\n",
    "for genus in unique_genera:\n",
    "    genus_df = usual_eccontri[usual_eccontri['Genus'] == genus]\n",
    "    # Sample a fraction (e.g., 10%) or a fixed number of rows for each genus\n",
    "    if len(genus_df) > 100:  # Example: sample up to 100 rows per genus\n",
    "        sampled_genus = genus_df.sample(n=100, random_state=42)\n",
    "    else:\n",
    "        sampled_genus = genus_df  # Keep all if fewer than the sample size\n",
    "    sampled_df = pd.concat([sampled_df, sampled_genus])\n",
    "\n",
    "print(sampled_df.head())\n",
    "print(sampled_df['Genus'].value_counts()) # See the distribution of genera in the sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Sites</th>\n",
       "      <th>Genus</th>\n",
       "      <th>abund_raw</th>\n",
       "      <th>rel_abund_raw</th>\n",
       "      <th>genome_EC_count</th>\n",
       "      <th>abund_contri</th>\n",
       "      <th>rel_abund_contri</th>\n",
       "      <th>norm_abund_contri</th>\n",
       "      <th>protein_name</th>\n",
       "      <th>...</th>\n",
       "      <th>organic_processes</th>\n",
       "      <th>overall_organic_process_score</th>\n",
       "      <th>has_metal</th>\n",
       "      <th>overall_synergy_score</th>\n",
       "      <th>organic_process_scores</th>\n",
       "      <th>corrosion_synergy_scores</th>\n",
       "      <th>Category</th>\n",
       "      <th>was_uncharacterized</th>\n",
       "      <th>version</th>\n",
       "      <th>core_corrosion_flag</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>284414</th>\n",
       "      <td>284414</td>\n",
       "      <td>site_13</td>\n",
       "      <td>Pseudomonas</td>\n",
       "      <td>28186.0</td>\n",
       "      <td>3.816816</td>\n",
       "      <td>1</td>\n",
       "      <td>28186.0</td>\n",
       "      <td>3.816816</td>\n",
       "      <td>0.040841</td>\n",
       "      <td>nucleoside diphosphate kinase (ndk) (ndp kinase) (nucleoside-2-p</td>\n",
       "      <td>...</td>\n",
       "      <td>oxidation; reduction</td>\n",
       "      <td>1.632993</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{\"oxidation\": 0.816496580927726, \"reduction\": 0.816496580927726}</td>\n",
       "      <td>{}</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>v9_proteins_pathways_fold</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1384130</th>\n",
       "      <td>1384130</td>\n",
       "      <td>site_63</td>\n",
       "      <td>Pseudomonas</td>\n",
       "      <td>36755.0</td>\n",
       "      <td>5.422421</td>\n",
       "      <td>1</td>\n",
       "      <td>36755.0</td>\n",
       "      <td>5.422421</td>\n",
       "      <td>0.053048</td>\n",
       "      <td>riboflavin biosynthesis protein ribd [</td>\n",
       "      <td>...</td>\n",
       "      <td>respiration; oxidation; reduction</td>\n",
       "      <td>2.527420</td>\n",
       "      <td>True</td>\n",
       "      <td>0.894427</td>\n",
       "      <td>{\"respiration\": 0.8944271909999159, \"oxidation\": 0.816496580927726, \"reduction\": 0.816496580927726}</td>\n",
       "      <td>{\"Fe-S\": 0.8944271909999159}</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>v9_proteins_pathways_fold</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1418278</th>\n",
       "      <td>1418278</td>\n",
       "      <td>site_64</td>\n",
       "      <td>Pseudomonas</td>\n",
       "      <td>28500.0</td>\n",
       "      <td>3.226537</td>\n",
       "      <td>1</td>\n",
       "      <td>28500.0</td>\n",
       "      <td>3.226537</td>\n",
       "      <td>0.083163</td>\n",
       "      <td>glutamate--cysteine ligase</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>v9_proteins_pathways_fold</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1414684</th>\n",
       "      <td>1414684</td>\n",
       "      <td>site_64</td>\n",
       "      <td>Pseudomonas</td>\n",
       "      <td>28500.0</td>\n",
       "      <td>3.226537</td>\n",
       "      <td>2</td>\n",
       "      <td>57000.0</td>\n",
       "      <td>6.453074</td>\n",
       "      <td>0.062336</td>\n",
       "      <td>homoserine kinase</td>\n",
       "      <td>...</td>\n",
       "      <td>synthesis; reduction</td>\n",
       "      <td>1.816497</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{\"synthesis\": 1.0, \"reduction\": 0.816496580927726}</td>\n",
       "      <td>{}</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>v9_proteins_pathways_fold</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469066</th>\n",
       "      <td>469066</td>\n",
       "      <td>site_22</td>\n",
       "      <td>Pseudomonas</td>\n",
       "      <td>450721.0</td>\n",
       "      <td>62.850223</td>\n",
       "      <td>1</td>\n",
       "      <td>450721.0</td>\n",
       "      <td>62.850223</td>\n",
       "      <td>0.813143</td>\n",
       "      <td>indolepyruvate ferredoxin oxido-reductase</td>\n",
       "      <td>...</td>\n",
       "      <td>oxidation; reduction</td>\n",
       "      <td>1.971197</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{\"oxidation\": 1.1547005383792515, \"reduction\": 0.816496580927726}</td>\n",
       "      <td>{}</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>v9_proteins_pathways_fold</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1183728</th>\n",
       "      <td>1183728</td>\n",
       "      <td>site_56</td>\n",
       "      <td>Desulfobacterium</td>\n",
       "      <td>412.0</td>\n",
       "      <td>0.064714</td>\n",
       "      <td>1</td>\n",
       "      <td>412.0</td>\n",
       "      <td>0.064714</td>\n",
       "      <td>0.000697</td>\n",
       "      <td>mannose-1-phosphate guanylyltransferase</td>\n",
       "      <td>...</td>\n",
       "      <td>synthesis</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{\"synthesis\": 1.0}</td>\n",
       "      <td>{}</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>v9_proteins_pathways_fold</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1181339</th>\n",
       "      <td>1181339</td>\n",
       "      <td>site_56</td>\n",
       "      <td>Desulfobacterium</td>\n",
       "      <td>412.0</td>\n",
       "      <td>0.064714</td>\n",
       "      <td>1</td>\n",
       "      <td>412.0</td>\n",
       "      <td>0.064714</td>\n",
       "      <td>0.000686</td>\n",
       "      <td>riboflavin-synthase</td>\n",
       "      <td>...</td>\n",
       "      <td>synthesis</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{\"synthesis\": 1.0}</td>\n",
       "      <td>{}</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>v9_proteins_pathways_fold</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1171677</th>\n",
       "      <td>1171677</td>\n",
       "      <td>site_56</td>\n",
       "      <td>Desulfobacterium</td>\n",
       "      <td>412.0</td>\n",
       "      <td>0.064714</td>\n",
       "      <td>1</td>\n",
       "      <td>412.0</td>\n",
       "      <td>0.064714</td>\n",
       "      <td>0.002315</td>\n",
       "      <td>gdp-l-fucose-synthase</td>\n",
       "      <td>...</td>\n",
       "      <td>synthesis; reduction</td>\n",
       "      <td>1.816497</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{\"synthesis\": 1.0, \"reduction\": 0.816496580927726}</td>\n",
       "      <td>{}</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>v9_proteins_pathways_fold</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1197304</th>\n",
       "      <td>1197304</td>\n",
       "      <td>site_56</td>\n",
       "      <td>Desulfobacterium</td>\n",
       "      <td>412.0</td>\n",
       "      <td>0.064714</td>\n",
       "      <td>1</td>\n",
       "      <td>412.0</td>\n",
       "      <td>0.064714</td>\n",
       "      <td>0.002641</td>\n",
       "      <td>formate--tetrahydrofolate ligase</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>v9_proteins_pathways_fold</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1175417</th>\n",
       "      <td>1175417</td>\n",
       "      <td>site_56</td>\n",
       "      <td>Desulfobacterium</td>\n",
       "      <td>412.0</td>\n",
       "      <td>0.064714</td>\n",
       "      <td>3</td>\n",
       "      <td>1236.0</td>\n",
       "      <td>0.194142</td>\n",
       "      <td>0.001749</td>\n",
       "      <td>methylenetetrahydrofolate-reductase [</td>\n",
       "      <td>...</td>\n",
       "      <td>reduction</td>\n",
       "      <td>0.816497</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{\"reduction\": 0.816496580927726}</td>\n",
       "      <td>{}</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>v9_proteins_pathways_fold</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1700 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           index    Sites             Genus  abund_raw  rel_abund_raw  \\\n",
       "idx                                                                     \n",
       "284414    284414  site_13       Pseudomonas    28186.0       3.816816   \n",
       "1384130  1384130  site_63       Pseudomonas    36755.0       5.422421   \n",
       "1418278  1418278  site_64       Pseudomonas    28500.0       3.226537   \n",
       "1414684  1414684  site_64       Pseudomonas    28500.0       3.226537   \n",
       "469066    469066  site_22       Pseudomonas   450721.0      62.850223   \n",
       "...          ...      ...               ...        ...            ...   \n",
       "1183728  1183728  site_56  Desulfobacterium      412.0       0.064714   \n",
       "1181339  1181339  site_56  Desulfobacterium      412.0       0.064714   \n",
       "1171677  1171677  site_56  Desulfobacterium      412.0       0.064714   \n",
       "1197304  1197304  site_56  Desulfobacterium      412.0       0.064714   \n",
       "1175417  1175417  site_56  Desulfobacterium      412.0       0.064714   \n",
       "\n",
       "         genome_EC_count  abund_contri  rel_abund_contri  norm_abund_contri  \\\n",
       "idx                                                                           \n",
       "284414                 1       28186.0          3.816816           0.040841   \n",
       "1384130                1       36755.0          5.422421           0.053048   \n",
       "1418278                1       28500.0          3.226537           0.083163   \n",
       "1414684                2       57000.0          6.453074           0.062336   \n",
       "469066                 1      450721.0         62.850223           0.813143   \n",
       "...                  ...           ...               ...                ...   \n",
       "1183728                1         412.0          0.064714           0.000697   \n",
       "1181339                1         412.0          0.064714           0.000686   \n",
       "1171677                1         412.0          0.064714           0.002315   \n",
       "1197304                1         412.0          0.064714           0.002641   \n",
       "1175417                3        1236.0          0.194142           0.001749   \n",
       "\n",
       "                                                             protein_name  \\\n",
       "idx                                                                         \n",
       "284414   nucleoside diphosphate kinase (ndk) (ndp kinase) (nucleoside-2-p   \n",
       "1384130                            riboflavin biosynthesis protein ribd [   \n",
       "1418278                                       glutamate--cysteine ligase    \n",
       "1414684                                                homoserine kinase    \n",
       "469066                          indolepyruvate ferredoxin oxido-reductase   \n",
       "...                                                                   ...   \n",
       "1183728                           mannose-1-phosphate guanylyltransferase   \n",
       "1181339                                               riboflavin-synthase   \n",
       "1171677                                            gdp-l-fucose-synthase    \n",
       "1197304                                  formate--tetrahydrofolate ligase   \n",
       "1175417                             methylenetetrahydrofolate-reductase [   \n",
       "\n",
       "         ...                  organic_processes overall_organic_process_score  \\\n",
       "idx      ...                                                                    \n",
       "284414   ...               oxidation; reduction                      1.632993   \n",
       "1384130  ...  respiration; oxidation; reduction                      2.527420   \n",
       "1418278  ...                               None                      0.000000   \n",
       "1414684  ...               synthesis; reduction                      1.816497   \n",
       "469066   ...               oxidation; reduction                      1.971197   \n",
       "...      ...                                ...                           ...   \n",
       "1183728  ...                          synthesis                      1.000000   \n",
       "1181339  ...                          synthesis                      1.000000   \n",
       "1171677  ...               synthesis; reduction                      1.816497   \n",
       "1197304  ...                               None                      0.000000   \n",
       "1175417  ...                          reduction                      0.816497   \n",
       "\n",
       "        has_metal overall_synergy_score  \\\n",
       "idx                                       \n",
       "284414       True              0.000000   \n",
       "1384130      True              0.894427   \n",
       "1418278      True              0.000000   \n",
       "1414684      True              0.000000   \n",
       "469066       True              0.000000   \n",
       "...           ...                   ...   \n",
       "1183728      True              0.000000   \n",
       "1181339      True              0.000000   \n",
       "1171677      True              0.000000   \n",
       "1197304      True              0.000000   \n",
       "1175417      True              0.000000   \n",
       "\n",
       "                                                                                      organic_process_scores  \\\n",
       "idx                                                                                                            \n",
       "284414                                      {\"oxidation\": 0.816496580927726, \"reduction\": 0.816496580927726}   \n",
       "1384130  {\"respiration\": 0.8944271909999159, \"oxidation\": 0.816496580927726, \"reduction\": 0.816496580927726}   \n",
       "1418278                                                                                                   {}   \n",
       "1414684                                                   {\"synthesis\": 1.0, \"reduction\": 0.816496580927726}   \n",
       "469066                                     {\"oxidation\": 1.1547005383792515, \"reduction\": 0.816496580927726}   \n",
       "...                                                                                                      ...   \n",
       "1183728                                                                                   {\"synthesis\": 1.0}   \n",
       "1181339                                                                                   {\"synthesis\": 1.0}   \n",
       "1171677                                                   {\"synthesis\": 1.0, \"reduction\": 0.816496580927726}   \n",
       "1197304                                                                                                   {}   \n",
       "1175417                                                                     {\"reduction\": 0.816496580927726}   \n",
       "\n",
       "             corrosion_synergy_scores Category was_uncharacterized  \\\n",
       "idx                                                                  \n",
       "284414                             {}        3               False   \n",
       "1384130  {\"Fe-S\": 0.8944271909999159}        2               False   \n",
       "1418278                            {}        2               False   \n",
       "1414684                            {}        2               False   \n",
       "469066                             {}        3               False   \n",
       "...                               ...      ...                 ...   \n",
       "1183728                            {}        3               False   \n",
       "1181339                            {}        3               False   \n",
       "1171677                            {}        3               False   \n",
       "1197304                            {}        3               False   \n",
       "1175417                            {}        3               False   \n",
       "\n",
       "                           version core_corrosion_flag  \n",
       "idx                                                     \n",
       "284414   v9_proteins_pathways_fold               False  \n",
       "1384130  v9_proteins_pathways_fold               False  \n",
       "1418278  v9_proteins_pathways_fold               False  \n",
       "1414684  v9_proteins_pathways_fold               False  \n",
       "469066   v9_proteins_pathways_fold               False  \n",
       "...                            ...                 ...  \n",
       "1183728  v9_proteins_pathways_fold               False  \n",
       "1181339  v9_proteins_pathways_fold               False  \n",
       "1171677  v9_proteins_pathways_fold               False  \n",
       "1197304  v9_proteins_pathways_fold               False  \n",
       "1175417  v9_proteins_pathways_fold               False  \n",
       "\n",
       "[1700 rows x 40 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Rule-Based System\n",
    "def identify_high_confidence_corrosion(row):\n",
    "    \"\"\"Identifies high-confidence corrosion cases based on rules.\"\"\"\n",
    "\n",
    "    physicochemical_conditions_for_corrosion = [\"Redox potential is extreme\", # This comes from physicochemical repository on the parallel data\n",
    "                                             \"pH is low\",\n",
    "                                             \"EC is high\",\n",
    "                                             \"Salinity is high\"]\n",
    "\n",
    "    for bacteria in usual_list:\n",
    "        if bacteria.lower() in row.get('Genus', '').lower():  \n",
    "           has_high_confidence_bacteria = True\n",
    "           break\n",
    "\n",
    "    #if some pysicochemical conditios\n",
    "    has_high_corrosion_conditions = None\n",
    "    for conditions in physicochemical_conditions_for_corrosion:\n",
    "\n",
    "        if conditions.lower() in row.get('location', '').lower(): \n",
    "           has_high_corrosion_conditions = True\n",
    "           break\n",
    "\n",
    "    if has_high_confidence_bacteria and has_high_corrosion_conditions:\n",
    "       return True\n",
    "    else:\n",
    "       return False\n",
    "\n",
    "# 2. Calculate relevant Descriptive columns (scores)\n",
    "def descriptive_marker_scores(results):\n",
    "    \"\"\"\n",
    "    Calculate Scores with statistical and database for potential corrosion markers based on descriptive columns\n",
    "    \"\"\"\n",
    "    results['core_corrosion_flag'] = results.apply(identify_high_confidence_corrosion, axis=1)\n",
    "\n",
    "    return results\n",
    "\n",
    "descriptive_marker_scores(sampled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'Sites', 'Genus', 'abund_raw', 'rel_abund_raw',\n",
       "       'genome_EC_count', 'abund_contri', 'rel_abund_contri',\n",
       "       'norm_abund_contri', 'protein_name', 'uniprot_id', 'EC', 'enzyme_names',\n",
       "       'enzyme_class', 'pathways', 'hierarchy', 'metals_consolidated',\n",
       "       'corrosion_mechanisms', 'corrosion_relevance_score',\n",
       "       'corrosion_relevance', 'metal_scores', 'overall_metal_score',\n",
       "       'corrosion_mechanism_scores', 'overall_corrosion_score',\n",
       "       'functional_categories', 'overall_functional_score',\n",
       "       'corrosion_keyword_groups', 'corrosion_keyword_scores',\n",
       "       'overall_keyword_score', 'corrosion_synergies', 'organic_processes',\n",
       "       'overall_organic_process_score', 'has_metal', 'overall_synergy_score',\n",
       "       'organic_process_scores', 'corrosion_synergy_scores', 'Category',\n",
       "       'was_uncharacterized', 'version'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usual_eccontri.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'combined_score'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/MIC/2_Micro/.venv/lib/python3.10/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'combined_score'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 16\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m avg_score \u001b[38;5;241m<\u001b[39m overall_avg:\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWARNING: Scoring system underprioritizes known corrosive Genus!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m \u001b[43mvalidate_scoring\u001b[49m\u001b[43m(\u001b[49m\u001b[43musual_eccontri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musual_list\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[79], line 8\u001b[0m, in \u001b[0;36mvalidate_scoring\u001b[0;34m(enriched_df, usual_list)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_df\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo known corrosive Genus found in results!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m avg_score \u001b[38;5;241m=\u001b[39m \u001b[43mvalidation_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcombined_score\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m      9\u001b[0m overall_avg \u001b[38;5;241m=\u001b[39m enriched_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcombined_score\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation: Known corrosive Genus average score = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_score\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (vs overall \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moverall_avg\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/MIC/2_Micro/.venv/lib/python3.10/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/MIC/2_Micro/.venv/lib/python3.10/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'combined_score'"
     ]
    }
   ],
   "source": [
    "def validate_scoring(enriched_df, usual_list):\n",
    "    \"\"\"Validates if known corrosive Genus rank higher in combined_score.\"\"\"\n",
    "    validation_df = enriched_df[enriched_df['Genus'].isin(usual_list)]\n",
    "    \n",
    "    if validation_df.empty:\n",
    "        raise ValueError(\"No known corrosive Genus found in results!\")\n",
    "    \n",
    "    avg_score = validation_df['combined_score'].mean()\n",
    "    overall_avg = enriched_df['combined_score'].mean()\n",
    "    \n",
    "    print(f\"Validation: Known corrosive Genus average score = {avg_score:.2f} (vs overall {overall_avg:.2f})\")\n",
    "    if avg_score < overall_avg:\n",
    "        print(\"WARNING: Scoring system underprioritizes known corrosive Genus!\")\n",
    "\n",
    "\n",
    "validate_scoring(usual_eccontri, usual_list)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6825079,
     "sourceId": 10969174,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6825231,
     "sourceId": 10969366,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6825260,
     "sourceId": 10996867,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7185333,
     "sourceId": 11466080,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
