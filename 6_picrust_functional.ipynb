{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10969174,"sourceType":"datasetVersion","datasetId":6825079},{"sourceId":10969366,"sourceType":"datasetVersion","datasetId":6825231},{"sourceId":10996867,"sourceType":"datasetVersion","datasetId":6825260}],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"colab":{"machine_shape":"hm","provenance":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Sequence Analysis and Functional Prediction Pipeline\n\n## 1. Introduction\nThis notebook analyzes the functional and sequence relationships between newly identified bacteria and known corrosion-influencing microorganisms. The analysis builds upon previous findings where:\n- Statistical significance was established between the selected bacteria and corrosion risk (Notebook 3)\n- Literature validation confirmed corrosion influence for many bacteria (Notebook 4)\n- Evolutionary relationships were mapped through phylogenetic analysis (Notebook 5)\n\nThe study focuses on bacteria from operational heating and cooling water systems, primarily in Germany. Using 16S rRNA data (bootstrap-validated from Notebook 5), this analysis employs PICRUSt2 to predict metabolic functions and compare functional profiles between different bacterial groups.\n\n### Analysis Approaches\nWe implement two classification strategies:\n\n1. Simple Classification:\n   - Known corrosion-causing bacteria (usual_taxa)\n   - Other bacteria (combining checked_taxa and core_taxa)\n\n2. Detailed Classification:\n   - Known corrosion-causing bacteria (usual_taxa)\n   - Pure checked bacteria (exclusive to checked_taxa)\n   - Pure core bacteria (exclusive to core_taxa)\n   - Checked-core bacteria (overlap between checked and core taxa)\n\nThis detailed approach allows for more nuanced analysis of functional profiles and better understanding of potential corrosion mechanisms across different bacterial groups.\n\n### Analysis Goals:\n- Predict metabolic functions from 16S sequences\n- Focus on corrosion-relevant pathways (sulfur/iron metabolism)\n- Compare functional profiles between known corrosion-causing bacteria and newly identified candidates\n- Validate whether statistical correlations reflect genuine metabolic capabilities associated with corrosion processes\n\n### Directory Structure:\n Following is the structure of the notebook data named data_picrus  \ndata_tree  \n ├── sequences/  \n │   ├── known.fasta : sequences of known corrosion-causing bacteria  \n │   ├── candidate.fasta : sequences of potential new corrosion-causing bacteria  \n |   └── other files  \n data_picrus  \n └── picrust_results/  \n      ├── known_bacteria/  \n      |               ├── EC_predictions/       : enzyme predictions  \n      |               ├── pathway_predictions/  : metabolic pathway abundance  \n      |               ├── KO_predictions/       : KEGG ortholog predictions  \n      |               └── other_picrust_files/  \n      ├── candidate_bacteria/  \n      |               ├── EC_predictions/       : enzyme predictions  \n      |               ├── pathway_predictions/  : metabolic pathway abundance  \n      |               ├── KO_predictions/       : KEGG ortholog predictions  \n      |               └── other_picrust_files/  : final comparison summary\n      ├── core_bacteria/\n      |               ├── EC_predictions/       : enzyme predictions  \n      |               ├── pathway_predictions/  : metabolic pathway abundance  \n      |               ├── KO_predictions/       : KEGG ortholog predictions  \n      |               └── other_picrust_files/  \n      │      \n      └── functional_comparison.xlsx  \n\nPicrust2 works using its reference database that was installed with the package   \n~/miniconda3/envs/picrust2/lib/python3.9/site-packages/picrust2/default_files/prokaryotic/pro_ref\n\nAbout picrust2  \nhttps://evomics.org/wp-content/uploads/2015/01/presentation_evomics-05-picrust_01-18-15.pdf","metadata":{"id":"cDED5j4zCqR_"}},{"cell_type":"markdown","source":"# 2. Loading and Preparing the Data\n\n## 2.1 Colab Initialisation","metadata":{"id":"LeIn7RIBCqSD"}},{"cell_type":"code","source":"import os\nos.path.exists('/content/drive/MyDrive')","metadata":{"id":"m2gkBXEzwKkG","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"*marzo* 22","metadata":{"id":"zB6nXz7xEeWf"}},{"cell_type":"code","source":"# Colab specific\nfrom google.colab import drive\nfrom google.colab import files\nimport os\n\ndrive.mount('/content/drive')\n\n#change the path\nos.chdir('/content/drive/MyDrive/MIC/data_picrust')","metadata":{"id":"5U6gm_R_JQjt","outputId":"b4265fd4-0986-41ea-d152-c9a0802d0214","trusted":true,"execution":{"iopub.status.busy":"2025-03-28T19:15:10.335944Z","iopub.execute_input":"2025-03-28T19:15:10.336333Z","iopub.status.idle":"2025-03-28T19:15:10.406674Z","shell.execute_reply.started":"2025-03-28T19:15:10.336295Z","shell.execute_reply":"2025-03-28T19:15:10.404935Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Making sure to use same python version for compatibility\n!sudo apt-get update -y\n!sudo apt-get install python3.10\n!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.10 1\n!python --version","metadata":{"id":"xcyAS5xNUyNx","outputId":"2363a940-de04-4d06-9e40-ef71d00c4343","trusted":true,"execution":{"iopub.status.busy":"2025-03-30T16:40:00.119822Z","iopub.execute_input":"2025-03-30T16:40:00.120203Z","iopub.status.idle":"2025-03-30T16:40:25.523473Z","shell.execute_reply.started":"2025-03-30T16:40:00.120166Z","shell.execute_reply":"2025-03-30T16:40:25.522148Z"}},"outputs":[{"name":"stdout","text":"Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\nGet:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]        \nGet:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\nGet:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\nGet:5 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]      \nGet:6 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]      \nGet:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]           \nGet:8 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [69.9 kB]\nGet:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\nGet:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,381 kB]\nGet:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\nGet:12 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [55.7 kB]\nGet:13 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [4,049 kB]\nHit:14 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease   \nGet:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,045 kB]\nGet:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,538 kB]\nGet:17 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [35.2 kB]\nGet:18 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [82.7 kB]\nGet:19 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,790 kB]\nGet:20 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [3,892 kB]\nGet:21 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,686 kB]\nGet:22 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [33.6 kB]\nGet:23 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [46.8 kB]\nGet:24 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,239 kB]\nGet:25 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,737 kB]\nGet:26 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [47.7 kB]\nFetched 30.2 MB in 3s (10.6 MB/s)                            \nReading package lists... Done\nW: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following additional packages will be installed:\n  libpython3.10 libpython3.10-dev libpython3.10-minimal libpython3.10-stdlib\n  python3.10-dev python3.10-minimal\nSuggested packages:\n  python3.10-venv python3.10-doc binfmt-support\nThe following packages will be upgraded:\n  libpython3.10 libpython3.10-dev libpython3.10-minimal libpython3.10-stdlib\n  python3.10 python3.10-dev python3.10-minimal\n7 upgraded, 0 newly installed, 0 to remove and 163 not upgraded.\nNeed to get 12.7 MB of archives.\nAfter this operation, 1,024 B of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 python3.10-dev amd64 3.10.12-1~22.04.9 [508 kB]\nGet:2 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpython3.10-dev amd64 3.10.12-1~22.04.9 [4,763 kB]\nGet:3 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpython3.10 amd64 3.10.12-1~22.04.9 [1,949 kB]\nGet:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 python3.10 amd64 3.10.12-1~22.04.9 [508 kB]\nGet:5 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpython3.10-stdlib amd64 3.10.12-1~22.04.9 [1,850 kB]\nGet:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 python3.10-minimal amd64 3.10.12-1~22.04.9 [2,263 kB]\nGet:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpython3.10-minimal amd64 3.10.12-1~22.04.9 [815 kB]\nFetched 12.7 MB in 1s (9,879 kB/s)             \ndebconf: unable to initialize frontend: Dialog\ndebconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 7.)\ndebconf: falling back to frontend: Readline\n(Reading database ... 127400 files and directories currently installed.)\nPreparing to unpack .../0-python3.10-dev_3.10.12-1~22.04.9_amd64.deb ...\nUnpacking python3.10-dev (3.10.12-1~22.04.9) over (3.10.12-1~22.04.7) ...\nPreparing to unpack .../1-libpython3.10-dev_3.10.12-1~22.04.9_amd64.deb ...\nUnpacking libpython3.10-dev:amd64 (3.10.12-1~22.04.9) over (3.10.12-1~22.04.7) ...\nPreparing to unpack .../2-libpython3.10_3.10.12-1~22.04.9_amd64.deb ...\nUnpacking libpython3.10:amd64 (3.10.12-1~22.04.9) over (3.10.12-1~22.04.7) ...\nPreparing to unpack .../3-python3.10_3.10.12-1~22.04.9_amd64.deb ...\nUnpacking python3.10 (3.10.12-1~22.04.9) over (3.10.12-1~22.04.7) ...\nPreparing to unpack .../4-libpython3.10-stdlib_3.10.12-1~22.04.9_amd64.deb ...\nUnpacking libpython3.10-stdlib:amd64 (3.10.12-1~22.04.9) over (3.10.12-1~22.04.7) ...\nPreparing to unpack .../5-python3.10-minimal_3.10.12-1~22.04.9_amd64.deb ...\nUnpacking python3.10-minimal (3.10.12-1~22.04.9) over (3.10.12-1~22.04.7) ...\nPreparing to unpack .../6-libpython3.10-minimal_3.10.12-1~22.04.9_amd64.deb ...\nUnpacking libpython3.10-minimal:amd64 (3.10.12-1~22.04.9) over (3.10.12-1~22.04.7) ...\nSetting up libpython3.10-minimal:amd64 (3.10.12-1~22.04.9) ...\nSetting up python3.10-minimal (3.10.12-1~22.04.9) ...\nSetting up libpython3.10-stdlib:amd64 (3.10.12-1~22.04.9) ...\nSetting up libpython3.10:amd64 (3.10.12-1~22.04.9) ...\nSetting up python3.10 (3.10.12-1~22.04.9) ...\nSetting up libpython3.10-dev:amd64 (3.10.12-1~22.04.9) ...\nSetting up python3.10-dev (3.10.12-1~22.04.9) ...\nProcessing triggers for man-db (2.10.2-1) ...\nProcessing triggers for libc-bin (2.35-0ubuntu3.4) ...\n/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n\nupdate-alternatives: using /usr/bin/python3.10 to provide /usr/bin/python3 (python3) in auto mode\nPython 3.10.12\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"__Importing PICRUST IN COLAB__","metadata":{"id":"VWqOI6IDD1qW"}},{"cell_type":"code","source":"'''# Install miniconda and initialize:\n!wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n!bash Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local/miniconda3\n!conda config --add channels defaults\n!conda config --add channels bioconda\n!conda config --add channels conda-forge\n# Imports for colab\nimport condacolab\nimport sys\nsys.path.append('/usr/local/miniconda3/lib/python3.7/site-packages/')\n\n# Install PICRUSt2 and its dependencies\n%conda install -c bioconda -c conda-forge picrust2=2.4.1 -y\n# Verify installations%\n%conda list | grep picrust2'''","metadata":{"id":"uKimriI3hmTq","trusted":true,"vscode":{"languageId":"javascript"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Using Pro colab","metadata":{"id":"HzaNXN9suvgG"}},{"cell_type":"code","source":"'''import sys\nprint([module for module in sys.modules if 'tensorflow' in module])'''","metadata":{"id":"qF94FGxn2iUL","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set up memory footprint support libraries\n!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n!pip install gputil\n!pip install psutil\n%pip install humanize\n%pip install memory_profiler\nimport psutil\nimport humanize\nimport os\nimport GPUtil as GPU\nGPUs = GPU.getGPUs()\nfrom psutil import virtual_memory\nram_gb = virtual_memory().total / 1e9\nprint('Runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n\nif ram_gb < 20:\n  print('Not using a high-RAM runtime')\nelse:\n  print('Using a high-RAM runtime!')","metadata":{"id":"3gWJfdx3Ni1f","outputId":"521b2c2c-a879-45b1-fcbb-9b016a0801e7","trusted":true,"execution":{"iopub.status.busy":"2025-03-30T16:40:25.524746Z","iopub.execute_input":"2025-03-30T16:40:25.525082Z","iopub.status.idle":"2025-03-30T16:40:45.816984Z","shell.execute_reply.started":"2025-03-30T16:40:25.525051Z","shell.execute_reply":"2025-03-30T16:40:45.815861Z"}},"outputs":[{"name":"stdout","text":"Collecting gputil\n  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nBuilding wheels for collected packages: gputil\n  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for gputil: filename=GPUtil-1.4.0-py3-none-any.whl size=7392 sha256=fc1c0a25f43c7653bc975e5c12a130cce804c39edfb5b883ef209ad1d71ac0f7\n  Stored in directory: /root/.cache/pip/wheels/a9/8a/bd/81082387151853ab8b6b3ef33426e98f5cbfebc3c397a9d4d0\nSuccessfully built gputil\nInstalling collected packages: gputil\nSuccessfully installed gputil-1.4.0\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (5.9.5)\nRequirement already satisfied: humanize in /usr/local/lib/python3.10/dist-packages (4.11.0)\nNote: you may need to restart the kernel to use updated packages.\nCollecting memory_profiler\n  Downloading memory_profiler-0.61.0-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from memory_profiler) (5.9.5)\nDownloading memory_profiler-0.61.0-py3-none-any.whl (31 kB)\nInstalling collected packages: memory_profiler\nSuccessfully installed memory_profiler-0.61.0\nNote: you may need to restart the kernel to use updated packages.\nRuntime has 33.7 gigabytes of available RAM\n\nUsing a high-RAM runtime!\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"### Kaggle / Colab","metadata":{"id":"hTKk2e3eYelt"}},{"cell_type":"code","source":"!python3 --version","metadata":{"id":"-hJqhsT3Mcwr","outputId":"78d0a9a3-342e-438e-e0e7-f21bb34cc46a","trusted":true,"execution":{"iopub.status.busy":"2025-03-30T16:40:45.818024Z","iopub.execute_input":"2025-03-30T16:40:45.818299Z","iopub.status.idle":"2025-03-30T16:40:45.945059Z","shell.execute_reply.started":"2025-03-30T16:40:45.818272Z","shell.execute_reply":"2025-03-30T16:40:45.944050Z"}},"outputs":[{"name":"stdout","text":"Python 3.10.12\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!pip install psutil\nimport psutil\n!pip install biopython\nfrom IPython import get_ipython\nfrom IPython.display import display\n!pip install biom-format\n%pip install umap-learn\n!pip install lxml pandas\n!pip install pyarrow\n!pip install kneed\n!pip install scipy\n%pip install \"dask[complete]\"\n!pip install fuzzywuzzy\n!pip install python-Levenshtein","metadata":{"id":"CfyrnbtLIh2D","outputId":"2f4503ab-ae09-4391-bf63-e2d826d5e9a1","trusted":true,"execution":{"iopub.status.busy":"2025-03-30T16:40:45.946158Z","iopub.execute_input":"2025-03-30T16:40:45.946467Z","iopub.status.idle":"2025-03-30T16:42:05.747481Z","shell.execute_reply.started":"2025-03-30T16:40:45.946438Z","shell.execute_reply":"2025-03-30T16:42:05.746213Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (5.9.5)\nCollecting biopython\n  Downloading biopython-1.85-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from biopython) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->biopython) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->biopython) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->biopython) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->biopython) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->biopython) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->biopython) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->biopython) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->biopython) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->biopython) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->biopython) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->biopython) (2024.2.0)\nDownloading biopython-1.85-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: biopython\nSuccessfully installed biopython-1.85\nCollecting biom-format\n  Downloading biom-format-2.1.16.tar.gz (11.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from biom-format) (8.1.7)\nRequirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.10/dist-packages (from biom-format) (1.26.4)\nRequirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from biom-format) (1.13.1)\nRequirement already satisfied: pandas>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from biom-format) (2.2.3)\nRequirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from biom-format) (3.12.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.9.2->biom-format) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.9.2->biom-format) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.9.2->biom-format) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.9.2->biom-format) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.9.2->biom-format) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.9.2->biom-format) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.20.0->biom-format) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.20.0->biom-format) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.20.0->biom-format) (2025.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=0.20.0->biom-format) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.9.2->biom-format) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.9.2->biom-format) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.9.2->biom-format) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.9.2->biom-format) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.9.2->biom-format) (2024.2.0)\nBuilding wheels for collected packages: biom-format\n  Building wheel for biom-format (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for biom-format: filename=biom_format-2.1.16-cp310-cp310-linux_x86_64.whl size=12159048 sha256=c5723a89af1fdbb1957b5dd4b6ae14f54485988cf910b5c9ab363420a9772cac\n  Stored in directory: /root/.cache/pip/wheels/8e/a9/f9/197fd5a0e5bbab5f2e03c89194f6c194bed7af5d7a8c8759f3\nSuccessfully built biom-format\nInstalling collected packages: biom-format\nSuccessfully installed biom-format-2.1.16\nCollecting umap-learn\n  Downloading umap_learn-0.5.7-py3-none-any.whl.metadata (21 kB)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.26.4)\nRequirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.13.1)\nRequirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.2.2)\nRequirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (0.60.0)\nCollecting pynndescent>=0.5 (from umap-learn)\n  Downloading pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from umap-learn) (4.67.1)\nRequirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn) (0.43.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->umap-learn) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->umap-learn) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->umap-learn) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->umap-learn) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->umap-learn) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->umap-learn) (2.4.1)\nRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from pynndescent>=0.5->umap-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22->umap-learn) (3.5.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->umap-learn) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->umap-learn) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->umap-learn) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->umap-learn) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->umap-learn) (2024.2.0)\nDownloading umap_learn-0.5.7-py3-none-any.whl (88 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.8/88.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: pynndescent, umap-learn\nSuccessfully installed pynndescent-0.5.13 umap-learn-0.5.7\nNote: you may need to restart the kernel to use updated packages.\nRequirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (5.3.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.3)\nRequirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.22.4->pandas) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.22.4->pandas) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.22.4->pandas) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.22.4->pandas) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.22.4->pandas) (2024.2.0)\nRequirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (19.0.0)\nCollecting kneed\n  Downloading kneed-0.8.5-py3-none-any.whl.metadata (5.5 kB)\nRequirement already satisfied: numpy>=1.14.2 in /usr/local/lib/python3.10/dist-packages (from kneed) (1.26.4)\nRequirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from kneed) (1.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.2->kneed) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.2->kneed) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.2->kneed) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.2->kneed) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.2->kneed) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.2->kneed) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.14.2->kneed) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.14.2->kneed) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.14.2->kneed) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.14.2->kneed) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.14.2->kneed) (2024.2.0)\nDownloading kneed-0.8.5-py3-none-any.whl (10 kB)\nInstalling collected packages: kneed\nSuccessfully installed kneed-0.8.5\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.13.1)\nRequirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from scipy) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<2.3,>=1.22.4->scipy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<2.3,>=1.22.4->scipy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<2.3,>=1.22.4->scipy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<2.3,>=1.22.4->scipy) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<2.3,>=1.22.4->scipy) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<2.3,>=1.22.4->scipy) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2.3,>=1.22.4->scipy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2.3,>=1.22.4->scipy) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<2.3,>=1.22.4->scipy) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<2.3,>=1.22.4->scipy) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<2.3,>=1.22.4->scipy) (2024.2.0)\nRequirement already satisfied: dask[complete] in /usr/local/lib/python3.10/dist-packages (2024.11.2)\nRequirement already satisfied: click>=8.1 in /usr/local/lib/python3.10/dist-packages (from dask[complete]) (8.1.7)\nRequirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from dask[complete]) (3.1.0)\nRequirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.10/dist-packages (from dask[complete]) (2024.9.0)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from dask[complete]) (24.2)\nRequirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from dask[complete]) (1.4.2)\nRequirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from dask[complete]) (6.0.2)\nRequirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from dask[complete]) (0.12.1)\nRequirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.10/dist-packages (from dask[complete]) (8.5.0)\nRequirement already satisfied: pyarrow>=14.0.1 in /usr/local/lib/python3.10/dist-packages (from dask[complete]) (19.0.0)\nCollecting lz4>=4.3.2 (from dask[complete])\n  Downloading lz4-4.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\nRequirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=4.13.0->dask[complete]) (3.21.0)\nRequirement already satisfied: locket in /usr/local/lib/python3.10/dist-packages (from partd>=1.4.0->dask[complete]) (1.0.0)\nRequirement already satisfied: bokeh>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from dask[complete]) (3.6.2)\nRequirement already satisfied: jinja2>=2.10.3 in /usr/local/lib/python3.10/dist-packages (from dask[complete]) (3.1.4)\nRequirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.10/dist-packages (from dask[complete]) (1.26.4)\nRequirement already satisfied: pandas>=2.0 in /usr/local/lib/python3.10/dist-packages (from dask[complete]) (2.2.3)\nRequirement already satisfied: dask-expr<1.2,>=1.1 in /usr/local/lib/python3.10/dist-packages (from dask[complete]) (1.1.19)\nRequirement already satisfied: distributed==2024.11.2 in /usr/local/lib/python3.10/dist-packages (from dask[complete]) (2024.11.2)\nRequirement already satisfied: msgpack>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from distributed==2024.11.2->dask[complete]) (1.1.0)\nRequirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from distributed==2024.11.2->dask[complete]) (5.9.5)\nRequirement already satisfied: sortedcontainers>=2.0.5 in /usr/local/lib/python3.10/dist-packages (from distributed==2024.11.2->dask[complete]) (2.4.0)\nRequirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from distributed==2024.11.2->dask[complete]) (3.0.0)\nRequirement already satisfied: tornado>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from distributed==2024.11.2->dask[complete]) (6.3.3)\nRequirement already satisfied: urllib3>=1.26.5 in /usr/local/lib/python3.10/dist-packages (from distributed==2024.11.2->dask[complete]) (2.3.0)\nRequirement already satisfied: zict>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from distributed==2024.11.2->dask[complete]) (3.0.0)\nRequirement already satisfied: contourpy>=1.2 in /usr/local/lib/python3.10/dist-packages (from bokeh>=3.1.0->dask[complete]) (1.3.1)\nRequirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from bokeh>=3.1.0->dask[complete]) (11.0.0)\nRequirement already satisfied: xyzservices>=2021.09.1 in /usr/local/lib/python3.10/dist-packages (from bokeh>=3.1.0->dask[complete]) (2024.9.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.10.3->dask[complete]) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->dask[complete]) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->dask[complete]) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->dask[complete]) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->dask[complete]) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->dask[complete]) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->dask[complete]) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0->dask[complete]) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0->dask[complete]) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0->dask[complete]) (2025.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0->dask[complete]) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.24->dask[complete]) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.24->dask[complete]) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.24->dask[complete]) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.24->dask[complete]) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.24->dask[complete]) (2024.2.0)\nDownloading lz4-4.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: lz4\nSuccessfully installed lz4-4.4.3\nNote: you may need to restart the kernel to use updated packages.\nRequirement already satisfied: fuzzywuzzy in /usr/local/lib/python3.10/dist-packages (0.18.0)\nCollecting python-Levenshtein\n  Downloading python_levenshtein-0.27.1-py3-none-any.whl.metadata (3.7 kB)\nCollecting Levenshtein==0.27.1 (from python-Levenshtein)\n  Downloading levenshtein-0.27.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\nCollecting rapidfuzz<4.0.0,>=3.9.0 (from Levenshtein==0.27.1->python-Levenshtein)\n  Downloading rapidfuzz-3.12.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\nDownloading python_levenshtein-0.27.1-py3-none-any.whl (9.4 kB)\nDownloading levenshtein-0.27.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.5/161.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading rapidfuzz-3.12.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein, python-Levenshtein\nSuccessfully installed Levenshtein-0.27.1 python-Levenshtein-0.27.1 rapidfuzz-3.12.2\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# 2.2. Importing Libraries,  Making Directories and Loading Data","metadata":{"id":"FiHOVZiIuvgH"}},{"cell_type":"code","source":"# Standard library imports\nimport os\nimport sys\nimport ast\nimport subprocess\nimport logging\nimport time\nfrom datetime import datetime\nimport shutil\nfrom io import StringIO\nfrom pathlib import Path\nimport re\nimport json\n# Data processing and analysis\nimport pandas as pd\nimport numpy as np\nimport openpyxl\nimport seaborn as sns\nimport networkx as nx\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport plotly.io as pio\nimport matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom matplotlib.colors import to_rgba, LinearSegmentedColormap\nimport matplotlib.patches as mpatches\n# Machine learning and statistical analysis\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\nfrom sklearn.decomposition import PCA, NMF\nfrom sklearn.cluster import AgglomerativeClustering, KMeans\nfrom sklearn.manifold import TSNE\nimport umap\nimport scipy\nfrom scipy import stats\nfrom scipy.cluster import hierarchy\nimport scipy.cluster.hierarchy as sch\nfrom statsmodels.stats.multitest import multipletests\nfrom scipy.spatial.distance import pdist\nfrom scipy.stats import spearmanr, kruskal, mannwhitneyu\nfrom kneed import KneeLocator\nfrom scipy.signal import savgol_filter\nfrom joblib import Parallel, delayed\n\n# Bioinformatics\nfrom Bio import SeqIO\nfrom Bio.Seq import Seq\nfrom Bio.SeqRecord import SeqRecord\nfrom biom import Table, load_table\nfrom biom.util import biom_open\n\n# Web and data retrieval\nimport requests\nimport xml.etree.ElementTree as ET\nfrom lxml import etree\n\n# Utility libraries\nimport gzip\nimport random\nfrom natsort import natsorted\nfrom typing import Dict, List, Tuple, Set, Optional\nimport pickle\nimport gc\nimport joblib\nimport h5py\nimport os\nos.environ['DISPLAY'] = ':0'","metadata":{"id":"eVWYFfyQIh2E","trusted":true,"execution":{"iopub.status.busy":"2025-03-30T16:42:05.750735Z","iopub.execute_input":"2025-03-30T16:42:05.751121Z","iopub.status.idle":"2025-03-30T16:42:45.460562Z","shell.execute_reply.started":"2025-03-30T16:42:05.751085Z","shell.execute_reply":"2025-03-30T16:42:45.459590Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Set up logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s'\n)\n\n# Directory Structure Definitions\nSIMPLE_BASE = {\n    'known': 'simple_known_mic',\n    'other': 'simple_candidate_mic'\n}\n\nDETAILED_BASE = {\n    'known': 'detailed_known_mic',\n    'pure_checked': 'detailed_pure_checked_mic',\n    'pure_core': 'detailed_pure_core_mic',\n    'checked_core': 'detailed_checked_core_mic'\n}\n\nSUBDIRS = [\n    'EC_predictions',\n    'pathway_predictions',\n    'KO_predictions',\n    'other_picrust_files'\n]\n\n'''\n# Base Paths\nif \"google.colab\" in sys.modules:\n    base_dir = Path(\"/content/drive/MyDrive/MIC/data_picrust\")\nelse:\n    base_dir = Path(\"/home/beatriz/MIC/2_Micro/data_picrust\")\n\n#base dir for small files to git\nbase_dir = Path(\"/home/beatriz/MIC/2_Micro/data_picrust\")\n\nabundance_excel= Path(\"/home/beatriz/MIC/2_Micro/data_Ref/merged_to_sequence.xlsx\")\nfasta_file_final = Path(\"/home/beatriz/MIC/2_Micro/data_qiime/results_match_gg/final_sequences_gg.fasta\")\naligned_fasta = Path(\"/home/beatriz/MIC/2_Micro/data_qiime/results_match_gg/aligned-dna-sequences_gg.fasta\")\n\n# Create output directory if it doesn't exist\noutput_base = base_dir / \"output_base\"\noutput_base.mkdir(parents=True, exist_ok=True)\n# large galaxies input and output #large size dir for large files hosted instead in kaggle\nlarge_dir = Path(\"/home/beatriz/MIC\")\nlarge_dir.mkdir(parents=True, exist_ok=True)\n# databases\ndb_dir = large_dir / \"Databases\"\n# input galaxies and uniprots\ninput_galaxy = large_dir / \"data_galaxies\"\n# Directory to output large files # eccontris, compilated db\noutput_large = large_dir / \"output_large\"\noutput_large.mkdir(parents=True, exist_ok=True)\n\n# for colab\n# Create output directory if it doesn't exist\nbase_dir = Path(\"/content/drive/MyDrive/MIC/data_picrust/\")\nbase_dir.mkdir(parents=True, exist_ok=True)\nabundance_excel= Path(\"/content/drive/MyDrive/MIC/data_picrust/merged_to_sequence.xlsx\")\nfasta_file_final = Path(\"/content/drive/MyDrive/MIC/data_picrust/final_sequences_gg.fasta\")\naligned_fasta = Path(\"/content/drive/MyDrive/MIC/data_picrust/aligned-dna-sequences_gg.fasta\")\n\noutput_base = base_dir  # Separate output directory\noutput_base.mkdir(parents=True, exist_ok=True)\nlarge_dir = Path(\"/content/drive/MyDrive/MIC/\")\nlarge_dir.mkdir(parents=True, exist_ok=True)\ndb_dir = Path(\"/content/drive/MyDrive/MIC/Databases\")\ndb_dir.mkdir(parents=True, exist_ok=True)\ninput_galaxy = large_dir / \"data_galaxies\"\n# Directory to output large files # eccontris, compilated db\noutput_large = large_dir / \"output_large\"\noutput_large.mkdir(parents=True, exist_ok=True)\n'''\n# For Kaggle work\n# Input datasets (read-only in Kaggle)\nbase_dir = Path(\"/kaggle/input/new-picrust\") #base dir for small files to git /kaggle/input/new-picrust\n\n# Files in small input directory\nabundance_excel= base_dir / \"merged_to_sequence.xlsx\" # inside input small sizes input\nfasta_file_final = base_dir  / \"final_sequences_gg.fasta\" # inside input small sizes\n\n# Output for small files has to be changed for vscode no to push it to git\noutput_base = Path(\"/kaggle/working/output_base\")\noutput_base.mkdir(parents=True, exist_ok=True)\n#datasets large galaxies and databases\ndb_dir = Path(\"/kaggle/input/databases/Databases\")\ninput_galaxy = Path(\"/kaggle/input/data-galaxies\")\n\n# Directory to output large files # eccontris, compilated db\nlarge_dir =  Path(\"/kaggle/working/\")\n\n# Directory to output large files # eccontris, compilated db\noutput_large = large_dir / \"output_large\"\noutput_large.mkdir(parents=True, exist_ok=True)","metadata":{"id":"Jhg73Rb9CqSF","outputId":"e37fcf65-c146-4510-9b29-f9a1fea39523","trusted":true,"execution":{"iopub.status.busy":"2025-03-30T16:42:45.462025Z","iopub.execute_input":"2025-03-30T16:42:45.462830Z","iopub.status.idle":"2025-03-30T16:42:45.471117Z","shell.execute_reply.started":"2025-03-30T16:42:45.462793Z","shell.execute_reply":"2025-03-30T16:42:45.470075Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"The fasta file come from the Alternative Sequences finding from the Greenes Genes Database, from the taxonomy in this study made in section 7 in the 5_Sequences_qiime notebook: final_sequences_gg.fasta. Abundance dataframe come from the data from notebook 4 merged_to_sequence.xlsx sheet=core_check_usual_taxa which is a unified df between 3 different groups explained previously: cora_taxa (>20% 60 abundance features), usual_taxa (17 high literature ranking bacteria influencing corrosion) and checked_taxa (30 statistically significant to the corrosion risk label) in total 85 features.","metadata":{"id":"COW1kGFZCqSG"}},{"cell_type":"code","source":"import psutil\nimport time\nimport threading\nimport pandas as pd\nimport os\n\n# Global flag to control monitoring\nmonitoring_active = False\n\ndef background_monitor(interval_seconds=30, log_file='resource_usage_log.csv'):\n    \"\"\"\n    Background thread to monitor system resources and log to CSV\n    \"\"\"\n    global monitoring_active\n    monitoring_active = True\n\n    # Initialize the CSV file with headers if it doesn't exist\n    if not os.path.exists(log_file):\n        with open(log_file, 'w') as f:\n            # Corrected: Added delimiter argument\n            f.write('timestamp,cpu_percent,ram_percent,ram_available_gb\\n')\n\n    print(f\"Starting background resource monitoring. Logging to {log_file} every {interval_seconds} seconds.\")\n    print(\"Run stop_monitoring() when you want to stop.\")\n\n    while monitoring_active:\n        # Get resource usage\n        cpu = psutil.cpu_percent()\n        ram = psutil.virtual_memory().percent\n        ram_avail = psutil.virtual_memory().available / (1024**3)\n\n        # Log to file\n        timestamp = time.strftime('%Y-%m-%d %H:%M:%S')\n        with open(log_file, 'a') as f:\n            # Corrected: Use explicit delimiter when writing\n            f.write(f'{timestamp},{cpu},{ram},{ram_avail:.2f}\\n')\n\n        time.sleep(interval_seconds)\n\ndef start_monitoring(interval_seconds=30):\n    \"\"\"\n    Start monitoring resources in the background\n    \"\"\"\n    # Start the monitoring in a background thread\n    monitor_thread = threading.Thread(\n        target=background_monitor,\n        args=(interval_seconds,),\n        daemon=True  # This ensures the thread will stop when the notebook kernel stops\n    )\n    monitor_thread.start()\n    return \"Monitoring started. Run your notebook normally. Call stop_monitoring() when done.\"\n\ndef stop_monitoring():\n    \"\"\"\n    Stop the background monitoring\n    \"\"\"\n    global monitoring_active\n    monitoring_active = False\n    time.sleep(1)  # Give a second for thread to finish\n    return \"Monitoring stopped. Resource data saved to 'resource_usage_log.csv'\"\n\ndef plot_resource_log(log_file='resource_usage_log.csv'):\n    \"\"\"\n    Plot the logged resource data\n    \"\"\"\n    import matplotlib.pyplot as plt\n    from datetime import datetime\n\n    # Read the log file\n    df = pd.read_csv(log_file)\n\n    # Convert timestamp to datetime\n    df['timestamp'] = pd.to_datetime(df['timestamp'])\n\n    # Calculate minutes from start\n    start_time = df['timestamp'].min()\n    df['minutes'] = (df['timestamp'] - start_time).dt.total_seconds() / 60\n\n    # Create plot\n    plt.figure(figsize=(12, 8))\n\n    plt.subplot(2, 1, 1)\n    plt.plot(df['minutes'], df['cpu_percent'], 'b-')\n    plt.title('CPU Usage')\n    plt.ylabel('Percent')\n    plt.ylim(0, 100)\n    plt.grid(True, alpha=0.3)\n\n    plt.subplot(2, 1, 2)\n    plt.plot(df['minutes'], df['ram_percent'], 'r-')\n    plt.title('RAM Usage')\n    plt.xlabel('Time (minutes)')\n    plt.ylabel('Percent')\n    plt.ylim(0, 100)\n    plt.grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()\n\n    # Print summary statistics\n    print(f\"Monitoring duration: {df['minutes'].max():.1f} minutes\")\n    print(f\"Average CPU: {df['cpu_percent'].mean():.1f}%\")\n    print(f\"Peak CPU: {df['cpu_percent'].max():.1f}%\")\n    print(f\"Average RAM: {df['ram_percent'].mean():.1f}%\")\n    print(f\"Peak RAM: {df['ram_percent'].max():.1f}%\")\n    print(f\"Minimum available RAM: {df['ram_available_gb'].min():.2f} GB\")","metadata":{"id":"3SvrKEB0rzas","trusted":true,"execution":{"iopub.status.busy":"2025-03-30T16:42:45.472271Z","iopub.execute_input":"2025-03-30T16:42:45.472603Z","iopub.status.idle":"2025-03-30T16:42:45.498866Z","shell.execute_reply.started":"2025-03-30T16:42:45.472563Z","shell.execute_reply":"2025-03-30T16:42:45.497883Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"start_monitoring(interval_seconds=60)  # Log every minute","metadata":{"id":"ad2wh7MYtDkt","outputId":"ea77143a-c64d-449f-f099-f8f0bb0e506c","trusted":true,"execution":{"iopub.status.busy":"2025-03-30T16:42:45.502388Z","iopub.execute_input":"2025-03-30T16:42:45.502781Z","iopub.status.idle":"2025-03-30T16:42:45.529368Z","shell.execute_reply.started":"2025-03-30T16:42:45.502744Z","shell.execute_reply":"2025-03-30T16:42:45.528224Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"'Monitoring started. Run your notebook normally. Call stop_monitoring() when done.'"},"metadata":{}},{"name":"stdout","text":"Starting background resource monitoring. Logging to resource_usage_log.csv every 60 seconds.\nRun stop_monitoring() when you want to stop.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Integrated taxa from origin genus as headers with levels 6 for the genera, 7 for the GID, muss be cleaned\nIntegrated_T = pd.read_excel(abundance_excel, sheet_name='core_check_usual_taxa', header=[0,1,2,3,4,5,6,7], engine ='openpyxl')\n# Drop first row (index 0) and first column in one chain\nIntegrated_T = Integrated_T.drop(index=0).drop(Integrated_T.columns[0], axis=1)\nIntegrated_T= Integrated_T.astype({'Sites': str})\nIntegrated_T['Sites'] = Integrated_T['Sites'].fillna('Source')\n# Remove 'Unnamed' level names\nIntegrated_T.columns = Integrated_T.columns.map(lambda x: tuple('' if 'Unnamed' in str(level) else level for level in x))\n# Changing dtypes to category whiles respecting structure\nIntegrated_T[\"Category\"] = Integrated_T[\"Category\"].astype(\"Int64\")\nIntegrated_T= Integrated_T.set_index(\"Sites\")\npre_Integrated = Integrated_T.T","metadata":{"id":"qzMCSdlPuvgI","trusted":true,"execution":{"iopub.status.busy":"2025-03-30T16:42:45.530477Z","iopub.execute_input":"2025-03-30T16:42:45.530903Z","iopub.status.idle":"2025-03-30T16:42:45.834080Z","shell.execute_reply.started":"2025-03-30T16:42:45.530861Z","shell.execute_reply":"2025-03-30T16:42:45.832959Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"Integrated_T.tail()","metadata":{"id":"aZSzaNSQuvgI","outputId":"6403157c-e9cb-4ee8-8062-d2ff308b0696","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2.3. Making Sequences for Picrust fasta file\n\nPicrust Functional Analyiss requires a biom table with otus as index, samples as headers and abundance as values. The present biom has genus names but is needs instead Otus instead. The other input file for picrust is the representative sequences table that consist of the sequences per genera followed by the frequency of that genera on the whole sample, this is done directly by the software. The fasta file requires the otus instead of the genera names and the sequences non aligned coming from notebook 5. The following scrips will formate the data to picrust.","metadata":{"id":"sdOLY5SPuvgI"}},{"cell_type":"code","source":"# Read and modify sequences\nnew_records = []\nfor record in SeqIO.parse(fasta_file_final, \"fasta\"):\n    match = re.search(r\"\\s(\\d+)\\s\", record.description)  # Look for digits surrounded by spaces\n    if match:\n        otu_id = match.group(1)\n    else:\n        print(f\"Warning: Could not extract OTU ID from description: {record.description}\")\n        continue  # Skip this record if OTU ID not found\n\n    # Create new record with only OTU as ID\n    new_record = SeqRecord(\n        record.seq,\n        id=otu_id,\n        description=\"\"  # Empty description to keep only ID\n    )\n    new_records.append(new_record)\n\n# Write modified FASTA\noutput_fasta_path = output_base / \"sequences_for_picrust.fasta\"\n\nSeqIO.write(new_records, output_fasta_path, \"fasta\")","metadata":{"id":"cizMvGCGuvgI","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2.4. Making of Dataframes for 2 Different Pipelines\nThe following script is the path to the biom file but also to the Integrate dataframe which create dataframes that discriminate its origin in order to pass then through picrust different pipelines, to know: Simple_Base that compares the known bacteria namely usual_taxa against the other features to understand their relationships on the function of their metabolism, an additional group is put forward as simply_candidate_mic which corresponds to the bacteria no previously linked to corrosion but showing an statistical significance with the risk label, those come from the checked_taxa and in this study are: genera(GID): Bulleida (154); Mycoplana (471), Oxobacter (512) and Oerskovia (). Also as showing an favor behaviour against corrosion are presented: Phenylobacterium (549), Gelria(334), Porphyrobacteria (564) and Tepidimonas (712)\nSIMPLE_BASE = {'known': 'simple_known_mic', 'other': 'simple_candidate_mic'}\nThe second pipeline comprises a more detailed separation of the bacteria and that is: The Known bacteria as previously, pure_checked corresponding to the statistical significant genera, pure_core correspondent to the core taxa on the systems and the combination of the core and checked taxa.\nDETAILED_BASE = {'known': 'detailed_known_mic','pure_checked': 'detailed_pure_checked_mic',\n    'pure_core': 'detailed_pure_core_mic', 'checked_core': 'detailed_checked_core_mic'}","metadata":{"id":"RN-pFp1cuvgI"}},{"cell_type":"markdown","source":"__Making the Integrated dataframe__\nThe original dataframe has a column for source, indicating from which df  came from (core, usual, checked), this script proceses that datadrame into individual dfs and the combined preserving the source for further analysis. The Integrated dataframe continues to be process on the next step to become the biom abundance df.","metadata":{"id":"akl8MkZjuvgI"}},{"cell_type":"code","source":"def process_integrated_data(df):\n    \"\"\"\n    Process the integrated DataFrame to create a new DataFrame with clear column names\n    and preserve all values including source information.\n\n    Parameters:\n    df (pandas.DataFrame): Input DataFrame with MultiIndex index and site columns\n\n    Returns:\n    pandas.DataFrame: Processed DataFrame with clear structure\n    \"\"\"\n\n    # Extract genera and GIDs from the index MultiIndex\n    genera = df.index.get_level_values(6)[1:]  # Skip first row\n    gids = pd.to_numeric(df.index.get_level_values(7)[1:], errors='coerce')\n\n    # Create a new DataFrame with the extracted information\n    result_df = pd.DataFrame({\n        'Genus': genera,\n        'GID': gids\n    })\n\n    # Add the site values from the original DataFrame\n    for col in df.columns:\n        result_df[col] = df.iloc[1:][col].values\n\n    # Clean up the DataFrame\n    result_df['GID'] = pd.to_numeric(result_df['GID'], errors='coerce')\n    result_df = result_df.dropna(subset=['GID'])\n    result_df['GID'] = result_df['GID'].astype(int)\n\n    return result_df\n\ndef get_taxa_groups(df):\n    \"\"\"\n    Separate the processed DataFrame into different taxa groups based on Source column\n\n    Parameters:\n    df (pandas.DataFrame): Processed DataFrame from process_integrated_data()\n\n    Returns:\n    dict: Dictionary containing DataFrames for different taxa groups\n    \"\"\"\n    # Split the data into groups based on 'Source' column patterns\n\n    # Known corrosion bacteria (any pattern with 'us')\n    known_bacteria = df[df['Source'].str.contains('us', case=False, na=False)]\n\n    # Pure checked bacteria (only 'chk' without 'core' or 'us')\n    pure_checked = df[\n        df['Source'].str.contains('chk', case=False, na=False) &\n        ~df['Source'].str.contains('core|us', case=False, na=False)\n    ]\n\n    # Pure core bacteria (only 'core' without 'chk' or 'us')\n    pure_core = df[\n        df['Source'].str.contains('core', case=False, na=False) &\n        ~df['Source'].str.contains('chk|us', case=False, na=False)\n    ]\n\n    # Checked-core bacteria (contains both 'core' and 'chk' but no 'us')\n    checked_core = df[\n        df['Source'].str.contains('chk.*core|core.*chk', case=False, na=False) &\n        ~df['Source'].str.contains('us', case=False, na=False)\n    ]\n\n    # Create groups dictionary\n    taxa_groups = {\n        'known_bacteria': known_bacteria,\n        'pure_checked': pure_checked,\n        'pure_core': pure_core,\n        'checked_core': checked_core\n    }\n\n    # Print summary statistics\n    print(\"\\nDetailed Classification Results:\")\n    print(f\"Known corrosion bacteria: {len(known_bacteria)}\")\n    print(f\"Pure checked bacteria: {len(pure_checked)}\")\n    print(f\"Pure core bacteria: {len(pure_core)}\")\n    print(f\"Checked-core bacteria: {len(checked_core)}\")\n\n    # Verify total matches expected\n    total_classified = len(known_bacteria) + len(pure_checked) + len(pure_core) + len(checked_core)\n    print(f\"\\nTotal classified taxa: {total_classified}\")\n    print(f\"Total in dataset: {len(df)}\")\n\n    return taxa_groups\n\n# Usage example:\nIntegrated = process_integrated_data(pre_Integrated)\n\n# Get the groups\ntaxa_groups = get_taxa_groups(Integrated)\n\n# Access individual groups -\nknown_bacteria = taxa_groups['known_bacteria']\npure_core = taxa_groups['pure_core']\npure_checked = taxa_groups['pure_checked']\nchecked_core = taxa_groups['checked_core']","metadata":{"id":"MWukq8fUCqSH","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2.5. Making the Abundanc Biom dataframe for Picrust\n\nThe final biom should have as index the Otus numbers no the genera names and a clean formate","metadata":{"id":"g56zbUtiuvgJ"}},{"cell_type":"code","source":"# droping source and genus and putting GID as index\npre_biom= Integrated.drop(columns=[\"Source\", \"GID\"])\npre_biom= pre_biom.set_index(\"Genus\").astype(str)\n# Ensure all data values are float\npre_biom = pre_biom.astype(float)","metadata":{"id":"Qn6xPmvfuvgJ","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"__changing genera to otus__","metadata":{"id":"mI5nSqwXuvgJ"}},{"cell_type":"code","source":"# Create genus to OTU mapping from FASTA headers\ngenus_to_otu = {}\nfor record in SeqIO.parse(fasta_file_final, \"fasta\"):\n    parts = record.description.split()\n    if len(parts) >= 3:\n        genus = parts[0]\n        otu = parts[1]  # We'll use the first OTU number\n        genus_to_otu[genus] = otu\n\n# Print a few mappings to verify\nprint(\"Sample genus to OTU mappings:\")\nfor i, (genus, otu) in enumerate(list(genus_to_otu.items())[:5]):\n    print(f\"{genus} -> {otu}\")\n\n# Replace genus with OTU in the index\npre_biom.index = pre_biom.index.map(lambda x: genus_to_otu.get(x, x))\n\n# Remove the 'Genus' name from the index\npre_biom.index.name = \"OTU\"","metadata":{"id":"amKUleGLuvgJ","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"__Calculation counts for picrust2__","metadata":{"id":"eFYJS2KuuvgJ"}},{"cell_type":"code","source":"scaling_factor = 10000\n# Multiply by scaling factor and round to nearest integer\ncount_pre_biom = np.round(pre_biom * scaling_factor).astype(int)\ncount_pre_biom","metadata":{"id":"bwLLzgWLuvgJ","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"__Creating the biom table formate__","metadata":{"id":"Gu2uuKLHuvgJ"}},{"cell_type":"code","source":"# Create BIOM table with type specification\nbiom_table = Table(data=count_pre_biom.values,\n                  observation_ids=count_pre_biom.index.astype(str),\n                  sample_ids=count_pre_biom.columns.astype(str),\n                  type=\"OTU table\",\n                  create_date=datetime.now().isoformat(),\n                  generated_by=\"BIOM-Format\",\n                  matrix_type=\"sparse\",\n                  matrix_element_type=\"float\")\n\n# Save with explicit format\noutput_path = output_base / \"count_abundance_85.biom\"\n\nwith biom_open(output_path, 'w') as f:\n    biom_table.to_hdf5(f, generated_by=\"BIOM-Format\")","metadata":{"id":"PsCMsci7w8R7","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Validate the table structure\nprint(\"\\nValidating table...\")\n!biom validate-table -i {output_path}\n#/home/beatriz/MIC/2_Micro/data_picrust/count_abundance_85.biom\n\n# Show table info\n!biom summarize-table -i {output_path}","metadata":{"id":"gV5uEFS_uvgJ","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Validating table...\n\nThe input file is a valid BIOM-formatted file.\nNum samples: 70\nNum observations: 85\nTotal count: 56747993\nTable density (fraction of non-zero values): 0.405\n\nCounts/sample summary:\n Min: 181800.000\n Max: 990578.000\n Median: 851078.500\n Mean: 810685.614\n Std. dev.: 157876.192\n Sample Metadata Categories: None provided\n Observation Metadata Categories: None provided\n\nCounts/sample detail:\nsite_69: 181800.000\nsite_67: 217903.000\nsite_70: 270600.000\nsite_26: 582999.000\nsite_21: 589725.000","metadata":{"id":"KzS6DEGb-PJG"}},{"cell_type":"markdown","source":"# 3. Making the representative sequences\n\n__Convert Abundance Biom table and the Sequences into a QIIME2 artifact__","metadata":{"id":"YmMvQVofuvgK"}},{"cell_type":"code","source":"def create_rep_seqs_with_freq(sequence_file, pre_biom_df, output_fasta):\n    \"\"\"\n    Create representative sequences with frequencies written to output\n\n    Args:\n        sequence_file: Path to FASTA file with OTU sequences\n        pre_biom_df: DataFrame with abundance data\n        output_fasta: Path to save sequences with frequencies\n    \"\"\"\n    try:\n        # Calculate total frequency for each OTU\n        total_frequencies = round(pre_biom_df.sum(axis=1), 2)\n\n        with open(output_fasta, 'w') as out:\n            for record in SeqIO.parse(sequence_file, \"fasta\"):\n                otu_id = record.id\n\n                if otu_id in total_frequencies.index:\n                    freq = total_frequencies[otu_id]\n                    sequence = str(record.seq)\n\n                    # Write sequence with frequency to FASTA\n                    out.write(f\">{otu_id} {sequence} {freq}\\n\")\n\n        # First lines of the file\n        print(\"Representative Sequences head:\")\n        with open(output_fasta, 'r') as f:\n            for i, line in enumerate(f):\n                if i < 1:  # Show first 3 sequences (header + sequence lines)\n                    print(line.strip())\n        return output_fasta\n\n    except Exception as e:\n        print(f\"Error occurred: {e}\")\n    return None\n\n\n# Representative sequences\nsequences_for_picrust = output_base / \"sequences_for_picrust.fasta\"\n\noutput_fasta = output_base / \"representative_sequences\"\n\nrepres_sequ = create_rep_seqs_with_freq(sequences_for_picrust, pre_biom, output_fasta)\nrepres_sequ","metadata":{"id":"yXcOOPN7uvgK","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"__Disclamer:__ These notebook was mean to do the analysis of the functional mechanisms of bacteria using picrust2, however the capacity of the laptop was no sufficient to run it, nor colab on public library, nor a virtual machine, that is the reason why the analysis was undertaken in the galaxy website, where the data resides.\nhttps://usegalaxy.eu/  \nusername= magicalex238","metadata":{"id":"Ku8lUve9uvgK"}},{"cell_type":"markdown","source":"## 3.1. Classifying Bacteria by their Source DataFrame\nTwo distinct classification approaches are implemented to categorize bacteria. The simple approach (get_bacteria_sources_simple) divides bacteria into known corrosion-causers (usual_taxa) and candidates (all others). The detailed approach (get_bacteria_sources_detailed) provides finer categorization by separating bacteria into known corrosion-causers, pure checked taxa, pure core taxa, and those present in both checked and core datasets. Please notice that this function uses df Integrated for source clasification and no abundance.biom which will be used for the picrust2 pipeline.","metadata":{"id":"Zv-yUkfmCqSN"}},{"cell_type":"code","source":"def get_bacteria_sources_simple(Integrated_df):\n    \"\"\"\n    Simple classification:\n    1. Known (anything with 'us')\n    2. All others (combined chk, core, chk-core)\n    \"\"\"\n    # Get genera and gids from column levels 6 and 7\n    genera = Integrated_df[\"Genus\"]\n    gids = Integrated_df[\"GID\"]\n\n    # Look for Source in the data, not index\n    sources = Integrated_df['Source'] if 'Source' in Integrated_df.columns else None\n\n    known_bacteria = {}     # usual_taxa\n    other_bacteria = {}     # everything else\n\n    sources_found = set()\n    source ={}\n    patterns = ['us', 'core-us', 'chk-us', 'chk-core-us']\n\n    for i, (genus, gid) in enumerate (zip(genera, gids)):\n        if source is not None:  # Check if source exists for this genus\n            source = str(sources.iloc[i]).strip().lower()\n            sources_found.add(source)\n\n            if source in patterns:\n                known_bacteria[genus] = int(gid) if str(gid).isdigit() else gid\n            else:\n                other_bacteria[genus] = int(gid) if str(gid).isdigit() else gid\n\n    print(\"\\nSimple Classification Results:\")\n    print(f\"Known corrosion bacteria: {len(known_bacteria)}\")\n    print(f\"Other bacteria: {len(other_bacteria)}\")\n    print(\"\\nSources found:\", sources_found)\n\n    return {\n        'known_bacteria': known_bacteria,\n        'other_bacteria': other_bacteria\n    }\n\ndef get_bacteria_sources_detailed(Integrated_df):\n    \"\"\"\n    Detailed classification with all possible combinations:\n    1. Known (usual_taxa)\n    2. Pure checked (only 'chk')\n    3. Pure core (only 'core')\n    4. Checked-core (overlap 'chk-core')\n    \"\"\"\n\n    genera = Integrated_df[\"Genus\"]\n    gids = Integrated_df[\"GID\"]\n\n    sources = Integrated_df['Source'] if 'Source' in Integrated_df.columns else None\n\n    known_bacteria = {}      # usual_taxa\n    pure_checked = {}        # only 'chk' checked_taxa\n    pure_core = {}          # only 'core' core_taxa\n    checked_core = {}       # 'chk-core' checked and core taxa\n    source ={}\n    sources_found = set()\n    patterns = ['us', 'core-us', 'chk-us', 'chk-core-us']\n\n    for i, (genus, gid) in enumerate (zip(genera, gids)):\n        if source is not None:  # Check if source exists for this genus\n            source = str(sources.iloc[i]).strip().lower()\n            sources_found.add(source)\n\n            if source in patterns:\n                known_bacteria[genus] = int(gid) if str(gid).isdigit() else gid\n                continue\n\n            # Then handle other combinations\n            if source == 'chk':\n                pure_checked[genus] = gid\n            elif source == 'core':\n                pure_core[genus] = gid\n            elif 'chk-core' in source:\n                checked_core[genus] = gid\n\n    print(\"\\nDetailed Classification Results:\")\n    print(f\"Known corrosion bacteria: {len(known_bacteria)}\")\n    print(f\"Pure checked bacteria: {len(pure_checked)}\")\n    print(f\"Pure core bacteria: {len(pure_core)}\")\n    print(f\"Checked-core bacteria: {len(checked_core)}\")\n    print(\"\\nSources found:\", sources_found)\n\n    return {\n        'known_bacteria': known_bacteria,\n        'pure_checked': pure_checked,\n        'pure_core': pure_core,\n        'checked_core': checked_core\n    }","metadata":{"id":"5bDVrPwWCqSR","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sources_simple = get_bacteria_sources_simple(Integrated)\n\nsources_detail = get_bacteria_sources_detailed(Integrated)","metadata":{"id":"LSxCpNd8Yelz","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Extracting the genus lists for each group:\nknown_bacteria_list = list(sources_detail['known_bacteria'].keys())\npure_checked_list = list(sources_detail['pure_checked'].keys())\npure_core_list = list(sources_detail['pure_core'].keys())\nchecked_core_list = list(sources_detail['checked_core'].keys())","metadata":{"id":"QtD3K9OIYelz","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The lists will be utilised later in order to groupby this list int he analysis","metadata":{"id":"ZCA_PPxRYel0"}},{"cell_type":"markdown","source":"## 3.2. Prepare picrust data and Creating Directories for PICRUSt2 Input\nThe check_missing_genera function processes the integrated data and handles data quality control. Known problematic genera (e.g., 'Clostridium_sensu_stricto_12', 'Oxalobacteraceae_unclassified') are flagged for exclusion to prevent analysis errors. The function also creates an organized directory structure as outlined in the introduction, with separate paths for different bacterial classifications (known_mic, candidate_mic, etc.) and their respective analysis outputs (EC_predictions, pathway_predictions, KO_predictions). Following function prepares the data for picrust analysis but both dataframes the abundance.biom and Integrated have some bacteria that were no sequenciated mostly cause are no known specimens. So it is necesary to do same procedure to both dfs.","metadata":{"id":"23P2k1QwCqSR"}},{"cell_type":"code","source":"def prepare_picrust_data(Integrated_df, aligned_file, function_type='simple'):\n    \"\"\"\n    Prepare data for PICRUSt analysis with choice of  function_type method\n\n    Args:\n        Integrated_df: Input DataFrame\n        aligned_file: Path to aligned sequences\n        function_type: 'simple' or 'detailed'\n    \"\"\"\n    # Get bacteria source_groups based on chosen  function_type\n    if  function_type == 'simple':\n        source_groups = get_bacteria_sources_simple(Integrated_df)\n    else:\n        source_groups= get_bacteria_sources_detailed(Integrated_df)\n\n    # Create appropriate directory structure\n    create_directory_structure(function_type)\n\n    return source_groups\n\ndef create_directory_structure(function_type='simple'):\n    \"\"\"Create directory structure for PICRUSt analysis\"\"\"\n    base_dir = Path(\"/home/beatriz/MIC/2_Micro/data_picrust\")\n    base_dir.mkdir(parents=True, exist_ok=True)\n\n    if function_type == 'simple':\n        directories = SIMPLE_BASE\n    else:\n        directories = DETAILED_BASE\n\n    # Create all required directories\n    for dir_name in directories.values():\n        for subdir in SUBDIRS:\n            (base_dir / dir_name / subdir).mkdir(parents=True, exist_ok=True)\n    logging.info(\"Directory structure created successfully\")\n\n    return True\n\n'''  except Exception as e:\n    logging.error(f\"Error creating directory structure: {str(e)}\")\n    return False'''","metadata":{"id":"bNfnbXfKCqSS","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def verify_input_files():\n    \"\"\"Verify that input files exist and are readable\"\"\"\n    missing_files = []\n\n    if not fasta_file.exists():\n        missing_files.append(str(fasta_file))\n    if not biom_table.exists():\n        missing_files.append(str(biom_table))\n\n    if missing_files:\n        logging.error(f\"Missing input files: {', '.join(missing_files)}\")\n        return False\n\n    logging.info(\"All input files found\")\n    return True","metadata":{"id":"q4v7JVU8uvgK","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4. PICRUSt Pipeline Definition\nThe pipeline processes the aligned sequence data from notebook 5 that has or not undergo cleaning of the sequences as previously done on section 2. Also processes the biom_table in order to account on this anylsis on abundance. It queries the PICRUSt database to predict potential metabolic pathways for each genus. This prediction is based on evolutionary relationships and known genomic capabilities of related organisms.","metadata":{"id":"RUwADMKZCqSS"}},{"cell_type":"code","source":"def run_picrust2_pipeline(fasta_file, biom_file, output_dir):\n    \"\"\"\n    Run the main PICRUSt2 pipeline on input sequences and BIOM table.\n\n    Args:\n        fasta_file: Path to the aligned sequences FASTA file.\n        biom_file: Path to the BIOM table (without extra columns).\n        output_dir: Directory for PICRUSt2 output.\n    \"\"\"\n    try:\n        # Run main PICRUSt2 pipeline\n        cmd = [\n            'picrust2_pipeline.py',\n            '-s', fasta_file,        # Input FASTA file with aligned sequences\n            '-i', biom_file,         # BIOM table with abundance data\n            '-o', output_dir,        # Output directory\n            '--processes', '4',      # Parallel processes\n            '--verbose',\n            '--min_align', '0.25'    # Note the split here\n        ]\n        subprocess.run(cmd, check=True)\n\n        # Add pathway descriptions if the pathway file exists\n        pathway_file = os.path.join(output_dir, 'pathways_out/path_abun_unstrat.tsv.gz')\n        if os.path.exists(pathway_file):\n            cmd_desc = [\n                'add_descriptions.py',\n                '-i', pathway_file,\n                '-m', 'PATHWAY',\n                '-o', os.path.join(output_dir, 'pathways_with_descriptions.tsv')\n            ]\n            subprocess.run(cmd_desc, check=True)\n\n        print(f\"PICRUSt2 pipeline completed successfully for {output_dir}\")\n        return True\n\n    except subprocess.CalledProcessError as e:\n        print(f\"Error running PICRUSt2: {e}\")\n        return False","metadata":{"id":"srMpS5DkCqSS","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 5. Analysis of Pathways\nThe analysis focuses on metabolic pathways known to be involved in microbially influenced corrosion, including sulfur metabolism, organic acid production, iron metabolism, and biofilm formation. These pathways were selected based on documented mechanisms of known corrosion-inducing bacteria. Separate pipeline runs for simple and detailed classifications ensure proper pathway analysis for each bacterial group.","metadata":{"id":"Rx_DyzHbCqSS"}},{"cell_type":"code","source":"def analyze_functional_profiles(picrust_output_dir, bacteria_list):\n    \"\"\"\n    Analyze functional profiles with focus on corrosion-relevant pathways\n\n    Parameters:\n    picrust_output_dir: directory containing PICRUSt2 output\n    bacteria_list: list of bacteria names to analyze\n    \"\"\"\n    # Define corrosion-relevant pathways\n    relevant_pathways = [\n        'Sulfur metabolism',\n        'Iron metabolism',\n        'Energy metabolism',\n        'Biofilm formation',\n        'Metal transport',\n        'ochre formation',\n        'iron oxide deposits',\n        'iron precipitation',\n        'rust formation',\n        'organic acid production',\n        'acetate production',\n        'lactate metabolism',\n        'formate production',\n    ]\n\n    try:\n        # Read PICRUSt2 output\n        pathway_file = os.path.join(picrust_output_dir, 'pathways_with_descriptions.tsv')\n        pathways_df = pd.read_csv(pathway_file, sep='\\t')\n\n        # Filter for relevant pathways\n        filtered_pathways = pathways_df[\n            pathways_df['description'].str.contains('|'.join(relevant_pathways),\n                                                  case=False,\n                                                  na=False)]\n\n        # Calculate pathway abundances per bacteria\n        pathway_abundances = filtered_pathways.groupby('description').sum()\n\n        # Calculate pathway similarities between bacteria\n        pathway_similarities = {}\n        for bacteria in bacteria_list:\n            if bacteria in pathways_df.columns:\n                similarities = pathways_df[bacteria].corr(pathways_df[list(bacteria_list)])\n                pathway_similarities[bacteria] = similarities\n\n        # Predict functional potential\n        functional_predictions = {}\n        for pathway in relevant_pathways:\n            pathway_presence = filtered_pathways[\n                filtered_pathways['description'].str.contains(pathway, case=False)\n            ]\n            if not pathway_presence.empty:\n                functional_predictions[pathway] = {\n                    'presence': len(pathway_presence),\n                    'mean_abundance': pathway_presence.mean().mean(),\n                    'max_abundance': pathway_presence.max().max()\n                }\n\n        # Calculate correlation scores\n        correlation_scores = {}\n        for bacteria in bacteria_list:\n            if bacteria in pathways_df.columns:\n                correlations = pathways_df[bacteria].corr(\n                    pathways_df[filtered_pathways.index]\n                )\n                correlation_scores[bacteria] = {\n                    'mean_correlation': correlations.mean(),\n                    'max_correlation': correlations.max(),\n                    'key_pathways': correlations.nlargest(5).index.tolist()\n                }\n\n        comparison_results = {\n            'pathway_similarities': pathway_similarities,\n            'functional_predictions': functional_predictions,\n            'correlation_scores': correlation_scores,\n            'pathway_abundances': pathway_abundances.to_dict()\n        }\n\n        return filtered_pathways, comparison_results\n\n    except Exception as e:\n        print(f\"Error in pathway analysis: {str(e)}\")\n        return None, None","metadata":{"id":"8eP8MAidCqSS","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5.2. Testing the pipeline","metadata":{"id":"i-EEla9jCqSS"}},{"cell_type":"code","source":"'''# ---- RUNNING THE PIPELINE ----\n\n# Set paths\nfasta_file = Path('/home/beatriz/MIC/2_Micro/data_tree/accession_sequences.fasta')\nabundance_biom_file =  Path('/home/beatriz/MIC/2_Micro/data_picrust/abundance_accession.biom')\noutput_dir = 'picrust_output'\n\n# List of bacteria to analyze\nbacteria_of_interest = ['Azospira', 'Brachybacterium', 'Bulleidia']\n\n# Run PICRUSt2\nif run_picrust2_pipeline(aligned_fasta_file,\n                         abundance_biom_file,\n                         output_dir\n                        ):\n    # Analyze functional profiles if the pipeline completes successfully\n    filtered_pathways, abundances = analyze_functional_profiles(output_dir, bacteria_of_interest)'''","metadata":{"id":"mnwckS6sCqST","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 6. Functional Analysis\n## 6.1 Running picrust full pipeline 1\nThe analysis workflow begins by categorizing bacteria into source groups using the classification functions. These categorized data are then processed through the PICRUSt pipeline to predict metabolic capabilities. The functional analysis examines pathway presence, abundance, and correlations between different bacterial groups to identify potential corrosion-related metabolic patterns.","metadata":{"id":"03KIf3UaCqST"}},{"cell_type":"code","source":"def run_functional_analysis(df, Integrated_df, aligned_file, analysis_type='simple'):\n    \"\"\"\n    Run complete functional analysis pipeline for either simple or detailed classification\n\n    Parameters:\n    df: Input DataFrame\n    aligned_file: Path to aligned sequences file\n    analysis_type: 'simple' or 'detailed'\n    \"\"\"\n    try:\n        print(f\"\\n{'='*50}\")\n        print(f\"Starting {analysis_type} classification analysis\")\n        print(f\"{'='*50}\")\n\n        # Prepare data and get source groups\n        print(\"\\nStep 1: Preparing data...\")\n\n        source_groups = prepare_picrust_data(Integrated_df, aligned_file, function_type=analysis_type)\n\n        if not source_groups:\n            raise ValueError(\"Failed to prepare data: No source groups returned\")\n\n        # Base directory for PICRUSt output\n        base_dir = Path(\"~MIC/2_Micro/data_picrust\")\n\n        results = {}\n\n        if analysis_type == 'simple':\n            # Run analysis for simple classification\n            # Known bacteria\n            known_output_dir = base_dir /SIMPLE_BASE['known']\n            success_known = run_picrust2_pipeline(aligned_file, df, str(known_output_dir))\n            if success_known:\n                results_known = analyze_functional_profiles(str(known_output_dir),\n                                                        source_groups['known_bacteria'].keys())\n\n            # Other bacteria\n            other_output_dir = base_dir / SIMPLE_BASE['other']\n            success_other = run_picrust2_pipeline(aligned_file, str(other_output_dir))\n            if success_other:\n                results_other = analyze_functional_profiles(str(other_output_dir),\n                                                        source_groups['other_bacteria'].keys())\n\n        else:\n            # Run analysis for detailed classification\n            for group, dir_name in DETAILED_BASE.items():\n\n                # Known bacteria\n                known_output_dir = base_dir / DETAILED_BASE['known']\n                success_known = run_picrust2_pipeline(aligned_file, str(known_output_dir))\n                if success_known:\n                    results_known = analyze_functional_profiles(str(known_output_dir),\n                                                            source_groups['known_bacteria'].keys())\n\n                # Pure checked bacteria\n                checked_output_dir = base_dir /  DETAILED_BASE['pure_checked']\n                success_checked = run_picrust2_pipeline(aligned_file, str(checked_output_dir))\n                if success_checked:\n                    results_checked = analyze_functional_profiles(str(checked_output_dir),\n                                                            source_groups['pure_checked'].keys())\n\n                # Pure core bacteria\n                core_output_dir = base_dir /DETAILED_BASE['pure_core']\n                success_core = run_picrust2_pipeline(aligned_file, str(core_output_dir))\n                if success_core:\n                    results_core = analyze_functional_profiles(str(core_output_dir),\n                                                            source_groups['pure_core'].keys())\n\n                # Checked-core bacteria\n                checked_core_output_dir = base_dir /DETAILED_BASE['checked_core']\n                success_checked_core = run_picrust2_pipeline(aligned_file, str(checked_core_output_dir))\n                if success_checked_core:\n                    results_checked_core = analyze_functional_profiles(str(checked_core_output_dir),\n                                                                    source_groups['checked_core'].keys())\n    except subprocess.CalledProcessError as e:\n        print(f\"Error running PICRUSt2: {e}\")\n\n        return \"Analysis completed successfully\"","metadata":{"id":"uHpbek-BCqST","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''# Run the analysis for both types\n# Simple source classification\nsimple_results = run_functional_analysis(biom_table, aligned_file, analysis_type='simple') # output_biom\n\n# Detailed source classification\ndetailed_results = run_functional_analysis(biom_table, aligned_file, analysis_type='detailed')'''","metadata":{"id":"3NyEekbBCqST","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6.2 Running picrust full pipeline 2","metadata":{"id":"X4rP1kdUCqST"}},{"cell_type":"code","source":"def run_picrust2_pipeline(fasta_file, output_dir, min_align =0.5):\n    \"\"\"\n    Run PICRUSt2 pipeline with improved error handling and path management\n\n    Args:\n        fasta_file: Path to aligned sequences fasta file (str or Path)\n        output_dir: Directory for PICRUSt2 output (str or Path)\n    \"\"\"\n    # Convert paths to strings\n    fasta_file = str(fasta_file)\n    output_dir = str(output_dir)\n\n    try:\n        # Verify picrust2 is available\n        picrust_check = subprocess.run(['which', 'picrust2_pipeline.py'],\n                                     capture_output=True,\n                                     text=True)\n        if picrust_check.returncode != 0:\n            raise RuntimeError(\"picrust2_pipeline.py not found. Please ensure PICRUSt2 is properly installed.\")\n\n        # Create output directory\n        os.makedirs(output_dir, exist_ok=True)\n\n        # Construct command as a single string\n        cmd = f\"picrust2_pipeline.py -s {fasta_file} -i {fasta_file} -o {output_dir} --processes 1 --verbose\"\n\n        # Run pipeline\n        print(f\"Running command: {cmd}\")\n        process = subprocess.run(cmd,\n                               shell=True,  # Use shell to handle command string\n                               check=True,\n                               capture_output=True,\n                               text=True)\n\n        print(\"PICRUSt2 Output:\")\n        print(process.stdout)\n\n        if process.stderr:\n            print(\"Warnings/Errors:\")\n            print(process.stderr)\n\n        # Add descriptions if pathway file exists\n        pathway_file = os.path.join(output_dir, 'pathways_out/path_abun_unstrat.tsv.gz')\n        if os.path.exists(pathway_file):\n            desc_cmd = f\"add_descriptions.py -i {pathway_file} -m PATHWAY -o {os.path.join(output_dir, 'pathways_with_descriptions.tsv')}\"\n            subprocess.run(desc_cmd, shell=True, check=True)\n\n        print(f\"PICRUSt2 pipeline completed successfully for {output_dir}\")\n        return True\n\n    except subprocess.CalledProcessError as e:\n        print(f\"Error running PICRUSt2 command: {e}\")\n        print(f\"Command output: {e.output}\")\n        return False\n    except Exception as e:\n        print(f\"Error in pipeline: {str(e)}\")\n        return False","metadata":{"id":"5A2a1CNLCqSW","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''# For original sequences\naligned_file = aligned_fasta\nsuccess = run_picrust2_pipeline(aligned_file, output_large)\n\n# For improved sequences\noptimized_file = output_large / \"picrust_optimized_sequences.fasta\")\noptimized_output = Path(\"~/MIC/2_Micro/data_picrust/optimized_results\")\nsuccess_opt = run_picrust2_pipeline(optimized_file, optimized_output)'''","metadata":{"id":"Hbhd5NNkCqSX","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 7. Findings and Analysis","metadata":{"id":"6EDOctt8uvgP"}},{"cell_type":"markdown","source":"The PICRUSt2 pipeline generated a series of interconnected files revealing the functional potential of the microbial community. These files collectively map metabolic pathways, enzymatic functions, and taxonomic relationships, providing a multi-layered view of microbial functional capabilities across samples. Detailed view of the files found in the folder ~data_picrust are located in the manuscript.","metadata":{"id":"RXu7ACOAuvgP"}},{"cell_type":"markdown","source":"Picrust_Result_SEPP and Picrust_Result_EPA contain the descriptions, pathways and abundance of the full pipeline of picrust.","metadata":{"id":"Se5YK4UfuvgP"}},{"cell_type":"code","source":"MetaCyc_EPA_path = input_galaxy / \"Galaxy19_PICRUSt2_Add_descriptions_on_data_8.tabular\"\nPicrust_Result= pd.read_csv(MetaCyc_EPA_path, sep = \"\\t\")\nPicrust_Result_EPA= pd.read_csv(MetaCyc_EPA_path, sep = \"\\t\")\nPicrust_Result_EPA.set_index(\"description\", inplace=True)\nPicrust_Result_EPA = Picrust_Result_EPA.drop(\"pathway\", axis=1)\nPicrust_Result_EPA.index.name = \"pathway\"\nMetaCyc_SEPP_path = input_galaxy / \"Galaxy35_Add_descriptions_SEPP.tabular\"\nPicrust_Result_SEPP= pd.read_csv(MetaCyc_SEPP_path, sep = \"\\t\")\nPicrust_Result_SEPP.set_index(\"description\", inplace=True)\nPicrust_Result_SEPP = Picrust_Result_SEPP.drop(\"pathway\", axis=1)\nPicrust_Result_SEPP.index.name = \"pathway\"","metadata":{"id":"BZ8njtbRuvgP","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7.1. Placement Algorithm EPA vs SEPP\nnsti_SEPP and nsti_EPA Corresponds to a sample-wide measure of how closely related the microbial taxa in that sample are to known reference genomes with two different placement algoritms.","metadata":{"id":"H39IXM7-uvgP"}},{"cell_type":"code","source":"nsti_path_EPA = Path(input_galaxy  / \"Galaxy13_EC_weighted_nsti.tabular\")\nnsti_EPA= pd.read_csv(nsti_path_EPA, sep = \"\\t\")\nnsti_path_SEPP = Path(input_galaxy  / \"Galaxy20_EC_weighted_nsti_SEPP.tabular\")\nnsti_SEPP= pd.read_csv(nsti_path_SEPP, sep = \"\\t\")","metadata":{"id":"TW5Je0oouvgP","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create the scatter plot\nplt.figure(figsize=(10, 6))\nplt.scatter(nsti_EPA['sample'], nsti_EPA['weighted_NSTI'], alpha=0.5, label= \"EPA\", color=\"blue\")\nplt.scatter(nsti_SEPP['sample'], nsti_SEPP['weighted_NSTI'], alpha=0.5, label= \"SEPP\", color=\"gray\")\n\n# Add the threshold line\nplt.axhline(y=0.15, color='black', linestyle='--', label='Threshold (0.15)')\n\n# Customize the plot\nplt.title('NSTI Values by Site')\nplt.xlabel('Site')\nplt.ylabel('NSTI Value')\nplt.legend()\n\n# Rotate x-axis labels if there are many samples\nplt.xticks(rotation=90)\n\n# Adjust layout and display the plot\nplt.tight_layout()\nplt.show()","metadata":{"id":"ESTXja6nuvgP","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Interestingly, the results are no as expected, it was though that the algorithm for placing the sequences more convenient for the present samples was SEPP because it is design specially for 16sRNA samples and diverse microbios communities, however the samples show another story. I fail to realise that the present data has been validated with the greenes genes database with the purpose of finding more compatibility with the picrust2 database, and therefore the EPA algoritm is performing much better on the all of samples using EPA placement algoritm.","metadata":{"id":"7_fb468duvgP"}},{"cell_type":"markdown","source":"## 7.2. Explore Pathway Patterns\nThe pathway analysis strategy is to do a preliminar exploration before diving into specific hypotheses about organic matter metabolism and corrosion. It was chosen to start with unbiased exploratory data analysis of the PICRUSt pathways. The aim is to let the data reveal natural patterns without preconceptions. That helps to identify unexpected relationships between pathways, providing a baseline understanding of pathway distributions and relationships. This will guide subsequent targeted analyses of corrosion-relevant pathways.\nThe following script takes multiple perspectives in order to visualise the data without bias and let it reveal itself. We do PCA for linear patterns, NMF for modular organization, UMAP for non-linear relationships and take different clustering approaches. The aim being to look for natural Patterns without predefined categories, so that strong strong correlations can be identified regardless of pathway type. It is visualised the distribution of pathway abundances, correlation structure, hierarchical relationships and non-linear patterns","metadata":{"id":"dUPl-r2huvgQ"}},{"cell_type":"markdown","source":"__Category Dict__","metadata":{"id":"gfLGAWTXuvgQ"}},{"cell_type":"code","source":"# Define category dict outside so that all charts can use same dict\n\ncategory_dict = Integrated_T.T.iloc[0, 0:-1].to_dict()\n\n# Define colors and categories\ncategory_colors = {1: '#008800',  # Dark green\n                   2: '#FF8C00',  # Dark orange\n                   3: '#FF0000'}   # Red\n\ncategories_labels = {1: 'Normal Operation',\n              2: 'Early Warning',\n              3: 'System Failure'}","metadata":{"id":"bSQwysg7uvgQ","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def explore_pathway_patterns(df):\n    \"\"\"\n    Explore pathway patterns using multiple analytical approaches\n    \"\"\"\n    # Standardize data\n    scaler = StandardScaler()\n    scaled_data = scaler.fit_transform(df)\n\n    results = {}\n\n    def plot_exploration_results(df, results, category_dict, category_colors, categories_labels):\n        \"\"\"\n        Create visualizations for the exploratory analysis with consistent category colors\n        \"\"\"\n        # 1. Distribution of pathway abundances with category colors - side by side\n        plt.figure(figsize=(12, 6))\n\n        # Create dictionary to store abundances by category\n        category_abundances = {cat_id: [] for cat_id in categories_labels.keys()}\n\n        # Group abundances by category\n        for site_col in df.columns:\n            if site_col.startswith('site_'):\n                site_num = int(site_col.split('_')[1])\n                category = category_dict.get(f'site_{site_num}', 0)\n                if category in category_abundances:\n                    category_abundances[category].extend(df[site_col].values)\n\n        # Plot distribution for each category side by side\n        for category_id in categories_labels.keys():\n            sns.histplot(data=category_abundances[category_id],\n                        bins=50,\n                        color=category_colors[category_id],\n                        label=categories_labels[category_id],\n                        alpha=0.6,\n                        multiple=\"layer\")\n\n        plt.title('Distribution of Pathway Abundances by Category')\n        plt.xlabel('Abundance')\n        plt.yscale('log')\n        plt.legend()\n        plt.tight_layout()\n        plt.show()\n\n    # 2. PCA Dimensionality Reduction\n    pca = PCA(n_components=5)\n    X_pca = pca.fit_transform(scaled_data)\n    results['pca'] = {\n        'components': X_pca,\n        'explained_variance': pca.explained_variance_ratio_,\n        'loadings': pd.DataFrame(\n            pca.components_.T,\n            index=df.columns,\n            columns=[f'PC{i+1}' for i in range(5)])}\n\n    # 3 NMF for pathway modules\n    nmf = NMF(n_components=5, init='random', random_state=0, max_iter=400)\n    W = nmf.fit_transform(df.clip(lower=0))\n    H = nmf.components_\n    results['nmf'] = {\n        'W': pd.DataFrame(W, index=df.index, columns=[f'NMF{i+1}' for i in range(5)]), # Pathway contributions\n        'H': pd.DataFrame(H, columns=df.columns, index=[f'NMF{i+1}' for i in range(5)]), # Sample patterns\n        'reconstruction_err': nmf.reconstruction_err_ }\n\n    # 4 UMAP for non-linear patterns\n    umap_reducer = umap.UMAP(random_state=0)\n    umap_result = umap_reducer.fit_transform(scaled_data)\n    results['umap'] = pd.DataFrame(umap_result, index=df.index, columns=['UMAP1', 'UMAP2'])\n\n    # 5. Multiple Clustering Approaches / Hierarchical clustering\n    linkage_matrix = hierarchy.linkage(scaled_data, method='ward')\n\n    # Try different numbers of clusters\n    cluster_results = {}\n    for n_clusters in [5, 10, 15]:\n        # Hierarchical\n        hc = AgglomerativeClustering(n_clusters=n_clusters)\n        hc_labels = hc.fit_predict(scaled_data)\n\n        # K-means\n        km = KMeans(n_clusters=n_clusters, random_state=0)\n        km_labels = km.fit_predict(scaled_data)\n\n        cluster_results[n_clusters] = {'hierarchical': pd.Series(hc_labels, index=df.index, name='cluster'),\n            'kmeans': pd.Series(km_labels, index=df.index, name='cluster')}\n\n    results['clustering'] = cluster_results\n    results['linkage'] = linkage_matrix\n\n    # 4. Correlation Analysis/Spearman correlation for non-linear relationships\n    corr_matrix = spearmanr(df.T)[0]\n    results['correlation'] = pd.DataFrame(corr_matrix, index=df.index, columns=df.index)\n\n    return results, X_pca\n\ndef plot_exploration_results(df, results, category_dict, category_colors, categories_labels):\n    \"\"\"\n    Create visualizations for the exploratory analysis with colored categories_labels\n    \"\"\"\n    # Modified PCA visualization with categories\n    plt.figure(figsize=(15, 10))\n\n    # Create subplots\n    plt.subplot(1, 2, 1)\n    plt.plot(range(1, 6), results['pca']['explained_variance'], 'bo-')\n    plt.title('PCA Explained Variance')\n    plt.xlabel('Component')\n    plt.ylabel('Explained Variance Ratio')\n\n    plt.subplot(1, 2, 2)\n\n    # Get PCA components\n    pca_data = results['pca']['components']\n\n    # Plot each category separately to create the legend\n    for category_id in categories_labels.keys():\n        # Get indices for current category\n        category_mask = [category_dict.get(f'site_{i+1}', 0) == category_id\n                        for i in range(len(pca_data))]\n\n        # Plot points for current category\n        plt.scatter(pca_data[category_mask, 0],\n                   pca_data[category_mask, 1],\n                   c=category_colors[category_id],\n                   label=categories_labels[category_id],\n                   alpha=0.6)\n\n    plt.title('PCA First Two Components')\n    plt.xlabel('PC1')\n    plt.ylabel('PC2')\n    plt.legend()\n    plt.tight_layout()\n    plt.show()\n\n\n    # 3. UMAP visualization with categories\n    plt.figure(figsize=(10, 8))\n    umap_df = results['umap']\n\n    for category_id in categories_labels.keys():\n        category_mask = [category_dict.get(f'site_{i+1}', 0) == category_id\n                        for i in range(len(umap_df))]\n        category_data = umap_df[category_mask]\n\n        plt.scatter(category_data['UMAP1'],\n                   category_data['UMAP2'],\n                   c=category_colors[category_id],\n                   label=categories_labels[category_id],\n                   alpha=0.6)\n\n    plt.title('UMAP Projection of Pathways by Category')\n    plt.xlabel('UMAP1')\n    plt.ylabel('UMAP2')\n    plt.legend()\n    plt.tight_layout()\n    plt.show()\n\n    # 4. Hierarchical clustering dendrogram - simplified version\n    plt.figure(figsize=(15, 10))\n    dendrogram = hierarchy.dendrogram(\n        results['linkage'],\n        labels=df.index,  # Use index , columns instead of index\n        leaf_rotation=90,\n        leaf_font_size=8\n    )\n    plt.title('Pathway Clustering Dendrogram')\n    plt.tight_layout()\n    plt.show()\n\n    # 5. Correlation heatmap\n    plt.figure(figsize=(12, 12))\n    mask = np.triu(np.ones_like(results['correlation']))\n\n    # Create a custom colormap that uses our category colors\n    custom_cmap = sns.diverging_palette(220, 20, as_cmap=True)\n\n    sns.heatmap(results['correlation'],\n                mask=mask,\n                cmap=custom_cmap,\n                center=0,\n                vmin=-1,\n                vmax=1)\n    plt.title('Pathway Correlation Heatmap')\n    plt.tight_layout()\n    plt.show()\n\ndef identify_key_patterns(df, results):\n    \"\"\"\n    Identify and summarize key patterns in the data\n    \"\"\"\n    patterns = {}\n\n    # Find highly correlated pathway groups\n    corr = results['correlation']\n    high_corr = pd.DataFrame(\n        [(i, j, corr.loc[i,j])\n         for i in corr.index\n         for j in corr.index\n         if i < j and abs(corr.loc[i,j]) > 0.8],\n        columns=['pathway1', 'pathway2', 'correlation']\n    ).sort_values('correlation', ascending=False)\n\n    # Find pathways with strong PCA loadings\n    loadings = results['pca']['loadings']\n    strong_loadings = pd.DataFrame({\n        'PC1_contribution': abs(loadings['PC1']),\n        'PC2_contribution': abs(loadings['PC2'])\n    }).sort_values('PC1_contribution', ascending=False)\n\n    patterns['high_correlations'] = high_corr\n    patterns['strong_loadings'] = strong_loadings\n\n    return patterns","metadata":{"id":"TvoGFdS5uvgQ","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calling the function for the pipeline using EPA algoritm\nresults_SEPP, X_pca_SEPP = explore_pathway_patterns(Picrust_Result_SEPP)\nplot_exploration_results(Picrust_Result_SEPP, results_SEPP, category_dict, category_colors, categories_labels)\npatterns_SEPP = identify_key_patterns(Picrust_Result_SEPP, results_SEPP)","metadata":{"id":"TrS9Ii_GuvgQ","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calling the function for the pipeline using EPA algoritm\nresults_EPA, X_pca_EPA = explore_pathway_patterns(Picrust_Result_EPA)\nplot_exploration_results(Picrust_Result_EPA, results_EPA, category_dict, category_colors, categories_labels)\npatterns_EPA = identify_key_patterns(Picrust_Result_EPA, results_EPA)","metadata":{"id":"EDmJPqM7uvgQ","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"__Discussing first results__\n\nThe distribution of pathway abundances shows a typical microbial community pattern with few dominant pathways, suggesting key metabolic processes are essential across samples. PCA analysis reveals that only two components explain over 80% of the variance, indicating that metabolism in these systems might be driven by two major functional groups. The UMAP visualization confirms this binary pattern through two distinct clusters, demonstrating the robustness of this separation across different dimensional reduction techniques. The hierarchical clustering dendrogram further validates this division by showing two major branches, which notably align with previously observed physicochemical patterns in our Pourbaix plot analysis. The correlation heatmap exhibits strong relationships between specific pathway groups, suggesting coordinated metabolic activities that require detailed pathway mapping for full biological interpretation. EPA sequence placement shows much better differenciation on the pc plot.","metadata":{"id":"R59lKZ60uvgQ"}},{"cell_type":"markdown","source":"## 7.3. Distribution of pathway abundances and Heatmap Hierarchies\nIn the following script we map the column pathway on the dataframe Picrust_Result_raw to the actual names provided by the Galaxy website that corresponds to the MetaCyc pathways. We will end up with the original Picrust_Results df with disernible names.After the 20 most abundant pathways will be plotted and the heatmap with the hierarchichal pathways drawn.","metadata":{"id":"RI1NL6_uuvgQ"}},{"cell_type":"code","source":"# Define category dict outside so that all charts can use same dict\n\ncategory_dict = Integrated_T.T.iloc[0, 0:-1].to_dict()\n\n# Define colors and categories\ncategory_colors = {1: '#008800',  # Dark green\n                   2: '#FF8C00',  # Dark orange\n                   3: '#FF0000'}   # Red\n\ncategories_labels = {1: 'Normal Operation',\n              2: 'Early Warning',\n              3: 'System Failure'}","metadata":{"id":"Zczh30NruvgQ","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def analyze_pathway_patterns(df, mean_abundances, category_dict, top_n=20):\n    \"\"\"\n    Create two separate visualizations for pathway analysis:\n    1. Stacked bar chart of top pathways by system state\n    2. Correlation heatmap of top pathways\n\n    Parameters:\n    df: DataFrame with pathway data\n    mean_abundances: Series with pre-calculated mean abundances\n    category_dict: Dictionary mapping sites to risk categories\n    top_n: Number of top pathways to display\n    \"\"\"\n    # Get top pathways\n    top_pathways = mean_abundances.nlargest(top_n)\n\n    # 1. Stacked Bar Chart\n    plt.figure(figsize=(15, 8))\n\n    # Prepare data for stacking\n    pathway_data = []\n    for pathway in top_pathways.index:\n        cat_means = {}\n        for cat in [1, 2, 3]:\n            cat_sites = [site for site, c in category_dict.items() if c == cat]\n            if cat_sites:\n                cat_means[cat] = df.loc[pathway, cat_sites].mean()\n            else:\n                cat_means[cat] = 0\n        pathway_data.append((pathway, cat_means))\n\n    # Create stacked bars\n    bottoms = np.zeros(len(top_pathways))\n    for cat in [1, 2, 3]:\n        values = [d[1][cat] for d in pathway_data]\n        plt.bar(range(len(top_pathways)), values, bottom=bottoms,\n                label=categories_labels[cat], color=category_colors[cat], alpha=0.7)\n        bottoms += values\n\n    plt.title('Top 20 Most Abundant Pathways by System State', fontsize=14, pad=20)\n    plt.xlabel('Pathway', fontsize=12)\n    plt.ylabel('Mean Abundance', fontsize=12)\n    plt.xticks(range(len(top_pathways)), top_pathways.index,\n               rotation=45, ha='right', fontsize=10)\n    plt.legend(title='System State', title_fontsize=12, fontsize=10)\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.show()\n\n    # 2. Correlation Heatmap (separate figure)\n    plt.figure(figsize=(15, 12))\n    top_data = df.loc[top_pathways.index]\n    corr = top_data.T.corr()\n\n    # Create mask for upper triangle\n    mask = np.triu(np.ones_like(corr), k=1)\n\n    # Create heatmap with improved readability\n    sns.heatmap(corr,\n                mask=mask,\n                cmap='coolwarm',\n                center=0,\n                annot=True,\n                fmt='.2f',\n                square=True,\n                cbar_kws={'label': 'Correlation Coefficient'},\n                annot_kws={'size': 8})\n\n    plt.title('Pathway Correlation Heatmap\\n(Top 20 Most Abundant)',\n              fontsize=14,\n              pad=20)\n    plt.xticks(rotation=45, ha='right')\n    plt.yticks(rotation=0)\n    plt.tight_layout()\n    plt.show()\n\n    return corr, top_data\n\n#Calculate mean abundances and run analysis\nmean_abundances_epa = Picrust_Result_EPA.mean(axis=1)\ncorr_epa, top_data = analyze_pathway_patterns(Picrust_Result_EPA, mean_abundances_epa, category_dict)","metadata":{"id":"praqb_f6uvgQ","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"__Discussing the 20 biggest metabolisms and their Hierarchical Heatmap__\nThe metabolic pathway analysis reveals aerobic respiration as the dominant metabolism, showing approximately 75% higher abundance than other pathways across all systems. Correlation analysis highlights strong relationships between aerobic respiration and key metabolic processes, including TCA cycles and amino acid biosynthesis pathways, particularly those involved in biofilm formation. While these patterns provide insights into the overall metabolic landscape, a more detailed analysis separating corroded and non-corroded systems, along with integration of physicochemical variables and risk labels, would be necessary for actionable conclusions about corrosion processes.","metadata":{"id":"meleGILNuvgR"}},{"cell_type":"markdown","source":"## 7.4. Distribution of Reactions abundances and Heatmap Hierarchies","metadata":{"id":"_LmHp51DYel2"}},{"cell_type":"code","source":"#parsing pathways (PWY) to the reactions (RXN), parce has a single column with 575 rows, that will mean that the patways can be more than once with different reactions\nparce_path = input_galaxy / \"Galaxy17_parsed_mapfile.tabular\"\nparce= pd.read_csv(parce_path, sep = \"\\t\")","metadata":{"id":"ISSI1EOxyxkB","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"However attemps to parse the parce df were no suscessful, therefore it was used elsewhere.","metadata":{"id":"KSBe_WevhaZy"}},{"cell_type":"code","source":"print(parce.head(20))","metadata":{"id":"65XT_sm0R0lC","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# reaction is a regroup file comprises the list of reactions in the index and the sites with abundances, similar to the pathways with abundances master file\n# whiles pathways has 366 rows (pathway), react has 2956 rows(reactions)\nreact_path = input_galaxy / \"Galaxy18_regrouped_infile.tabular\"\nreact= pd.read_csv(react_path, sep = \"\\t\")\nreact = react.set_index(\"function\")\nreact.index = react.index.astype(str)\n# Sort columns numerically\ndef sort_sites_numerically(df):\n    # Extract site numbers\n    site_numbers = [int(col.replace('site_', '')) for col in df.columns if col.startswith('site_')]\n\n    # Create sorted column list\n    sorted_cols = ['site_' + str(num) for num in sorted(site_numbers)]\n\n    # Return reordered dataframe\n    return df[sorted_cols]\nreact = sort_sites_numerically(react)","metadata":{"id":"PPFMKGrgYel2","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The parce dataframe was used for parsing the names of the function on the df reaction but the results were no human readable, so an api call was done to the rea, kegg dbs to get the names, however none of them gave results, a manual retrieval of the top 20 was done through https://gem-aureme.genouest.org/.","metadata":{"id":"kYplXnPPv-j-"}},{"cell_type":"code","source":"react['mean_abundances'] = react.mean(axis=1)\n\n# Get the top 20 most abundant functions\ntop_functions = react['mean_abundances'].nlargest(20)\ntop_functions.index = top_functions.index.astype(str)","metadata":{"id":"QDPL2I7v65sa","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"top_functions.index","metadata":{"id":"ctzF9Q1CPW6o","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"rxn_dict = {'DNA-DIRECTED-DNA-POLYMERASE-RXN': 'DNA-directed DNA Polymerase',\n 'RXN-11135' : 'DNA helicase (DNA repair), Rad3 type' ,\n 'NADH-DEHYDROG-A-RXN': 'NADH dehydrogenase' ,\n '2.7.13.3-RXN': 'histidine kinase' ,\n 'PEPTIDYLPROLYL-ISOMERASE-RXN': 'peptidylprolyl isomerase ',\n '3-OXOACYL-ACP-REDUCT-RXN': '3-oxoacyl-(acyl-carrier-protein)',\n 'RXN-10060': '3-oxocerotoyl-[acp] reductase',\n 'RXN-10655': '3-oxo-cis-Δ7-tetradecenoyl-[acp] reductase',\n 'RXN-10659': '3-oxo-cis-Δ9-hexadecenoyl-[acp] reductase',\n 'RXN-11476': '3-oxo-glutaryl-[acp] methyl ester reductase',\n 'RXN-11480': '3-oxo-pimeloyl-[acp] methyl ester reductase',\n 'RXN-13008': '3-oxo-docosapentaenoyl [acp][c] ',\n 'RXN-16616': '(5Z)-3-oxo-tetradec-5-enoyl-[acyl-carrier-protein] reductase',\n 'RXN-16622': '(7Z)-3-oxo-hexadec-7-enoyl-[acp] reductase',\n 'RXN-16626': '(9Z)-3-oxo-octadec-9-enoyl-[acp] reductase',\n 'RXN-16630': '(11Z)-3-oxo-icos-11-enoyl-[acp] reductase',\n 'RXN-9514': 'acetoacetyl-[acyl-carrier protein] reductase',\n 'RXN-9518': '3-hydroxyhexanoyl-[acyl-carrier protein] reductase',\n 'RXN-9524': '3-oxo-octanoyl-[acyl-carrier protein] reductase',\n 'RXN-9528': '3-oxo-decanoyl-[acyl-carrier protein] reductase'}\n\nclean_rxn_dict = {k.strip(): v for k, v in rxn_dict.items()}\n\nreact= react.rename(index =clean_rxn_dict)","metadata":{"id":"_tOz3yil85rb","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def analyze_reaction_patterns(top_functions, mean_abundances, category_dict):\n    \"\"\"\n    Analyzes pathway patterns for a DataFrame with 'function' as index and 'Sites' as columns.\n    This is the FINAL, CORRECTED implementation.\n    \"\"\"\n    # 1. Stacked Bar Chart\n    plt.figure(figsize=(15, 8))\n\n    function_data = []\n    for function in top_functions.index:\n        cat_means = {}\n        for cat in [1, 2, 3]:\n            cat_sites = [site for site, c in category_dict.items() if c == cat]\n            # Optimized site selection:\n            relevant_sites = list(df.columns.intersection(cat_sites)) # More efficient intersection\n            if relevant_sites:\n                cat_means[cat] = df.loc[function, relevant_sites].mean()\n            else:\n                cat_means[cat] = 0\n        function_data.append((function, cat_means))\n\n    bottoms = np.zeros(len(top_functions))\n    for cat in [1, 2, 3]:\n        values = [d[1][cat] for d in function_data]\n        plt.bar(range(len(top_functions)), values, bottom=bottoms,\n                label=categories_labels[cat], color=category_colors[cat], alpha=0.7)\n        bottoms += values\n\n    plt.title('Top 20 Most Abundant Functions by System State', fontsize=14, pad=20)\n    plt.xlabel('Function', fontsize=12)\n    plt.ylabel('Mean Abundance', fontsize=12)\n    plt.xticks(range(len(top_functions)), top_functions.index, rotation=45, ha='right', fontsize=10)\n    plt.legend(title='System State', title_fontsize=12, fontsize=10)\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.show()\n\n    # 2. Correlation Heatmap\n    plt.figure(figsize=(15, 12))\n    top_data = df.loc[top_functions.index]\n    # Convert all columns of top_data to numeric, coercing errors to NaN\n    top_data = top_data.apply(pd.to_numeric, errors='coerce')\n\n    # Drop rows with any NaN values to ensure only numeric data is used for correlation\n    top_data = top_data.dropna(axis=1, how='all')\n    # Check if there are any columns left after dropping NaNs\n    if top_data.empty:\n        print(\"Warning: DataFrame is empty after dropping NaN columns. Skipping correlation heatmap.\")\n        return None  # Or return an empty DataFrame or a placeholder\n\n    corr = top_data.T.corr()  # Transpose for function correlation\n\n    mask = np.triu(np.ones_like(corr), k=1)  # Mask for upper triangle\n    sns.heatmap(corr, mask=mask, cmap='coolwarm', center=0, annot=True, fmt='.2f',\n                square=True, cbar_kws={'label': 'Correlation Coefficient'}, annot_kws={'size': 8})\n\n    plt.title('Function Correlation Heatmap\\n(Top 20 Most Abundant)', fontsize=14, pad=20)\n    plt.xticks(rotation=45, ha='right')\n    plt.yticks(rotation=0)\n    plt.tight_layout()\n    plt.show()\n\n    return corr, top_data\n\n# Convert numeric columns to the correct data type\nfor col in react.columns[1:]:  # Exclude 'function' column\n    try:\n        react[col] = pd.to_numeric(react[col], errors='coerce') # Skip errors but convert rest\n    except ValueError:\n        print(f\"Could not convert column '{col}' to numeric. Check its contents.\")\n        # Handle the error or investigate the column for non-numeric values\n\n# Calculate the mean after type conversion\nmean_abundances_react = react.mean(axis=1, numeric_only=True) # Specify only numeric in case strings remain\ncorr_react, top_data = analyze_pathway_patterns(react, mean_abundances_react, category_dict)","metadata":{"id":"A7kaHJFkYel2","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Reactions chart confirm our label system, the clear progression of abundance across our system states (normal operation → early warning → system failure) is particularly compelling evidence that these reactions are directly involved in the corrosion process rather than just coincidental. Also the top reactions corresponds to core taxa that however are seem to be implicated on the corrosion failure. Additionally the fact that most of the reactions deal with oxo groups on an aromatic ring, further reinfor the study intuition that oxalic and acetic acid can be a good dummy compounds to represent organic matter on the TOC. These small organic acids are likely end products or intermediates of the metabolic pathways involving the 3-oxoacyl compounds so abundant in the data.","metadata":{"id":"tYrg4kbRUSll"}},{"cell_type":"markdown","source":"","metadata":{"id":"U-Xa6aVgUSGf"}},{"cell_type":"markdown","source":"# 8. Mapping the Pathways back to the Genera\n\nThe result we obtained from the picrust pipeline contain the following dataframes, here described so it would be possible to parse. Following are the files description with the shape\n| Picrust_Result | Picrust_Result | Picrust_Result | parce | parce | parce | ECcontri | ECcontri | ECcontri | ECcontri | React | React |\n|---|---|---|---|---|---|---|---|---|---|---|---|\n| pathway | description | Sites/abund | pathway | RXN | EC number | EC number | varios abundances | Sites | OTU | Sites/abund | Reactions |\n|366,72|366,72|366,72|574,1|574,1|574,1|1491288, 9|1491288, 9|1491288, 9|1491288, 9| (2955, 71)|(2955, 71)|\n","metadata":{"id":"w0Sbd6A7uvgR"}},{"cell_type":"code","source":"# ECcontri and KOcontri files contain sample, function (EC/KO number), taxon (genus/OTU ID), and abundance metrics.\nECcontri_path = input_galaxy / \"Galaxy26_contrib.tabular\"\n\n#ECcontri_path =  Path(base_dir / \"Galaxy26_contrib.tabular\") # for Kaggle\nECcontri= pd.read_csv(ECcontri_path, sep = \"\\t\")\n#KOcontri_path = Path(large_dir / \"Galaxy30-[KO_pred_metagenome_contrib].tabular\")\n#KOcontri= pd.read_csv(KOcontri_path, sep = \"\\t\")","metadata":{"id":"RGMCpOrRuvgR","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(ECcontri.head(), ECcontri.shape)","metadata":{"id":"pxTskbBWUg0Q","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 8.1. Mapping Genera to Otu","metadata":{"id":"HB57hdTiuvgR"}},{"cell_type":"code","source":"# Mapping the Genera to Otu for the Taxonomy assigment requeriment\ndef create_otu_mapping(fasta_file_final):\n    \"\"\"Creates a DataFrame mapping OTUs to genera from a FASTA file\n    Args: fasta_file (str): Path to FASTA file\n    Returns: pd.DataFrame: DataFrame with columns ['Genus', 'OTU']\n    \"\"\"\n    mapping_data = []\n\n    for record in SeqIO.parse(fasta_file_final, \"fasta\"):\n        # Split description to get genus and OTU\n        parts = record.description.split()\n        genus = parts[0]\n        otu = parts[1]  # Take first OTU number\n\n        mapping_data.append({'Genus': genus,'OTU': otu})\n\n    # Create DataFrame\n    df = pd.DataFrame(mapping_data).sort_values('Genus')\n\n    return df\n\notu_mapping = create_otu_mapping(fasta_file_final)\n# Change the name of the Otus since they using taxon\notu_mapping = otu_mapping.rename(columns={\"OTU\" : \"taxon\"})\n\notu_path = output_base / \"otu_mapping.tsv\"\notu_mapping.to_csv(otu_path, sep='\\t', index= False)","metadata":{"id":"C_7z90_iuvgR","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Data Extraction from the Parce File: The parce file, containing pathway, reaction, and EC number information, is used to extract EC numbers. These EC numbers are intended to link to the corresponding pathways.\n\nInitial Mapping Approach: Pathways (from Picrust_Result) were mapped to the parce file to ensure accurate EC–pathway links.\nReactions were similarly mapped to maintain correct RXN–pathway links.\nThese mappings were then used to update the ECcontri dataframe, which represents stratified pathway abundance contributions (including KO/EC, taxon, taxon abundance, etc.).\nUnmapped EC numbers were kept separate for review.\nRevised Mapping Strategy: Due to incomplete overlap between the EC numbers and pathways in the parce file and ECcontri, the strategy was adjusted. Instead of relying solely on the parce file, the mapping now integrates the 'description' and 'pathways' columns directly from the Picrust_Result file into ECcontri. This integration is performed by matching on the 'Site' column, rather than using the EC number from the function column.\n\nFinal Integration: The taxonomy assignment file (linking OTUs to genera) is joined with ECcontri via the taxon column. This final combined dataset (Picrust_Result joined with ECcontri) provides complete pathway descriptions and abundance data for subsequent visualizations.","metadata":{"id":"3SWsDEMsuvgR"}},{"cell_type":"markdown","source":"### Pathway Mapping Analysis\n\nThere were identified a discrepancy between EC predictions and pathway abundances. Found 61 pathways with EC number evidence that were not included in final predictions. Total number of reference pathways: 574 (from MetaCyc), total pathways in final predictions: 366, example missing pathway: PWY-6486 supported by EC:4.2.1.41\n\nImplications\nThis finding suggests that the pathway prediction pipeline might be filtering out potentially relevant pathways despite having supporting EC evidence. This could impact the biological interpretation of the functional profiles and warrants further investigation.\nSo in this study we mapped the pathways dataframe directly to the parce file and in doing so, we have also the reaction information, avoiding the discrepancy with the Picrust_Result missing pathways.","metadata":{"id":"kgN_STGJj9Qz"}},{"cell_type":"markdown","source":"## 8.2. Map Econtri to pathways","metadata":{"id":"bCs2cimyuvgS"}},{"cell_type":"markdown","source":"It is possible now directly map the description and the pathway from Picrust_Result into ECcontri because each site can have several pathways, so we reshaping the Picrust_Result to long format and so that each row corresponds to a pathway for a given site. It is no possible to do this on a go using the whole 1491288 rows on ECcontri, so it would have to be done on agreggated data, as suggested by McKinney, 2010.\nSource: McKinney, W. (2010). Data Structures for Statistical Computing in Python. Retrieved from https://pandas.pydata.org/\n","metadata":{"id":"nEkDJg5zuvgS"}},{"cell_type":"code","source":"# Reshape Picrust_Result to long format: each row now corresponds to a pathway for a given site\npicrust_long = Picrust_Result.melt(id_vars=['pathway', 'description'],\n                                   var_name='sample',\n                                   value_name='abundance')\n\n# Filter out rows where the abundance is 0\npicrust_long = picrust_long[picrust_long['abundance'] > 0]\n\n# Aggregate pathway info per site\nmapping = picrust_long.groupby('sample').agg({\n    'pathway': lambda x: list(x),\n    'description': lambda x: list(x)\n}).reset_index()\n\n# Merge the aggregated mapping with ECcontri\nECcontri_agg_site = pd.merge(ECcontri, mapping, on='sample', how='left')","metadata":{"id":"npxOZlx8uvgS","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":" # Add genus information from otu_mapping\nECcontri_agg_site['taxon'] = ECcontri_agg_site['taxon'].astype(str)\notu_mapping['taxon'] = otu_mapping['taxon'].astype(str)\n\nECcontri_otu= pd.merge(ECcontri_agg_site, otu_mapping, on='taxon', how='left', validate='m:1')\n\nunmapped = ECcontri_otu['Genus'].isna().sum()\nif unmapped > 0:\n    print(f\"Warning: {unmapped} rows could not be mapped to genera\")\n# Rename columns: here \"description\" becomes \"pathway\" and \"pathway\" becomes \"npath\"\nECcontri_otu  = ECcontri_otu.rename(columns={\"sample\":\"Sites\", \"function\": \"EC\", \"taxon\": \"OTU\", \"description\":\"pathway\", \"pathway\":\"npath\",\n                                     \"taxon_abun\": \"abund_raw\", \"taxon_function_abun\": \"abund_contri\", \"taxon_rel_abun\": \"rel_abund_raw\",\n                                       \"taxon_rel_function_abun\": \"rel_abund_contri\", \"norm_taxon_function_contrib\" :\"norm_abund_contri\", \"genome_function_count\":\"genome_EC_count\"})\n# Organize columns in logical groups\ncols_order = ['Sites', 'Genus', 'OTU', 'EC', # Identification columns\n              'npath', 'pathway', # Pathway information\n              'abund_raw', 'rel_abund_raw', # Raw abundance metrics\n              'genome_EC_count', 'abund_contri', 'rel_abund_contri', 'norm_abund_contri'] # Contribution metrics\n# Reorder columns, takes like 4 minutes on this slow laptop\nECcontri_otu = ECcontri_otu[cols_order]","metadata":{"id":"5EXHu-QeuvgS","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 8.3 Critical comparison of the abundances on ECcontri_otu\nabund_contri: is the absolute contribution of a specific genus to a specific function   \nrel_abund_contri is the relative contribution (percentage)   \nnorm_abund_contri is the normalized contribution   ","metadata":{"id":"2eC_YFQdY1Yj"}},{"cell_type":"code","source":"fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n\n# Plot 1: rel_abund_contri vs. abund_contri\nsns.scatterplot(data=ECcontri_otu, x='abund_contri', y='rel_abund_contri', ax=axes[0])\naxes[0].set_title(\"Rel vs Absolute Abundance Contribution\")\naxes[0].set_xlabel(\"Absolute Abundance Contribution\")\naxes[0].set_ylabel(\"Relative Abundance Contribution\")\n\n# Plot 2: norm_abund_contri vs. abund_contri\nsns.scatterplot(data=ECcontri_otu, x='abund_contri', y='norm_abund_contri', ax=axes[1])\naxes[1].set_title(\"Norm vs Absolute Abundance Contribution\")\naxes[1].set_xlabel(\"Absolute Abundance Contribution\")\naxes[1].set_ylabel(\"Normalized Abundance Contribution\")\n\n# Plot 3: rel_abund_contri vs. norm_abund_contri\nsns.scatterplot(data=ECcontri_otu, x='rel_abund_contri', y='norm_abund_contri', ax=axes[2])\naxes[2].set_title(\"Rel vs Norm Abundance Contribution\")\naxes[2].set_xlabel(\"Relative Abundance Contribution\")\naxes[2].set_ylabel(\"Normalized Abundance Contribution\")\n\nplt.tight_layout()\nplt.show()","metadata":{"id":"RaauSfmoP0tb","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"From the three abundances it is seen that the absolute abundance is less granular as the relative and normalised abundances. The absolute abundance tends to lead to higher relative abundance with a strong positive correlation. Seems that it aggreegates some data and there are fewer values. The Normallise Abundance removes some of the direct correlation between absolute and relative abundance and highlights differences between individual bacteria or proteins. This alignes with ther biology that same EC is present in many bacteria with varying expression levels of proteins. Since normalisation removes sample size effects, it reveals true variations across bacteria, since protein expression is highly variable within ECs, the normalise abundance appears to be the most suitable for comparisons moving forward.","metadata":{"id":"O2P3cmYSbuNs"}},{"cell_type":"code","source":"# Count unique proteins per EC\nEC_counts = ECcontri_otu.groupby(\"EC\")[\"OTU\"].nunique()\nprint(f\"Unique proteins per EC\", EC_counts.describe())  # Check expected range","metadata":{"id":"xQOtjMOhZQzj","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"ECcontri_otu is a comprehensive dataframe that combines site locations, taxonomic information (genera and OTUs), enzyme classifications (ECs), and pathways (code for pathway (npath) and description (pathway)). The associated abundance metrics belong to the original ECcontri. The abundance metrics include:\nabund_raw: The original count of each organism (OTU) at each site\nrel_abund_raw: The relative abundance of each organism at each site, expressed as a proportion of total counts\ngenome_function_count represents the predicted number of copies of a particular EC number (enzyme) in an organism's genome. This prediction comes from PICRUSt's hidden-state prediction process, which infers gene family abundances for each organism based on its phylogenetic placement relative to reference genomes\nabund_contri: The contribution of each organism to a specific enzyme function, calculated by multiplying the raw abundance by the number of copies of that enzyme in the organism's genome\nrel_abund_contri: The relative contribution of each organism to the enzyme function, accounting for both abundance and genome copy number\nnorm_abund_contri: The normalized contribution metric that allows comparison across different sites and functions\n\n## 8.4. Statistical Analysis of the Genome Function Count","metadata":{"id":"MPaVL4T0uvgS"}},{"cell_type":"code","source":"# Analyze genome_function_count\nprint(\"Genome function count statistics:\")\nprint(\"\\nOverall statistics:\")\nprint(ECcontri_otu['genome_EC_count'].describe())\n\n# Look at distribution by EC number\nprint(\"\\nExample EC numbers and their genome counts:\")\nec_counts = ECcontri_otu.groupby('EC')['genome_EC_count'].agg(['unique', 'mean', 'max']).head()\nprint(ec_counts)\n\n# Check if genome_function_count is consistent for each OTU-EC pair\nprint(\"\\nCheck if genome_EC_count is consistent for OTU-EC combinations:\")\nconsistency_check = ECcontri_otu.groupby(['OTU', 'EC'])['genome_EC_count'].nunique()\ninconsistent = consistency_check[consistency_check > 1]\nif len(inconsistent) > 0:\n    print(f\"Found {len(inconsistent)} OTU-EC pairs with inconsistent genome counts\")\nelse:\n    print(\"Genome counts are consistent for all OTU-EC pairs\")\n\n# Explain the metrics in the dataframe\nprint(\"\\nDataframe Components:\")\nprint(\"1. Abundance Metrics:\")\nprint(\"   - abund_raw: Raw abundance of each organism in each site\")\nprint(\"   - abund_contri: Organism's abundance contribution to function/pathway\")\nprint(\"   - rel_abund_raw: Original relative abundance\")\nprint(\"   - rel_abund_contri: Relative abundance contribution to pathway\")\nprint(\"   - norm_abund_contri: Normalized abundance contribution\")\nprint(\"\\n2. Genome Function Count:\")\nprint(\"   Number of copies of each EC (enzyme) in organism's genome\")","metadata":{"id":"xeKQezwyuvgS","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Genome function count statistics:\n\nOverall statistics:\ncount    1.491288e+06\nmean     1.390277e+00\nstd      1.071974e+00\nmin      1.000000e+00\n25%      1.000000e+00\n50%      1.000000e+00\n75%      1.000000e+00\nmax      1.000000e+01\nName: genome_EC_count, dtype: float64\n\nExample EC numbers and their genome counts:\n                                    unique      mean  max\nEC                                                       \nEC:1.1.1.1              [3, 2, 1, 5, 4, 8]  2.310375    8\nEC:1.1.1.100  [8, 5, 2, 3, 4, 9, 10, 6, 1]  4.237317   10\nEC:1.1.1.102                           [1]  1.000000    1\nEC:1.1.1.103                           [1]  1.000000    1\nEC:1.1.1.105                           [1]  1.000000    1\n\nCheck if genome_EC_count is consistent for OTU-EC combinations:\nGenome counts are consistent for all OTU-EC pairs\n\nDataframe Components:\n1. Abundance Metrics:\n   - abund_raw: Raw abundance of each organism in each site\n   - abund_contri: Organism's abundance contribution to function/pathway\n   - rel_abund_raw: Original relative abundance\n   - rel_abund_contri: Relative abundance contribution to pathway\n   - norm_abund_contri: Normalized abundance contribution\n\n2. Genome Function Count:\n   Number of copies of each EC (enzyme) in organism's genome","metadata":{"id":"xpjxR3vk-PJK"}},{"cell_type":"markdown","source":"Analysis of genome_function_count(genome_EC_count) shows that most organisms typically have just one copy of any given enzyme (EC number) in their genome, with 75% of all cases showing a single copy. However, there is notable variation, with some organisms having up to 10 copies of certain enzymes. The average across all cases is 1.4 copies per enzyme per organism.\nSome enzymes show more variation than others. For example:\n\nEC:1.1.1.1 varies from 1 to 8 copies across different organisms\nEC:1.1.1.100 shows the widest range, from 1 to 10 copies\nMany enzymes (like EC:1.1.1.102, 103, 105) consistently appear as single copies\n\nImportantly, the copy number is consistent for each organism-enzyme combination across all sites, indicating this is a stable genomic characteristic.\n____________________________________________","metadata":{"id":"MA-sQJpQuvgS"}},{"cell_type":"markdown","source":"\n\nNow ECcontri_otu has several rows and columns providing information of the EC contribution to the metrics to each enzime aka EC number to the sites, genera combination, however the pathways are from origin link to most of the sites. This is perhaps because the methos infwee dunxriona bAWS ON XOMON sets of reference genomes.  Then, same environment in this case heating and cooling water systems poses similar organisms with similar pathways, the difference being on the abundance. So in order for this data to be usable, it is necesary to parse the EC into human readable information from a external enzyme databases to retrieve functional information about an EC number. Common resources include:\n\nUniProt: query UniProt’s REST API to get enzyme details by searching with the EC number.\nExPASy Enzyme Database: Provides enzyme information based on EC numbers.\nBRENDA: A comprehensive enzyme database that can be queried either via its web interface or programmatically (e.g., using the bioservices Python package). Following script creates an EnzymeRetriever class that handles API requests to UniProt, processes unique EC numbers to avoid duplicate requests\nAdds protein names, functions, and UniProt IDs to ECcontri_otu df and includes rate limiting to avoid API restrictions.\nThe retrieval was done localy using vscode and via colab because the retrieval require a superior ram and cpu, it took batches spread on several days.\n\n## 8.5. Retrieval of protein names from Uniprot though Api call","metadata":{"id":"voefPo-VuvgS"}},{"cell_type":"code","source":"class ColabEnzymeRetriever:\n    def __init__(self, batch_size=100, save_every=5):\n        self.uniprot_api = \"https://rest.uniprot.org/uniprotkb/search\"\n        self.batch_size = batch_size\n        self.save_every = save_every\n        self.results_file = Path('uniprot_results.tsv')\n        self.state_file = Path('retrieval_state.json')\n        self.processed_pairs: Set[Tuple[str, str]] = set()\n        self.existing_results = None\n        self.logger = logging.getLogger(__name__)\n        self.logger.setLevel(logging.INFO)\n\n        if not self.logger.handlers:\n            handler = logging.StreamHandler()\n            formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n            handler.setFormatter(formatter)\n            self.logger.addHandler(handler)\n\n    def load_existing_results(self, file_path: Path) -> pd.DataFrame:\n        \"\"\"Load and validate existing results\"\"\"\n        if file_path.exists():\n            try:\n                self.existing_results = pd.read_csv(file_path, sep='\\t')\n                self.logger.info(f\"Loaded {len(self.existing_results)} existing results\")\n\n                # Build set of processed pairs\n                self.processed_pairs = set()\n                for _, row in self.existing_results.iterrows():\n                    if pd.notna(row['ec_number']) and pd.notna(row['organism']):\n                        ec_num = str(row['ec_number']).strip()\n                        org = str(row['organism']).split()[0].strip()\n                        self.processed_pairs.add((ec_num, org))\n\n                return self.existing_results\n            except Exception as e:\n                self.logger.error(f\"Error loading results file: {e}\")\n                self.existing_results = pd.DataFrame(\n                    columns=['uniprot_id', 'ec_number', 'protein_name', 'organism', 'score']\n                )\n                return self.existing_results\n\n        self.existing_results = pd.DataFrame(\n            columns=['uniprot_id', 'ec_number', 'protein_name', 'organism', 'score']\n        )\n        return self.existing_results\n\n    def get_uniprot_info(self, ec: str, organism: str) -> Optional[dict]:\n        \"\"\"Get UniProt information for a specific EC-organism pair\"\"\"\n        if (ec, organism) in self.processed_pairs:\n            return None\n\n        query = f'({ec}) AND (organism_name:\"{organism}*\")'\n        params = {\n            'query': query,\n            'format': 'tsv',\n            'fields': 'id,ec,protein_name,organism_name',\n            'size': 10\n        }\n\n        max_retries = 3\n        for attempt in range(max_retries):\n            try:\n                response = requests.get(self.uniprot_api, params=params)\n                response.raise_for_status()\n                time.sleep(0.5)\n\n                lines = response.text.strip().split('\\n')\n                if len(lines) < 2:\n                    return None\n\n                best_match = None\n                best_score = -float('inf')\n\n                for line in lines[1:]:\n                    parts = line.split('\\t')\n                    if len(parts) < 4:\n                        continue\n\n                    uniprot_id, ec_numbers, protein_name, organism_name = parts\n\n                score = 0\n                if organism_name and isinstance(organism_name, str):\n                    name_parts = organism_name.split()\n                    genus = name_parts[0] if name_parts else \"\"\n\n                    # Exact genus match gets highest score\n                    if genus.lower() == organism.lower():\n                        score += 500\n                        # Prefer entries with just the genus name\n                        if len(name_parts) == 1:\n                            score += 300\n                        # Heavily penalize strain designations or subspecies\n                        elif len(name_parts) > 2 or any(char.isdigit() for char in organism_name):\n                            score -= 400\n\n                    if score > -float('inf'):\n                        if ec.replace('EC:', '') in ec_numbers.split('; '):\n                            score += 150\n\n                            if score > best_score:\n                                best_score = score\n                                best_match = {\n                                    'uniprot_id': uniprot_id,\n                                    'ec_number': ec,\n                                    'protein_name': protein_name,\n                                    'organism': organism_name,\n                                    'score': score\n                                }\n\n                return best_match if best_match else None\n\n            except requests.exceptions.RequestException as e:\n                if attempt < max_retries - 1:\n                    time.sleep(2 ** attempt)\n                    continue\n                self.logger.error(f\"Error fetching data from UniProt: {e}\")\n                return None\n\n    def process_remaining_pairs(self, unique_pairs: pd.DataFrame, start_ec: str) -> pd.DataFrame:\n        \"\"\"Process remaining pairs with enforced starting point\"\"\"\n        # Ensure EC format consistency\n        if not start_ec.startswith('EC:'):\n            start_ec = f\"EC:{start_ec.replace('EC:', '')}\"\n\n        # Sort and filter pairs\n        unique_pairs = unique_pairs.sort_values(['EC', 'Genus']).reset_index(drop=True)\n        unique_pairs = unique_pairs[unique_pairs['EC'] >= start_ec].reset_index(drop=True)\n\n        if len(unique_pairs) == 0:\n            self.logger.warning(f\"No EC numbers found after {start_ec}\")\n            return self.existing_results\n\n        self.logger.info(f\"Starting processing from {unique_pairs.iloc[0]['EC']}\")\n        total_pairs = len(unique_pairs)\n\n        results = []\n        for idx in range(0, total_pairs, self.batch_size):\n            batch = unique_pairs.iloc[idx:idx + self.batch_size]\n            batch_results = []\n\n            self.logger.info(f\"\\nProcessing batch {idx//self.batch_size + 1} of {total_pairs//self.batch_size + 1}\")\n            self.logger.info(f\"Progress: {idx}/{total_pairs} pairs ({(idx/total_pairs)*100:.1f}%)\")\n\n            current_ec = None\n            for _, row in batch.iterrows():\n                if current_ec != row['EC']:\n                    current_ec = row['EC']\n                    self.logger.info(f\"\\nProcessing EC number: {current_ec}\")\n\n                if (row['EC'], row['Genus']) not in self.processed_pairs:\n                    result = self.get_uniprot_info(row['EC'], row['Genus'])\n                    if result:\n                        batch_results.append(result)\n                        self.processed_pairs.add((row['EC'], row['Genus']))\n\n            if batch_results:\n                results.extend(batch_results)\n\n                # Save progress periodically\n                if (idx//self.batch_size) % self.save_every == 0:\n                    combined_results = pd.concat(\n                        [self.existing_results, pd.DataFrame(results)],\n                        ignore_index=True\n                    )\n                    combined_results.to_csv(self.results_file, sep='\\t', index=False)\n                    self.logger.info(f\"Saved {len(combined_results)} total results to file\")\n\n        # Final save\n        final_results = pd.concat(\n            [self.existing_results, pd.DataFrame(results)],\n            ignore_index=True\n        )\n        final_results.to_csv(self.results_file, sep='\\t', index=False)\n\n        return final_results\n\ndef continue_enzyme_retrieval(unique_pairs: pd.DataFrame, existing_results_file: Path, start_ec: str):\n    \"\"\"Main function to continue enzyme data retrieval\"\"\"\n    retriever = ColabEnzymeRetriever(batch_size=100)\n\n    # Load existing results\n    retriever.load_existing_results(existing_results_file)\n\n    # Ensure input data is properly formatted\n    if isinstance(unique_pairs, str):\n        unique_pairs = pd.read_csv(unique_pairs, sep='\\t')\n    elif isinstance(unique_pairs, pd.DataFrame):\n        unique_pairs = unique_pairs.copy(deep=False)\n    else:\n        raise ValueError(\"Input must be either a file path or a pandas DataFrame\")\n\n    # Validate and prepare input data\n    required_columns = ['EC', 'Genus']\n    if not all(col in unique_pairs.columns for col in required_columns):\n        raise ValueError(f\"Input data must contain columns: {required_columns}\")\n\n    unique_pairs['EC'] = unique_pairs['EC'].astype(str).apply(lambda x: f\"EC:{x.replace('EC:', '')}\")\n    unique_pairs['Genus'] = unique_pairs['Genus'].astype(str).str.strip()\n    unique_pairs = unique_pairs[['EC', 'Genus']].drop_duplicates()\n\n    # Process remaining pairs\n    results_df = retriever.process_remaining_pairs(unique_pairs, start_ec)\n\n    # Save final results\n    final_path = Path('uniprot_results_final.tsv')\n    results_df.to_csv(final_path, sep='\\t', index=False)\n\n    return results_df","metadata":{"id":"hTO90O2duvgT","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''uniprot_results_path = Path(base_dir '/uniprot_results.tsv')\n# Usage (after uploading files to Colab), ECcontri_otu was made in colab because it was too big to upload after transformed\nresults = continue_enzyme_retrieval(ECcontri_otu, uniprot_results_path, start_ec=\"x.3.1.12\" )'''","metadata":{"id":"mowtkQwJuvgT","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 8.6. Cleaning and Preparing Retrieved Data to integrate to ECContri","metadata":{"id":"nARoxMBKuvgT"}},{"cell_type":"code","source":"df_1_path = Path(base_dir / \"uniprot_1_4_sorted.tsv\") # First file retrieved on first run 4 am\ndf_2_path = Path(base_dir / \"uniprot_2_1.38_sorted.tsv\") # Same file retrieven when corrupted around 1:38 following day\ndf_3_path = Path(base_dir / \"uniprot_3_sorted.tsv\") # Rerun done trying to get following EC numbers\ndf_4_path = Path(base_dir / \"uniprot_4_missing_sorted.tsv\") # Final run in missing data\n\ndf_1 = pd.read_csv(df_1_path, sep='\\t')\ndf_2 = pd.read_csv(df_2_path, sep='\\t')\ndf_3 = pd.read_csv(df_3_path, sep='\\t')\ndf_4 = pd.read_csv(df_4_path, sep='\\t')\nprint(len(df_1), len(df_2), len(df_3), len(df_4))","metadata":{"id":"9r_F4kMtuvgU","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# extract EC and Genus from the Retrieved files, so I need to join them first\nretrieved = pd.concat([df_1, df_2, df_3, df_4], axis = 0)\n# unique pairs on our data\nunique_pairs = ECcontri_otu[['EC', 'Genus']].drop_duplicates()","metadata":{"id":"9a2RbXR1uvgU","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 8.7 Extracting the Genus from the retrieved_pairs","metadata":{"id":"DwbF_HFmuvgU"}},{"cell_type":"code","source":"# Function to extract the Genus from the organism str\ndef extract_genus(organism_str):\n    # Assumes Genus is the first word that starts with an uppercase letter.\n    match = re.search(r'([A-Z][a-z]+)', organism_str)\n    return match.group(1) if match else None\n# Creating a Genus column in the retrieved dataframe.\nretrieved['Genus'] = retrieved['organism'].astype(str).apply(extract_genus)\n\n# if there are duplicates, we want the best entry based on score:\nretrieved_unique = retrieved.sort_values('score', ascending=False)\\\n                            .drop_duplicates(subset=['ec_number', 'Genus'])","metadata":{"id":"jfKMErDWuvgU","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"unique_pairs Galaxy data:{len(unique_pairs)}, Uniprot retrieved data:{len(retrieved_unique)}\")","metadata":{"id":"MZiDFgoGgmmJ","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Merging using a left join on the two keys (EC_number and Genus). Plus an indicator of missing data.\nECcontri_Uniprot  = pd.merge(\n    ECcontri_otu,\n    retrieved_unique[['ec_number', 'Genus', 'protein_name', 'score', 'uniprot_id']],\n    left_on=['EC', 'Genus'],\n    right_on=['ec_number', 'Genus'],\n    how='left',\n    suffixes=('', '_retr')\n)\nprint(ECcontri_Uniprot.shape) # Very slow 1 minute, can kill the kernel","metadata":{"id":"yoQasxuauvgU","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ECcontri_Uniprot = ECcontri_Uniprot.drop(columns = [\"OTU\",\t\"ec_number\",\t\"npath\", \"pathway\",\t\"score\"])","metadata":{"id":"elCz1V27O-E3","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 8.8. Missing Values\nECcontri_uniprot_info is the final df mixed and is keep for reference only purposes. With the missing unique df I will retrive again the rest of the missing values","metadata":{"id":"qA_nz2qFuvgU"}},{"cell_type":"code","source":"#Rows with no match from retrieved_unique will have '_merge' value of 'left_only'\nmerged_unique = pd.merge(\n    unique_pairs,\n    retrieved_unique,\n    left_on=['EC', 'Genus'],\n    right_on=['ec_number', 'Genus'],\n    how='left',\n    indicator=True\n)\n\n# Filter unique pairs missing from retrieved data\nECcontri_missing = merged_unique[merged_unique['_merge'] == 'left_only']\nprint(\"Missing unique pairs count:\", ECcontri_missing.shape[0])\nECcontri_missing = ECcontri_missing[[\"EC\", \"Genus\"]]\nfile_path = os.path.join(output_base, \"ECcontri_missing.tsv\")\nECcontri_missing.to_csv(file_path, sep='\\t', index=False)","metadata":{"id":"q6iKlSuHuvgU","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 8.9 Cleaning Protein Names on ECcontri_Uniprot","metadata":{"id":"9lMQJ2mM8ZGp"}},{"cell_type":"code","source":"from fuzzywuzzy import fuzz\n\ndef enhanced_clean_protein_name(name):\n    \"\"\"\n    Enhanced protein name cleaning that builds on the original function\n    with additional normalization steps:\n\n    1. Handles EC numbers appropriately\n    2. Removes parenthetical content and brackets\n    3. Removes duplicated terms and standardizes spacing\n    4. Handles specific protein families consistently\n    5. Standardizes capitalizations and hyphenations\n    6. Removes common suffixes that don't change the protein identity\n    7. Normalizes common protein name patterns\n    \"\"\"\n    if pd.isna(name):\n        return \"Uncharacterized protein\"\n\n    # Save EC numbers for later reattachment if needed\n    ec_match = re.search(r'(EC\\s*[\\d\\.]+)', name)\n    ec_number = ec_match.group(1) if ec_match else None\n\n    # If the name is just an EC number in any format, return it\n    if re.match(r'^[\\s\\(\\)]*EC\\s*[\\d\\.]+[\\s\\(\\)]*$', name):\n        return name.strip()\n\n    # Remove EC numbers and content in parentheses\n    name = re.sub(r'\\(EC\\s*[\\d\\.]+\\)', '', name)\n    name = re.sub(r'\\([^)]*\\)', '', name)\n    name = re.sub(r'\\[[^\\]]*\\]', '', name)  # Also remove content in square brackets\n\n    # Convert to lowercase for better matching\n    name = name.lower()\n\n    # Standardize spacing around hyphens, slashes\n    name = re.sub(r'[\\s]*[\\-\\/][\\s]*', '-', name)\n\n    # Remove common suffixes that don't change protein identity\n    name = re.sub(r'\\s+(protein|domain|enzyme|family|subunit|chain|component|type|complex|fragment|precursor)$', '', name)\n\n    # Remove specific protein ID suffixes (like FabG)\n    name = re.sub(r'\\s+[a-z]+\\d+$', '', name)  # e.g., \"reductase FabG\" -> \"reductase\"\n\n    # Normalize common protein name patterns\n    replacements = {\n        # Standardize dehydrogenases\n        r'(\\w+)\\s*dehydrogenase': r'\\1-dehydrogenase',\n\n        # Standardize reductases\n        r'(\\w+)\\s*reductase': r'\\1-reductase',\n        r'\\[acyl-carrier-protein\\]': 'acp',\n        r'\\[acyl carrier protein\\]': 'acp',\n        r'3-oxoacyl-acp reductase': '3-oxoacyl-acp-reductase',\n\n        # Standardize synthetases\n        r'(\\w+)\\s*synthase': r'\\1-synthase',\n        r'(\\w+)\\s*synthetase': r'\\1-synthetase',\n\n        # Standardize common protein terms\n        r'alcohol dehydrogenase': 'alcohol-dehydrogenase',\n        r'glutathione dehydrogenase': 'glutathione-dehydrogenase',\n        r's-glutathione dehydrogenase': 'glutathione-dehydrogenase',\n        r'threonine dehydrogenase': 'threonine-dehydrogenase',\n        r'l-threonine dehydrogenase': 'threonine-dehydrogenase'\n    }\n\n    for pattern, replacement in replacements.items():\n        name = re.sub(pattern, replacement, name)\n\n    # Split into words and remove duplicates while preserving order\n    words = name.split()\n    seen = set()\n    unique_words = []\n    for word in words:\n        if word not in seen:\n            seen.add(word)\n            unique_words.append(word)\n\n    # Rejoin words\n    name = ' '.join(unique_words)\n\n    # Remove specific redundant patterns\n    redundant_patterns = [\n        (r'enzyme\\s+enzyme', 'enzyme'),\n        (r'synthase\\s+synthase', 'synthase'),\n        (r'transferase\\s+transferase', 'transferase'),\n        (r'-glucan\\s+glucan', 'glucan'),\n        (r'protein\\s+protein', 'protein'),\n        (r'reductase\\s+reductase', 'reductase'),\n        (r'dehydrogenase\\s+dehydrogenase', 'dehydrogenase')\n    ]\n\n    for pattern, replacement in redundant_patterns:\n        name = re.sub(pattern, replacement, name, flags=re.IGNORECASE)\n\n    # Reattach EC number if it was the main identifier\n    if ec_number and len(name.strip()) < 5:  # If the remaining name is very short\n        name = f\"{name} {ec_number}\" if name.strip() else ec_number\n\n    return name.strip()\n\ndef normalize_dataset(df, name_col='protein_name', sample_size=20):\n    \"\"\"\n    Apply enhanced protein name cleaning to a dataset and\n    show before/after examples\n    \"\"\"\n    # Create a copy with normalized names\n    normalized_df = df.copy()\n    normalized_df['normalized_protein'] = normalized_df[name_col].apply(enhanced_clean_protein_name)\n\n    # Display samples of the normalization\n    samples = df[[name_col]].drop_duplicates().sample(min(sample_size, df[name_col].nunique()))\n\n    print(f\"Original unique protein names: {df[name_col].nunique()}\")\n    samples['normalized'] = samples[name_col].apply(enhanced_clean_protein_name)\n\n    print(\"\\nSample normalization results:\")\n    for _, row in samples.iterrows():\n        print(f\"\\nOriginal:   {row[name_col]}\")\n        print(f\"Normalized: {row['normalized']}\")\n\n    # Count reduction in unique names\n    original_count = df[name_col].nunique()\n    normalized_count = normalized_df['normalized_protein'].nunique()\n    reduction = original_count - normalized_count\n    reduction_pct = 100 * reduction / original_count if original_count > 0 else 0\n\n    print(f\"\\nNormalized unique protein names: {normalized_count}\")\n    print(f\"Reduction: {reduction} ({reduction_pct:.1f}%)\")\n\n    # Find similar protein names that are now treated as the same\n    if normalized_count < original_count:\n        print(\"\\nExamples of proteins that were normalized to the same name:\")\n        norm_to_orig = {}\n\n        # Build mapping of normalized names to original names\n        for _, row in samples.iterrows():\n            norm_name = row['normalized']\n            orig_name = row[name_col]\n\n            if norm_name not in norm_to_orig:\n                norm_to_orig[norm_name] = []\n\n            if orig_name not in norm_to_orig[norm_name]:\n                norm_to_orig[norm_name].append(orig_name)\n\n        # Display examples where multiple original names map to the same normalized name\n        examples_shown = 0\n        for norm_name, orig_names in norm_to_orig.items():\n            if len(orig_names) > 1 and examples_shown < 5:\n                print(f\"\\nNormalized to: {norm_name}\")\n                for orig in orig_names:\n                    print(f\"  - {orig}\")\n                examples_shown += 1\n\n    return normalized_df","metadata":{"id":"pnWf_c20TPbA","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Standardize EC format: Extract numbers without the 'EC:' prefix\nECcontri_Uniprot['EC_clean'] = ECcontri_Uniprot['EC'].str.replace('EC:', '', regex=False)\nECcontri_Uniprot = ECcontri_Uniprot.drop(columns = [\"EC\"])\nECcontri_Uniprot = ECcontri_Uniprot.rename(columns={\"EC_clean\": \"EC\"})\n\n# Calling the function to normalise the name\nECcontri_Uniprot['protein_name'] = ECcontri_Uniprot['protein_name'].apply(enhanced_clean_protein_name)\n\n# Saving the data\nECcontri_Uniprot_path = output_large / 'ECcontri_Uniprot.parquet'\nECcontri_Uniprot.to_parquet(ECcontri_Uniprot_path)","metadata":{"id":"ZOok4gH71k5L","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ECcontri_Uniprot_path = output_large / 'ECcontri_Uniprot.parquet'\nECcontri_Uniprot = pd.read_parquet(ECcontri_Uniprot_path)","metadata":{"id":"u0mYrM2gI8TY","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 8.10 Calculate & Visualize Total Protein Count per EC","metadata":{"id":"pABT9cmAcoqI"}},{"cell_type":"code","source":"ECcontri_Uniprot.tail(20)","metadata":{"id":"Jhc_pFhXTsdR","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Grouping by EC and counting unique proteins\nec_protein_counts = ECcontri_Uniprot.groupby('EC')['protein_name'].nunique().reset_index()\nec_protein_counts.columns = ['EC', 'Protein_Count']\n\n# Plot histogram\nplt.figure(figsize=(10, 6))\nsns.histplot(ec_protein_counts['Protein_Count'], bins=50, kde=True)\nplt.xlabel(\"Total Protein Count per EC\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Distribution of Protein Counts per EC\")\nplt.show()","metadata":{"id":"fJNiW0QlVsV7","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"It is notice that EC wont scale with protein abundance proportionally as the skewed plot shows, this is notice onthe Uniprot tables where a EC has hundred if no thousands of proteins and hundred of organisms. There might be threshold effects were a minimum protein abundance will be relevant for a microorganism metabolism and hence we could stablish those threshold in order to visualise better those protein that are really relevant to the bacteria in question.","metadata":{"id":"YZ-VErJjg3zC"}},{"cell_type":"markdown","source":"## 8.11 Computing the Knee point for Genus of Protein Significance\nHere it is computed the knee point where the activity of protein drops for each genus","metadata":{"id":"XxWk1eJGmp5u"}},{"cell_type":"code","source":"def knee_point_analysis(df, abundance_col):\n    # Dictionary to store knee points for each genus\n    knee_points = {}\n\n    # Calculate total number of genera to determine plot grid size\n    num_genera = df[\"Genus\"].nunique()\n    num_rows = int(np.ceil(np.sqrt(num_genera)))  # Number of rows in subplot grid\n    num_cols = num_rows  # Same for columns in a square grid\n    # Adjust figure size based on the number of genera\n    figsize = (num_cols * 3, num_rows * 3)  # Dynamically adjust figure size\n\n    # Plot setup\n    fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=figsize)  # Adjust for ~85 genera\n    axes = axes.flatten()\n    num_subplots = num_rows * num_cols\n    for i, (genus, group) in enumerate(df.groupby(\"Genus\")):\n        group = group.sort_values(abundance_col, ascending=False).reset_index(drop=True)\n        x = np.arange(len(group))  # Protein rank\n        y = group[abundance_col].values  # Abundance values\n\n        # Apply Savitzky-Golay smoothing\n        y_smoothed = savgol_filter(y, window_length=7, polyorder=3)\n\n        # Knee detection (concave function, decreasing trend)\n        kneedle = KneeLocator(x, y_smoothed, curve=\"convex\", direction=\"decreasing\", online=True, S=1.5)\n\n        if kneedle.knee is not None:\n            knee_points[genus] = group.iloc[kneedle.knee][abundance_col]\n\n        # Plot for visualization - blue for data\n        axes[i].plot(x, y, color='blue', label=genus)\n        axes[i].plot(x, y_smoothed, color='blue', alpha=0.7, label=f\"{genus} (Smoothed)\")\n        if kneedle.knee is not None:\n            axes[i].axvline(kneedle.knee, color='red', linestyle=\"--\", label=\"Knee Point\")\n        axes[i].set_title(genus, fontsize=8)\n        axes[i].set_ylabel(abundance_col)\n        axes[i].tick_params(axis='both', which='major', labelsize=6)\n        legend = axes[i].legend(fontsize=6, handlelength=1)\n\n    # Remove empty subplots (after the loop)\n    for i in range(num_genera, num_subplots):\n        fig.delaxes(axes[i])  # Remove the extra subplot from the figure\n\n    plt.tight_layout(pad=0.5)\n    plt.show()\n\n    # Manual overrides at the end as in your original code\n    manual_overrides = {\n        \"Methylocystis\": 4000, \"Smithella\": 4000, \"Thermincola\": 750, \"Acidovorax\": 10500,\n        \"Anoxybacillus\": 3000, \"Pseudorhodoferax\": 7500, \"Pseudoxanthomonas\": 5000,\n        \"Phreatobacter\": 3000, \"Propionibacterium\": 1500, \"Novosphingobium\": 1500,\n        \"Herbaspirillum\": 2000, \"Azospira\": 10000\n    }\n\n    # Convert knee points dictionary to DataFrame for analysis\n    knee_df = pd.DataFrame(knee_points.items(), columns=[\"Genus\", \"Knee_Abundance\"])\n\n    # Apply manual overrides\n    for genus, override_value in manual_overrides.items():\n        # If genus exists in knee_df, update it; otherwise, add it\n        if genus in knee_df[\"Genus\"].values:\n            knee_df.loc[knee_df[\"Genus\"] == genus, \"Knee_Abundance\"] = override_value\n        else:\n            knee_df = pd.concat([knee_df, pd.DataFrame({\"Genus\": [genus], \"Knee_Abundance\": [override_value]})],\n                              ignore_index=True)\n    # Create a dictionary mapping each genus to its threshold value\n    genus_to_threshold = knee_df.set_index('Genus')['Knee_Abundance'].to_dict()\n\n    return knee_df, genus_to_threshold\n\n# Call the function and store the result\nknee_df, genus_to_threshold = knee_point_analysis(ECcontri_Uniprot, \"norm_abund_contri\")","metadata":{"id":"vSx0nkBPnC5Y","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The knee points in red are consistent with protein drop in protein abundance between each genera. Some genera have a steeper drop, making the knee point clear, while others have a gradual decline, which might make the knee less biologically meaningful. Some of the knee points were manually override in order to make the selection more homogeneus.","metadata":{"id":"KTHudmFdvjs9"}},{"cell_type":"markdown","source":"### Identifying the Uniquenes datapoints of the Data\nAll 1.5 millon rows are unique?, would a EC-Genus-Site be unique or is it protein_name-Genus-Site combination?","metadata":{"id":"Hokee4LzA090"}},{"cell_type":"code","source":"def identify_uniqueness_factors(df):\n    \"\"\"\n    Analyzes what column combinations create uniqueness in the original dataframe.\n    \"\"\"\n    # Original columns\n    columns = ['Sites', 'Genus', 'abund_raw', 'rel_abund_raw', 'genome_EC_count',\n              'abund_contri', 'rel_abund_contri', 'norm_abund_contri', 'protein_name',\n              'uniprot_id', 'EC']\n\n    # Test various column combinations for uniqueness\n    combinations = [\n        ['Sites', 'Genus', 'EC'],\n        ['Sites', 'Genus', 'protein_name'],\n        ['Sites', 'Genus', 'EC', 'protein_name'],\n        ['Sites', 'Genus', 'EC', 'abund_contri'],\n        ['Sites', 'Genus', 'EC', 'abund_raw'],\n        ['Sites', 'Genus', 'EC', 'protein_name', 'abund_contri'],\n        # Test if all columns together make rows unique\n        columns\n    ]\n\n    results = {}\n    for combo in combinations:\n        # Count how many duplicates exist with this combination\n        duplicate_count = df.duplicated(combo, keep=False).sum()\n        unique_count = len(df) - duplicate_count\n        percent_unique = (unique_count / len(df)) * 100\n\n        results[tuple(combo)] = {\n            'total_rows': len(df),\n            'unique_rows': unique_count,\n            'duplicate_rows': duplicate_count,\n            'percent_unique': percent_unique\n        }\n\n        # If we found a perfectly unique combination, print it prominently\n        if duplicate_count == 0:\n            print(f\"PERFECT MATCH: {combo} creates unique rows\")\n\n    # Print all results\n    for combo, stats in results.items():\n        print(f\"Columns {combo}:\")\n        print(f\"  Unique rows: {stats['unique_rows']} ({stats['percent_unique']:.2f}%)\")\n        print(f\"  Duplicates: {stats['duplicate_rows']}\")\n\n    return results\n\nuniquenes= identify_uniqueness_factors(ECcontri_Uniprot)","metadata":{"id":"JWwX3SWj-KMO","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The same EC number (enzyme function) can be performed by different proteins and the same protein can have multiple EC numbers (multiple enzymatic functions), that mens the reltion EC-Genus-Sites is unique","metadata":{"id":"2ymxNjJ4BMZs"}},{"cell_type":"markdown","source":"### Cleaning anc collecting garbage","metadata":{"id":"oVnGj7D_-PJL"}},{"cell_type":"code","source":"del picrust_long\ndel retrieved\ndel retrieved_unique\ndel unique_pairs\ndel df_1\ndel df_2\ndel df_3\ndel df_4\ndel ECcontri\ndel ECcontri_agg_site\n#del ECcontri_otu","metadata":{"id":"2rdKHvKj-PJM","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gc\ngc.collect()","metadata":{"id":"AwyO_mme-PJM","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Data Retrieval Completion Note\nAfter multiple retrieval attempts, 12,656 pairs remain unmapped out of approximately 1,500,000 total rows (0.84%). Given this small percentage and the diminishing returns from further retrieval attempts, we concluded that this level of completeness is acceptable for analysis.\n______________________________","metadata":{"id":"9d2KfaHxuvgU"}},{"cell_type":"markdown","source":"#9. Building a Dictionary from Databases","metadata":{"id":"5lnzHgflRGFV"}},{"cell_type":"markdown","source":"Data Normalization and Mapping\nIt was ensured that all protein/EC data (including EC numbers, KO numbers, and reaction IDs) were parsed and cleaned. This identifiers were mapped to their corresponding metabolic pathways using databases such as KEGG, MetaCyc, and BioCyc.\n\nIdentifying Metal-Related Proteins:\ncross-reference proteins with metal-related databases (BRENDA, MetalPDB, TransportDB) where crossreferenced to flag those with direct metal-binding or metal-transport roles. Then consolidate similar metal terms (e.g., “iron”, “Fe”, “ferric”) into a unified field to improve consistency in later analyses.\n\n\n\nFinal Assembly:\nI compile the data into a final dictionary/table that includes all relevant columns (Protein, EC/KO, Metabolism, Pathway, Metal Interaction, MIC Function). This allows me to search programmatically for the functional roles of proteins that are influential in corrosion studies.","metadata":{"id":"eNr6ECYly6LJ"}},{"cell_type":"markdown","source":"## 9.1 Setting up Paths and Parsing the Dataframes","metadata":{"id":"-FEargpNkucP"}},{"cell_type":"code","source":"def setup_paths():\n    \"\"\"Set up paths for database access\"\"\"\n\n    # Database paths\n    db_paths = {\n        'enzyme': db_dir / 'enzyme',\n        'enzyme_class': db_dir / 'enzclass.txt',\n        'enzyme_brenda' : db_dir/ 'brenda_2024.txt',\n        'ko': db_dir / 'ko',\n        'ko_hierarchy': db_dir / 'ko_hierarchy.txt',\n        'pathway': db_dir / 'pathway',\n        'module': db_dir / 'module',\n        'reaction': db_dir / 'reaction',\n        'compound': db_dir / 'compound',\n        'metalpdb': db_dir / 'flat_db_file.xml',\n        'ko_pathway': db_dir / 'ec_pathway.list'\n    }\n\n    return db_paths\n\n#  Calling the paths\nif __name__ == \"__main__\":\n    paths = setup_paths()\n    # Print paths to verify\n    for db_name, path in paths.items():\n        print(f\"{db_name}: {path}\")\n        print(f\"Exists: {path.exists()}\")","metadata":{"id":"0efKvfwuHc0f","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Brenda Enzyme Parse Brenda\n\nhttps://www.brenda-enzymes.org/download.php\n\nChang A., Jeske L., Ulbrich S., Hofmann J., Koblitz J., Schomburg I., Neumann-Schaal M., Jahn D., Schomburg D.\nBRENDA, the ELIXIR core data resource in 2021: new developments and updates. (2021), Nucleic Acids Res., 49:D498-D508.\nDOI: 10.1093/nar/gkaa1025 PubMed: 33211880","metadata":{"id":"7P2TUJLNWX3B"}},{"cell_type":"code","source":"import logging\nlogging.basicConfig(level=logging.INFO)\n\ndef parse_brenda_file():\n    \"\"\"Parse BRENDA database file for detailed enzyme information\"\"\"\n    paths = setup_paths()\n    enzyme_brenda_path = paths['enzyme_brenda']\n\n    ec_detailed_info = {}\n    current_ec = None\n    in_enzyme_entry = False\n\n    try:\n        with open(enzyme_brenda_path, 'r') as f:\n          for line in f:\n              line = line.strip()\n\n              # Skip empty lines\n              if not line:\n                  continue\n\n              # Check for the end of an entry\n              if line == \"///\":\n                  current_ec = None\n                  in_enzyme_entry = False\n                  continue\n\n              # Process ID line - identify enzyme entries\n              if line.startswith('ID\\t'):\n                  current_ec = line.split('\\t')[1]\n\n                  # Skip \"spontaneous\" and other non-EC entries\n                  if not any(c.isdigit() for c in current_ec):\n                      current_ec = None\n                      in_enzyme_entry = False\n                      continue\n\n                  # Initialize proper EC entry\n                  ec_detailed_info[current_ec] = {\n                      'metals': [],\n                      'cofactors': [],\n                      'reactions': [],\n                      'substrates': [],\n                      'inhibitors': []\n                  }\n                  in_enzyme_entry = True\n\n              # Only process other lines if we're in a valid enzyme entry\n              elif in_enzyme_entry and current_ec:\n                  if line.startswith('ME\\t'):\n                      # Extract metal information\n                      metal_info = line.split('\\t')[1]\n                      ec_detailed_info[current_ec]['metals'].append(metal_info)\n\n                  elif line.startswith('CF\\t'):\n                      # Extract cofactor information\n                      cofactor_info = line.split('\\t')[1]\n                      ec_detailed_info[current_ec]['cofactors'].append(cofactor_info)\n\n                  elif line.startswith('RE\\t'):\n                      # Extract detailed reaction information\n                      reaction_info = line.split('\\t')[1]\n                      ec_detailed_info[current_ec]['reactions'].append(reaction_info)\n\n                  elif line.startswith('SP\\t') or line.startswith('NSP\\t'):\n                      # Extract substrate information\n                      substrate_info = line.split('\\t')[1]\n                      ec_detailed_info[current_ec]['substrates'].append(substrate_info)\n\n                  elif line.startswith('IN\\t'):\n                      # Extract inhibitor information\n                      inhibitor_info = line.split('\\t')[1]\n                      ec_detailed_info[current_ec]['inhibitors'].append(inhibitor_info)\n\n        # Verify we have valid EC numbers\n        ec_detailed_info = {ec: info for ec, info in ec_detailed_info.items()\n                            if ec.count('.') == 3 and all(part.isdigit() for part in ec.split('.'))}\n\n    except Exception as e:\n        logging.error(\"Error parsing BRENDA file: %s\", e)\n        return {}\n    return ec_detailed_info\n\nbrenda_data = parse_brenda_file()\n#brenda_data","metadata":{"id":"mkW7GPMhvlL3","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def process_brenda_data(brenda_data):\n    \"\"\"Process BRENDA data to extract clean metal information while keeping other data intact\"\"\"\n    processed_data = {}\n\n    # Common metal ions to look for\n    metal_patterns = {'iron': ['Fe2+', 'Fe3+', 'iron', 'ferrous', 'ferric'],\n        'manganese': ['Mn2+', 'manganese'],\n        'copper': ['Cu+', 'Cu2+', 'copper'],\n        'nickel': ['Ni2+', 'nickel'],\n        'cobalt': ['Co2+', 'cobalt'],\n        'magnesium': ['Mg2+', 'magnesium'],\n        'calcium': ['Ca2+', 'calcium'],\n        'Mo': ['Mo', 'molybdenum'],\n        'V5+': ['V5+', 'vanadium'],\n        'Al3+': ['Al3+', 'aluminum'],\n        'Cr3+': ['Cr3+', 'chromium'],\n        'zinc': ['Zn2+', 'zinc'],\n        'sodium': ['Na+', 'sodium', 'NaCl'],\n        'potassium': ['K+', 'potassium', 'KCl'],\n        'selenium': ['selenium', 'Se'],\n        'barium': ['Ba2+', 'barium'],\n        'phosphate': ['HPO4-2', 'PO4-3', 'phosphate', 'phosphates'],\n        'nitrate': ['NO3-', 'nitrate', 'nitrates'],\n        'nitrite': ['NO2-', 'nitrite', 'nitrites'],\n        'chloride': ['Cl-', 'chloride', 'chlorine'],\n        'sulfate': ['SO4-2', 'sulfate', 'sulfates'],\n        'sulfide': ['S', 'sulfide', 'sulfides'],\n        'thiosulfate': [ 'S'],\n        's-s': ['S']\n    }\n    # Pathway categories collecting all terms\n    pathway_categories = {\n        'organic_acid_metabolism': [\n            'acetate', 'acetic acid', 'acetyl',\n            'oxalate', 'oxalic acid',\n            'organic acid', 'fatty acid',\n            'carboxylic acid'\n        ],\n        'metal_organic_interaction': [\n            'siderophore', 'metal binding',\n            'iron complex', 'metal transport',\n            'metallophore', 'metal organic'\n        ],\n        'biofilm_formation': [\n            'biofilm', 'exopolysaccharide',\n            'extracellular matrix', 'adhesion'\n        ],\n        'halogen_related': [\n            'halogen', 'chloride', 'bromide',\n            'halide', 'dehalogenation'\n        ],\n        'sulfur': [\n            'sulfur', 'sulfate', 'sulfide',\n            'thiosulfate', 'sulfite', 'sulfonate'\n        ],\n        'electron_transfer': [\n            'cytochrome', 'electron transport',\n            'oxidoreductase', 'redox'\n        ],\n        'carbon_metabolism': [\n            'carbon fixation', 'carbon utilization',\n            'carbohydrate metabolism'\n        ],\n        'ph_modulation': [\n            'acid', 'alkaline', 'proton pump',\n            'pH homeostasis'\n        ],\n        'temp_response': [\n            'heat shock', 'cold shock',\n            'temperature response'\n        ]\n    }\n\n    # Define organic matter categories\n    organic_categories = {\n        'degradation': ['degradation', 'breakdown', 'catabolism'],\n        'synthesis': ['biosynthesis', 'anabolism', 'synthesis'],\n        'transport': ['transport', 'uptake', 'export'],\n        'modification': ['modification', 'conversion', 'transformation']\n    }\n\n    for ec_number, data in brenda_data.items():\n        processed_data[ec_number] = {\n            'cofactors': data.get('cofactors', []),\n            'reactions': data.get('reactions', []),\n            'substrates': data.get('substrates', []),\n            'inhibitors': data.get('inhibitors', []),\n            'raw_metals': data.get('metals', []),\n            'clean_metals': [],\n            'pathway_categories': {},\n            'organic_processes': {}\n\n        }\n\n        # Extract clean metal names\n        for entry in data.get('metals', []):\n            entry_lower = entry.lower()\n            for metal in metal_patterns:\n                metal_lower = metal.lower()\n                if metal_lower in entry_lower:\n                    if metal not in processed_data[ec_number]['clean_metals']:\n                        processed_data[ec_number]['clean_metals'].append(metal)\n\n        # Create a single text string for pathway searching\n        all_text = ' '.join([\n            ' '.join(data.get('reactions', [])),\n            ' '.join(data.get('substrates', [])),\n            ' '.join(data.get('cofactors', []))\n        ]).lower()\n\n        # Add corrosion relevance information\n        processed_data[ec_number]['corrosion_metals_from_brenda'] = [\n            metal for metal in processed_data[ec_number]['clean_metals']\n            if metal in ['Fe2+', 'Fe3+', 'iron', 'Mn2+', 'manganese', 'Cu+', 'Cu2+',\n                         'copper', 'Ni2+', 'nickel', 'Co2+', 'cobalt']\n        ]\n        ## Add pathway relevance information, would it no be better to search for this relevance on the pathway database?\n        for category, terms in pathway_categories.items():\n            if any(term.lower() in all_text for term in terms):\n                processed_data[ec_number]['pathway_categories'][category] = True\n\n        # Check for organic matter processes\n        for category, terms in organic_categories.items():\n            if any(term.lower() in all_text for term in terms):\n                processed_data[ec_number]['organic_processes'][category] = True\n\n        # Calculate corrosion relevance score based on metals\n        if processed_data[ec_number]['corrosion_metals_from_brenda']:\n            processed_data[ec_number]['corrosion_relevance'] = 'high'\n        elif processed_data[ec_number]['clean_metals']:\n            processed_data[ec_number]['corrosion_relevance'] = 'medium'\n        else:\n            processed_data[ec_number]['corrosion_relevance'] = 'low'\n\n    return processed_data\n\nbrenda_en= process_brenda_data(brenda_data)\n#brenda_en.keys()","metadata":{"id":"d2S3JREa6rg7","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Enzyme names\nThe database containing enzyme names and EC numbers\n\nwget https://www.enzyme-database.org/downloads/enzyme-database.sql.gz","metadata":{"id":"EA-pSl0dvd0W"}},{"cell_type":"code","source":"","metadata":{"id":"Zqezyb_GuEQL","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def read_enzyme_names():\n    \"\"\"Read and parse enzyme file to get EC numbers and their names\"\"\"\n    paths = setup_paths()\n    enzyme_path = paths['enzyme']\n\n    ec_to_names = {}  # More descriptive name\n    with open(enzyme_path, 'r') as f:\n        for line in f:\n            parts = line.strip().split('\\t')\n            if len(parts) >= 2:\n                ec_number = parts[0]\n                names = parts[1].split('; ')\n                ec_to_names[ec_number] = names\n\n    for ec_number, names_list in ec_to_names.items():\n\n        cleaned_names = [enhanced_clean_protein_name(name) if isinstance(name, str) else str(name) for name in names_list]\n        ec_to_names[ec_number] = cleaned_names\n\n    return ec_to_names\nec_to_names = read_enzyme_names()\n#list(ec_to_names.items())[:100]","metadata":{"id":"L6FSLRy-E1_a","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Enzyme Class\nThe enzyme classification system (text-based hierarchy).","metadata":{"id":"aUUP7OAF9iaM"}},{"cell_type":"code","source":"def read_enzyme_class():\n    paths = setup_paths()\n    ec_file_path = paths['enzyme_class']\n\n    enzyme_class = {}\n\n    with open(ec_file_path, 'r') as f:\n        for line in f:\n            # Format is like \"1. 1. 1.-    With NAD(+) or NADP(+) as acceptor.\"\n            if line.strip() and any(line.startswith(str(i)) for i in range(1, 7)):\n                parts = line.strip().split('  ')\n                if len(parts) >= 2:\n                    ec_id = parts[0].replace(' ', '')\n                    desc = parts[1].strip()\n                    enzyme_class[ec_id] = desc\n    return enzyme_class\nenzyme_class = read_enzyme_class()\n#list(enzyme_class.items())[:100]","metadata":{"id":"6FTH4E8gZfdu","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Ko list\n\nwget -O ec_pathway.list \"https://rest.kegg.jp/link/pathway/ec\"","metadata":{"id":"GBIfRLP65Jzc"}},{"cell_type":"code","source":"def read_ec_pathway_mapping():\n    \"\"\"Read EC to pathway mapping file downloaded from KEGG\"\"\"\n\n    paths = setup_paths()\n    ko_pathway_path = paths['ko_pathway']\n\n    ec_to_pathway = {}\n\n    try:\n        with open(ko_pathway_path , 'r') as f:\n            for line in f:\n                parts = line.strip().split('\\t')\n                if len(parts) == 2:\n                    ec_id = parts[0].replace('ec:', '')\n                    pathway_id = parts[1].replace('path:', '')\n\n                    if ec_id not in ec_to_pathway:\n                        ec_to_pathway[ec_id] = []\n                    ec_to_pathway[ec_id].append(pathway_id)\n\n        print(f\"Loaded pathway mappings for {len(ec_to_pathway)} EC numbers\")\n    except Exception as e:\n        print(f\"Error reading EC-pathway mapping: {e}\")\n        return {}\n\n    return ec_to_pathway\n\nec_to_pathway = read_ec_pathway_mapping()\n#ec_to_pathway","metadata":{"id":"EEH4ACcm558K","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Ko Database\n A new variable mapping  KO numbers to EC numbers from KEGG KO file.\nrsync -avz rsync://rest.kegg.jp/kegg/pathway/ .","metadata":{"id":"CBtayQh4eQ82"}},{"cell_type":"code","source":"def read_ko_data():\n    \"\"\"Read and parse KEGG KO file\"\"\"\n    paths = setup_paths()\n    ko_file_path = paths['ko']\n\n    ko_info = {}\n    with open(ko_file_path, 'r') as f:\n        for line in f:\n            if line.startswith('K'):\n                parts = line.strip().split('\\t')\n                if len(parts) > 1:\n                    ko_info[parts[0]] = {\n                        'definition': parts[1],\n                        'pathway': parts[2] if len(parts) > 2 else ''\n                    }\n    return ko_info\n\nko_ec =read_ko_data()\n#list(ko_ec.items())[:100]","metadata":{"id":"Nz9K72kaKM52","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Ko Hierarchi Database\nHierarchy of KO numbers (helps in pathway mapping).\n","metadata":{"id":"iBwbp_UkeTI6"}},{"cell_type":"code","source":"def read_ko_hierarchy():\n    paths = setup_paths()\n    ko_path = paths['ko_hierarchy']\n\n    hierarchy = {\n        'A': {},  # Top level\n        'B': {},  # ko Category\n        'C': {},  # Pathway\n        'D': {}   # KO/Enzyme\n    }\n\n    current = {'A': None, 'B': None, 'C': None}\n\n    with open(ko_path, 'r') as f:\n        for line in f:\n            if line.startswith('A'):\n                parts = line.strip().split()\n                id = parts[1]\n                name = ' '.join(parts[2:])\n                hierarchy['A'][id] = name\n                current['A'] = id\n\n            elif line.startswith('B'):\n                parts = line.strip().split()\n                id = parts[1]\n                name = ' '.join(parts[2:])\n                hierarchy['B'][id] = {'name': name, 'parent': current['A']}\n                current['B'] = id\n\n            elif line.startswith('C'):\n                parts = line.strip().split()\n                id = parts[1]\n                name = ' '.join(parts[2:])\n                if '[PATH:' in name:\n                    path_parts = name.split('[PATH:')\n                    name = path_parts[0].strip()\n                    path_id = path_parts[1].split(']')[0]\n                else:\n                    path_id = None\n\n                hierarchy['C'][id] = {\n                    'name': name,\n                    'parent': current['B'],\n                    'path_id': path_id\n                }\n                current['C'] = id\n\n            elif line.startswith('D'):\n                parts = line.strip().split()\n                ko_id = parts[1]\n                name = ' '.join(parts[2:])\n\n                # Extract EC numbers if present\n                ec_numbers = []\n                if '[EC:' in name:\n                    ec_part = name.split('[EC:')[1].split(']')[0]\n                    ec_numbers = ec_part.split()\n                    name = name.split('[EC:')[0].strip()\n\n                hierarchy['D'][ko_id] = {\n                    'name': name,\n                    'parent': current['C'],\n                    'ec_numbers': ec_numbers\n                }\n\n    return hierarchy\n\nko_hierarchy = read_ko_hierarchy()\n#list(ko_hierarchy.items())[:10]","metadata":{"id":"l-PjltuxE1_b","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Reaction Data\n Reaction-level information.\n\n!wget -c \"ftp://ftp.genome.jp/pub/kegg/reaction/reaction.tar.gz\"","metadata":{"id":"4-kOhcGCcn-i"}},{"cell_type":"code","source":"def read_reaction_data():\n    paths = setup_paths()\n    reaction_file_path = paths['reaction']\n\n    reaction_info = {}\n\n    with open(reaction_file_path, 'r') as f:\n        for line in f:\n            line = line.strip()\n            if not line:\n                continue\n\n            parts = line.split(None, 1)  # Split on first whitespace\n            if len(parts) >= 2:\n                rxn_id = parts[0]\n                desc_parts = parts[1].split(';')\n\n                # First part is reaction name\n                name = desc_parts[0].strip()\n\n                # Rest might contain equation\n                equation = desc_parts[1].strip() if len(desc_parts) > 1 else \"\"\n\n                reaction_info[rxn_id] = {\n                    'name': name,\n                    'equation': equation\n                }\n\n    return reaction_info\n\nreaction_equation = read_reaction_data()\n#reaction_equation","metadata":{"id":"s5Pd6vUTLR4M","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Pathway Database\nhemical compounds database.\n\nwget https://biocyc.org/download.shtml\n\nwget https://www.brenda-enzymes.org/download.php\n\n","metadata":{"id":"LIyE7Tx5chQ6"}},{"cell_type":"code","source":"def read_pathway_data():\n    paths = setup_paths()\n    pathway_path = paths['pathway']\n\n    pathway_info = {}\n    with open(pathway_path, 'r') as f:\n          for line in f:\n              parts = line.strip().split('\\t')\n              if len(parts) >= 2:\n                  pathway_id = parts[0]\n                  pathway_name = parts[1]\n                  pathway_info[pathway_id] = pathway_name\n    return pathway_info\n\npathway_data = read_pathway_data()\n#pathway_data","metadata":{"id":"xu5sqGIPc0op","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Module Database","metadata":{"id":"zxYkP8l0dCXo"}},{"cell_type":"code","source":"def read_module_data():\n    paths = setup_paths()\n    module_path = paths['module']\n\n    module_info = {}\n    with open(module_path, 'r') as f:\n      for line in f:\n          parts = line.strip().split('\\t')\n          if len(parts) >= 2:\n              module_id = parts[0]\n              module_desc = parts[1]\n              module_info[module_id] = module_desc\n    return module_info\n\nmodule_info = read_module_data()\n#module_info","metadata":{"id":"0Ue-7KZpdEfe","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Compound Database","metadata":{"id":"W2GClcq8dmrP"}},{"cell_type":"code","source":"def read_compound_data():\n    paths = setup_paths()\n    compound_path = paths['compound']\n\n    compound_info = {}\n    with open(compound_path, 'r') as f:\n            for line in f:\n                parts = line.strip().split('\\t')\n                if len(parts) >= 2:\n                    compound_id = parts[0]\n                    compound_names = parts[1].split('; ')\n                    compound_info[compound_id] = {\n                        'name': compound_names[0],\n                        'synonyms': compound_names[1:] if len(compound_names) > 1 else []\n                    }\n    return compound_info\n\ncompound_info = read_compound_data()\n#compound_info","metadata":{"id":"K9riHOnadqSm","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Metal pdb\nMetalPDB in 2018: a database of metal sites in biological macromolecular structures.\nPutignano V., Rosato A., Banci L., Andreini C.\nNucleic Acids Res. 2018 Jan;46(D1):D459-D464. [PMID: 29077942]\n","metadata":{"id":"hi9MNHn55BT3"}},{"cell_type":"code","source":"def parse_metalpdb_xml():\n    \"\"\"Parse MetalPDB XML file to extract metal-binding information\"\"\"\n    paths = setup_paths()\n    metalpdb_path = paths['metalpdb']\n\n    metal_binding_data = {}\n\n    try:\n        # Use a more tolerant parser\n        parser = etree.XMLParser(recover=True)\n        tree = etree.parse(metalpdb_path, parser)\n        root = tree.getroot()\n\n        # Process each site\n        for site in root.findall('.//site'):\n            # Extract site information\n            site_name = site.findtext('site_name')\n            pdb_code = site.findtext('pdb_code')\n            site_nuclearity = site.findtext('site_nuclearity')\n\n            # Process each metal in the site\n            for metal in site.findall('.//metal'):\n                metal_symbol = metal.findtext('periodic_symbol')\n                metal_name = metal.findtext('periodic_name')\n                coordination_number = metal.findtext('coordination_number')\n                geometry = metal.findtext('geometry')\n\n                # Process ligands\n                ligands = []\n                for ligand in metal.findall('.//ligand'):\n                    residue_name = ligand.findtext('residue_name')\n                    residue_num = ligand.findtext('residue_pdb_number')\n                    chain = ligand.findtext('chain_letter')\n                    binding_type = ligand.findtext('endo_exo')\n\n                    # Process donor atoms\n                    donors = []\n                    for donor in ligand.findall('.//donor'):\n                        distance = donor.findtext('distance')\n                        atom_name = donor.findtext('atom_pdb_name')\n                        atom_symbol = donor.findtext('atom_symbol')\n                        interaction_type = donor.findtext('interaction_type')\n\n                        donors.append({\n                            'distance': distance,\n                            'atom_name': atom_name,\n                            'atom_symbol': atom_symbol,\n                            'interaction_type': interaction_type\n                        })\n\n                    ligands.append({\n                        'residue_name': residue_name,\n                        'residue_number': residue_num,\n                        'chain': chain,\n                        'binding_type': binding_type,\n                        'donors': donors\n                    })\n\n                # Get the protein/molecule information\n                site_chains = []\n                for chain in site.findall('.//site_chain'):\n                    molecule_name = chain.findtext('molecule_name')\n                    molecule_type = chain.findtext('molecule_type')\n                    chain_letter = chain.findtext('letter')\n\n                    site_chains.append({\n                        'molecule_name': molecule_name,\n                        'molecule_type': molecule_type,\n                        'chain_letter': chain_letter\n                    })\n\n                # Create a unique key for this metal site\n                metal_site_key = f\"{pdb_code}_{site_name}_{metal_symbol}\"\n\n                # Store the data\n                metal_binding_data[metal_site_key] = {\n                    'pdb_code': pdb_code,\n                    'site_name': site_name,\n                    'site_nuclearity': site_nuclearity,\n                    'metal': {\n                        'symbol': metal_symbol,\n                        'name': metal_name,\n                        'coordination_number': coordination_number,\n                        'geometry': geometry\n                    },\n                    'ligands': ligands,\n                    'site_chains': site_chains\n                }\n\n    except Exception as e:\n        logging.error(\"Error parsing MetalPDB XML: %s\", e)\n        return {}\n\n    return metal_binding_data\nmetal_binding_data = parse_metalpdb_xml()\n#metal_binding_data","metadata":{"id":"MhoKLqYQihT8","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Extracting metal binding patterns from metal binding data","metadata":{"id":"7ScqMYjZ3dIM"}},{"cell_type":"code","source":"def extract_metal_coordination_patterns(metal_binding_data):\n    \"\"\"Extract metal coordination patterns from MetalPDB data\"\"\"\n\n    # Track metal coordination patterns\n    metal_coordination = {}\n    metal_residue_binding = {}\n\n    for site_key, site_data in metal_binding_data.items():\n        metal_symbol = site_data['metal']['symbol']\n\n        # Track coordination environments\n        coord_num = site_data['metal']['coordination_number']\n        geometry = site_data['metal']['geometry']\n        coord_key = f\"{metal_symbol}_{coord_num}_{geometry}\"\n\n        if coord_key not in metal_coordination:\n            metal_coordination[coord_key] = 0\n        metal_coordination[coord_key] += 1\n\n        # Track metal-residue binding\n        if metal_symbol not in metal_residue_binding:\n            metal_residue_binding[metal_symbol] = {}\n\n        for ligand in site_data['ligands']:\n            residue = ligand['residue_name']\n            if residue not in metal_residue_binding[metal_symbol]:\n                metal_residue_binding[metal_symbol][residue] = 0\n            metal_residue_binding[metal_symbol][residue] += 1\n\n    return {\n        'coordination': metal_coordination, # metal_coordination,\n        'residue_binding': metal_residue_binding# metal_residue_binding\n    }\n\nmetal_patterns = extract_metal_coordination_patterns(metal_binding_data)\n#metal_patterns","metadata":{"id":"2SwP7fDN3so5","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### EC to reaction Mapping","metadata":{"id":"jdl9jF7TikIR"}},{"cell_type":"code","source":"def create_ec_to_reaction_mapping():\n    # Get EC to enzyme names mapping\n    ec_to_names = read_enzyme_names()\n\n    # Get reaction data\n    reaction_info = read_reaction_data()\n\n    # Create a mapping from EC to reactions\n    ec_to_reaction = {}\n\n    # Use string pattern matching to find EC numbers in reaction names\n    for rxn_id, rxn_info in reaction_info.items():\n        rxn_name = rxn_info['name'].lower()\n\n        # Look through all EC numbers and their names\n        for ec, names in ec_to_names.items():\n            enzyme_text = ' '.join(names).lower()\n\n            # Check for common significant words\n            if any(word in rxn_name for word in enzyme_text.split() if len(word) > 4):\n                if ec not in ec_to_reaction:\n                    ec_to_reaction[ec] = []\n                if rxn_id not in ec_to_reaction[ec]:\n                    ec_to_reaction[ec].append(rxn_id)\n\n    return ec_to_reaction\n\nec_to_rxn = create_ec_to_reaction_mapping()\n#ec_to_rxn","metadata":{"id":"FH9jMXKKSJM7","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 9.2 Creating an Integrated Database","metadata":{"id":"QQ3q5YlPYZWJ"}},{"cell_type":"markdown","source":"### Consolidation Metals from Different Origins Helper Function","metadata":{"id":"XBwBw-ufrWsh"}},{"cell_type":"code","source":"# Standard mapping: lower-case keys for matching, with standard symbols as values\nmetal_mapping = {\n    'iron': 'Fe',\n    'fe': 'Fe',\n    'ferrous': 'Fe',\n    'ferric': 'Fe',\n    'heme': 'Fe',\n    'iron-sulfur': 'Fe',\n    'fe2+': 'Fe',\n    'fe3+': 'Fe',\n    'manganese': 'Mn',\n    'mn': 'Mn',\n    'manganous': 'Mn',\n    'manganic': 'Mn',\n    'manganese oxidation': 'Mn',\n    'metal oxide': 'Mn',\n    'copper': 'Cu',\n    'cu+': 'Cu',\n    'cu2+': 'Cu',\n    'nickel': 'Ni',\n    'ni2+': 'Ni',\n    'cobalt': 'Co',\n    'co2+': 'Co',\n    'zinc': 'Zn',\n    'zn2+': 'Zn',\n    'calcium': 'Ca',\n    'ca2+': 'Ca',\n    'molybdenum': 'Mo',\n    'mo': 'Mo',\n    'vanadium': 'V5+',\n    'v5+': 'V5+',\n    'aluminum': 'Al3+',\n    'al3+': 'Al3+',\n    'chromium': 'Cr3+',\n    'cr3+': 'Cr3+',\n    'sodium': 'Na',\n    'na+': 'Na',\n    'nacl': 'Na',\n    'potassium': 'K',\n    'k+': 'K',\n    'kcl': 'K',\n    'selenium': 'Se',\n    'se': 'Se',\n    'barium': 'Ba2+',\n    'ba2+': 'Ba2+',\n    'sulfate': 'S',\n    'sulfide': 'S',\n    'thiosulfate': 'S',\n    's-s': 'S',\n    'sulfur': 'S',\n    'sulfur oxidation': 'S',\n    'srb': 'S',\n    'hydrogen': 'H',\n    'h2': 'H',\n    'h2o': 'H',\n    'h2s': 'H',\n    'phosfate': 'po4-3',\n    'nitrate': 'NO3-',\n    'nitrite': 'NO2',\n    'chloride': 'Cl-'\n }\n\ndef consolidate_metal_terms(brenda_metals, text_detected_metals):\n    \"\"\"\n    Consolidates metal names from BRENDA and text mining into standardized symbols.\n\n    Parameters:\n        brenda_metals (list of str): Metals obtained from BRENDA data.\n        text_detected_metals (list of str): Metals detected from text mining.\n\n    Returns:\n        list: Consolidated list of unique, standardized metal symbols.\n    \"\"\"\n    consolidated = set()\n    all_metals = (brenda_metals or []) + (text_detected_metals or [])\n\n    for metal in all_metals:\n        metal_norm = metal.strip().lower()\n        # Check if the normalized term matches any key in the standard mapping\n        for key, symbol in metal_mapping.items():\n            if key in metal_norm:\n                consolidated.add(symbol)\n                break\n        else:\n            # If no mapping is found, add the original\n            consolidated.add(metal.strip())\n    return list(consolidated)","metadata":{"id":"HXsb57_SraU0","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Creating a consolidated DataBase","metadata":{"id":"8BNjB4FHE1_e"}},{"cell_type":"markdown","source":"","metadata":{"id":"gZMB95fLE1_e"}},{"cell_type":"markdown","source":"Brenda Enzyme Database. (n.d.). In BRENDA. Retrieved from https://www.brenda-enzymes.org (APA citation format)\nMetalPDB: a database of metal sites in biological macromolecular structures. (n.d.). Retrieved from http://metalpdb.cerm.unifi.it (APA citation format)\n\n### Main Function","metadata":{"id":"nHVkndSJpuS4"}},{"cell_type":"code","source":"def create_metabolism_database():\n    \"\"\"\n    Build a list of dictionaries, each representing a single EC record.\n    This approach is for conversion to a DataFrame, with proper error handling\n    and data validation.\n    \"\"\"\n    try:\n        # Read all necessary files\n        ec_to_names = read_enzyme_names() or {}\n        enzyme_class = read_enzyme_class() or {}\n        reaction_equation = read_reaction_data() or {}\n        ko_ec = read_ko_data() or {}\n        ko_hierarchy = read_ko_hierarchy() or {}\n        pathway_data = read_pathway_data() or {}\n        module_info = read_module_data() or {}\n        compound_info = read_compound_data() or {}\n        brenda_en = process_brenda_data(brenda_data) or {}\n        metal_binding_data = parse_metalpdb_xml() or {}\n        metal_patterns = extract_metal_coordination_patterns(metal_binding_data)\n        ec_pathway_mapping = read_ec_pathway_mapping() or {}\n\n        print(f\"Loaded: {len(ec_to_names)} enzymes, {len(pathway_data)} pathways, {len(brenda_en)} BRENDA entries\")\n    except Exception as e:\n        print(f\"Error loading data sources: {e}\")\n        return []\n\n    # metal and corrosion keywords\n    metal_terms = {\n        'iron': ['iron', 'iron reduction','fe', 'ferrous', 'ferric', 'heme', 'iron-sulfur', 'Fe2+', 'Fe3+'],\n        'sulfur': ['sulfate', 'sulfide', 'thiosulfate', 'S-S', 'sulfur', 'srb', 'sulfur oxidation', 'sulfur reduction'],\n        'hydrogen': ['hydrogen', 'hydrogenase', 'h2'],\n        'manganese': ['Mn2+', 'manganese', 'mn', 'manganous', 'manganic', 'manganese oxidation', 'metal oxide'],\n        'biofilm': ['exopolysaccharide', 'biofilm', 'adhesin', 'eps', 'polysaccharide'],\n        'copper': ['Cu+', 'Cu2+', 'copper'],\n        'nickel': ['Ni2+', 'nickel'],\n        'cobalt': ['Co2+', 'cobalt'],\n        'calcium': ['Ca2+', 'calcium'],\n        'Mo': ['Mo', 'molybdenum'],\n        'V5+': ['V5+', 'vanadium'],\n        'Al3+': ['Al3+', 'aluminum'],\n        'Cr3+': ['Cr3+', 'chromium'],\n        'zinc': ['Zn2+', 'zinc'],\n        'sodium': ['Na+', 'sodium', 'NaCl'],\n        'potassium': ['K+', 'potassium', 'KCl'],\n        'selenium': ['selenium', 'Se'],\n        'barium': ['Ba2+', 'barium'],\n        'chloride': ['chloride', 'cl-'],\n        'nitrate': ['nitrate', 'NO3-'],\n        'nitrite': ['nitrite', 'NO2-'],\n        'phosphate': ['phosphate', 'po4-3']\n    }\n\n    # Corrosion mechanism classification\n    corrosion_mechanisms = {\n          'direct_eet': ['cytochrome', 'electron transfer', 'conductive pili', 'nanowire', 'mtrABC', 'omcS','oxidoreductase', 'redox', 'reductase', 'oxidase'],\n          'indirect_eet': ['shuttle', 'mediator', 'redox mediator'],\n          'acid_production': ['acid', 'acidification', 'fermentation', 'lactic acid', 'formic acid', 'acetic acid', 'oxalic acid', 'organic acid', 'acetate production', 'lactate metabolism', 'formate production'],\n          'h2_consumption': ['hydrogenase', 'hydrogen uptake', 'hydrogen consumption', 'h2', 'H2 oxidation', 'H2ase'],\n          'o2_consumption': ['oxidase', 'oxygen reduction', 'aerobic respiration','oxygen reduc', 'aerobic respiration', 'oxygen consum'],\n          'biofilm_formation': ['polysaccharide', 'adhesin', 'biofilm', 'EPS', 'extracellular polymeric substance', 'curli', 'exopolymer', 'extracellular matrix', 'adhesion', 'colonization', 'attachment'],\n          'sulfur_metabolism': ['sulfate reduc', 'sulfide', 'sulfite', 'thiosulfate', 'sulfur oxidation', 'SRB'],\n          'metal_transformation': ['iron reduction', 'manganese oxidation', 'metal oxide', 'ochre formation', 'iron oxide deposits', 'iron precipitation', 'rust formation',],\n          'iron_metabolism': ['iron reduc', 'ferric reduc', 'iron oxid', 'ferrous oxid'],\n          'metal_chelation': ['siderophore', 'metal binding', 'chelator', 'metallophore', 'iron complex', 'metal transport'],\n          'carbon_metabolism': ['carbon fixation', 'carbon utilization', 'carbohydrate metabolism', 'glycolysis', 'TCA cycle'],\n          'ph_modulation': ['acid tolerance', 'alkaline tolerance', 'proton pump', 'pH homeostasis', 'pH stress']\n    }\n\n\n\n    # Get EC to reaction mapping\n    try:\n        ec_to_rxn = create_ec_to_reaction_mapping()\n    except Exception as e:\n        print(f\"Error creating EC to reaction mapping: {e}\")\n        ec_to_rxn =  {}\n\n    # Prepare a list to store all records\n    ec_records = []\n\n    # Track statistics for validation\n    stats = {\n        'total_enzymes': 0,\n        'with_brenda_data': 0,\n        'with_reactions': 0,\n        'with_pathways': 0,\n        'with_ko': 0,\n        'with_metal_involvement': 0,\n        'with_corrosion_mechanisms': 0\n    }\n\n    # populate from ec_to_names for basic enzyme names\n    for ec_number, names in ec_to_names.items():\n        stats['total_enzymes'] += 1\n\n        # Data validation for EC number format\n        if not (ec_number.count('.') == 3 and all(part.isdigit() for part in ec_number.split('.'))):\n            print(f\"Warning: Invalid EC number format: {ec_number}\")\n            continue\n\n        record ={\n            'ec_number': ec_number,\n            'enzyme_names':  names if isinstance(names, list) else [str(names)],\n            'enzyme_class': None,\n            'pathways': [],\n            'hierarchy': [],\n            'ko': [],\n            'reactions': [],\n            'compounds': [],\n            'modules': []\n        }\n\n        # Add pathways from EC-pathway mapping\n        if ec_number in ec_pathway_mapping:\n            for pathway_id in ec_pathway_mapping[ec_number]:\n                # Standardize to map prefix\n                std_id = pathway_id\n                if pathway_id.startswith('ec'):\n                    std_id = 'map' + pathway_id[2:]\n\n                # Look up the pathway name\n                if std_id in pathway_data:\n                    pathway_name = pathway_data[std_id]\n                    if pathway_name not in record['pathways']:\n                        record['pathways'].append(pathway_name)\n\n        # Add pathways from KO data\n        if ec_number in ko_ec and isinstance(ko_ec[ec_number], list):\n            for path in ko_ec[ec_number]:\n                if path not in record['pathways']:\n                    record['pathways'].append(path)\n        elif ec_number in ko_ec and isinstance(ko_ec[ec_number], dict) and 'pathway' in ko_ec[ec_number]:\n            path = ko_ec[ec_number]['pathway']\n            if path not in record['pathways']:\n                record['pathways'].append(path)\n\n        # Add KO IDs\n        ko_ids = []\n        for ko, data in ko_ec.items():\n            if isinstance(data, dict) and 'definition' in data and f\"[EC:{ec_number}]\" in data['definition']:\n                ko_ids.append(ko)\n        record['ko'] = ko_ids\n\n        if ko_ids:\n            stats['with_ko'] += 1\n\n        # Build reaction list\n        rxns = ec_to_rxn.get(ec_number, [])\n        for rxn_id in rxns:\n            if rxn_id in reaction_equation:\n                eqn = reaction_equation.get(rxn_id, {}).get('equation', 'Unknown')\n                record['reactions'].append({'id': rxn_id, 'equation': eqn})\n\n                # Add compounds involved in this reaction\n                for compound_id in reaction_equation.get(rxn_id, {}).get('compounds', []):\n                    if compound_id in compound_info:\n                        compound_data = compound_info[compound_id]\n                        if compound_data not in record['compounds']:\n                            record['compounds'].append(compound_data)\n\n        if rxns:\n            stats['with_reactions'] += 1\n\n        # Add module information\n        for module_id, module_desc in module_info.items():\n            if f\"[EC:{ec_number}]\" in module_desc:\n                record['modules'].append({'id': module_id, 'description': module_desc})\n\n        # Reconcile metals from BRENDA with text mining\n        record['metals_from_brenda'] = []\n        record['corrosion_metals_from_brenda'] = []\n\n        # Add BRENDA metal information\n        if brenda_en and ec_number in brenda_en:\n            record['metals_from_brenda'] = brenda_en[ec_number].get('clean_metals', [])\n            record['corrosion_metals_from_brenda'] = brenda_en[ec_number].get('corrosion_metals_from_brenda', [])\n            stats['with_brenda_data'] += 1\n\n        ec_records.append(record)\n\n    # Count records with pathways\n    pathway_count = 0\n\n    # Add pathway information from pathway_data\n    for rec in ec_records:\n        # Get KO terms for this EC number\n        ko_ids = rec.get('ko', [])\n\n        # For each KO, check if it has a pathway\n        for ko_id in ko_ids:\n            if ko_id in ko_ec and 'pathway' in ko_ec[ko_id]:\n                pathway_id = ko_ec[ko_id]['pathway']\n\n                # If the pathway ID exists in pathway_data, add it\n                if pathway_id and pathway_id in pathway_data:\n                    pathway_name = pathway_data[pathway_id]\n                    if pathway_name not in rec['pathways']:\n                        rec['pathways'].append(pathway_name)\n\n        # If records has pathways it would update them\n        if rec['pathways']:\n            stats['with_pathways'] += 1\n            pathway_count += 1\n\n    # After processing all records, print count\n    pathway_count = sum(1 for rec in ec_records if rec['pathways'])\n    print(f\"Added pathway information to {pathway_count} records\")\n\n    # Add enzyme class info\n    for rec in ec_records:\n        try:\n            ec_number = rec['ec_number']\n            ec_prefix = '.'.join(ec_number.split('.')[:2])\n\n            # Try exact match first\n            if ec_prefix in enzyme_class:\n                rec['enzyme_class'] = enzyme_class[ec_prefix]\n            # Then try pattern match\n            else:\n                pattern_key = f\"{ec_prefix}.-.-\"\n                if pattern_key in enzyme_class:\n                    rec['enzyme_class'] = enzyme_class[pattern_key]\n        except Exception as e:\n            print(f\"Error processing enzyme class for {rec.get('ec_number')}: {e}\")\n\n    # Add metal_involved & corrosion_mechanisms\n    for rec in ec_records:\n        try:\n            # Combine name and class text properly\n            names_text = ' '.join(rec.get('enzyme_names', []))\n            class_text = rec.get('enzyme_class', '')\n            all_text = f\"{names_text} {class_text}\".lower()\n\n            # Add reaction text for more context\n            reaction_text = ' '.join([r.get('equation', '') for r in rec['reactions']])\n            all_text += f\" {reaction_text.lower()}\"\n\n            # Reconcile BRENDA metals with text mining\n            detected_metals = {}\n\n            # First add metals from BRENDA\n            for metal in rec.get('metals_from_brenda', []):\n                detected_metals[metal] = True\n\n            # Then add metals from text mining\n            for metal, terms in metal_terms.items():\n                if any(term.lower() in all_text for term in terms):\n                    detected_metals[metal] = True\n\n            # Store consolidated metals\n            rec['metals_involved'] = list(detected_metals.keys())\n\n            if detected_metals:\n                stats['with_metal_involvement'] += 1\n\n            rec['metals_consolidated']= consolidate_metal_terms(\n                rec.get('metals_from_brenda', []),\n                rec.get('metals_involved', [])\n            )\n            # corrosion_mechanisms (use a set for efficiency)\n            corrosion_mechs = set()\n            for mech, terms in corrosion_mechanisms.items():\n                if any(term.lower() in all_text for term in terms):\n                    corrosion_mechs.add(mech)\n\n            rec['corrosion_mechanisms'] = list(corrosion_mechs)\n\n            if corrosion_mechs:\n                stats['with_corrosion_mechanisms'] += 1\n\n        except Exception as e:\n            print(f\"Error processing metal/mechanisms data for {rec.get('ec_number')}: {e}\")\n\n    # Add metal binding potential from MetalPDB\n    try:\n        # Assuming parse_metalpdb_xml and extract_metal_coordination_patterns are defined\n        metal_binding_data = parse_metalpdb_xml()\n        metal_patterns = extract_metal_coordination_patterns(metal_binding_data)\n\n        print(f\"Extracted coordination patterns for {len(metal_patterns.get('coordination', {}))} metal-coordination environments\")\n        print(f\"Extracted residue binding patterns for {len(metal_patterns.get('residue_binding', {}))} metals\")\n\n        # Add metal binding information to records\n        for rec in ec_records:\n            rec['metal_binding_info'] = {}\n\n            # Check metals from BRENDA or detected in text\n            all_metals = set(rec.get('metals_from_brenda', []) + rec.get('metals_involved', []))\n\n            for metal in all_metals:\n                # Try to map to standard symbol\n                for metal_name, symbol in metal_mapping.items():\n                    if metal_name in metal.lower() or symbol.lower() in metal.lower():\n                        # Check if we have binding data for this metal\n                        if symbol in metal_patterns.get('residue_binding', {}):\n                            # Get top binding residues\n                            residue_counts = metal_patterns['residue_binding'][symbol]\n                            top_residues = sorted(residue_counts.items(), key=lambda x: x[1], reverse=True)[:5]\n\n                            rec['metal_binding_info'][symbol] = {\n                                'common_residues': [res for res, count in top_residues],\n                                'binding_count': sum(residue_counts.values())\n                            }\n\n                            # Check for common coordination geometries\n                            geometries = [\n                                key.split('_')[2:] for key, count in metal_patterns['coordination'].items()\n                                if key.startswith(f\"{symbol}_\")\n                            ]\n                            if geometries:\n                                rec['metal_binding_info'][symbol]['common_geometries'] = geometries[:3]\n\n        # Add this to corrosion relevance calculation\n        corrosion_metals = ['Fe', 'Mn', 'Cu', 'Ni', 'Co', 'Zn', 'Al', 'Cr']\n        for rec in ec_records:\n            # Add binding score to corrosion relevance\n            binding_score = sum(\n                2 for metal in corrosion_metals\n                if metal in rec.get('metal_binding_info', {})\n            )\n\n            # Update corrosion score\n            if 'corrosion_relevance_score' in rec:\n                rec['corrosion_relevance_score'] += binding_score\n            else:\n                rec['corrosion_relevance_score'] = binding_score\n\n    except Exception as e:\n        print(f\"Error processing metal binding data: {e}\")\n\n    # Integrate KO Hierarchy\n    if 'D' in ko_hierarchy:\n        for ko, info in ko_hierarchy.get('D', {}).items():\n            for ec in info.get('ec_numbers', []):\n                # find matching records\n                for rec in ec_records:\n                    if rec['ec_number'] == ec:\n                        try:\n                            parent_c = info.get('parent')\n                            if parent_c and 'C' in ko_hierarchy and parent_c in ko_hierarchy['C']:\n                                path_info = ko_hierarchy['C'][parent_c]\n                                parent_b = path_info.get('parent')\n                                if parent_b and 'B' in ko_hierarchy and parent_b in ko_hierarchy['B']:\n                                    hi_category = ko_hierarchy['B'][parent_b].get('name', '')\n                                    pathway = path_info.get('name', '')\n\n                                    # Use sets to efficiently track unique values\n                                    if pathway and pathway not in rec['pathways']:\n                                        rec['pathways'].append(pathway)\n\n                                    hierarchy = f\"{hi_category} > {pathway}\"\n                                    if hierarchy and hierarchy not in rec['hierarchy']:\n                                        rec['hierarchy'].append(hierarchy)\n                        except Exception as e:\n                            print(f\"Error processing KO hierarchy for {rec.get('ec_number')}: {e}\")\n\n    # Calculate corrosion relevance score\n    for rec in ec_records:\n        try:\n            # Base score on metal involvement and corrosion mechanisms\n            metal_score = len(rec.get('metals_involved', [])) * 1.5\n            mech_score = len(rec.get('corrosion_mechanisms', [])) * 2\n            pathway_score = 0\n\n            # Add scores for relevant pathways\n            corrosion_pathway_terms = corrosion_pathway_terms = ['iron', 'sulfur', 'oxide', 'corrosion', 'metal', 'biofilm',\n                              'ochre', 'acid', 'rust', 'precipitation', 'electron transfer',\n                              'redox', 'siderophore', 'chelation', 'acidification',\n                              'hydrogen', 'oxygen consumption', 'degradation']\n\n            for pathway in rec.get('pathways', []):\n                if any(term in pathway.lower() for term in corrosion_pathway_terms):\n                    pathway_score += 1\n\n            # Calculate final score\n            rec['corrosion_relevance_score'] = metal_score + mech_score + pathway_score\n\n            # Categorize\n            if rec['corrosion_relevance_score'] >= 5:\n                rec['corrosion_relevance'] = 'high'\n            elif rec['corrosion_relevance_score'] >= 2:\n                rec['corrosion_relevance'] = 'medium'\n            else:\n                rec['corrosion_relevance'] = 'low'\n\n        except Exception as e:\n            print(f\"Error calculating corrosion score for {rec.get('ec_number')}: {e}\")\n            rec['corrosion_relevance_score'] = 0\n            rec['corrosion_relevance'] = 'unknown'\n\n    # Filter records without content\n    filtered_ec_records = []\n    for record in ec_records:\n        protein_name = record.get('protein_name', \"\").lower()\n        enzyme_names = record.get('enzyme_names', [])\n        ec_number = record.get('ec_number', \"\")\n\n        # Condition 1: At least one valid identifier must be present\n        has_valid_protein = \"uncharacterized\" not in protein_name and len(protein_name) > 2\n        has_valid_enzyme = any(len(name) > 2 for name in enzyme_names)\n        has_valid_ec = ec_number.count('.') == 3 and all(part.isdigit() for part in ec_number.split('.'))\n\n        # Condition 2: Check for valuable data that should be preserved\n        has_mechanisms = len(record.get('corrosion_mechanisms', [])) > 0\n        has_pathways = len(record.get('pathways', [])) > 0\n        has_metal_involvement = len(record.get('metals_consolidated', [])) > 0\n\n        # Include record if it meets either condition\n        if (has_valid_protein or has_valid_enzyme or has_valid_ec) or \\\n           (has_mechanisms or has_pathways or has_metal_involvement):\n            filtered_ec_records.append(record)\n\n    # Replace original list with filtered version\n    ec_records = filtered_ec_records\n\n    # Print summary statistics\n    print(\"\\nMetabolism Database Summary:\")\n    print(f\"Total enzyme records: {stats['total_enzymes']}\")\n    print(f\"Records with BRENDA data: {stats['with_brenda_data']} ({stats['with_brenda_data']/stats['total_enzymes']*100:.1f}%)\")\n    print(f\"Records with reactions: {stats['with_reactions']} ({stats['with_reactions']/stats['total_enzymes']*100:.1f}%)\")\n    print(f\"Records with pathways: {stats['with_pathways']} ({stats['with_pathways']/stats['total_enzymes']*100:.1f}%)\")\n    print(f\"Records with KO terms: {stats['with_ko']} ({stats['with_ko']/stats['total_enzymes']*100:.1f}%)\")\n    print(f\"Records with metal involvement: {stats['with_metal_involvement']} ({stats['with_metal_involvement']/stats['total_enzymes']*100:.1f}%)\")\n    print(f\"Records with corrosion mechanisms: {stats['with_corrosion_mechanisms']} ({stats['with_corrosion_mechanisms']/stats['total_enzymes']*100:.1f}%)\")\n\n    # Validate the data - check for missing essential fields\n    validation_issues = []\n    for i, rec in enumerate(ec_records):\n        if not rec.get('ec_number'):\n            validation_issues.append(f\"Record {i} missing EC number\")\n        if not rec.get('enzyme_names'):\n            validation_issues.append(f\"EC {rec.get('ec_number')} missing enzyme names\")\n\n    if validation_issues:\n        print(\"\\nValidation Issues:\")\n        for issue in validation_issues[:10]:  # Show first 10 issues\n            print(f\"- {issue}\")\n        if len(validation_issues) > 10:\n            print(f\"...and {len(validation_issues) - 10} more issues\")\n    else:\n        print(\"\\nValidation: All records have essential fields.\")\n\n    return ec_records","metadata":{"id":"Smqh2fy8-PJP","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Takes around 10 min\nec_records = create_metabolism_database()","metadata":{"id":"G_pX1Jqj4KNZ","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ec_metadata = pd.DataFrame(ec_records)","metadata":{"id":"HgVjVgyVjib0","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Loaded pathway mappings for 3889 EC numbers   \nLoaded: 8235 enzymes, 578 pathways, 6710 BRENDA entries   \nAdded pathway information to 3888 records   \nExtracted coordination patterns for 1586 metal-coordination environments   \nExtracted residue binding patterns for 65 metals   \n\nMetabolism Database Summary:   \nTotal enzyme records: 8235   \nRecords with BRENDA data: 6710 (81.5%)  \nRecords with reactions: 6405 (77.8%)  \nRecords with pathways: 3888 (47.2%)  \nRecords with KO terms: 4873 (59.2%)  \nRecords with metal involvement: 7873 (95.6%)  \nRecords with corrosion mechanisms: 6131 (74.5%)  \n\nValidation: All records have essential fields..  ","metadata":{"id":"2TO-o4FC3933"}},{"cell_type":"code","source":"ec_metadata.head()","metadata":{"id":"RmQeLU0ZE1_g","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Saving the database","metadata":{"id":"yCE1edxRX7Sf"}},{"cell_type":"code","source":"# Making sure the output directory exists\noutput_large = Path(\"/kaggle/working/output_large\")\n\n# Save to JSON with timing\njson_path = output_large / \"ec_records.json\"\nprint(f\"Starting JSON save to {json_path}...\")\nstart_time = time.time()\n\ntry:\n    with open(json_path, 'w') as f:\n        json.dump(ec_records, f)\n\n    end_time = time.time()\n    elapsed = end_time - start_time\n    size_mb = os.path.getsize(json_path) / 1024 / 1024\n\n    print(f\"Successfully saved to {json_path} in {elapsed:.2f} seconds ({size_mb:.2f} MB)\")\nexcept Exception as e:\n    print(f\"Error saving to JSON: {e}\")","metadata":{"id":"1TknGS_gE1_h","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"stop_monitoring()\nplot_resource_log()\n\n# Rename previous log to preserve it\nimport os\nos.rename('resource_usage_log.csv', 'section1_resource_log.csv')\n\n# Start fresh monitoring for next section\nstart_monitoring(interval_seconds=60)\n","metadata":{"id":"gslwYmAyyWJ9","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Corrosion-Specific Filtering:\nI filter proteins that are involved in key corrosion mechanisms (electron transfer, biofilm formation, sulfate reduction, iron oxidation, etc.). I classify proteins into different MIC categories (e.g., redox proteins, sulfate reducers, oxidizers) and identify those that are common across all categories versus those enriched in specific groups.\n\nStatistical Testing:\nI build a table of protein–genus combinations with their classifications (including metabolism, pathway, metal interaction, and MIC function). I then perform statistical tests (Kruskal–Wallis with appropriate post-hoc tests and FDR corrections) to identify which combinations show significant differential abundance between the MIC categories. Only proteins with a significant difference will be carried forward for further analysis.","metadata":{"id":"pm0Bj8j71rUw"}},{"cell_type":"markdown","source":"ec_records: [{'ec_number': '1.1.1.1',\n  'enzyme_names': ['alcohol dehydrogenase',\n   'aldehyde reductase',\n   'ADH',\n   'alcohol dehydrogenase (NAD)',\n   'aliphatic alcohol dehydrogenase',\n   'ethanol dehydrogenase',\n   'NAD-dependent alcohol dehydrogenase',\n   'NAD-specific aromatic alcohol dehydrogenase',\n   'NADH-alcohol dehydrogenase',\n   'NADH-aldehyde dehydrogenase',\n   'primary alcohol dehydrogenase',\n   'yeast alcohol dehydrogenase'],","metadata":{"id":"3n7sW03WDVOP"}},{"cell_type":"code","source":"# Load from JSON with timing\njson_path = output_large/ \"ec_records.json\"\nprint(f\"Starting JSON load from {json_path}...\")\nstart_time = time.time()\n\nwith open(json_path, 'r') as f:\n    ec_records = json.load(f)\n\nend_time = time.time()\nelapsed = end_time - start_time\nprint(f\"Loaded {len(ec_records)} records from JSON in {elapsed:.2f} seconds\")","metadata":{"id":"Sf9ZTMVIE1_i","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ECcontri_Uniprot_path = output_large / 'ECcontri_Uniprot.parquet'\nECcontri_Uniprot = pd.read_parquet(ECcontri_Uniprot_path)","metadata":{"id":"spPAz5h286uL","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 9.3.  Building Enriched Dataframe of ECcontri\n","metadata":{"id":"6zqpU2g8Xpkv"}},{"cell_type":"code","source":"def enrich_eccontri_data(eccontri_df, ec_records):\n    \"\"\"\n    Enrich the ECcontri_Uniprot dataframe with complete information from ec_records dictionary\n\n    Parameters:    eccontri_df : pandas DataFrame original ECcontri_Uniprot data with EC numbers in format EC:x.x.x.x\n                   ec_records : list of Dictionary where keys are EC numbers (without 'EC:' prefix) and values are metadata dictionaries\n\n    Returns:       enriched_df : pandas DataFrame with additional metadata columns\n    \"\"\"\n    # Make a copy to avoid modifying the original\n    enriched_df = eccontri_df.copy()\n\n    start_time = time.time()\n\n    # Create dictionaries for faster lookups\n    print(\"Creating lookup dictionaries...\")\n\n    # EC number dictionary\n    ec_dict = {record['ec_number']: record for record in ec_records if 'ec_number' in record}\n    print(f\"Created EC dictionary with {len(ec_dict)} entries\")\n\n    # Protein name dictionary\n    protein_name_dict = {}\n    for record in ec_records:\n        enzyme_names = record.get('enzyme_names', [])\n        if isinstance(enzyme_names, list):\n            for name in enzyme_names:\n                if name:  # Skip empty names\n                    protein_name_dict[name.lower()] = record\n        elif enzyme_names:  # If it's a string and not empty\n            protein_name_dict[str(enzyme_names).lower()] = record\n    print(f\"Created protein name dictionary with {len(protein_name_dict)} entries\")\n\n    # Create a mapping dictionary to store all matches\n    idx_to_metadata = {}\n\n    # Add all metadata columns\n    metadata_columns = ['enzyme_names', 'enzyme_class', 'pathways', 'hierarchy',\n                        'metals_involved', 'metals_consolidated', 'corrosion_mechanisms',\n                        'corrosion_relevance_score', 'corrosion_relevance']\n\n    for col in metadata_columns:\n        enriched_df[col] = None\n\n    # Define metal and corrosion terms within the function\n    metal_terms = {\n        'iron': ['iron', 'iron reduction','fe', 'ferrous', 'ferric', 'heme', 'iron-sulfur', 'Fe2+', 'Fe3+', 'rust', 'ochre', 'iron oxide', 'iron precipitation', 'siderophore', 'ferritin'],\n        'sulfur': ['sulfate', 'sulfide', 'thiosulfate', 'S-S', 'sulfur', 'sulfite', 'sulfonate', 'cysteine', 'methionine'],\n        'hydrogen': ['hydrogen', 'hydrogenase', 'h2', 'hydrogen uptake', 'hydrogen evolution', 'proton reduction'],\n        'manganese': ['Mn2+', 'manganese', 'mn', 'manganous', 'manganic', 'manganese oxidation', 'metal oxide', 'metal oxide', 'manganese oxide', 'MnO2'],\n        'biofilm': ['exopolysaccharide', 'biofilm', 'adhesin', 'eps', 'polysaccharide', 'extracellular matrix', 'colonization', 'attachment', 'surface adherence'],\n        'copper': ['Cu+', 'Cu2+', 'copper', 'cupric', 'cuprous', 'copper oxide', 'copper corrosion'],\n        'nickel': ['Ni2+', 'nickel', 'nickelous', 'nickel oxidation', 'nickel reduction'],\n        'cobalt': ['Co2+', 'cobalt', 'cobaltous', 'cobalamin', 'vitamin B12'],\n        'calcium': ['Ca2+', 'calcium', 'calcium carbonate', 'calcite', 'calcium precipitation'],\n        'Mo': ['Mo', 'molybdenum', 'molybdopterin', 'molybdenum cofactor'],\n        'V5+': ['V5+', 'vanadium', 'vanadate', 'vanadyl'],\n        'Al3+': ['Al3+', 'aluminum', 'aluminate', 'aluminum oxide'],\n        'Cr3+': ['Cr3+', 'chromium', 'chromate', 'dichromate', 'chromium oxide'],\n        'zinc': ['Zn2+', 'zinc', 'zinc finger', 'zinc oxide'],\n        'sodium': ['Na+', 'sodium', 'NaCl', 'sodium transport', 'sodium gradient'],\n        'potassium': ['K+', 'potassium', 'KCl','potassium transport', 'potassium channel'],\n        'selenium': ['selenium', 'Se', 'selenocysteine', 'selenoprotein', 'selenite'],\n        'barium': ['Ba2+', 'barium', 'barium sulfate', 'barite'],\n        'phosphate': ['HPO4-2', 'PO4-3', 'phosphate', 'phosphates'],\n        'nitrate': ['NO3-', 'nitrate', 'nitrates'],\n        'nitrite': ['NO2-', 'nitrite', 'nitrites'],\n        'chloride': ['Cl-', 'chloride', 'chlorine'],\n        'magnesium': ['Mg2+', 'magnesium', 'magnesium oxide'],\n        'chlorine': ['Cl-', 'chloride', 'chlorine'],\n        }\n\n    # Corrosion mechanism classification\n    corrosion_mechanisms = {\n        'direct_eet': ['cytochrome', 'electron transfer', 'conductive pili', 'nanowire', 'mtrABC', 'omcS','oxidoreductase',  'redox', 'reductase', 'oxidase', 'electron conduit', 'direct electron transfer'],\n        'indirect_eet': ['shuttle', 'mediator', 'redox mediator', 'electron shuttle', 'flavin', 'quinone', 'humic substance'],\n        'acid_production': ['acid', 'acidification', 'fermentation', 'lactic acid', 'formic acid', 'acetic acid','oxalic acid', 'organic acid', 'acetate production', 'lactate metabolism', 'formate production', 'proton generation', 'low pH'],\n        'h2_consumption': ['hydrogenase', 'hydrogen uptake', 'hydrogen consumption', 'h2', 'H2 oxidation', 'H2ase', 'hydrogen metabolism'],\n        'o2_consumption': ['oxidase', 'oxygen reduction', 'aerobic respiration','oxygen reduc', 'aerobic respiration', 'oxygen consum', 'oxygen scavenging', 'oxygen stress', 'oxidative phosphorylation'],\n        'biofilm_formation': ['polysaccharide', 'adhesin', 'biofilm', 'EPS', 'extracellular polymeric substance', 'curli''exopolymer', 'extracellular matrix', 'adhesion', 'colonization', 'attachment', 'surface adherence', 'biofilm maturation'],\n        'sulfur_metabolism': ['sulfate reduc', 'sulfide', 'sulfite', 'thiosulfate', 'sulfur oxidation', 'SRB', 'sulfur disproportionation', 'sulfate-reducing bacteria', 'sulfur respiration'],\n        'metal_transformation': ['iron reduction', 'manganese oxidation', 'metal oxide', 'ochre formation', 'iron oxide deposits', 'iron precipitation', 'rust formation', 'metal deposition', 'metal solubilization', 'mineral dissolution', 'mineral precipitation'],\n        'iron_metabolism': ['iron reduc', 'ferric reduc', 'iron oxid', 'ferrous oxid', 'iron uptake', 'iron transport', 'iron storage', 'iron homeostasis', 'siderophore production'],\n        'metal_chelation': ['siderophore', 'metal binding', 'chelator', 'metallophore', 'iron complex', 'metal transport', 'chelation', 'metal complexation', 'metal sequestration'],\n        'carbon_metabolism': ['carbon fixation', 'carbon utilization', 'carbohydrate metabolism', 'glycolysis', 'TCA cycle', 'carbon flux', 'carbon assimilation'],\n        'ph_modulation': ['acid tolerance', 'alkaline tolerance', 'proton pump', 'pH homeostasis', 'pH stress', 'pH regulation', 'acid resistance']\n    }\n    # a boolean 'has_metal' column\n    enriched_df['has_metal'] = False\n\n     # Define progress reporting\n    total_rows = len(enriched_df)\n    log_interval = max(1, min(10000, total_rows // 20))  # Log at most 20 times, minimum every 10000 rows\n\n    print(f\"Processing {total_rows} rows with logging every {log_interval} rows\")\n\n    # try protein name matches\n    print(\"Performing protein name matches...\")\n    # Get rows without EC+Genus matches\n    remaining_indices = set(enriched_df.index) - set(idx_to_metadata.keys())\n    mask_remaining = enriched_df.index.isin(remaining_indices)\n    mask_valid_protein = enriched_df['protein_name'].notna() & (enriched_df['protein_name'] != \"Uncharacterized protein\")\n    mask_protein_match = mask_remaining & mask_valid_protein\n\n    # This part still needs row-by-row processing for fuzzy matching\n    protein_matches = 0\n    for idx in enriched_df.index[mask_protein_match]:\n        protein_name = enriched_df.loc[idx, 'protein_name'].lower()\n\n        # Direct lookup in protein name dictionary\n        if protein_name in protein_name_dict:\n            idx_to_metadata[idx] = protein_name_dict[protein_name]\n            protein_matches += 1\n        else:\n            # Try partial matches\n            for name, record in protein_name_dict.items():\n                if name in protein_name or protein_name in name:\n                    idx_to_metadata[idx] = record\n                    protein_matches += 1\n                    break\n\n    print(f\"Found {protein_matches} protein name matches\")\n\n    # For any remaining rows, try EC-only matching\n    print(\"Performing EC-only matches...\")\n    # Get rows without matches so far\n    remaining_indices = set(enriched_df.index) - set(idx_to_metadata.keys())\n    mask_remaining = enriched_df.index.isin(remaining_indices)\n    mask_valid_ec = enriched_df['EC'].notna()\n    mask_ec_match = mask_remaining & mask_valid_ec\n\n    ec_only_matches = 0\n    for idx in enriched_df.index[mask_ec_match]:\n        ec_num = enriched_df.loc[idx, 'EC']\n        if ec_num in ec_dict:\n            idx_to_metadata[idx] = ec_dict[ec_num]\n            ec_only_matches += 1\n\n    print(f\"Found {ec_only_matches} EC-only matches\")\n\n    # Apply all metadata in one go based on the matches we found\n    print(\"Applying metadata to matched rows...\")\n    for idx, metadata in idx_to_metadata.items():\n\n        # Only proceed if we have metadata (either from EC or from protein/enzyme name)\n        if metadata is not None:\n\n            # Add basic metadata\n            if 'enzyme_names' in metadata and metadata['enzyme_names']:\n                if isinstance(metadata['enzyme_names'], list):\n                    enriched_df.at[idx, 'enzyme_names'] = '; '.join(metadata['enzyme_names'])\n                else:\n                    enriched_df.at[idx, 'enzyme_names'] = str(metadata['enzyme_names'])\n\n            if 'enzyme_class' in metadata and metadata['enzyme_class']:\n                enriched_df.at[idx, 'enzyme_class'] = metadata['enzyme_class']\n\n            if 'pathways' in metadata and metadata['pathways']:\n                if isinstance(metadata['pathways'], list):\n                    enriched_df.at[idx, 'pathways'] = '; '.join(metadata['pathways'])\n                else:\n                    enriched_df.at[idx, 'pathways'] = str(metadata['pathways'])\n\n            if 'hierarchy' in metadata and metadata['hierarchy']:\n                if isinstance(metadata['hierarchy'], list):\n                    enriched_df.at[idx, 'hierarchy'] = '; '.join(metadata['hierarchy'])\n                else:\n                    enriched_df.at[idx, 'hierarchy'] = str(metadata['hierarchy'])\n\n            # add for the consolidated metals field:\n            if 'metals_consolidated' in metadata and metadata['metals_consolidated']:\n                if isinstance(metadata['metals_consolidated'], list):\n                    enriched_df.at[idx, 'metals_consolidated'] = '; '.join(metadata['metals_consolidated'])\n                else:\n                    enriched_df.at[idx, 'metals_consolidated'] = str(metadata['metals_consolidated'])\n\n            # add corrosion_relevance category:\n            if 'corrosion_relevance' in metadata:\n                enriched_df.at[idx, 'corrosion_relevance'] = metadata['corrosion_relevance']\n\n                # Directly use the metadata from ec_records\n            if 'metals_involved' in metadata and metadata['metals_involved']:\n                if isinstance(metadata['metals_involved'], list):\n                    enriched_df.at[idx, 'metals_involved'] = '; '.join(metadata['metals_involved'])\n                    enriched_df.at[idx, 'has_metal'] = len(metadata['metals_involved']) > 0\n                else:\n                    enriched_df.at[idx, 'metals_involved'] = str(metadata['metals_involved'])\n                    enriched_df.at[idx, 'has_metal'] = bool(metadata['metals_involved'])\n\n            if 'corrosion_mechanisms' in metadata and metadata['corrosion_mechanisms']:\n                if isinstance(metadata['corrosion_mechanisms'], list):\n                    enriched_df.at[idx, 'corrosion_mechanisms'] = '; '.join(metadata['corrosion_mechanisms'])\n                else:\n                    enriched_df.at[idx, 'corrosion_mechanisms'] = str(metadata['corrosion_mechanisms'])\n\n            if 'corrosion_relevance_score' in metadata:\n                try:\n                    # Ensure score is converted to float\n                    enriched_df.at[idx, 'corrosion_relevance_score'] = float(metadata['corrosion_relevance_score'])\n                except (ValueError, TypeError):\n                    print(f\"Row {idx}: Could not convert score {metadata['corrosion_relevance_score']} to float\")\n                    enriched_df.at[idx, 'corrosion_relevance_score'] = None\n\n    # Final report\n    end_time = time.time()\n    total_time = end_time - start_time\n    print(f\"Completed enrichment in {total_time:.2f} seconds\")\n    print(f\"Processed {total_rows} rows at {total_rows/total_time:.1f} rows/second\")\n\n    # Count non-null values in the metadata columns to see success rate\n    metadata_counts = {col: enriched_df[col].notnull().sum() for col in metadata_columns}\n    print(\"\\nMetadata population statistics:\")\n    for col, count in metadata_counts.items():\n        print(f\"  {col}: {count} rows ({count/total_rows*100:.1f}%)\")\n\n    return enriched_df","metadata":{"id":"--jwekk9SHE0","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Enrich the data directly from EC records dictionary > 20 minutes @ 12GB\npre_ECcontri_Uniprot_enriched= enrich_eccontri_data(ECcontri_Uniprot, ec_records)","metadata":{"id":"E0B3BgFljnTw","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Count occurrences of each value in both columns\nenzyme_counts = pre_ECcontri_Uniprot_enriched['enzyme_names'].value_counts()\nprotein_counts = pre_ECcontri_Uniprot_enriched['protein_name'].value_counts()\n\n# Get the number of unique values in each column\nenzyme_unique_count = pre_ECcontri_Uniprot_enriched['enzyme_names'].nunique()\nprotein_unique_count = pre_ECcontri_Uniprot_enriched['protein_name'].nunique()\n\n# Print the results\nprint(\"Enzyme Names - Unique Values:\", enzyme_unique_count)\nprint(\"Enzyme Names - Top 5 Occurrences:\\n\", enzyme_counts.head())\nprint(\"\\nProtein Names - Unique Values:\", protein_unique_count)\nprint(\"Protein Names - Top 5 Occurrences:\\n\", protein_counts.head())","metadata":{"id":"zOraHS9I8Adf","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Enzyme vs. Protein Naming Precision is reflected on this statistics. There are 1,788 unique enzyme names coming from the ec_record combined databases download in this study, in comparison to 4,327 unique protein names coming from the Api retrival from Uniprot. This suggests api retrieval can be more time consuming but rewarding.\n\nDistribution Pattern:\n\nThe top 5 enzyme names all appear exactly 2,407 times each this suggests these enzymes might be universally present across the microorganisms. These top enzymes are primarily involved in basic cellular processes (DNA replication and tRNA charging), which makes sense as they're fundamental to all cellular life\n\nUncharacterized Proteins:\n\n\"Uncharacterized protein\" are the name of the fields could no be retrieved and it is present as the most common protein name (272,716 occurrences)\n\nBifunctional Enzymes:\n\nFour of the top five protein names are bifunctional enzymes (containing \"[Includes: ... ; ...]\"), which means they perform two distinct enzymatic functions, often in related metabolic pathways.\n\nFundamental vs. Specialized Functions:\n\nTop enzymes are involved in fundamental cellular processes (translation, DNA replication). This suggests the dataset has good coverage of core metabolic functions.","metadata":{"id":"jrp1424qRNgT"}},{"cell_type":"code","source":"'''# To load ec_records:\nwith open(output_large / \"ec_records.json\", \"r\") as f:\n    ec_records = json.load(f)'''","metadata":{"id":"wRAQqvT8E1_l","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 9.4 Preparing the Enriched DF for entering the Pipeline\nBefore the data enters the pipeline it was noticed that around 18% of the protein-names were Uncharacterized proteins that is because the retrieval from uniprot was no totally suscessful, however in the process of enriching the data, the enzyme name was added with many other entities and so, the already enriched data will have those entries known as Uncharacterized, replaced by the enzyme name that was retrieved from the many other db and compiled on ec_records","metadata":{"id":"G4hqE4YG4vvU"}},{"cell_type":"code","source":"pre_ECcontri_Uniprot_enriched.columns","metadata":{"id":"mKzCbk5_6Yxh","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define category dict outside\ncategory_dict = Integrated_T.T.iloc[0, 0:-1].to_dict()\n\n# Define colors and categories\ncategory_colors = {1: '#008800',  # Dark green\n                   2: '#FF8C00',  # Dark orange\n                   3: '#FF0000'}   # Red\n\ncategories_labels = {1: 'Normal Operation',\n              2: 'Early Warning',\n              3: 'System Failure'}\n# Add Category on eccontry\ncat_ECcontri_Uniprot_enriched = pre_ECcontri_Uniprot_enriched.reset_index()\ncat_ECcontri_Uniprot_enriched['Category'] = cat_ECcontri_Uniprot_enriched['Sites'].map(category_dict)","metadata":{"id":"xndh2u1oqEWU","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Assign unique identifier\n- Each unique combination of Site, Genus, and Protein name is assigned a unique integer ID\n- IDs are generated using pandas' groupby().ngroup() function, which creates sequential integers\n- A separate mapping file (protein_genus_site_id_mapping.csv) preserves the connection between IDs and the original values for traceability\n- Add a flag column to know which of the last protein was name replaced","metadata":{"id":"kJcp5J3rGES3"}},{"cell_type":"code","source":"def assign_unique_identifier(df):\n    \"\"\"\n    Assigns a unique identifier to all protein-genus pairs using Site, Category, Genus,\n    and protein_name (including replaced enzyme names).\n    \"\"\"\n    df = df.copy()\n\n    # Flag uncharacterized proteins\n    df['was_uncharacterized'] = df['protein_name'] == 'Uncharacterized protein'\n\n    # First check for true duplicates using the correct unique combination\n    exact_dups = df.duplicated(['Sites', 'Genus', 'EC']).sum()\n    if exact_dups > 0:\n        print(f\"Warning: Found {exact_dups} duplicate rows based on Sites-Genus-EC\")\n    else:\n        print(\"✓ No duplicates found with Sites-Genus-EC criteria\")\n\n    # Split the dataframe into two parts\n    characterized = df[~df['was_uncharacterized']].copy()\n    uncharacterized = df[df['was_uncharacterized']].copy()\n\n    # Assign sequential IDs\n    characterized['idx'] = range(len(characterized))\n    uncharacterized['idx'] = range(len(characterized), len(characterized) + len(uncharacterized))\n\n    # Record the cutoff point\n    unchar_id_start = len(characterized)\n    print(f\"Original proteins have IDs 0-{unchar_id_start-1}\")\n    print(f\"Uncharacterized proteins have IDs {unchar_id_start}+\")\n\n    # Recombine the dataframes (uncharacterized at the end)\n    sorted_df = pd.concat([characterized, uncharacterized], ignore_index=True)\n    # After recombination, verify ordering\n    print(\"\\n=== ID Range Verification ===\")\n    print(f\"First 5 IDs: {sorted_df.index[:5].tolist()}\")\n    print(f\"Last 5 IDs: {sorted_df.index[-5:].tolist()}\")\n    print(f\"Last protein names:\\n{sorted_df.tail(5)['protein_name']}\")\n    # Create mapping dataframe for reference\n    id_mapping = sorted_df[['idx', 'Sites', 'Category', 'Genus', 'protein_name', 'EC', 'abund_contri', 'was_uncharacterized']].drop_duplicates()\n\n    # Set index and sort by ID (to ensure order is preserved)\n    sorted_df = sorted_df.set_index('idx').sort_index()\n\n    return sorted_df, id_mapping\nid_ECcontri_Uniprot_enriched, id_mapping = assign_unique_identifier(cat_ECcontri_Uniprot_enriched)","metadata":{"id":"uTiNDJ2RF59p","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"id_ECcontri_Uniprot_enriched.head()","metadata":{"id":"p0khm1pKh5YD","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def correct_uncharacterized_proteins(df):\n    \"\"\"\n    Corrects 'Uncharacterized protein' entries and handles abundance-aware duplicates.\n    \"\"\"\n    print(\"\\n=== Starting Protein Correction ===\")\n    print(f\"Input shape: {df.shape}\")\n\n    # Create a copy\n    corrected_df = df.copy(deep=False)\n\n    # Replacement\n    print(\"=== Replacing uncharacterized ===\")\n    unchar_mask = corrected_df['protein_name'] == 'Uncharacterized protein'\n    valid_mask = corrected_df['enzyme_names'].notna() & (corrected_df['enzyme_names'] != '')\n    corrected_df.loc[unchar_mask & valid_mask, 'protein_name'] = corrected_df.loc[unchar_mask & valid_mask, 'enzyme_names']\n\n    # Check for duplicates with accurate criteria\n    duplicates = corrected_df.duplicated(['Sites', 'Genus', 'EC'], keep=False)\n    if duplicates.any():\n        print(f\"Warning: Found {duplicates.sum()} duplicates after correction\")\n    else:\n        print(\"✓ All rows remain unique based on Sites-Genus-EC\")\n\n    return corrected_df\n\n# Apply correction\nECcontri_Uniprot_enriched = correct_uncharacterized_proteins(id_ECcontri_Uniprot_enriched)","metadata":{"id":"ciu8zphzPDXX","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ECcontri_Uniprot_enriched.head()","metadata":{"id":"bjO3IzZSiKhP","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Preserve my intended order of letting which were uncharacterised at the end 1.200.000\nECcontri_Uniprot_enriched.sort_values('idx', inplace=True)","metadata":{"id":"4o21zFxCw0_U","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Saving the new corrected dataframe\nECcontri_Uniprot_enriched.to_parquet(output_large / 'ECcontri_Uniprot_enriched.parquet')","metadata":{"id":"FHS2imGegOMv","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load colab already has corrected the name and put identifiers at this point and category included\nECcontri_Uniprot_enriched = pd.read_parquet(output_large / 'ECcontri_Uniprot_enriched.parquet')","metadata":{"execution":{"iopub.status.busy":"2025-03-30T18:06:03.073692Z","iopub.execute_input":"2025-03-30T18:06:03.074142Z","iopub.status.idle":"2025-03-30T18:06:05.918254Z","shell.execute_reply.started":"2025-03-30T18:06:03.074110Z","shell.execute_reply":"2025-03-30T18:06:05.917349Z"},"id":"BM42zmBmgOMv","trusted":true},"outputs":[],"execution_count":93},{"cell_type":"code","source":"'''del cat_ECcontri_Uniprot_enriched\ndel id_ECcontri_Uniprot_enriched\ndel pre_ECcontri_Uniprot_enriched\n'''\ngc.collect()","metadata":{"id":"pf0TmrNPwCuM","trusted":true,"execution":{"iopub.status.busy":"2025-03-28T16:13:07.804255Z","iopub.execute_input":"2025-03-28T16:13:07.804579Z","iopub.status.idle":"2025-03-28T16:13:08.277758Z","shell.execute_reply.started":"2025-03-28T16:13:07.804552Z","shell.execute_reply":"2025-03-28T16:13:08.276497Z"},"outputId":"53e45e31-c70b-486c-e705-5972ef6fb6f4"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ECcontri_Uniprot_enriched.head()","metadata":{"id":"LYaF0CJOpIoZ","trusted":true,"execution":{"iopub.status.busy":"2025-03-28T16:13:08.280249Z","iopub.execute_input":"2025-03-28T16:13:08.280590Z","iopub.status.idle":"2025-03-28T16:13:08.344470Z","shell.execute_reply.started":"2025-03-28T16:13:08.280563Z","shell.execute_reply":"2025-03-28T16:13:08.343139Z"},"outputId":"44ffaeee-4ad8-49a1-de37-cbf511ba7b9b"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Count occurrences of each value in both columns\nenzyme_counts = ECcontri_Uniprot_enriched['enzyme_names'].value_counts()\nprotein_counts = ECcontri_Uniprot_enriched['protein_name'].value_counts()\n\n# Get the number of unique values in each column\nenzyme_unique_count = ECcontri_Uniprot_enriched['enzyme_names'].nunique()\nprotein_unique_count = ECcontri_Uniprot_enriched['protein_name'].nunique()\n\n# Print the results\nprint(\"Enzyme Names - Unique Values:\", enzyme_unique_count)\nprint(\"Enzyme Names - Top 5 Occurrences:\\n\", enzyme_counts.head())\nprint(\"\\nProtein Names - Unique Values:\", protein_unique_count)\nprint(\"Protein Names - Top 5 Occurrences:\\n\", protein_counts.head())","metadata":{"id":"V59Z9_c0Q4ir","trusted":true,"execution":{"iopub.status.busy":"2025-03-30T18:06:15.439591Z","iopub.execute_input":"2025-03-30T18:06:15.439941Z","iopub.status.idle":"2025-03-30T18:06:16.193529Z","shell.execute_reply.started":"2025-03-30T18:06:15.439912Z","shell.execute_reply":"2025-03-30T18:06:16.192247Z"}},"outputs":[{"name":"stdout","text":"Enzyme Names - Unique Values: 1910\nEnzyme Names - Top 5 Occurrences:\n enzyme_names\nanthocyanidin-synthase; leucocyanidin oxygenase; leucocyanidin,2-oxoglutarate:oxygen oxido-reductase; ans                                                                                      88919\nmrna guanylyltransferase; rngtt; ceg1; mrna capping; messenger rna guanylyltransferase; protein                                                                                                80379\nzinc d-ala-d-ala carboxypeptidase; zn2+ g peptidase; d-alanyl-d-alanine hydrolase; d-alanyl-d-alanine-cleaving carboxypeptidase; dd-carboxypeptidase; g; dd-carboxypeptidase-transpeptidase    32902\nnitric-oxide-synthase; nos; nitric oxide-synthetase; endothelium-derived relaxation factor-forming; endothelium-derived relaxing factor-synthase; no-synthase; nadph-diaphorase                22872\npentalenolactone f-synthase; pend; pntd; ptld                                                                                                                                                  18316\nName: count, dtype: int64\n\nProtein Names - Unique Values: 5515\nProtein Names - Top 5 Occurrences:\n protein_name\nmultifunctional fusion protein                        5936\nbifunctional protein fold                             4617\ncoenzyme a biosynthesis bifunctional protein coabc    4367\nbifunctional purine biosynthesis protein purh         4310\nriboflavin biosynthesis protein                       4169\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":94},{"cell_type":"markdown","source":"The replacement of the Uncharacterised protein names has been suscessful and now it has been replaced by the enzyme names. The problem with the duplicates has been resolved. The uncharacterized proteins were 18% now there are none. The id were left as plain numbers so to make the computational load smaller since we working with small space more over the id is mean to trace done the combinations and it is no really involved in the calculations, it is more for tracing and mapping at the end.\n\nl-Histidine is one of the 20 standard proteinogenic amino acids present in proteins of all living organisms. Histidine biosynthesis seems to be conserved in all organisms including archaea (Lee et al., 2008),","metadata":{"id":"G67GpdlQauBT"}},{"cell_type":"markdown","source":"Making the category dictionary in case it starts from here","metadata":{"id":"0PVjx1_TrqhG"}},{"cell_type":"code","source":"# Define category dict outside\ncategory_dict = Integrated_T.T.iloc[0, 0:-1].to_dict()\n\n# Define colors and categories\ncategory_colors = {1: '#008800',  # Dark green\n                   2: '#FF8C00',  # Dark orange\n                   3: '#FF0000'}   # Red\n\ncategories_labels = {1: 'Normal Operation',\n              2: 'Early Warning',\n              3: 'System Failure'}","metadata":{"id":"8J-xTtj2rusm","trusted":true,"execution":{"iopub.status.busy":"2025-03-30T18:06:16.194982Z","iopub.execute_input":"2025-03-30T18:06:16.195390Z","iopub.status.idle":"2025-03-30T18:06:16.203054Z","shell.execute_reply.started":"2025-03-30T18:06:16.195350Z","shell.execute_reply":"2025-03-30T18:06:16.201646Z"}},"outputs":[],"execution_count":95},{"cell_type":"markdown","source":"## 9.5. Filtering pairs Bacteria-Protein by significance to the risk category\n\nThe analyze_corrosion_proteins function establishes a systematic framework for identifying protein-genus pairs associated with corrosion across different risk categories. The function begins by preparing the data, converting enzyme records to a searchable dictionary and mapping sites to risk categories, then proceeds to track which sites contain each protein-genus combination to maintain traceability throughout the analysis.\nAfter the initial preparation, the function orchestrates a series of analytical steps through specialized helper functions that reshape the data, perform statistical tests across risk categories, integrate biological metadata, prioritize markers based on combined statistical and biological significance, and finally organize results into specialized groups for interpretation. This sequential approach ensures a comprehensive assessment that considers both statistical significance and biological relevance, ultimately producing a multifaceted view of protein-genus pairs that may contribute to microbial corrosion processes under varying environmental conditions.\n\nThe abundance taken is the absolute contribution to abundance (abund_contri) this is because this metric is comparable across samples and reflex the biolofical influence without relativising it. Ultimately the goal is to identiy the most influencial bacteria.\nA separation of the significative inverse relationship is done and elsewhere analysed.","metadata":{"id":"Dq8C9vSywZjZ"}},{"cell_type":"code","source":"def analyze_corrosion_proteins(eccontri_df, alpha=0.05, balance_genera=False, per_genus_count=10, genus_to_threshold=None):\n    \"\"\"\n    Comprehensive analysis of protein-genus pairs for corrosion relevance\n\n    Parameters:\n    eccontri_df : pandas DataFrame The enriched ECcontri_Uniprot dataframe with EC, protein_name, Genus, abundance, etc.\n    ec_records : List of dictionaries with EC, protein_name metadata with corrosion relevance information\n    alpha : float,  Significance level for statistical tests (default: 0.05)\n    balance_genera : bool, Whether to balance representation across genera (default: False)\n    per_genus_count : intNumber of markers to include per genus if balancing (default: 10)\n    genus_to_threshold : dict, Optional mapping of genus to abundance thresholds for filtering (default: None)\n\n    Returns:\n     results :  Dictionary containing various analysis results\n    \"\"\"\n    print(\"Starting corrosion protein analysis...\")\n\n    # Use existing category mapping unconditionally to ensure it exists\n    category_dict = Integrated_T.T.iloc[0, 0:-1].astype(int).to_dict()\n\n    # Use existing category map\n    if 'Category' not in eccontri_df.columns:\n        eccontri_df['Category'] = eccontri_df['Sites'].map(category_dict)\n\n    print(f\"Analyzing {len(eccontri_df)} data points across {len(eccontri_df['Sites'].unique())} Sites...\")\n\n    # Create mapping of protein-genus pairs to their sites\n    protein_genus_sites = {}\n    site_groups = eccontri_df.groupby(['Genus', 'protein_name'])['Sites'].apply(list)\n    for (genus, protein), sites in site_groups.items():\n        protein_genus_sites[(genus, protein)] = list(set(sites))\n\n    # create base matrix\n    base_matrix = prepare_base_matrix_stats(eccontri_df)\n\n    # Reshape data for analysis and calculate abundance matrix\n    print(\"calculating abundancy pattern...\")\n    output_dir = \"pattern_analysis_results\"\n    pattern_df = process_in_resumable_chunks(base_matrix,  output_dir, eccontri_df, chunk_size=50000)\n\n    print(\"Merging pattern data with existing metadata...\")\n    # merging the pattern-stats dataframe with the eccontri df\n    eccontri_df = eccontri_df.reset_index(drop=False)\n\n    integrated_results = pd.merge(pattern_df, eccontri_df[['idx', 'enzyme_names', 'enzyme_class', 'pathways', 'hierarchy',\n                         'metals_involved', 'metals_consolidated', 'corrosion_mechanisms', 'corrosion_relevance_score',\n                         'corrosion_relevance', 'has_metal']], on='idx', how='left')\n\n    # Clasify ubiquitous, niche genus-protein names by mechanism, pathways\n    classified_results = classify_pathways_by_specificity(integrated_results)\n\n    # Separate positive and inverse patterns\n    print(\"Separating positive and inverse patterns by corr\")\n    increasing_results, inverse_results, constant_df = separate_by_correlation(classified_results)\n\n    # Prioritize positive results only\n    print(\"Prioritizing markers based on statistical and biological relevance...\")\n    prioritized_markers = prioritize_markers(increasing_results)\n\n    # Create marker groups based on whether we're using balanced markers or not\n    if balance_genera:\n        print(f\"Balancing genus representation (top {per_genus_count} per genus)...\")\n        balanced_markers = balance_genus_representation(\n            prioritized_markers,\n            per_genus_count=per_genus_count,\n            genus_thresholds=genus_to_threshold\n        )\n        print(f\"Created balanced dataset with {len(balanced_markers)} markers from {len(prioritized_markers['Genus'].unique())} genera\")\n\n        # Create marker groups from balanced markers\n        print(\"Creating specialized marker groups from balanced markers...\")\n        marker_groups = create_marker_groups(balanced_markers)\n    else:\n        # Create marker groups from prioritized markers (original behavior)\n        print(\"Creating specialized marker groups...\")\n        marker_groups = create_marker_groups(prioritized_markers)\n\n    print(\"Analysis complete!\")\n\n    results_dict = {\n        'pattern_data': pattern_df,\n        'integrated_results': integrated_results,\n        'classified_results': classified_results,\n        'increasing_markers': increasing_results,\n        'prioritized_markers': prioritized_markers,\n        'marker_groups': marker_groups,\n        'inverse_markers': inverse_results\n    }\n\n    # Add balanced markers if they were created\n    if balance_genera:\n        results_dict['balanced_markers'] = balanced_markers\n\n    return results_dict","metadata":{"execution":{"iopub.status.busy":"2025-03-30T18:06:16.204834Z","iopub.execute_input":"2025-03-30T18:06:16.205133Z","iopub.status.idle":"2025-03-30T18:06:16.224816Z","shell.execute_reply.started":"2025-03-30T18:06:16.205106Z","shell.execute_reply":"2025-03-30T18:06:16.223774Z"},"id":"aoMYUGIDoMc-","trusted":true},"outputs":[],"execution_count":96},{"cell_type":"markdown","source":"### Determining Abundance Patters\nThe determine_abundance_pattern function implements a sophisticated classification system for protein-genus abundance profiles across risk categories, prioritizing patterns with potential corrosion relevance.\n\nThis classification focuses on corrosion-relevant trends by highlighting proteins that proliferate in higher risk environments, those that spike during transitional conditions (category 2), and those exclusively abundant in severe corrosion conditions (category 3), while deprioritizing proteins with mixed patterns or those predominantly found in normal conditions (category 1).\n\nThe algorithm identifies distinct patterns\nwith 2 transitions between 3 categories, we have 3² = 9 possible patterns:\n\n1. cat1 < cat2 < cat3 (steadily increasing)  # Increasing  \n2. cat1 < cat2 = cat3 (increases then plateaus)  # Increases  \n3. cat1 < cat2 > cat3 (peaks at cat2) #Increases  to be decided by biology\n4. cat1 = cat2 < cat3 (plateaus then increases) # Increases    \n5. cat1 = cat2 = cat3 (consistent across all)  # Consistent to be droped  \n6. cat1 = cat2 > cat3 (plateaus then decreases)  # ambigous to be droped  \n7. cat1 > cat2 < cat3 (valley at cat2)  # mixed to be decided by biology\n8. cat1 > cat2 = cat3 (decreases then plateaus)  # Inverse df  \n9. cat1 > cat2 > cat3 (steadily decreasing)  # Inverse df  ","metadata":{"id":"XDxX599EwamS"}},{"cell_type":"code","source":"'''\ncat1 < cat2 < cat3 (increasing) # increasing\ncat1 < cat2 = cat3 (increases then plateaus) v# increasing\ncat1 < cat2 > cat3 but cat1 < cat3 (peak at cat2, but still higher at end) # increasing\ncat1 < cat2 > cat3 and cat1 = cat3 (perfect peak at cat2) # increasing\ncat1 < cat2 > cat3 and cat1 > cat3 (peak at cat2, ends lower) drop\ncat1 = cat2 < cat3 (plateaus then increases) # increasing\ncat1 = cat2 = cat3 (consistent across all) drop\ncat1 = cat2 > cat3 (plateaus then decreases) drop\ncat1 > cat2 > cat3 (decreasing) # decrising\ncat1 > cat2 = cat3 (decreases then plateaus) # decrising\ncat1 > cat2 < cat3 but cat1 > cat3 (valley at cat2, but still lower at end) #drop\ncat1 > cat2 < cat3 and cat1 = cat3 (perfect valley at cat2) #drop\ncat1 > cat2 < cat3 and cat1 < cat3 (valley at cat2, ends higher) # increasing'''","metadata":{"execution":{"execution_failed":"2025-03-30T18:38:54.170Z"},"id":"K1CbJl7OiFn4","trusted":true,"outputId":"c09a3b0b-d661-4c3d-e326-c85cda652b41"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Statistical Analysis and Pattern Recognition\nThe patterns previously analysed were implemented to find the pairs (genera-protein) that comply with the requirement of the pattern recognised as to make correlation with the risk category. With those patterns plus the statistical medians, mean and counts values, the samples were assessed to determine if they comply with the patterns and then get classified. To those results, a significance test was done to track non-parametrical data while also taking into account that the data is not only about absence/presence but abundance differences. Additionally, log2 fold change calculations between categories provided quantitative measures of abundance magnitude differences, helping prioritize proteins with substantial increases in higher risk categories. Ultimately, a mask was run in order to select the samples which were statistically significant <0.05 and also the ones only in category 3. In this way, the analysis can narrow down which samples are more relevant going forward.","metadata":{"id":"vK1j45XyoIpE"}},{"cell_type":"code","source":"ECcontri_Uniprot_enriched= ECcontri_Uniprot_enriched.drop(columns=\"index\").reset_index()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.171Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(ECcontri_Uniprot_enriched)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_contri = ECcontri_Uniprot_enriched.iloc[:5000, :].copy()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.171Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate the total abundance for each (Genus, protein_name) group\nsample_contri['total_abundance'] = sample_contri.groupby(['Genus', 'protein_name'])['norm_abund_contri'].transform('sum')\n# Create a mapping that counts the original entries per protein-genus pair\n# Add a duplication counter for each genus-protein pair\nsample_contri['dup_count'] = sample_contri.groupby(['Genus', 'protein_name']).cumcount()\n# Add a unique identifier combining genus, protein, and duplication count\nsample_contri['unique_id'] = sample_contri['Genus'] + '_' + sample_contri['protein_name'] + '_' + sample_contri['dup_count'].astype(str)\n# Create a mapping DataFrame before analysis\nmapping_df = sample_contri[['unique_id', 'Genus', 'protein_name', 'dup_count', 'idx', 'total_abundance']]","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.171Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def prepare_base_matrix_stats(eccontri_df):\n\n    # Aggregate keeping track of idx\n    base_matrix = eccontri_df.groupby(['Sites', 'Genus', 'protein_name', 'Category'], observed=True, as_index=False)['norm_abund_contri'].sum()\n    return base_matrix\n\n    return base_matrix\n    \n# Generate base matrix\nbase_matrix = prepare_base_matrix_stats(sample_contri) ","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.171Z"},"id":"gI8uGZud7qVU"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#base_matrix = prepare_base_matrix_stats(sample_contri) # ECcontri_Uniprot_enriched","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.171Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Pattern Analysis\nThis function serve to study the pattern distribution in the data outside the pipeline and it is here located, so to give a justification on the way the pattern analysis in the pipeline continues","metadata":{}},{"cell_type":"markdown","source":"It is seen that the data behaves on specificity rather than abundance on the granular level. It was seen previously that pathways and reactions were caracterised by abundance of each of them rather than for the specificity of these activities. Protein-genera presence seem to have an specificity given by the corrosion stage. \"This aligns with the fact that a cellular gene set has to be self sufficient in the sense that cells generally import metabolites but no functional proteins [Mushegian and Koonin, 1996].\" So for the proteins to be available for them, they have to produce their own, hence the ones with the genes to produce the proteins are the ones that survive . The presence of the genus-protein pair therefore are marked by the degree of corrosion being the pioonier species the first to colonise and have a maximum on category 2, as it seem in the data with the \"usual_taxa\" which agrees with the teory. \n ","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore', message='invalid value encountered in greater')\nwarnings.filterwarnings('ignore', message='invalid value encountered in less')\ndef perform_abundance_analysis(base_matrix, alpha=0.05, n_jobs=-1):\n    \"\"\"\n    Performs analysis on protein-genus pairs focusing on category-specific patterns.\n\n    This function:\n    1. Identifies patterns based on category presence/absence\n    2. Calculates prevalence and specificity metrics\n    3. Identifies category-specific and abundant proteins\n\n    Parameters:  base_matrix :  DataFrame with columns 'Genus', 'protein_name', 'Category', 'norm_abund_contri'\n    alpha : float, default=0.05   Threshold for prevalence significance\n    n_jobs : int, default=-1   Number of parallel jobs to run\n\n    Returns:\n    --------\n    tuple: (results_df, category_stats)\n        results_df: DataFrame with results and patterns\n        category_stats: Dictionary with summary statistics\n    \"\"\"\n    # Start timer\n    start_time = datetime.now()\n    print(f\"Processing started at {start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n    \n    # Get unique categories and genus-protein pairs\n    categories = sorted(base_matrix[\"Category\"].unique())\n    print(f\"Found {len(categories)} categories: {categories}\")\n\n    # define pairs\n    pairs = base_matrix[['Genus', 'protein_name']].drop_duplicates()\n    \n    print(f\"Total unique genus-protein pairs unique: {len(pairs)}\")\n\n    # Pre-compute statistics for all genus-protein-category combinations\n    print(\"Computing statistics for all genus-protein-category combinations...\")\n    grouped_stats = base_matrix.groupby(['Genus', 'protein_name', 'Category'])['norm_abund_contri'].agg([\n        ('mean', 'mean'),\n        ('median', 'median'),\n        ('std', 'std'),\n        ('count', 'count'),\n        ('sum', 'sum'),\n        ('values', lambda x: list(x))\n    ]).reset_index()\n    \n    # Calculate total samples per category for prevalence calculation\n    samples_per_category = base_matrix.groupby('Category')['Sites'].nunique().to_dict()\n    print(f\"Samples per category: {samples_per_category}\")\n\n    # Convert to dictionary for faster lookups\n    keys = list(zip(grouped_stats['Genus'], grouped_stats['protein_name'], grouped_stats['Category']))\n    values = grouped_stats[['mean', 'median', 'std', 'count', 'sum', 'values']].to_dict('records')\n    grouped_dict = dict(zip(keys, values))\n\n    # Create a pivot table to easily see category presence\n    category_pivot = grouped_stats.pivot_table(\n        index=['Genus', 'protein_name'],\n        columns='Category',\n        values='count',\n        fill_value=0\n    ).reset_index()\n\n    # Add columns for category presence\n    for cat in categories:\n        category_pivot[f'present_in_cat{cat}'] = category_pivot[cat] > 0\n\n    # Define function to process each pair\n    def process_pair(row_data):\n        # Get category presence for this pair\n        genus = row_data['Genus']\n        protein = row_data['protein_name']\n\n        pivot_row = category_pivot[\n            (category_pivot['Genus'] == genus) &\n            (category_pivot['protein_name'] == protein)]\n\n        if pivot_row.empty:\n            return None\n\n        pivot_row = pivot_row.iloc[0]\n\n        # Check which categories have data\n        present_cats = []\n        for cat in categories:\n            if pivot_row[f'present_in_cat{cat}']:\n                present_cats.append(cat)\n\n        # Skip if no categories have data default values.\n        if not present_cats:\n            return {'Genus': genus,'protein_name': protein,'pattern': 'no_category_data'}\n    \n        # Compile stats for each category\n        cat_stats = {}\n        for cat in categories:\n            key = (genus, protein, cat)\n            if key in grouped_dict:\n                cat_stats[f\"mean_cat{cat}\"] = grouped_dict[key]['mean']\n                cat_stats[f\"median_cat{cat}\"] = grouped_dict[key]['median']\n                cat_stats[f\"std_cat{cat}\"] = grouped_dict[key]['std']\n                cat_stats[f\"count_cat{cat}\"] = grouped_dict[key]['count']\n                cat_stats[f\"sum_cat{cat}\"] = grouped_dict[key]['sum']\n                \n                # Calculate prevalence (percentage of samples in this category where this protein appears)\n                if samples_per_category[cat] > 0:\n                    # Count distinct sites where this protein-genus pair appears in this category\n                    sites_with_pair = base_matrix[\n                        (base_matrix['Genus'] == genus) & \n                        (base_matrix['protein_name'] == protein) & \n                        (base_matrix['Category'] == cat)]['Sites'].nunique()\n                    \n                    cat_stats[f\"prevalence_cat{cat}\"] = sites_with_pair / samples_per_category[cat]\n                else:\n                    cat_stats[f\"prevalence_cat{cat}\"] = 0\n            else:\n                cat_stats[f\"mean_cat{cat}\"] = np.nan\n                cat_stats[f\"median_cat{cat}\"] = np.nan\n                cat_stats[f\"std_cat{cat}\"] = np.nan\n                cat_stats[f\"count_cat{cat}\"] = 0\n                cat_stats[f\"sum_cat{cat}\"] = 0\n                cat_stats[f\"prevalence_cat{cat}\"] = 0\n\n        # Determine pattern based on category presence\n        pattern = \"unknown\"\n        category_str = ''.join(str(c) for c in present_cats)\n\n        # First determine presence pattern\n        if category_str == '123':\n            presence_pattern = 'all_categories'\n        elif category_str == '12':\n            presence_pattern = 'cat1_and_cat2'\n        elif category_str == '13':\n            presence_pattern = 'cat1_and_cat3'\n        elif category_str == '23':\n            presence_pattern = 'cat2_and_cat3'\n        elif category_str == '1':\n            presence_pattern = 'only_cat1'\n        elif category_str == '2':\n            presence_pattern = 'only_cat2'\n        elif category_str == '3':\n            presence_pattern = 'only_cat3'\n        else:\n            presence_pattern = 'unknown'\n\n        # For pairs in all categories, determine abundance pattern\n        if presence_pattern == 'all_categories':\n            mean1 = cat_stats[\"mean_cat1\"]\n            mean2 = cat_stats[\"mean_cat2\"]\n            mean3 = cat_stats[\"mean_cat3\"]\n\n            if mean1 < mean2 and mean2 < mean3:\n                pattern = \"increasing_abundance\"\n            elif mean1 > mean2 and mean2 > mean3:\n                pattern = \"decreasing_abundance\"\n            elif mean1 < mean2 and mean2 > mean3:\n                pattern = \"peak_at_cat2\"\n            elif mean1 > mean2 and mean2 < mean3:\n                pattern = \"valley_at_cat2\"\n            else:\n                pattern = \"mixed_abundance\"\n        else:\n            # For pairs not in all categories, use the presence pattern\n            pattern = presence_pattern\n\n        # Calculate specificity score (how specific is this protein to its categories)\n        # Higher value = more specific to certain categories\n        total_abundance = sum(cat_stats[f\"sum_cat{cat}\"] for cat in categories)\n        if total_abundance > 0:\n            # Calculate Gini coefficient-like measure of specificity\n            # 0 = evenly distributed, 1 = present in only one category\n            abundances = [cat_stats[f\"sum_cat{cat}\"] for cat in categories]\n            abundances = [a/total_abundance for a in abundances if a > 0]\n            \n            if len(abundances) > 1:\n                specificity = 1 - (sum(abundances) / len(abundances)) / max(abundances)\n            else:\n                specificity = 1.0  # Completely specific to one category\n        else:\n            specificity = 0.0\n\n        # Create result dictionary\n        result = {\n            'Genus': genus,\n            'protein_name': protein,\n            'pattern': pattern,\n            'presence_pattern': presence_pattern,\n            'category_count': len(present_cats),\n            'categories': category_str,\n            'specificity': specificity, \n            **cat_stats}\n\n        return result\n\n    # Process all pairs using parallel processing\n    results = Parallel(n_jobs=n_jobs)(delayed(process_pair)(row) for _, row in pairs.iterrows())\n\n    print(f\"Generated {len(results)} valid results\")\n\n    # Create results DataFrame\n    if not results:\n        print(\"Warning: No valid results were produced.\")\n        return pd.DataFrame(), {}\n\n    results_df = pd.DataFrame(results)\n\n    # Add total abundance column (sum across all categories)\n    results_df['total_abundance'] = 0\n    for cat in categories:\n        results_df['total_abundance'] += results_df[f'sum_cat{cat}'].fillna(0)\n\n    # Calculate log2fc for pairs in multiple categories\n    # Create masks for valid comparisons\n    mask_cat1_cat2 = (results_df['count_cat1'] > 0) & (results_df['count_cat2'] > 0)\n    mask_cat1_cat3 = (results_df['count_cat1'] > 0) & (results_df['count_cat3'] > 0)\n    mask_cat2_cat3 = (results_df['count_cat2'] > 0) & (results_df['count_cat3'] > 0)\n\n    # Initialize with NaN\n    results_df['log2fc_cat2_vs_cat1'] = np.nan\n    results_df['log2fc_cat3_vs_cat1'] = np.nan\n    results_df['log2fc_cat3_vs_cat2'] = np.nan\n\n    # Only calculate for valid pairs using vectorized operations\n    if sum(mask_cat1_cat2) > 0:\n        cat1_values = results_df.loc[mask_cat1_cat2, 'mean_cat1'].values\n        cat2_values = results_df.loc[mask_cat1_cat2, 'mean_cat2'].values\n        cat1_values = np.maximum(cat1_values, 1e-10)\n        cat2_values = np.maximum(cat2_values, 1e-10)\n        results_df.loc[mask_cat1_cat2, 'log2fc_cat2_vs_cat1'] = np.log2(cat2_values / cat1_values)\n\n    if sum(mask_cat1_cat3) > 0:\n        cat1_values = results_df.loc[mask_cat1_cat3, 'mean_cat1'].values\n        cat3_values = results_df.loc[mask_cat1_cat3, 'mean_cat3'].values\n        cat1_values = np.maximum(cat1_values, 1e-10)\n        cat3_values = np.maximum(cat3_values, 1e-10)\n        results_df.loc[mask_cat1_cat3, 'log2fc_cat3_vs_cat1'] = np.log2(cat3_values / cat1_values)\n\n    if sum(mask_cat2_cat3) > 0:\n        cat2_values = results_df.loc[mask_cat2_cat3, 'mean_cat2'].values\n        cat3_values = results_df.loc[mask_cat2_cat3, 'mean_cat3'].values\n        cat2_values = np.maximum(cat2_values, 1e-10)\n        cat3_values = np.maximum(cat3_values, 1e-10)\n        results_df.loc[mask_cat2_cat3, 'log2fc_cat3_vs_cat2'] = np.log2(cat3_values / cat2_values)\n    # Print a progress update\n    print(f\"Added log2 fold change calculations for {sum(mask_cat1_cat2)} cat1-cat2 pairs, {sum(mask_cat1_cat3)} cat1-cat3 pairs, {sum(mask_cat2_cat3)} cat2-cat3 pairs\")\n       \n    # Define significance based on prevalence thresholds\n    results_df['significant'] = False\n    for cat in categories:\n        high_prevalence_mask = results_df[f'prevalence_cat{cat}'] >= alpha\n        cat_specific_mask = results_df['pattern'] == f'only_cat{cat}'\n        results_df.loc[high_prevalence_mask & cat_specific_mask, 'significant'] = True\n\n    # Create a dictionary with category statistics\n    category_stats = {'total_pairs': len(results_df), 'significant_pairs': sum(results_df['significant'])    }\n\n    # Add pattern counts to stats\n    if 'pattern' in results_df.columns:\n        for pattern in results_df['pattern'].unique():\n            category_stats[f'pattern_{pattern}'] = sum(results_df['pattern'] == pattern)\n\n    results_df = results_df.reset_index(drop=False)\n    end_time = datetime.now()\n    duration = end_time - start_time\n    print(f\"Processing completed at {end_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n    print(f\"Total duration: {duration}\")\n\n    return results_df, category_stats","metadata":{"id":"OMJNFSdASogQ","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.172Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#18.45\npattern_result, category_stats = perform_abundance_analysis(base_matrix, alpha=0.05, n_jobs=-1)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.172Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"''' For a stronger processor capabilities \ndef process_in_resumable_chunks(base_matrix, output_dir, eccontri_df, chunk_size=50000):\n    \"\"\"\n    Process data in chunks while preserving all original rows with duplication tracking.\n    \"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Create duplication tracking in the original dataset\n    eccontri_df = eccontri_df.copy()\n    eccontri_df['dup_count'] = eccontri_df.groupby(['Genus', 'protein_name']).cumcount()\n    eccontri_df['unique_id'] = eccontri_df['Genus'] + '_' + eccontri_df['protein_name'] + '_' + eccontri_df['dup_count'].astype(str)\n    eccontri_df['total_abundance'] = eccontri_df.groupby(['Genus', 'protein_name'])['norm_abund_contri'].transform('sum')\n    \n    # Create reference mapping dataframe for later matching\n    mapping_df = eccontri_df[['unique_id', 'Genus', 'protein_name', 'dup_count', 'idx', 'total_abundance']].copy()\n    print(f\"Created reference mapping with {len(mapping_df)} entries\")\n    \n    # Get unique genus-protein pairs for chunking\n    pairs = base_matrix[['Genus', 'protein_name']].drop_duplicates()\n    total_pairs = len(pairs)\n    total_chunks = (total_pairs + chunk_size - 1) // chunk_size\n    \n    print(f\"Identified {total_pairs} unique genus-protein pairs\")\n    print(f\"Will process in {total_chunks} chunks of approximately {chunk_size} pairs each\")\n    \n    # Check for existing chunks\n    existing_chunks = [f for f in os.listdir(output_dir) if f.startswith('pattern_chunk_') and f.endswith('.parquet')]\n    existing_nums = sorted([int(f.split('_')[-1].split('.')[0]) for f in existing_chunks])\n    \n    print(f\"Found {len(existing_nums)} existing chunks out of {total_chunks} total needed\")\n    start_time = time.time()\n    \n    # Process remaining chunks\n    for chunk_num in range(1, total_chunks + 1):\n        if chunk_num in existing_nums:\n            print(f\"Skipping chunk {chunk_num}/{total_chunks} - already processed\")\n            continue\n        \n        chunk_start_time = time.time()\n        start_idx = (chunk_num - 1) * chunk_size\n        end_idx = min(start_idx + chunk_size, total_pairs)\n        \n        print(f\"Processing chunk {chunk_num}/{total_chunks}: pairs {start_idx} to {end_idx}\")\n        chunk_pairs = pairs.iloc[start_idx:end_idx]\n        \n        # Filter base_matrix to only include the pairs in this chunk\n        chunk_mask = base_matrix['Genus'].isin(chunk_pairs['Genus']) & base_matrix['protein_name'].isin(chunk_pairs['protein_name'])\n        chunk_data = base_matrix[chunk_mask]\n        \n        print(f\"Chunk {chunk_num} contains {len(chunk_data)} rows for {len(chunk_pairs)} unique genus-protein pairs\")\n        \n        # Run analysis on this chunk\n        chunk_results, _ = perform_abundance_analysis(chunk_data, eccontri_df)\n        \n        # Add total_abundance to chunk_results to facilitate matching\n        group_totals = eccontri_df.groupby(['Genus', 'protein_name'])['norm_abund_contri'].sum().reset_index()\n        group_totals.rename(columns={'norm_abund_contri': 'total_abundance'}, inplace=True)\n        chunk_results = pd.merge(chunk_results, group_totals, on=['Genus', 'protein_name'], how='left')\n        \n        # First match with primary entries (dup_count=0)\n        primary_map = mapping_df[mapping_df['dup_count'] == 0]\n        primary_merged = pd.merge(\n            chunk_results,\n            primary_map[['Genus', 'protein_name', 'idx', 'total_abundance']],\n            on=['Genus', 'protein_name', 'total_abundance'],\n            how='left'\n        )\n        \n        # Then handle duplicates\n        duplicates = mapping_df[mapping_df['dup_count'] > 0]\n        duplicate_results = []\n        \n        # Process in batches for better performance\n        for genus in chunk_results['Genus'].unique():\n            for protein in chunk_results[chunk_results['Genus'] == genus]['protein_name'].unique():\n                pattern_row = chunk_results[\n                    (chunk_results['Genus'] == genus) & \n                    (chunk_results['protein_name'] == protein)\n                ]\n                \n                if not pattern_row.empty:\n                    # Get all duplicates for this genus-protein pair\n                    dup_rows = duplicates[\n                        (duplicates['Genus'] == genus) & \n                        (duplicates['protein_name'] == protein)\n                    ]\n                    \n                    # Create new rows for each duplicate\n                    for _, dup in dup_rows.iterrows():\n                        new_row = pattern_row.iloc[0].copy()\n                        new_row['idx'] = dup['idx']\n                        new_row['dup_count'] = dup['dup_count']\n                        duplicate_results.append(new_row)\n        \n        # Combine primary and duplicate results\n        if duplicate_results:\n            duplicates_df = pd.DataFrame(duplicate_results)\n            chunk_with_idx = pd.concat([primary_merged, duplicates_df], ignore_index=True)\n        else:\n            chunk_with_idx = primary_merged\n            \n        # Save processed chunk\n        output_file = os.path.join(output_dir, f\"pattern_chunk_{chunk_num}.parquet\")\n        chunk_with_idx.to_parquet(output_file, index=False)\n        \n        chunk_duration = time.time() - chunk_start_time\n        print(f\"Saved results for chunk {chunk_num} to {output_file} in {chunk_duration:.2f} seconds\")\n        \n        # Estimate remaining time\n        processed_chunks = chunk_num - len(existing_nums)\n        if processed_chunks > 0:\n            avg_time_per_chunk = (time.time() - start_time) / processed_chunks\n            remaining_chunks = total_chunks - chunk_num\n            est_remaining_time = avg_time_per_chunk * remaining_chunks\n            print(f\"Estimated remaining time: {est_remaining_time:.2f} seconds ({est_remaining_time/3600:.2f} hours)\")\n    \n    # Combine all chunks\n    print(\"Combining all chunks...\")\n    all_chunks = []\n    for chunk_num in range(1, total_chunks + 1):\n        file_path = os.path.join(output_dir, f\"pattern_chunk_{chunk_num}.parquet\")\n        if os.path.exists(file_path):\n            chunk_df = pd.read_parquet(file_path)\n            all_chunks.append(chunk_df)\n    \n    if all_chunks:\n        combined_results = pd.concat(all_chunks, ignore_index=True)\n        print(f\"Combined {len(all_chunks)} chunks into final result with {len(combined_results)} rows\")\n        \n        # Verify all rows are included\n        missing_count = len(eccontri_df) - len(combined_results)\n        if missing_count > 0:\n            print(f\"Warning: {missing_count} rows from original dataset are missing in the final result\")\n        else:\n            print(\"All rows from original dataset preserved in final result\")\n            \n        return combined_results\n    else:\n        print(\"No chunks were processed\")\n        return pd.DataFrame()''''''","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.172Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#pattern_result= pattern_result.drop(columns=[\"index\"], axis=1)\npattern_result.head()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.172Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# First merge pattern_result with the unique entries (dup_count=0)\nprimary_map = mapping_df[mapping_df['dup_count'] == 0]\nmerged_primary = pd.merge(\n    pattern_result,\n    primary_map[['Genus', 'protein_name', 'idx', 'total_abundance']], \n    on=['Genus', 'protein_name', 'total_abundance'],\n    how='left'\n)\n\n# Then get all the duplicate entries\nduplicates = mapping_df[mapping_df['dup_count'] > 0]\n\n# For each duplicate, find its corresponding pattern in pattern_result and create a new row\nduplicate_results = []\nfor _, dup in duplicates.iterrows():\n    # Find the pattern row for this genus-protein\n    pattern_row = pattern_result[\n        (pattern_result['Genus'] == dup['Genus']) & \n        (pattern_result['protein_name'] == dup['protein_name']) &\n        (abs(pattern_result['total_abundance'] - dup['total_abundance']) < 1e-6)\n    ]\n    \n    if not pattern_row.empty:\n        # Create a new row with this duplicate's idx but pattern information\n        new_row = pattern_row.iloc[0].copy()\n        new_row['idx'] = dup['idx']\n        new_row['dup_count'] = dup['dup_count']\n        duplicate_results.append(new_row)\n\n# Convert duplicate results to DataFrame and append to merged_primary\nif duplicate_results:\n    duplicates_df = pd.DataFrame(duplicate_results)\n    pattern_df = pd.concat([merged_primary, duplicates_df], ignore_index=True)\nelse:\n    pattern_df = merged_primary\n\npattern_df.shape","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.172Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# To put on the main function after the chunking","metadata":{}},{"cell_type":"code","source":"# After obtaining pattern_df (which now should include the 'group_id' column from base_matrix)\nintegrated_results = pd.merge(pattern_df,\n                              sample_contri.reset_index()[['idx', 'Genus', 'protein_name', \n                                                          'enzyme_names', 'enzyme_class', 'pathways', 'hierarchy',\n                                                          'metals_involved', 'metals_consolidated', 'corrosion_mechanisms', \n                                                          'corrosion_relevance_score', 'corrosion_relevance', 'has_metal']],\n                              on=['Genus', 'protein_name', 'idx'],\n                              how='left')\nintegrated_results = integrated_results.set_index(\"idx\")\nintegrated_results = integrated_results.reset_index()\nintegrated_results.head()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.172Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"integrated_results.head()\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.173Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(12, 6))\nplt.hist(pattern_df[\"presence_pattern\"].dropna(), bins=20)\nplt.tick_params(axis='both', which='major', labelsize=10)\nplt.xlabel('presence_pattern')\nplt.ylabel('Frequency')\nplt.title('Distribution of categories')\nplt.xticks(rotation=30, ha='right') \nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.173Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Reset indexes to ensure proper merging\npattern_df = pattern_df.reset_index(drop=True)\nsample_contri = sample_contri.reset_index(drop=False)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.173Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Done inside the main function together with previous,\ndone here just for debugging","metadata":{}},{"cell_type":"code","source":"# Check how many rows matched during merge\nbefore_count = len(pattern_df)\nafter_count = len(integrated_results)\nprint(f\"Rows before merge: {before_count}, after merge: {after_count}\")\n\n# Verify idx values are matching\nprint(f\"Unique idx values in pattern_df: {pattern_df['idx'].nunique()}\")\nprint(f\"Unique idx values in ECcontri_Uniprot_enriched: {sample_contri['idx'].nunique()}\")\nprint(f\"Rows with matching idx: {sum(pattern_df['idx'].isin(sample_contri['idx']))}\")\n# Check if these columns exist in pattern_df before the merge\nprint(\"Columns in pattern_df:\", pattern_df.columns.tolist())\nprint(\"NaN counts in pattern_df:\")\nprint(pattern_df.isna().sum())\n\n# Check if these columns exist in eccontri_df before the merge\nprint(\"Columns in ECcontri_Uniprot_enriched:\", sample_contri.columns.tolist())\nprint(\"NaN counts in ECcontri_Uniprot_enriched:\")\nprint(sample_contri.isna().sum())","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.174Z"},"id":"e_X8aZcO7qVV","outputId":"8d8eedcc-c5b5-498e-a5f3-57ea3913108e"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Classify Housekeeping, Niche and Mixed Protein depending on pathways, mechanisms and hierarchy","metadata":{"id":"mR_DPU31yG_U"}},{"cell_type":"code","source":"def classify_pathways_by_specificity(integrated_results):\n    \"\"\"\n    Classify pathways as 'universal', 'niche-specific', or 'mixed' based on matching to universal pathways list\n\n    Parameters:\n        integrated_results : DataFrame with pathway information\n\n    Returns:\n        DataFrame with additional columns for pathway classification\n    \"\"\"\n    # Create a copy to avoid modifying the original\n    results = integrated_results.copy(deep=False)\n\n    # Define the universal pathways based on the document\n    universal_pathways = {\n        # Energy Production\n        \"glycolysis\": \"1.1. Glycolysis\",\n        \"gluconeogenesis\": \"1.2. Gluconeogenesis\",\n        \"pentose phosphate\": \"1.3. Pentose Phosphate Pathway\",\n        \"tca cycle\": \"1.4. Krebs/TCA Cycle\",\n        \"krebs cycle\": \"1.4. Krebs/TCA Cycle\",\n        \"citric acid cycle\": \"1.4. Krebs/TCA Cycle\",\n        \"electron transport chain\": \"1.5. Electron Transport Chain\",\n        \"etc\": \"1.5. Electron Transport Chain\",\n        \"fermentation\": \"1.6. Fermentation\",\n        \"atp synthase\": \"1.7. ATP Synthase\",\n\n        # Carbon Storage\n        \"fatty acid synthesis\": \"2.1. Fatty Acid Synthesis\",\n        \"fatty acid oxidation\": \"2.2. Fatty Acid Oxidation\",\n        \"beta-oxidation\": \"2.2. Fatty Acid Oxidation\",\n        \"amino acid degradation\": \"2.3. Amino Acid Degradation\",\n        \"glycogenesis\": \"2.4. Carbohydrate Storage\",\n        \"glycogenolysis\": \"2.4. Carbohydrate Storage\",\n        \"triacylglycerol synthesis\": \"2.5. Triacylglycerol Synthesis\",\n\n        # DNA/RNA/Protein\n        \"dna replication\": \"3.1. DNA Replication\",\n        \"dna polymerase\": \"3.1. DNA Replication\",\n        \"helicase\": \"3.1. DNA Replication\",\n        \"transcription\": \"3.2. Transcription\",\n        \"rna polymerase\": \"3.2. Transcription\",\n        \"translation\": \"3.3. Translation\",\n        \"ribosome\": \"3.3. Translation\",\n        \"protein folding\": \"3.4. Protein Folding and Chaperones\",\n        \"chaperone\": \"3.4. Protein Folding and Chaperones\",\n        \"proteasome\": \"3.5. Proteasome System\",\n        \"protease\": \"3.5. Proteasome System\",\n        \"amino acid biosynthesis\": \"3.6. Amino Acid Biosynthesis\",\n\n        # Membrane Transport\n        \"abc transporter\": \"4.1. ABC Transporters\",\n        \"pilus\": \"4.2. Pilus and Flagella Formation\",\n        \"flagella\": \"4.2. Pilus and Flagella Formation\",\n        \"peptidoglycan\": \"4.3. Cell Wall Maintenance\",\n        \"s-layer\": \"4.3. Cell Wall Maintenance\",\n        \"lipid membrane\": \"4.4. Lipid Membrane Synthesis\",\n        \"glycerophospholipid\": \"4.4. Lipid Membrane Synthesis\",\n\n        # Stress Response\n        \"oxidative stress\": \"5.1. Oxidative Stress Response\",\n        \"superoxide dismutase\": \"5.1. Oxidative Stress Response\",\n        \"catalase\": \"5.1. Oxidative Stress Response\",\n        \"peroxidase\": \"5.1. Oxidative Stress Response\",\n        \"heat shock\": \"5.2. Heat Shock Proteins\",\n        \"cold shock\": \"5.3. Cold Shock Proteins\",\n        \"sos\": \"5.4. SOS DNA Repair System\",\n        \"dna repair\": \"5.4. SOS DNA Repair System\",\n\n        # Biomolecule Synthesis\n        \"nucleotide biosynthesis\": \"6.2. Nucleotide Biosynthesis\",\n    }\n\n    # Add specific amino acid biosynthesis pathways\n    amino_acids = [\"isoleucine\", \"valine\", \"leucine\", \"alanine\", \"arginine\",\n                  \"asparagine\", \"aspartate\", \"cysteine\", \"glutamate\", \"glutamine\",\n                  \"glycine\", \"histidine\", \"lysine\", \"methionine\", \"phenylalanine\",\n                  \"proline\", \"serine\", \"threonine\", \"tryptophan\", \"tyrosine\"]\n\n    for aa in amino_acids:\n        universal_pathways[f\"{aa} biosynthesis\"] = \"6.1. Amino Acid Biosynthesis\"\n\n    # Initialize classification columns\n    results[\"pathway_classification\"] = \"niche-specific\"  # Default\n    results[\"universal_pathways_detected\"] = \"\"\n    results[\"niche_specific_pathways\"] = \"\"\n\n    # Function to classify a pathway\n    def classify_pathway(pathway_text):\n        if pd.isna(pathway_text) or pathway_text == \"\":\n            return \"unknown\", \"\", \"\"\n\n        pathway_text = pathway_text.lower()\n        matched_universals = []\n\n        # Check for universal pathway matches\n        for key, value in universal_pathways.items():\n            if re.search(r'\\b{}\\b'.format(re.escape(key)), pathway_text):\n                matched_universals.append(value)\n\n        # Determine remaining niche-specific content\n        niche_specific = pathway_text\n        for key in universal_pathways:\n            niche_specific = re.sub(r'\\b{}\\b'.format(re.escape(key)), \"\", niche_specific)\n\n        # Clean up niche_specific text\n        niche_specific = re.sub(r'\\s+', ' ', niche_specific).strip()\n        niche_specific = re.sub(r'[,;]\\s*[,;]', ',', niche_specific)\n        niche_specific = re.sub(r'^[,;]\\s*|\\s*[,;]$', '', niche_specific)\n\n        # Classify based on matches\n        if matched_universals and niche_specific:\n            classification = \"mixed\"\n        elif matched_universals:\n            classification = \"universal\"\n        elif niche_specific:\n            classification = \"niche-specific\"\n        else:\n            classification = \"unknown\"\n\n        return classification, \", \".join(set(matched_universals)), niche_specific\n\n    # Apply classification to pathways column if it exists\n    if 'pathways' in results.columns:\n        classifications = results['pathways'].apply(classify_pathway)\n        results[\"pathway_classification\"] = classifications.apply(lambda x: x[0])\n        results[\"universal_pathways_detected\"] = classifications.apply(lambda x: x[1])\n        results[\"niche_specific_pathways\"] = classifications.apply(lambda x: x[2])\n\n    # Count each classification\n    if 'pathway_classification' in results.columns:\n        class_counts = results['pathway_classification'].value_counts()\n        print(\"Pathway classification results:\")\n        for cls, count in class_counts.items():\n            print(f\"  - {cls}: {count} ({count/len(results):.1%})\")\n\n    return results\n\ndef generate_specificity_report(classified_results, output_file=None):\n    \"\"\"\n    Generate a report of pathway classifications with examples\n\n    Parameters:\n        classified_results : DataFrame with pathway classification\n        output_file : str, optional path to save Excel report\n\n    Returns:\n        Dict with summary statistics\n    \"\"\"\n    # Initialize summary statistics\n    summary = {\n        'total_pathways': len(classified_results),\n        'classification_counts': classified_results['pathway_classification'].value_counts().to_dict(),\n        'universal_pathway_counts': {},\n        'niche_specific_examples': {}\n    }\n\n    # Count occurrences of each universal pathway\n    if 'universal_pathways_detected' in classified_results.columns:\n        all_universal = []\n        for pathways in classified_results['universal_pathways_detected'].dropna():\n            if pathways:\n                all_universal.extend([p.strip() for p in pathways.split(',')])\n\n        from collections import Counter\n        summary['universal_pathway_counts'] = dict(Counter(all_universal))\n\n    # Get examples of niche-specific pathways\n    if 'niche_specific_pathways' in classified_results.columns:\n        niche_examples = classified_results.loc[\n            classified_results['pathway_classification'] == 'niche-specific',\n            'niche_specific_pathways'\n        ].dropna().unique()\n\n        summary['niche_specific_examples'] = list(niche_examples)[:20]  # Top 20 examples\n\n    # Create report DataFrames\n    # 1. Summary of classifications\n    classification_summary = pd.DataFrame({\n        'Classification': summary['classification_counts'].keys(),\n        'Count': summary['classification_counts'].values(),\n        'Percentage': [count/summary['total_pathways'] for count in summary['classification_counts'].values()]\n    })\n\n    # 2. Universal pathway frequency\n    universal_frequency = pd.DataFrame({\n        'Universal Pathway': summary['universal_pathway_counts'].keys(),\n        'Count': summary['universal_pathway_counts'].values(),\n    }).sort_values('Count', ascending=False)\n\n    # 3. Examples of each classification\n    examples = {}\n    for classification in ['universal', 'niche-specific', 'mixed']:\n        examples[classification] = classified_results.loc[\n            classified_results['pathway_classification'] == classification,\n            ['pathways', 'universal_pathways_detected', 'niche_specific_pathways']\n        ].head(10)\n\n    # Export to Excel if requested\n    if output_file:\n        with pd.ExcelWriter(output_file) as writer:\n            classification_summary.to_excel(writer, sheet_name='Classification Summary', index=False)\n            universal_frequency.to_excel(writer, sheet_name='Universal Pathway Freq', index=False)\n\n            for classification, df in examples.items():\n                if not df.empty:\n                    df.to_excel(writer, sheet_name=f'{classification.capitalize()} Examples', index=False)\n\n            # Additional sheet with all results\n            columns_to_include = ['Sites', 'Category', 'pathways', 'pathway_classification',\n                                 'universal_pathways_detected', 'niche_specific_pathways']\n            columns_available = [col for col in columns_to_include if col in classified_results.columns]\n            classified_results[columns_available].to_excel(writer, sheet_name='All Results', index=False)\n\n        print(f\"Specificity report saved to {output_file}\")\n\n    return summary","metadata":{"id":"W_Ovd0LHyReP","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.174Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"classified_results = classify_pathways_by_specificity(integrated_results)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.174Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"integrated_results.head()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.174Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Patterns Separtation\nIt was interesting to know if the inverse patterns genera had some protein on increasing pattern, a snippet was done and found none: <<Found 0 genera with mixed patterns out of 5780 inverse genera>> that is why it was decided to separate the inverse genera pattern out of the pipeline for further investigation.","metadata":{"id":"VUcwwFReAAON"}},{"cell_type":"code","source":"def separate_by_pattern(classified_results):\n    \"\"\"\n    Separates classified_results into groups based on their pattern\n    of presence across risk categories.\n    \n    Returns:\n      increasing_df : DataFrame with patterns suggesting increasing risk\n      inverse_df : DataFrame with patterns suggesting decreasing risk\n      constant_df : DataFrame with other patterns\n    \"\"\"\n    # Define patterns associated with increasing risk\n    increasing_patterns = ['only_cat3', 'cat2_and_cat3', 'increasing_abundance', 'valley_at_cat2']\n    \n    # Define patterns associated with decreasing risk\n    inverse_patterns = ['only_cat1', 'cat1_and_cat2', 'decreasing_abundance', 'peak_at_cat2']\n    \n    # Define patterns that are ambiguous or mixed\n    mixed_patterns = ['cat1_and_cat3', 'mixed_abundance', 'only_cat2', 'all_categories']\n    \n    # Separate based on patterns\n    increasing_df = classified_results[classified_results['pattern'].isin(increasing_patterns)].copy(deep=False)\n    inverse_df = classified_results[classified_results['pattern'].isin(inverse_patterns)].copy(deep=False)\n    constant_df = classified_results[classified_results['pattern'].isin(mixed_patterns)].copy(deep=False)\n    \n    # Check if there are any patterns not accounted for\n    unaccounted_patterns = set(classified_results['pattern'].unique()) - set(increasing_patterns + inverse_patterns + mixed_patterns)\n    if unaccounted_patterns:\n        print(f\"Warning: Found {len(unaccounted_patterns)} unaccounted patterns: {unaccounted_patterns}\")\n        # Add rows with unaccounted patterns to constant_df\n        unaccounted_df = classified_results[classified_results['pattern'].isin(unaccounted_patterns)]\n        constant_df = pd.concat([constant_df, unaccounted_df])\n    \n    print(f\"Separated {len(increasing_df)} increasing patterns, {len(inverse_df)} inverse patterns, and {len(constant_df)} mixed/other patterns.\")\n    \n    return increasing_df, inverse_df, constant_df","metadata":{"id":"fHfwD3dkAIG0","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.175Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"increasing_results, inverse_results, constant_df = separate_by_pattern(classified_results)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.175Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Checking for consistency on the pattern recognition\nThe functions were improved in an iterative manner to ensure that the patterns specified correspond with the correlation values found.\nThe pattern distribution now shows complete alignment between pattern types and correlation signs:\n\nDecreasing patterns have negative correlation values\nIncreasing patterns have positive correlation values\nProteins found only in category 3 show positive correlation\nProteins found only in category 1 show negative correlation\nConsistent patterns have zero correlation\nMixed patterns are distributed across different correlation types, as expected\nThis approach ensures biological relevance and statistical consistency in the classification of protein abundance patterns across different risk categories.","metadata":{"id":"JsNzT-EF1gIk"}},{"cell_type":"code","source":"# Plot pattern distribution\nplt.figure(figsize=(12, 6))\npattern_counts = integrated_results['pattern'].value_counts()\npattern_counts.plot(kind='bar')\nplt.title('Distribution of Abundance Patterns')\nplt.xlabel('Pattern Type')\nplt.ylabel('Count')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n# Heatmap of mean values across categories\nplt.figure(figsize=(10, 8))\ncat_means = integrated_results[['mean_cat1', 'mean_cat2', 'mean_cat3']].fillna(0)\nsns.heatmap(cat_means.corr(), annot=True, cmap='coolwarm')\nplt.title('Correlation between Category Mean Values')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.175Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Priorise Markers\nThe prioritize_markers function implements a scoring system that objectively ranks protein-genus pairs based on their potential relevance to corrosion processes. It evaluates each candidate using a multi-factor approach that integrates statistical evidence with biological context, assigning points for statistical significance (3 points), abundance pattern relevance (up to 2 points, with higher scores for patterns showing increased abundance in corrosion conditions), effect size magnitude (up to 2 points for large fold-changes), corrosion relevance derived from metadata (scaled appropriately), metal involvement (1 bonus point), and participation in multiple corrosion mechanisms (up to 2 additional points). The final combined score creates a comprehensive ranking that balances statistical rigor with biological plausibility, enabling focusing on the most promising corrosion-associated proteins for further investigation while filtering out less relevant background organisms.\nIn this step the inverse patterns are filter out and are left aside for further exploration","metadata":{"id":"YCHfeZHEtB9M"}},{"cell_type":"code","source":"def prioritize_markers(increasing_results):\n    \"\"\"Prioritize markers based on statistical and biological significance\"\"\"\n    # Copy to avoid modifying original\n    results = increasing_results.copy(deep=False)\n\n    # Initialize combined score\n    results['combined_score'] = 0.0\n\n    # Pattern checking (diagnostics)\n    pattern_counts = results['pattern'].value_counts()\n    print(f\"Abundance patterns before filtering: {pattern_counts}\")\n\n    # 1. PATTERN RELEVANCE COMPONENT (up to 3 points)\n    pattern_scores = {\n        'increasing_abundance': 3.0,  # Increasing across all categories\n        'only_cat3': 3.0,           # Only present in severe corrosion\n        'cat2_and_cat3': 2.5,       # Present in medium and high risk\n        'valley_at_cat2': 2.0,      # Low in medium, high in severe\n        'cat1_and_cat3': 1.0,       # Mixed pattern (low and high)\n        'all_categories': 0.5       # Present everywhere\n    }\n    results['combined_score'] += results['pattern'].map(pattern_scores).fillna(0.0)\n\n    # 2. PREVALENCE COMPONENT (up to 2 points)\n    # If a marker is present in many samples within its category, it's more reliable\n    if 'prevalence_cat3' in results.columns:\n        # Score based on prevalence in category 3 (high corrosion)\n        results['prevalence_score'] = 0.0\n        results.loc[results['prevalence_cat3'] >= 0.8, 'prevalence_score'] = 2.0           # Present in >80% of samples\n        results.loc[(results['prevalence_cat3'] >= 0.5) & (results['prevalence_cat3'] < 0.8), 'prevalence_score'] = 1.5  # 50-80%\n        results.loc[(results['prevalence_cat3'] >= 0.3) & (results['prevalence_cat3'] < 0.5), 'prevalence_score'] = 1.0  # 30-50%\n        results.loc[(results['prevalence_cat3'] > 0) & (results['prevalence_cat3'] < 0.3), 'prevalence_score'] = 0.5     # <30%\n        \n        results['combined_score'] += results['prevalence_score']\n\n    # 3. SPECIFICITY COMPONENT (up to 2 points)\n    if 'specificity' in results.columns:\n        # Higher specificity means more category-specific\n        results['specificity_score'] = 0.0\n        results.loc[results['specificity'] >= 0.9, 'specificity_score'] = 2.0            # Very high specificity\n        results.loc[(results['specificity'] >= 0.7) & (results['specificity'] < 0.9), 'specificity_score'] = 1.5  # High specificity\n        results.loc[(results['specificity'] >= 0.5) & (results['specificity'] < 0.7), 'specificity_score'] = 1.0  # Moderate specificity\n        \n        results['combined_score'] += results['specificity_score']\n\n    # 4-9. KEEP THE REMAINING SCORING COMPONENTS AS IS\n\n    # [... Rest of your scoring logic for metals, pathways, mechanisms, etc. ...]\n\n    # Sort by combined score (descending)\n    sorted_results = results.sort_values(\n        by=['combined_score'],\n        ascending=[False]\n    )\n    \n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] Prioritization complete\")\n   \n    return sorted_results # prioritized_markers","metadata":{"id":"IR0N6UBYsvY7","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.175Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prioritized_markers = prioritize_markers(increasing_results)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.175Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Balancing Genus Representation\n\nThe cher amount of protein per genus is overwehlming so this fuctions aims to prioritize the top protein per genus. An abundance threshold would be allocated, many proteins are detected at low abundance and maybe not metabolically significant. With a protein threshold the genus would be balanced and that would mean only proteins actively used by a microorganisms are actually included. This avoid over representation that have been seen without this filter, in which 2 genera dominate the top 500 pairs. Some proteins are only relevant when their abundance crosses a functional threshold (e.g., enzyme saturation or metabolic pathway activation).\nA protein consistently present at high abundance in a genus is more likely to be functionally relevant. So a knee point detection between each genus would identify where the abundance drops and at this point the threshold be set for each genus.That will prevent bias toward dominant genera, whiles highlighing functionally important proteins per genus. That will Keep biologically relevant diversity.\nAn idea for future studies would be to search the activation thresholds of key enzymes in the dataset and integrate them in this filtering process for better balancing. For the present pipeline the threshold effects would be taken into account those were visualised on this notebook section Computing the Knee point for Genus representation and from there we calculate the knee point and make a dictionary that is going to be utilised in this step for balancing genus respresentation.\n","metadata":{"id":"PCZ-qTrpel1y"}},{"cell_type":"code","source":"def balance_genus_representation(prioritized_markers, per_genus_count=10, genus_thresholds=None):\n    \"\"\"\n    Balance representation by selecting top proteins per genus based on combined score\n    and optionally applying abundance thresholds.\n\n    Parameters:\n        prioritized_markers: DataFrame with prioritized markers\n        per_genus_count: Number of top proteins to include per genus\n        genus_thresholds: Dict mapping genus names to abundance thresholds\n\n    Returns:\n        DataFrame with balanced genus representation\n    \"\"\"\n    # Group by genus and select top proteins for each\n    genera = prioritized_markers['Genus'].unique()\n    balanced_markers = pd.DataFrame()\n\n    for genus in genera:\n        genus_df = prioritized_markers[prioritized_markers['Genus'] == genus]\n\n        # Apply abundance threshold if provided\n        if genus_thresholds is not None and genus in genus_thresholds:\n            threshold = genus_thresholds[genus]\n            abundance_col = next((col for col in ['total_abundance', 'norm_abund_contri', 'abundance']\n                                if col in genus_df.columns), None)\n\n            if abundance_col is not None:\n                filtered_df = genus_df[genus_df[abundance_col] >= threshold]\n                # If filtering removed everything, fall back to top_count approach\n                if len(filtered_df) > 0:\n                    genus_df = filtered_df\n\n        # Get top proteins for this genus\n        top_genus = genus_df.head(per_genus_count)\n        balanced_markers = pd.concat([balanced_markers, top_genus])\n\n    return balanced_markers.sort_values('combined_score', ascending=False)\n","metadata":{"id":"FSMXsGOBepkJ","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.175Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"markers_df = balance_genus_representation(prioritized_markers, per_genus_count=10, genus_thresholds=None)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.175Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Creating Marker Groups\n\nThe create_marker_groups function organizes the prioritized results into specialized subsets that facilitate targeted analysis of protein-genus pairs with specific characteristics relevant to corrosion processes. It systematically categorizes markers into distinct groups based on multiple criteria: statistical significance, abundance patterns across risk categories, involvement in specific corrosion mechanisms, association with metals, and combined statistical-biological significance. By extracting and parsing all unique corrosion mechanisms from the dataset, the function creates dedicated groups for each mechanism type such as direct electron transfer or acid production. The function also identifies \"high-confidence\" markers that satisfy both statistical significance and biological relevance thresholds, providing a particularly valuable subset for further investigation.","metadata":{"id":"W7TWqLOHtSyR"}},{"cell_type":"code","source":"def create_marker_groups(markers_df, top_count=500, threshold_percentile=0.75):\n    \"\"\"\n    Create specialized marker groups based on various criteria\n\n    Parameters:\n        markers_df: DataFrame with markers (prioritized or balanced)\n        top_count: Number of top markers to include (default: 500)\n        threshold_percentile: Percentile for determining high-relevance markers (default: 0.75)\n\n    Returns:\n        Dictionary of marker groups by different criteria\n    \"\"\"\n    groups = {}\n\n    # 1. OVERALL TOP MARKERS\n    groups['top_markers'] = markers_df.head(top_count)\n\n    # 2. SIGNIFICANCE GROUPS (if available)\n    if 'significant' in markers_df.columns:\n        groups['significant_markers'] = markers_df[markers_df['significant'] == True]\n\n    # 3. CONFIDENCE SCORE GROUPS\n    # High-confidence markers based on combined score\n    score_thresholds = {\n        'high_confidence': 7.0,\n        'medium_confidence': 5.0,\n        'low_confidence': 3.0\n    }\n\n    for group_name, threshold in score_thresholds.items():\n        groups[group_name] = markers_df[markers_df['combined_score'] >= threshold]\n\n    # 4. ABUNDANCE PATTERN GROUPS\n    # Updated pattern names to match your data\n    pattern_groups = {\n        'increasing_abundance': 'Increasing with corrosion severity',\n        'peak_at_cat2': 'Peak at intermediate corrosion',\n        'only_cat3': 'Only present in severe corrosion',\n        'cat2_and_cat3': 'Present in medium and high corrosion',\n        'valley_at_cat2': 'Low in intermediate, high in severe corrosion',\n        'only_cat1': 'Only present in low corrosion',\n        'cat1_and_cat2': 'Present in low and medium corrosion',\n        'only_cat2': 'Only present in medium corrosion',\n        'cat1_and_cat3': 'Present in low and high corrosion',\n        'decreasing_abundance': 'Decreasing with corrosion severity',\n        'all_categories': 'Present across all categories'\n    }\n\n    for pattern, description in pattern_groups.items():\n        pattern_df = markers_df[markers_df['pattern'] == pattern]\n        if len(pattern_df) > 0:\n            groups[f'pattern_{pattern}'] = pattern_df\n\n    # 5. PREVALENCE GROUPS\n    prevalence_fields = ['prevalence_cat1', 'prevalence_cat2', 'prevalence_cat3']\n    for field in prevalence_fields:\n        if field in markers_df.columns:\n            # Define thresholds for high prevalence\n            high_threshold = 0.7  # Present in 70%+ of samples\n            medium_threshold = 0.4  # Present in 40-70% of samples\n            \n            # Create groups\n            groups[f'high_{field}'] = markers_df[markers_df[field] >= high_threshold]\n            groups[f'medium_{field}'] = markers_df[(markers_df[field] >= medium_threshold) & \n                                                  (markers_df[field] < high_threshold)]\n\n    # 6. SPECIFICITY GROUPS\n    if 'specificity' in markers_df.columns:\n        specificity_thresholds = {\n            'very_high_specificity': 0.9,  # Highly specific to one category\n            'high_specificity': 0.7,\n            'moderate_specificity': 0.5\n        }\n        \n        for name, threshold in specificity_thresholds.items():\n            groups[name] = markers_df[markers_df['specificity'] >= threshold]\n\n    # 7-10. KEEP THE REMAINING COMPONENT GROUPS AS IS\n    # Component score groups, mechanism groups, pathway groups, metal groups, etc.\n\n    # 7. COMPONENT SCORE GROUPS\n    component_score_fields = {\n        'metals_score': 'high_metals_relevance',\n        'pathways_score': 'high_pathway_relevance',\n        'hierarchy_score': 'high_hierarchy_relevance',\n        'mechanisms_score': 'high_mechanism_relevance',\n        'fc_score': 'high_effect_size'\n    }\n\n    for score_field, group_name in component_score_fields.items():\n        if score_field in markers_df.columns and not markers_df[score_field].isna().all():\n            # Use percentile-based threshold\n            threshold = markers_df[score_field].quantile(threshold_percentile)\n            if threshold > 0:  # Only create group if threshold is meaningful\n                groups[group_name] = markers_df[markers_df[score_field] >= threshold]\n\n    # 8. MECHANISM-SPECIFIC GROUPS\n    if 'corrosion_mechanisms' in markers_df.columns:\n        # Extract all unique mechanisms\n        all_mechanisms = set()\n        for mechs in markers_df['corrosion_mechanisms'].dropna():\n            if mechs:\n                all_mechanisms.update([m.strip() for m in mechs.split(';')])\n\n        # Create a group for each mechanism\n        for mechanism in all_mechanisms:\n            # Skip empty mechanisms\n            if not mechanism:\n                continue\n\n            mech_df = markers_df[\n                markers_df['corrosion_mechanisms'].fillna('').str.contains(mechanism)\n            ]\n            if len(mech_df) > 0:\n                # Clean mechanism name for group key\n                clean_mechanism = mechanism.replace(' ', '_').lower()\n                groups[f'mechanism_{clean_mechanism}'] = mech_df\n\n    # 9. PATHWAY-SPECIFIC GROUPS\n    if 'pathways' in markers_df.columns:\n        # Key pathways of interest\n        key_pathways = [\n            'metal metabolism', 'sulfur metabolism', 'electron transfer',\n            'biofilm formation', 'oxidative stress', 'metal resistance',\n            'acid production'\n        ]\n\n        for pathway in key_pathways:\n            path_df = markers_df[\n                markers_df['pathways'].fillna('').str.contains(pathway, case=False)\n            ]\n            if len(path_df) > 0:\n                # Clean pathway name for group key\n                clean_pathway = pathway.replace(' ', '_').lower()\n                groups[f'pathway_{clean_pathway}'] = path_df\n\n    # 10. PATHWAY CLASSIFICATION GROUPS\n    if 'pathway_classification' in markers_df.columns:\n        tier_groups = ['niche-specific', 'mixed', 'universal', 'unknown']\n        for tier in tier_groups:\n            tier_df = markers_df[markers_df['pathway_classification'] == tier]\n            if len(tier_df) > 0:\n                groups[f'tier_{tier}'] = tier_df\n\n    # 11. METAL-RELATED GROUPS\n    # Group by metal involvement flag\n    if 'has_metal' in markers_df.columns:\n        groups['metal_involved'] = markers_df[markers_df['has_metal'] == True]\n\n    # If metals_involved column exists, create groups for specific metals\n    if 'metals_involved' in markers_df.columns:\n        key_metals = ['Fe', 'iron', 'Cu', 'copper', 'Mn', 'manganese', 'S', 'sulfur']\n\n        for metal in key_metals:\n            metal_df = markers_df[\n                markers_df['metals_involved'].fillna('').str.contains(metal, case=False)\n            ]\n            if len(metal_df) > 0:\n                # Clean metal name for group key\n                clean_metal = metal.lower()\n                groups[f'metal_{clean_metal}'] = metal_df\n\n    # 12. EFFECT SIZE GROUPS\n    if 'log2fc_cat3_vs_cat1' in markers_df.columns:\n        groups['high_cat3_vs_cat1_change'] = markers_df[\n            markers_df['log2fc_cat3_vs_cat1'].abs() >= 2.0  # 4x change\n        ]\n    \n    if 'log2fc_cat3_vs_cat2' in markers_df.columns:\n        groups['high_cat3_vs_cat2_change'] = markers_df[\n            markers_df['log2fc_cat3_vs_cat2'].abs() >= 2.0  # 4x change\n        ]\n\n    # Add information about group sizes\n    group_sizes = {name: len(df) for name, df in groups.items()}\n    print(\"Marker groups created:\")\n    for name, size in sorted(group_sizes.items(), key=lambda x: x[1], reverse=True):\n        print(f\"  - {name}: {size} markers\")\n\n    return groups","metadata":{"id":"ctnaWreptO2n","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.176Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Analyzing 1491288 data points across 70 sites...\nNotice that these results only contain positive markers by abundance and functional significance.\nCompleted testing of 35118/37089 protein-genus pairs.\nFound 943 statistically significant pairs after FDR correction.\nIntegrating with corrosion metadata...\nAdding metadata for 37089 protein-genus pairs...\nMetadata integration complete.\nPrioritizing markers based on statistical and biological relevance...\nCalculating combined relevance scores...\nPrioritization complete.\nCreating specialized marker groups...\n<ipython-input-238-4958864b2107>:40: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '1.5' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  results.loc[(fc_magnitude >= 2) & (fc_magnitude < 4), 'fc_score'] = 1.5  # 4-16x change\nAnalysis complete!","metadata":{"id":"oRqL49qLQhqm"}},{"cell_type":"markdown","source":"### Generate Marker Report\nGenerates a report with the results","metadata":{"id":"4FSuWh3-rZqp"}},{"cell_type":"code","source":"def generate_streamlined_report(analysis_results, output_file=None, top_count=500, include_detailed_scores=True):\n    \"\"\"\n    Generate a streamlined report focusing on corrosion-relevant information with\n    genus/protein pairs and their relationship to corrosion.\n\n    Parameters:\n        analysis_results : dict with analysis results\n        output_file : str or Path, path to save Excel report (optional)\n        top_count : int, number of top markers to include in main report (default: 500)\n        include_detailed_scores : bool, whether to include detailed scoring sheet (default: True)\n\n    Returns:\n        dict with report DataFrames\n    \"\"\"\n    # Extract the results\n    prioritized_markers = analysis_results.get('prioritized_markers')\n    if prioritized_markers is None:\n        prioritized_markers = analysis_results.get('increasing_results')\n\n    print(f\"Shape of prioritized_markers: {prioritized_markers.shape}\")\n\n    # Check if there was a balanced version\n    was_balanced = 'balanced_markers' in analysis_results\n    if was_balanced:\n        top_markers = analysis_results['balanced_markers'].head(top_count)\n    else:\n        top_markers = prioritized_markers.head(top_count)\n\n    # Define core columns for main report\n    core_columns = [ 'idx',\n        'Genus', 'protein_name', 'EC', 'enzyme_names', 'corrosion_mechanisms',\n        'metals_involved', 'pathway_classification', 'pattern', 'specificity',\n        'combined_score', 'Sites'\n    ]\n\n    # Add category prevalence columns if they exist\n    prevalence_columns = [col for col in top_markers.columns if 'prevalence_cat' in col]\n    \n    # Add category mean columns if they exist\n    cat_columns = [col for col in top_markers.columns if col.startswith('mean_cat')]\n\n    # Keep only columns that actually exist in the DataFrame\n    available_core = [col for col in core_columns if col in top_markers.columns]\n    available_core += [col for col in prevalence_columns if col in top_markers.columns]\n\n    # Create main report with core columns\n    main_report = top_markers[available_core].copy(deep=False)\n\n    # Format column names for readability\n    pretty_columns = { 'idx': 'idx',\n        'Genus': 'Genus',\n        'protein_name': 'Protein Name',\n        'EC': 'EC Number',\n        'enzyme_names': 'Enzyme Names',\n        'corrosion_mechanisms': 'Corrosion Mechanisms',\n        'metals_involved': 'Metals Involved',\n        'pathway_classification': 'Pathway Classification',\n        'pattern': 'Abundance Pattern',\n        'specificity': 'Category Specificity',\n        'combined_score': 'Relevance Score',\n        'prevalence_cat1': 'Prevalence in Low Risk',\n        'prevalence_cat2': 'Prevalence in Medium Risk',\n        'prevalence_cat3': 'Prevalence in High Risk',\n        'Sites': 'Sites',\n        'pathways': 'Pathways',\n        'hierarchy': 'Function Hierarchy',\n        'niche_specific_pathways': 'Niche-Specific Pathways'\n    }\n\n    # Rename main report columns\n    main_report.columns = [pretty_columns.get(col, col) for col in main_report.columns]\n\n    # Define columns for the visualization sheet\n    viz_columns = ['Genus', 'protein_name']\n\n    # Add pathway, mechanism, and metal columns if they exist\n    optional_viz_columns = ['pathways', 'corrosion_mechanisms', 'metals_involved']\n    viz_columns.extend([col for col in optional_viz_columns if col in top_markers.columns])\n\n    # Add category means\n    viz_columns.extend(cat_columns)\n\n    # Create visualization data sheet\n    available_viz = [col for col in viz_columns if col in top_markers.columns]\n    viz_data = top_markers[available_viz].copy(deep=False)\n\n    # Rename visualization columns\n    viz_data.columns = [pretty_columns.get(col, col) for col in viz_data.columns]\n\n    # Create detailed scoring sheet if requested\n    detailed_scores = None\n    if include_detailed_scores:\n        # Define scoring component columns\n        score_columns = [\n            'combined_score', 'metals_score', 'pathways_score',\n            'hierarchy_score', 'mechanisms_score', 'fc_score', \n            'specificity', 'corrosion_final_score', 'significant'\n        ]\n\n        # Only include columns that exist\n        available_score = [col for col in score_columns if col in top_markers.columns]\n\n        # Create basic identification columns\n        available_id = [col for col in idx if col in top_markers.columns]\n\n        # Combine for detailed scores\n        detailed_scores = top_markers[available_id + available_score].copy()\n\n        # Rename detailed score columns\n        pretty_score_columns = {\n            'combined_score': 'Total Score',\n            'metals_score': 'Metals Score',\n            'pathways_score': 'Pathways Score',\n            'hierarchy_score': 'Hierarchy Score',\n            'mechanisms_score': 'Mechanisms Score',\n            'fc_score': 'Effect Size Score',\n            'specificity': 'Category Specificity',\n            'corrosion_final_score': 'Corrosion Relevance Score',\n            'significant': 'Statistically Significant'\n        }\n\n        detailed_scores.columns = [pretty_score_columns.get(col, pretty_columns.get(col, col))\n                                  for col in detailed_scores.columns]\n\n    # Create mechanism-focused data\n    mechanism_data = None\n    if 'corrosion_mechanisms' in top_markers.columns:\n        # Get relevant columns\n        mech_columns = ['Genus', 'protein_name', 'corrosion_mechanisms', 'combined_score']\n        available_mech = [col for col in mech_columns if col in top_markers.columns]\n\n        # Extract mechanisms data\n        mechanism_data = top_markers[available_mech].copy()\n\n        # Process mechanisms into separate rows\n        expanded_mechanisms = []\n\n        for _, row in mechanism_data.iterrows():\n            if pd.notna(row.get('corrosion_mechanisms')) and row.get('corrosion_mechanisms'):\n                mechanisms = [m.strip() for m in row['corrosion_mechanisms'].split(';')]\n                for mechanism in mechanisms:\n                    new_row = row.copy()\n                    new_row['mechanism'] = mechanism\n                    expanded_mechanisms.append(new_row)\n            else:\n                row['mechanism'] = 'unknown'\n                expanded_mechanisms.append(row)\n\n        # Create expanded mechanism DataFrame if there are entries\n        if expanded_mechanisms:\n            mechanism_data = pd.DataFrame(expanded_mechanisms)\n            mechanism_data.columns = [pretty_columns.get(col, col) if col != 'mechanism' else 'Mechanism'\n                                    for col in mechanism_data.columns]\n\n    # Export to Excel if output_file is provided\n    if output_file:\n        try:\n            with pd.ExcelWriter(output_file) as writer:\n                # Main report\n                main_report.to_excel(writer, sheet_name='Top Corrosion Markers', index=False)\n\n                # Visualization data\n                viz_data.to_excel(writer, sheet_name='Visualization Data', index=False)\n\n                # Detailed scores\n                if include_detailed_scores and detailed_scores is not None:\n                    detailed_scores.to_excel(writer, sheet_name='Detailed Scores', index=False)\n\n                # Mechanism-focused data\n                if mechanism_data is not None:\n                    mechanism_data.to_excel(writer, sheet_name='Mechanisms Focus', index=False)\n\n                # Create summary sheet\n                summary_data = {\n                    'Item': [\n                        'Analysis Date',\n                        'Total Markers Analyzed',\n                        'Top Markers Selected',\n                        'Genus Balancing Applied',\n                        'Pathway Classification Counts',\n                        'Pattern Counts',\n                        'File Generated'\n                    ],\n                    'Value': [\n                        datetime.now().strftime('%Y-%m-%d %H:%M'),\n                        len(prioritized_markers),\n                        top_count,\n                        'Yes' if was_balanced else 'No',\n                        str(top_markers['pathway_classification'].value_counts().to_dict()) if 'pathway_classification' in top_markers.columns else 'N/A',\n                        str(top_markers['pattern'].value_counts().to_dict()) if 'pattern' in top_markers.columns else 'N/A',\n                        os.path.basename(str(output_file)) if output_file else 'None'\n                    ]\n                }\n                summary_df = pd.DataFrame(summary_data)\n                summary_df.to_excel(writer, sheet_name='Summary', index=False)\n\n            print(f\"Report successfully generated with {top_count} markers\")\n            print(f\"Report saved to: {output_file}\")\n\n        except Exception as e:\n            print(f\"Error generating Excel report: {e}\")\n            print(\"Returning report DataFrames without saving to file.\")\n            \n    # Return the report DataFrames\n    report_dict = {\n        'main_report': main_report,\n        'visualization_data': viz_data\n    }\n\n    if include_detailed_scores and detailed_scores is not None:\n        report_dict['detailed_scores'] = detailed_scores\n\n    if mechanism_data is not None:\n        report_dict['mechanism_focus'] = mechanism_data\n\n    return report_dict","metadata":{"id":"43MU0cZEtkLw","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.176Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Run the analysis 40 min @ 20GB\nanalysis_results = analyze_corrosion_proteins(sample_contri)\n\n## calling it with balancing the generacomplete_results\n#analysis_results = analyze_corrosion_proteins(ECcontri_Uniprot_enriched, ec_records, balance_genera=True, per_genus_count=10)\n","metadata":{"id":"LL_64bSwkKLy","trusted":true,"outputId":"6b2bca83-9cbf-45d6-9727-a946d09e4b45","execution":{"execution_failed":"2025-03-30T18:38:54.176Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for key, value in analysis_results.items():\n    print(f\"{key}: {type(value)}\")\n","metadata":{"id":"WdZn8Rdeg3ax","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.176Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Saving the Results","metadata":{"id":"FZApOPOZj7QW"}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore', category=pd.io.pytables.PerformanceWarning)\n\noutput_file = output_large / \"complete_results.h5\"\n\nwith pd.HDFStore(output_file) as store:\n    for key, df in analysis_results.items():\n\n        if isinstance(df, (pd.DataFrame, pd.Series)):\n            store[key] = df  # Store DataFrame or Series\n        else:\n            print(f\"Skipping {key}: Dict save in excel {type(df)}\")\n\n# h5 is unreliable and can work depending on the environment","metadata":{"id":"clQoVvdIoakO","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.176Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Should be more universaly efficient so I am saving on any case\nimport json\nimport pandas as pd\n\n# Function to make objects JSON-serializable\ndef json_serialize(obj):\n    if isinstance(obj, pd.DataFrame):\n        return obj.to_dict(orient='records')\n    elif isinstance(obj, pd.Series):\n        return obj.to_dict()\n    elif hasattr(obj, 'tolist'):  # For numpy arrays\n        return obj.tolist()\n    else:\n        return str(obj)\n\n# Save your analysis results to JSON\ntry:\n    # Process each item in your results dictionary\n    serializable_results = {}\n\n    for key, value in analysis_results.items():\n        print(f\"Processing {key}...\")\n        if isinstance(value, pd.DataFrame):\n            # Save DataFrame to CSV as backup\n            csv_path = output_large / f\"{key}.csv\"\n            value.to_csv(csv_path, index=False)\n            print(f\"  Saved {key} to CSV at {csv_path}\")\n\n            # Convert to dictionary for JSON\n            serializable_results[key] = json_serialize(value)\n        else:\n            serializable_results[key] = json_serialize(value)\n\n    # Save the full JSON\n    json_path = output_large / \"complete_results.json\"\n    with open(json_path, 'w') as f:\n        json.dump(serializable_results, f)\n\n    print(f\"Successfully saved results to {json_path}\")\n\nexcept Exception as e:\n    print(f\"Error saving to JSON: {e}\")\n\n    # Fallback: Save individual DataFrames as CSV if JSON fails\n    print(\"Falling back to individual CSV files...\")\n    for key, value in analysis_results.items():\n        if isinstance(value, pd.DataFrame):\n            try:\n                csv_path = output_large / f\"{key}.csv\"\n                value.to_csv(csv_path, index=False)\n                print(f\"Saved {key} to CSV at {csv_path}\")\n            except Exception as csv_e:\n                print(f\"Error saving {key} to CSV: {csv_e}\")","metadata":{"id":"RRzOPBIKgOM1","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.176Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Generate the Excel report in the same directory\nreport_path =output_large/ \"corrosion_markers_report.xlsx\" #/content/drive/MyDrive/MIC/output_large/corrosion_markers_report.xlsx\nreport = generate_streamlined_report(analysis_results, report_path, top_count=500, include_detailed_scores=True)","metadata":{"id":"WFHpieA5HF1W","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.177Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Run second part of pipeline\nstop_monitoring()\nplot_resource_log()\n","metadata":{"id":"ZyB6nuG9xDQs","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.177Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 9.6 Summary of the Results  \n\nAnalysis Files Description  \n* statistical_results are statistical selection results, comprises results of comprehensive statistical hypothesis testing including Kruskal-Wallis H statistics, raw p-values, and FDR-corrected p-values (p_adjusted). This non-parametric testing identifies genus-protein pairs that show statistically significant differences in abundance across corrosion risk categories. Each entry includes sample counts per category, mean/median/standard deviation metrics, and effect size measurements between categories.\n\n* integrated_results combines statistical significance data with functional metadata from the ec_records dictionary. This dataset enriches statistical findings with biological context including enzyme names, EC numbers, metabolic pathways, functional hierarchies, metal interactions, and corrosion-related mechanisms. It serves as the foundation for subsequent analysis by linking statistical significance to biological function.\n\n* classified_results Annotates integrated results with pathway classifications using the comprehensive taxonomy of universal vs. niche-specific pathways. Each entry is labeled as \"universal\" (common across all bacteria/archaea), \"niche-specific\" (specialized for particular environments), or \"mixed\" (containing elements of both). This classification helps distinguish between housekeeping functions and potentially corrosion-specific adaptations. classification_summary is a quantitative breakdown of pathway classifications with counts and percentages for each category, providing insight into the distribution of universal vs. specialized pathways in corrosion environments. Universal_frequency is a detailed frequency analysis of specific universal pathways, showing which core metabolic functions are most prevalent in the dataset.\n\n* Correlation Separated Results has tree types of results increasing_results which is a subset of integrated results showing positive correlation with corrosion risk.inverse_results which is a subset showing inverse correlation with corrosion risk, These are preserved for analysis of potentially protective or competitive proteins that might inhibit corrosion and analyse on a different section. Lastly constant_results a subset showing no significant correlation with risk categories.  \n\n* prioritized_markers comprising only the increasing_results prioritized according to a comprehensive scoring system that considers: Statistical significance (correlation and pattern significance), biological relevance (mechanisms, hierarchy, pathways), corrosion-specific factors (metal involvement, corrosion terms), effect size components.  It contains following columns: Genus', 'protein_name', 'count_1', 'count_2', 'count_3', 'mean_1','mean_2', 'mean_3', 'median_1', 'median_2', 'median_3', 'std_1', 'std_2', 'std_3', 'lookup_key', 'present_in_1', 'present_in_2', 'present_in_3', 'pattern', 'log2fc_1_to_2', 'log2fc_2_to_3', 'max_log2fc', 'enzyme_names', 'enzyme_class', 'pathways', 'hierarchy','metals_involved', 'metals_consolidated', 'corrosion_mechanisms', 'corrosion_relevance_score', 'corrosion_relevance', 'has_metal','Sites', 'h_statistic', 'p_value', 'mean_cat1', 'median_cat1', 'std_cat1', 'samples_cat1', 'mean_cat2', 'median_cat2', 'std_cat2',       'samples_cat2', 'mean_cat3', 'median_cat3', 'std_cat3', 'samples_cat3','effect_size_1_to_2', 'effect_size_2_to_3', 'p_adjusted', 'significant','corr', 'combined_score', 'corr_score', 'metals_score','pathways_score', 'hierarchy_score', 'mechanisms_score', 'fc_score','corrosion_score_scaled', 'corrosion_cat_score', 'corrosion_final_score'\n\n* balanced_markers is an optional filtered subset of prioritized markers that ensures balanced representation across genera, preventing over-representation of abundant genera while maintaining significant proteins.\n* marker_groups are categorized groups from the top 100 markers at 75% confidence threshold, organized by biological function and relevance.  \n\n* complete_results.joblib is the complete results dictionary contains all analysis results preserved in a flexible parquet format.\n* corrosion_markers_report.xlsx is the excel report and contains all analysis results in separate sheets for easy exploration and presentation.\n° Top Corrosion Markers, Visualization Data, Detailed Scores, and Mechanisms Focus.\nThe Top Corrosion Markers sheet, df top_markers contains the most relevant genus-protein pairs with their abundance patterns, pathway classifications, and relevance scores. Columns : Genus', 'Protein Name', 'Enzyme Names', 'Corrosion Mechanisms', 'Metals Involved', 'Abundance Pattern', 'Relevance Score', 'Correlation with Corrosion'.\n° The Visualization Data sheet, df visualize includes additional columns optimized for creating plots and visual analysis. Columns: 'Genus', 'Protein Name', 'Pathways', 'Corrosion Mechanisms', 'Metals Involved', 'mean_1', 'mean_2', 'mean_3', 'mean_cat1', 'mean_cat2', 'mean_cat3'\n° The Detailed Scores sheet breaks down the contributing factors to each marker's final score, showing the relative importance of statistical, biological, and corrosion-specific components. Columns: 'Genus', 'Protein Name', 'Total Score', 'Correlation Score', 'Metals Score', 'Pathways Score', 'Hierarchy Score', 'Mechanisms Score', 'Effect Size Score', 'Corrosion Relevance Score', 'P-Value', 'Adjusted P-Value', 'Statistically Significant'.\n° Finally The Mechanisms Focus sheet df mechanisms, reorganizes the data to highlight specific corrosion mechanisms, allowing for targeted analysis of metal transformation, electron transfer, biofilm formation, and other key processes relevant to microbial corrosion. Columns are: 'Genus', 'Protein Name', 'Corrosion Mechanisms', 'Relevance Score', 'Mechanism'.\n\nThis pipeline provides a comprehensive analysis framework that progresses from statistical identification to biological classification and prioritization, creating both comprehensive and focused views of corrosion-relevant microbial proteins.","metadata":{"id":"sqAXwsa32F0r"}},{"cell_type":"markdown","source":"### Reading the Resulsts","metadata":{"id":"zhdDiHBq2F0r"}},{"cell_type":"code","source":"# Load\ncomplete_results = {}\nwith pd.HDFStore(output_large / \"complete_results.h5\", \"r\") as store:\n    for key in store.keys():\n        # Remove leading '/' from key\n        clean_key = key.lstrip('/')\n        complete_results[clean_key] = store[key]\n\nclassified_results= complete_results['classified_results']","metadata":{"id":"3cqLiBFY2F0s","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.177Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''# To read it back\nwith open(output_large / \"complete_results.json\", 'r') as f:\n    loaded_results = json.load(f)'''","metadata":{"id":"LsoMLELsj7QX","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.178Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Reading the Reports on Pathway Classification from Specificity Section","metadata":{"id":"_uo8Tc3_YCFb"}},{"cell_type":"code","source":"classification_report_path = output_large / \"classification_report.xlsx\"\nclassification_summary = generate_specificity_report(classified_results, classification_report_path)","metadata":{"id":"iW3dhr_ITcyI","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.178Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"classification_summary","metadata":{"id":"dPvtOi9N2F0s","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.178Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Niche-specific pathways are the most common (25,762 entries), followed by mixed pathways (11,004), with only a small number of unknown classifications (323). The universal pathway counts provide valuable insights into which core metabolic functions are most represented:\n\nAmino Acid Biosynthesis (3,587) is the most common universal pathway Translation (1,583) is the second most common Glycolysis/Gluconeogenesis (1,426 each) are also highly represented SOS DNA Repair System (1,409) is surprisingly prevalent\n\nThe niche-specific examples reveal many specialized metabolic pathways, including:\n\nSecondary metabolite biosynthesis Vitamin metabolism (thiamine, B6, riboflavin) Specialized carbon metabolism Several pathways involved in environmental adaptation\n\nThis results are at first sight surprising since the top pathway reported by picrust2 results gave aerobic respiration(cytochrome c)as the dominant metabolic pathway. This could be explained by the calculation, whiles picrust output ranks pathways by total abundance across all samples, the classification_summary counts the number of unique genus-protein pairs associated with each universal pathway category. So aerobic respiration is the most abundant expressed pathway with high activity levels since is a high flux energy generating pathway. Whiles amino acid biosynthesis involves more distintic proteins or greater diversity of functions that actually comprises many different enzymes and reactions for creating an array of aminoacids.","metadata":{"id":"RV62Z9oquSo3"}},{"cell_type":"code","source":"# Calling the report on\nreport_path =output_large/ \"corrosion_markers_report.xlsx\"\n\ntop_markers = pd.read_excel(report_path, sheet_name=\"Top Corrosion Markers\", engine=\"openpyxl\")\nvisualize = pd.read_excel(report_path, sheet_name=\"Visualization Data\", engine=\"openpyxl\")\nscores = pd.read_excel(report_path, sheet_name=\"Detailed Scores\", engine=\"openpyxl\")\nmechanisms = pd.read_excel(report_path, sheet_name=\"Mechanisms Focus\", engine=\"openpyxl\")","metadata":{"id":"rmR7f3AOP0tn","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.178Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mechanisms.columns","metadata":{"id":"L3tpFO3N2F0s","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.178Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"visualize.columns","metadata":{"id":"0ynN-YKf2F0t","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.178Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"scores.head()","metadata":{"id":"xLo5kiZg2F0t","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.179Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mechanisms.head()","metadata":{"id":"rq6xw6nW2F0t","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.179Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"increasing_markers= complete_results['increasing_markers']\nintegrated_results= complete_results['integrated_results']\ninverse_markers= complete_results['inverse_markers']\npattern_data= complete_results['pattern_data']\nprioritized_markers= complete_results['prioritized_markers']\nstatistical_results= complete_results['statistical_results']\n#balanced_markers= complete_results['balanced_markers']","metadata":{"id":"JKBOQFlxgOM3","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.179Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(top_markers[\"Genus\"].unique())","metadata":{"id":"92FgKubg9wwp","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.179Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(prioritized_markers[\"enzyme_names\"].unique())","metadata":{"id":"AajoTutGgOM4","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.179Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.patches as mpatches\n\n# Define colors and categories\ncategory_colors = {\n    1: '#008800',  # Dark green - Normal Operation\n    2: '#FF8C00',  # Dark orange - Early Warning\n    3: '#FF0000'   # Red - System Failure\n}\n\ncategories_labels = {\n    1: 'Normal Operation',\n    2: 'Early Warning',\n    3: 'System Failure'\n}\n\ndef plot_multiview_functional_landscape(prioritized_markers, top_n=25):\n    \"\"\"\n    Creates multiple visualizations of functional data across risk categories.\n\n    Args:\n        prioritized_markers (DataFrame): DataFrame containing marker data\n        top_n (int): Number of top markers to include\n\n    Returns:\n        matplotlib figure with multiple subplots showing different aspects of the data\n    \"\"\"\n    # Select top markers by score\n    top_markers = prioritized_markers.sort_values('combined_score', ascending=False).head(top_n)\n\n    # Create figure with multiple subplots\n    fig = plt.figure(figsize=(20, 16))\n\n    # 1. Pathway abundance by category (top left)\n    ax1 = plt.subplot2grid((2, 2), (0, 0))\n\n    # Group data by category and aggregate mechanisms\n    mech_by_category = []\n\n    for _, row in top_markers.iterrows():\n        if isinstance(row['corrosion_mechanisms'], str) and row['corrosion_mechanisms']:\n            mechanisms = [m.strip() for m in row['corrosion_mechanisms'].split(';')]\n            for mech in mechanisms:\n                if mech:  # Ensure mechanism is not empty\n                    mech_by_category.append({\n                        'Mechanism': mech,\n                        'Category 1': row['mean_1'] if pd.notna(row['mean_1']) else 0,\n                        'Category 2': row['mean_2'] if pd.notna(row['mean_2']) else 0,\n                        'Category 3': row['mean_3'] if pd.notna(row['mean_3']) else 0,\n                    })\n\n    # Convert to DataFrame and aggregate\n    mech_df = pd.DataFrame(mech_by_category)\n    agg_mech = mech_df.groupby('Mechanism').agg({\n        'Category 1': 'mean',\n        'Category 2': 'mean',\n        'Category 3': 'mean'\n    }).reset_index()\n\n    # Calculate total abundance and sort\n    agg_mech['Total'] = agg_mech['Category 1'] + agg_mech['Category 2'] + agg_mech['Category 3']\n    agg_mech = agg_mech.sort_values('Total', ascending=False).head(10)\n\n    # Melt for plotting\n    mech_plot = agg_mech.melt(\n        id_vars=['Mechanism'],\n        value_vars=['Category 1', 'Category 2', 'Category 3'],\n        var_name='Category',\n        value_name='Abundance'\n    )\n\n    # Map categories to labels\n    mech_plot['Category'] = mech_plot['Category'].map({\n        'Category 1': categories_labels[1],\n        'Category 2': categories_labels[2],\n        'Category 3': categories_labels[3]\n    })\n\n    # Create the plot\n    sns.barplot(\n        data=mech_plot,\n        x='Abundance',\n        y='Mechanism',\n        hue='Category',\n        palette=[category_colors[1], category_colors[2], category_colors[3]],\n        ax=ax1\n    )\n\n    ax1.set_title('Top Corrosion Mechanisms by Risk Category', fontsize=14)\n    ax1.set_xlabel('Mean Abundance', fontsize=12)\n    ax1.set_ylabel('Mechanism', fontsize=12)\n\n    # 2. Heatmap of top proteins across categories (top right)\n    ax2 = plt.subplot2grid((2, 2), (0, 1))\n\n    # Prepare data for heatmap - select top proteins by score\n    top_proteins = top_markers.sort_values('combined_score', ascending=False).head(50)\n\n    # Create descriptive row labels\n    protein_labels = [f\"{row['Genus']} - {row['protein_name'][:20]}...\" if len(row['protein_name']) > 20\n                     else f\"{row['Genus']} - {row['protein_name']}\" for _, row in top_proteins.iterrows()]\n\n    # Create data matrix for heatmap\n    heatmap_data = top_proteins[['mean_1', 'mean_2', 'mean_3']].copy()\n\n    # Calculate z-scores for better visualization\n    for i, col in enumerate(['mean_1', 'mean_2', 'mean_3']):\n        col_mean = heatmap_data[col].mean()\n        col_std = heatmap_data[col].std()\n        if col_std > 0:\n            heatmap_data[col] = (heatmap_data[col] - col_mean) / col_std\n\n    # Create heatmap\n    sns.heatmap(\n        heatmap_data,\n        cmap='coolwarm',\n        center=0,\n        annot=True,\n        fmt='.2f',\n        linewidths=0.5,\n        yticklabels=protein_labels,\n        xticklabels=[categories_labels[1], categories_labels[2], categories_labels[3]],\n        ax=ax2\n    )\n\n    ax2.set_title('Top Protein Abundance Patterns (Z-score)', fontsize=14)\n\n    # 3. Correlation analysis (bottom left)\n    ax3 = plt.subplot2grid((2, 2), (1, 0))\n\n    # Calculate correlation with risk progression for each protein\n    top_markers['Correlation'] = top_markers.apply(\n        lambda x: np.corrcoef([1, 2, 3], [x['mean_1'], x['mean_2'], x['mean_3']])[0, 1]\n        if not (pd.isna(x['mean_1']) or pd.isna(x['mean_2']) or pd.isna(x['mean_3']))\n        else np.nan,\n        axis=1\n    )\n\n    # Sort by correlation and select top + bottom 5\n    corr_markers = pd.concat([\n        top_markers.sort_values('Correlation', ascending=False).head(5),  # Most positive\n        top_markers.sort_values('Correlation').head(5)  # Most negative\n    ])\n\n    # Create labels\n    corr_labels = [f\"{row['Genus']} - {row['protein_name'][:15]}...\" if len(row['protein_name']) > 15\n                  else f\"{row['Genus']} - {row['protein_name']}\" for _, row in corr_markers.iterrows()]\n\n    # Plot correlation bars\n    sns.barplot(\n        x='Correlation',\n        y=corr_labels,\n        data=corr_markers,\n        hue=corr_labels,\n        palette='coolwarm',\n        legend=False,\n        ax=ax3\n    )\n\n    ax3.axvline(x=0, color='gray', linestyle='--')\n    ax3.set_title('Proteins with Strongest Correlation to Risk Progression', fontsize=14)\n    ax3.set_xlabel('Correlation Coefficient', fontsize=12)\n    ax3.set_ylabel('')\n\n    # 4. Genus distribution across categories (bottom right)\n    ax4 = plt.subplot2grid((2, 2), (1, 1))\n\n    # Group by genus and calculate mean abundance in each category\n    genus_abundance = top_markers.groupby('Genus').agg({\n        'mean_1': 'mean',\n        'mean_2': 'mean',\n        'mean_3': 'mean'\n    }).reset_index()\n\n    # Calculate total abundance and get top genera\n    genus_abundance['Total'] = genus_abundance['mean_1'] + genus_abundance['mean_2'] + genus_abundance['mean_3']\n    top_genera = genus_abundance.sort_values('Total', ascending=False).head(8)\n\n    # Melt for plotting\n    genera_plot = top_genera.melt(\n        id_vars=['Genus'],\n        value_vars=['mean_1', 'mean_2', 'mean_3'],\n        var_name='Category',\n        value_name='Abundance'\n    )\n\n    # Map categories to labels\n    genera_plot['Category'] = genera_plot['Category'].map({\n        'mean_1': categories_labels[1],\n        'mean_2': categories_labels[2],\n        'mean_3': categories_labels[3]\n    })\n\n    # Create the plot\n    sns.barplot(\n        data=genera_plot,\n        x='Genus',\n        y='Abundance',\n        hue='Category',\n        palette=[category_colors[1], category_colors[2], category_colors[3]],\n        ax=ax4\n    )\n\n    ax4.set_title('Top Genera Abundance by Risk Category', fontsize=14)\n    ax4.set_xlabel('Genus', fontsize=12)\n    ax4.set_ylabel('Mean Abundance', fontsize=12)\n    ax4.tick_params(axis='x', rotation=45)\n\n    # Overall title\n    plt.suptitle('Functional Landscape of Microbially Influenced Corrosion', fontsize=18, y=0.98)\n\n    plt.tight_layout(rect=[0, 0, 1, 0.96])\n\n    return fig","metadata":{"id":"0lBzAsZmgOM4","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.179Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_multiview_functional_landscape(prioritized_markers, top_n=125)","metadata":{"id":"1oLAf-1TgOM4","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.180Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Use your custom category dictionary and color mapping\ncategory_colors = {\n    1: '#008800',  # Dark green - Normal Operation\n    2: '#FF8C00',  # Dark orange - Early Warning\n    3: '#FF0000'   # Red - System Failure\n}\n\ncategories_labels = {\n    1: 'Normal Operation',\n    2: 'Early Warning',\n    3: 'System Failure'\n}\n\ndef extract_pathways(df, pathway_column='pathways'):\n    \"\"\"Extract and count unique pathways from a DataFrame.\"\"\"\n    all_pathways = []\n\n    for pathways in df[pathway_column].dropna():\n        if isinstance(pathways, str):\n            # Split by semicolons and clean up each pathway\n            pathway_list = [p.strip() for p in pathways.split(';')]\n            all_pathways.extend(pathway_list)\n\n    # Count occurrences of each pathway\n    pathway_counts = pd.Series(all_pathways).value_counts()\n\n    return pathway_counts\n\ndef categorize_pathway(pathway):\n    \"\"\"Categorize pathway into broader functional categories.\"\"\"\n    pathway = pathway.lower()\n\n    if any(term in pathway for term in ['metal', 'iron', 'copper', 'nickel', 'zinc']):\n        return 'Metal Metabolism'\n    elif any(term in pathway for term in ['sulfur', 'sulfate', 'sulfide']):\n        return 'Sulfur Metabolism'\n    elif any(term in pathway for term in ['nitrogen', 'nitrate', 'nitrite', 'ammonia']):\n        return 'Nitrogen Metabolism'\n    elif any(term in pathway for term in ['biofilm', 'quorum', 'adhesion']):\n        return 'Biofilm Formation'\n    elif any(term in pathway for term in ['oxidation', 'reduction', 'electron transport']):\n        return 'Redox Processes'\n    elif any(term in pathway for term in ['acid', 'acidic']):\n        return 'Acid Production'\n    elif any(term in pathway for term in ['exopolysaccharide', 'polysaccharide', 'eps']):\n        return 'EPS Production'\n    elif any(term in pathway for term in ['stress', 'resistance', 'tolerance']):\n        return 'Stress Response'\n    elif any(term in pathway for term in ['carbon', 'c-compound']):\n        return 'Carbon Metabolism'\n    elif any(term in pathway for term in ['amino acid', 'protein']):\n        return 'Protein/Amino Acid Metabolism'\n    elif any(term in pathway for term in ['lipid', 'fatty acid']):\n        return 'Lipid Metabolism'\n    elif any(term in pathway for term in ['transport']):\n        return 'Transport'\n    elif any(term in pathway for term in ['energy', 'atp']):\n        return 'Energy Metabolism'\n    elif any(term in pathway for term in ['degrade', 'degradation']):\n        return 'Degradation Pathways'\n    elif any(term in pathway for term in ['synthesis', 'biosynthesis']):\n        return 'Biosynthesis Pathways'\n    elif any(term in pathway for term in ['fermentation']):\n        return 'Fermentation'\n    elif any(term in pathway for term in ['respiration']):\n        return 'Respiratory Pathways'\n    elif any(term in pathway for term in ['metabolism']):\n        return 'General Metabolism'\n    else:\n        return 'Other Metabolic Processes'\n\ndef plot_pathway_analysis(prioritized_markers, top_n=100):\n    \"\"\"\n    Creates comprehensive visualizations of pathway data across risk categories.\n\n    Args:\n        prioritized_markers (DataFrame): DataFrame containing marker data\n        top_n (int): Number of top markers to include\n\n    Returns:\n        matplotlib figure with multiple subplots showing different aspects of pathway data\n    \"\"\"\n    plt.close('all')  # Close previous plots to avoid duplication\n\n    # Select top markers by score\n    top_markers = prioritized_markers.sort_values('combined_score', ascending=False).head(top_n)\n\n    # Extract pathways and their counts\n    pathway_counts = extract_pathways(top_markers)\n    print(f\"Total unique pathways found: {len(pathway_counts)}\")\n\n    # Filter pathways that appear at least twice\n    common_pathways = pathway_counts[pathway_counts >= 2].index.tolist()\n    print(f\"Common pathways (appearing ≥2 times): {len(common_pathways)}\")\n\n    # Create figure with multiple subplots\n    fig = plt.figure(figsize=(20, 16))\n\n    # 1. Pathway categories distribution (top left)\n    ax1 = plt.subplot2grid((2, 2), (0, 0))\n\n    # Categorize pathways\n    pathway_categories = {}\n    for pathway in common_pathways:\n        category = categorize_pathway(pathway)\n        if category in pathway_categories:\n            pathway_categories[category] += pathway_counts[pathway]\n        else:\n            pathway_categories[category] = pathway_counts[pathway]\n\n    # Create DataFrame for plotting\n    category_df = pd.DataFrame({\n        'Category': list(pathway_categories.keys()),\n        'Count': list(pathway_categories.values())\n    }).sort_values('Count', ascending=False)\n\n    print(f\"Number of distinct pathway categories: {len(category_df)}\")\n\n    # Create the plot - use simple approach to avoid hue/palette issues\n    bars = sns.barplot(\n        data=category_df,\n        x='Count',\n        y='Category',\n        color='skyblue',\n        ax=ax1\n    )\n\n    # Add count labels\n    for i, bar in enumerate(bars.patches):\n        bars.text(\n            bar.get_width() + 0.3,\n            bar.get_y() + bar.get_height()/2,\n            f\"{int(bar.get_width())}\",\n            ha='left',\n            va='center'\n        )\n\n    ax1.set_title('Distribution of Pathway Categories', fontsize=14)\n    ax1.set_xlabel('Count', fontsize=12)\n    ax1.set_ylabel('Pathway Category', fontsize=12)\n\n    # 2. Pathway network visualization (top right)\n    ax2 = plt.subplot2grid((2, 2), (0, 1))\n\n    # Create a graph for visualization\n    G = nx.Graph()\n\n    # Add nodes for pathway categories\n    for category, count in pathway_categories.items():\n        G.add_node(category, size=count, group=1)\n\n    # Add nodes for top genera\n    genera_counts = top_markers['Genus'].value_counts().head(10)\n    for genus, count in genera_counts.items():\n        G.add_node(genus, size=count*3, group=2)  # Multiply by 3 for better visibility\n\n    # Add edges between genera and pathway categories they contribute to\n    for _, row in top_markers.iterrows():\n        genus = row['Genus']\n        if genus not in genera_counts:\n            continue\n\n        if isinstance(row['pathways'], str) and row['pathways']:\n            pathways = [p.strip() for p in row['pathways'].split(';')]\n            categories_added = set()  # Track categories already connected to this genus\n\n            for pathway in pathways:\n                if pathway in common_pathways:\n                    category = categorize_pathway(pathway)\n\n                    # Only add each genus-category edge once\n                    if category not in categories_added:\n                        # Add edge if it doesn't exist\n                        if not G.has_edge(genus, category):\n                            G.add_edge(genus, category, weight=1)\n                        else:\n                            # Increase weight if edge exists\n                            G[genus][category]['weight'] += 1\n\n                        categories_added.add(category)\n\n    # Use spring layout for positioning\n    pos = nx.spring_layout(G, k=0.3, seed=42)\n\n    # Draw nodes with different colors for categories vs genera\n    category_nodes = [node for node in G.nodes() if G.nodes[node].get('group', 0) == 1]\n    genera_nodes = [node for node in G.nodes() if G.nodes[node].get('group', 0) == 2]\n\n    node_sizes_cat = [G.nodes[node].get('size', 10) * 20 for node in category_nodes]\n    node_sizes_gen = [G.nodes[node].get('size', 10) * 20 for node in genera_nodes]\n\n    nx.draw_networkx_nodes(G, pos, nodelist=category_nodes, node_color='lightblue',\n                           node_size=node_sizes_cat, alpha=0.8, ax=ax2)\n    nx.draw_networkx_nodes(G, pos, nodelist=genera_nodes, node_color='lightgreen',\n                           node_size=node_sizes_gen, alpha=0.8, ax=ax2)\n\n    # Draw edges with width based on weight\n    edge_weights = [G[u][v].get('weight', 1)/2 for u, v in G.edges()]\n    nx.draw_networkx_edges(G, pos, width=edge_weights, alpha=0.5, ax=ax2)\n\n    # Draw labels\n    nx.draw_networkx_labels(G, pos, font_size=9, font_family='sans-serif', ax=ax2)\n\n    ax2.set_title('Genus-Pathway Category Network', fontsize=14)\n    ax2.axis('off')\n\n    # 3. Category correlation with risk categories (bottom left)\n    ax3 = plt.subplot2grid((2, 2), (1, 0))\n\n    # Create a DataFrame to store pathway presence for each risk category\n    pathway_data = []\n\n    for _, row in top_markers.iterrows():\n        if isinstance(row['pathways'], str) and row['pathways']:\n            # Get pathways for this row\n            row_pathways = [p.strip() for p in row['pathways'].split(';')]\n\n            # Add data for common pathways\n            for pathway in row_pathways:\n                if pathway in common_pathways:\n                    pathway_data.append({\n                        'Pathway': pathway,\n                        'Category': categorize_pathway(pathway),\n                        'Category 1': row['mean_1'] if pd.notna(row['mean_1']) else 0,\n                        'Category 2': row['mean_2'] if pd.notna(row['mean_2']) else 0,\n                        'Category 3': row['mean_3'] if pd.notna(row['mean_3']) else 0,\n                    })\n\n    # Convert to DataFrame\n    pathway_df = pd.DataFrame(pathway_data)\n\n    # Aggregate by pathway category\n    agg_pathway = pathway_df.groupby('Category').agg({\n        'Category 1': 'mean',\n        'Category 2': 'mean',\n        'Category 3': 'mean'\n    }).reset_index()\n\n    # Calculate the correlation with corrosion progression (Cat1 -> Cat3)\n    agg_pathway['Correlation'] = agg_pathway.apply(\n        lambda x: np.corrcoef([1, 2, 3], [x['Category 1'], x['Category 2'], x['Category 3']])[0, 1],\n        axis=1\n    )\n\n    # Sort by correlation\n    agg_pathway = agg_pathway.sort_values('Correlation', ascending=False)\n\n    # Create a horizontal bar plot - use simple approach to avoid hue/palette issues\n    corr_colors = [\n        'red' if x > 0.7 else\n        'orangered' if x > 0.3 else\n        'darkorange' if x > 0 else\n        'royalblue' if x > -0.3 else\n        'blue'\n        for x in agg_pathway['Correlation']\n    ]\n\n    bars = sns.barplot(\n        data=agg_pathway,\n        x='Correlation',\n        y='Category',\n        color='steelblue',\n        ax=ax3\n    )\n\n    # Add correlation values\n    for i, bar in enumerate(bars.patches):\n        corr_value = agg_pathway.iloc[i]['Correlation']\n        bars.text(\n            bar.get_width() + 0.02 if corr_value >= 0 else bar.get_width() - 0.02,\n            bar.get_y() + bar.get_height()/2,\n            f\"{corr_value:.2f}\",\n            ha='left' if corr_value >= 0 else 'right',\n            va='center'\n        )\n\n    ax3.axvline(x=0, color='gray', linestyle='--')\n    ax3.set_title('Pathway Category Correlation with Risk Progression', fontsize=14)\n    ax3.set_xlabel('Correlation Coefficient', fontsize=12)\n    ax3.set_ylabel('Pathway Category', fontsize=12)\n\n    # 4. Heatmap of pathway categories across risk categories (bottom right)\n    ax4 = plt.subplot2grid((2, 2), (1, 1))\n\n    # Prepare data for heatmap\n    heatmap_data = agg_pathway.set_index('Category')[['Category 1', 'Category 2', 'Category 3']]\n\n    # Calculate z-scores within each row to highlight relative differences\n    heat_data_zscore = pd.DataFrame(index=heatmap_data.index, columns=heatmap_data.columns)\n    # Ensure all heatmap data is numeric\n    for col in heat_data_zscore.columns:\n        heat_data_zscore[col] = pd.to_numeric(heat_data_zscore[col], errors='coerce')\n\n    # Fill NaN values with zeros as you suggested\n    heat_data_zscore = heat_data_zscore.fillna(0)\n\n    for idx, row in heatmap_data.iterrows():\n        if row.std() > 0:  # Avoid division by zero\n            heat_data_zscore.loc[idx] = (row - row.mean()) / row.std()\n        else:\n            heat_data_zscore.loc[idx] = row - row.mean()\n\n    # Create the heatmap\n    cmap = LinearSegmentedColormap.from_list('BlueRed', ['#1E88E5', '#FFFFFF', '#E53935'])\n    # Make a numeric copy first\n    heat_data_numeric = heat_data_zscore.copy(deep=True)\n\n    # Create heatmap with numeric data\n    sns.heatmap(\n        heat_data_zscore,\n        cmap=cmap,\n        center=0,\n        annot=True,\n        fmt=\".2f\",\n        linewidths=0.5,\n        ax=ax4\n    )\n\n    ax4.set_title('Pathway Category Abundance Patterns (Z-score)', fontsize=14)\n\n    # Overall title\n    plt.suptitle('Metabolic Pathway Analysis in Microbially Influenced Corrosion', fontsize=18, y=0.98)\n\n    plt.tight_layout(rect=[0, 0, 1, 0.96])\n\n    return fig, category_df, agg_pathway  # Return additional data for further analysis\n\n# To use this function:\nfig, category_data, correlation_data = plot_pathway_analysis(prioritized_markers, top_n=100)","metadata":{"id":"tDdmOFl_EHpt","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.180Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 2. PATHWAY ENRICHMENT ANALYSIS\n\ndef plot_pathway_enrichment(classified_results):\n    \"\"\"\n    Creates a bubble chart showing pathway enrichment by risk category.\n    \"\"\"\n    # Extract top pathways from the dataset\n    pathway_data = []\n\n    for _, row in classified_results.iterrows():\n        if isinstance(row['pathways'], str) and row['pathways']:\n            pathways = [p.strip() for p in row['pathways'].split(';')]\n            # Take only the first 3 pathways to avoid overwhelming the plot\n            for pathway in pathways[:3]:\n                pathway_data.append({\n                    'pathway': pathway,\n                    'category': 'Category 1',\n                    'mean_abundance': row['mean_cat1'],\n                    'classification': row['pathway_classification']\n                })\n                pathway_data.append({\n                    'pathway': pathway,\n                    'category': 'Category 2',\n                    'mean_abundance': row['mean_cat2'],\n                    'classification': row['pathway_classification']\n                })\n                pathway_data.append({\n                    'pathway': pathway,\n                    'category': 'Category 3',\n                    'mean_abundance': row['mean_cat3'],\n                    'classification': row['pathway_classification']\n                })\n\n    # Convert to DataFrame\n    pathway_df = pd.DataFrame(pathway_data)\n    \n    # Group by pathway and category, calculate mean abundance\n    grouped = pathway_df.groupby(['pathway', 'category', 'classification'], as_index=False)['mean_abundance'].mean()\n\n    # Calculate fold change from Category 1 to 3\n    pivot = grouped.pivot_table(index='pathway', columns='category', values='mean_abundance').reset_index()\n    # Add a small epsilon to avoid division by zero\n    epsilon = 1e-10\n    # Try with a more defensive approach\n    try:\n        # First calculate normal fold change\n        pivot['fold_change'] = pivot['Category 3'] / (pivot['Category 1'] + epsilon)\n\n        # Identify problematic values before log2\n        mask_inf = np.isinf(pivot['fold_change'])\n        mask_nan = np.isnan(pivot['fold_change'])\n        if mask_inf.any() or mask_nan.any():\n            print(f\"Found {mask_inf.sum()} inf values and {mask_nan.sum()} NaN values\")\n\n        # Now compute log2fc more carefully\n        pivot['log2fc'] = np.where(\n            np.isfinite(pivot['fold_change']),\n            np.log2(np.maximum(pivot['fold_change'], epsilon)),\n            np.nan\n        )\n    except Exception as e:\n        print(f\"Error calculating log2fc: {e}\")\n    # Try to identify the problematic rows\n    for i, row in pivot.iterrows():\n        try:\n            test = np.log2(row['Category 3'] / (row['Category 1'] + epsilon))\n        except Exception as e:\n            print(f\"Problem at row {i}: {row['pathway']}, Cat1={row['Category 1']}, Cat3={row['Category 3']}\")\n    pivot['fold_change'] = pivot['Category 3'] / (pivot['Category 1']+ epsilon)\n    pivot['log2fc'] = np.log2(pivot['fold_change'])\n\n        # After calculating log2fc, immediately check for issues\n    print(\"Log2fc range:\", pivot['log2fc'].min(), \"to\", pivot['log2fc'].max())\n    print(\"NaN in log2fc:\", pivot['log2fc'].isna().sum())\n\n    # Replace -inf and inf with large negative and positive values\n    pivot['log2fc'] = pivot['log2fc'].replace([np.inf, -np.inf], [10, -10])\n\n    # Now proceed with your filtering\n    significant = pivot[np.isfinite(pivot['log2fc'])].copy()\n\n    # Simplify the filtering to isolate issues\n    significant = pivot[~pivot['log2fc'].isna()].copy()\n    print(\"Significant rows:\", len(significant))\n\n    # Merge back with groupby results\n    enrichment = pd.merge(grouped, pivot[['pathway', 'fold_change', 'log2fc']], on='pathway')\n\n    # Filter for significant enrichment and top pathways\n    significant = enrichment[~np.isnan(enrichment['log2fc'])]\n    significant = significant.sort_values('log2fc', ascending=False)\n\n    # Get top 15 enriched and top 15 depleted pathways\n    top_enriched = significant.nlargest(15, 'log2fc')\n    top_depleted = significant.nsmallest(15, 'log2fc')\n    plot_data = pd.concat([top_enriched, top_depleted])\n\n    # Avoiding nan values\n    plot_data = plot_data.dropna(subset=['log2fc', 'mean_abundance'])  \n    # Create a bubble chart\n    fig = px.scatter(\n        plot_data,\n        x='log2fc',\n        y='pathway',\n        size='mean_abundance',\n        color='classification',\n        facet_col='category',\n        hover_data=['fold_change', 'mean_abundance'],\n        height=800,\n        width=1200,\n        title='Pathway Enrichment Analysis Across Risk Categories',\n        labels={'log2fc': 'Log2 Fold Change (Cat3/Cat1)', 'pathway': 'Metabolic Pathway'},\n        color_discrete_map={'niche-specific': '#FF5722', 'mixed': '#2196F3', 'universal': '#4CAF50'}\n    )\n\n    fig.update_layout(yaxis={'categoryorder': 'total ascending'})\n    return fig\nPathway_enrichment = plot_pathway_enrichment(classified_results)","metadata":{"id":"OQEogebvk20f","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.180Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 3. NETWORK ANALYSIS OF CORROSION MECHANISMS\ndef plot_mechanism_network(prioritized_markers, threshold=0.7):\n    \"\"\"\n    Creates a network visualization showing relationships between genera, proteins, and mechanisms.\n    \"\"\"\n    import networkx as nx\n\n    # Create a networkx graph\n    G = nx.Graph()\n\n    # Add nodes for genera (blue), proteins (green), and mechanisms (red)\n    genera = set()\n    proteins = set()\n    mechanisms = set()\n\n    # Extract top markers\n    top_markers = prioritized_markers.sort_values('combined_score', ascending=False).head(50)\n\n    # Process each row\n    for _, row in top_markers.iterrows():\n        genus = row['Genus']\n        protein = row['protein_name']\n\n        genera.add(genus)\n        proteins.add(protein)\n\n        # Add genus-protein edge\n        G.add_edge(genus, protein, weight=row['combined_score'])\n\n        # Add mechanism nodes and edges\n        if isinstance(row['corrosion_mechanisms'], str) and row['corrosion_mechanisms']:\n            for mech in row['corrosion_mechanisms'].split(';'):\n                mech = mech.strip()\n                mechanisms.add(mech)\n                G.add_edge(protein, mech, weight=1)\n\n    # Node colors\n    color_map = []\n    for node in G.nodes():\n        if node in genera:\n            color_map.append('skyblue')\n        elif node in proteins:\n            color_map.append('limegreen')\n        else:\n            color_map.append('tomato')\n\n    # Node sizes\n    size_map = []\n    for node in G.nodes():\n        if node in genera:\n            size_map.append(300)  # Genera\n        elif node in proteins:\n            size_map.append(200)  # Proteins\n        else:\n            size_map.append(400)  # Mechanisms\n\n    # Create plot\n    plt.figure(figsize=(20, 20))\n    pos = nx.spring_layout(G, k=0.5, iterations=50)\n\n    # Draw the network\n    nx.draw_networkx_nodes(G, pos, node_color=color_map, node_size=size_map, alpha=0.8)\n    nx.draw_networkx_edges(G, pos, width=1.0, alpha=0.5)\n    nx.draw_networkx_labels(G, pos, font_size=10, font_family='sans-serif')\n\n    plt.title('Network of Genera, Proteins, and Corrosion Mechanisms', fontsize=20)\n    plt.axis('off')\n    return plt.gcf()\n","metadata":{"id":"_WHmx7sXlFYS","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.180Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mechanism_network = plot_mechanism_network(prioritized_markers)","metadata":{"id":"KVD8pZfSlGFF","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.180Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 4. METAL INVOLVEMENT ANALYSIS\ndef plot_metal_involvement(prioritized_markers):\n    \"\"\"\n    Creates a heatmap showing metal involvement by genus across risk categories.\n    \"\"\"\n    # Process metal involvement data\n    metal_data = []\n\n    for _, row in prioritized_markers.iterrows():\n        if isinstance(row['metals_involved'], str) and row['metals_involved']:\n            metals = [m.strip() for m in row['metals_involved'].split(',')]\n            for metal in metals:\n                metal_data.append({\n                    'Genus': row['Genus'],\n                    'metal': metal,\n                    'Category 1': row['mean_cat1'],\n                    'Category 2': row['mean_cat2'],\n                    'Category 3': row['mean_cat3'],\n                    'combined_score': row['combined_score']\n                })\n\n    # Convert to DataFrame\n    metal_df = pd.DataFrame(metal_data)\n\n    # Group by genus and metal\n    grouped = metal_df.groupby(['Genus', 'metal']).agg({\n        'Category 1': 'mean',\n        'Category 2': 'mean',\n        'Category 3': 'mean',\n        'combined_score': 'mean'\n    }).reset_index()\n\n    # Create a pivot table for the heatmap\n    pivot = grouped.pivot_table(\n        index='Genus',\n        columns='metal',\n        values='combined_score',\n        aggfunc='mean'\n    ).fillna(0)\n\n    # Sort by sum of values\n    pivot = pivot.loc[pivot.sum(axis=1).sort_values(ascending=False).index]\n    pivot = pivot[pivot.sum().sort_values(ascending=False).index]\n\n    # Create heatmap\n    plt.figure(figsize=(14, 10))\n    sns.heatmap(\n        pivot,\n        cmap='YlOrRd',\n        linewidths=0.5,\n        cbar_kws={'label': 'Combined Score'}\n    )\n\n    plt.title('Metal Involvement by Genus', fontsize=16)\n    plt.xticks(rotation=45, ha='right')\n    plt.tight_layout()\n    return plt.gcf()\n\nmetal_involvement = plot_metal_involvement(prioritized_markers)","metadata":{"id":"6ZU44A-Fl1Kr","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.181Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_metal_involvement(prioritized_markers):\n    \"\"\"\n    Creates a heatmap showing metal involvement by genus across risk categories,\n    using the actual metal names from your data.\n    \"\"\"\n    # Process metal involvement data\n    metal_data = []\n\n    for _, row in prioritized_markers.iterrows():\n        if isinstance(row['metals_involved'], str) and row['metals_involved']:\n            # Use the correct delimiter based on your data format (comma or semicolon)\n            metals = [m.strip() for m in row['metals_involved'].split(';')]\n            for metal in metals:\n                metal_data.append({\n                    'Genus': row['Genus'],\n                    'metal': metal,  # Keep the original metal name\n                    'Category 1': row['mean_cat1'],\n                    'Category 2': row['mean_cat2'],\n                    'Category 3': row['mean_cat3'],\n                    'combined_score': row['combined_score']\n                })\n\n    # Convert to DataFrame\n    metal_df = pd.DataFrame(metal_data)\n\n    # Group by genus and metal\n    grouped = metal_df.groupby(['Genus', 'metal']).agg({\n        'Category 1': 'mean',\n        'Category 2': 'mean',\n        'Category 3': 'mean',\n        'combined_score': 'mean'\n    }).reset_index()\n\n    # Create a pivot table for the heatmap\n    pivot = grouped.pivot_table(\n        index='Genus',\n        columns='metal',\n        values='combined_score',\n        aggfunc='mean'\n    ).fillna(0)\n\n    # Sort by sum of values\n    pivot = pivot.loc[pivot.sum(axis=1).sort_values(ascending=False).index]\n    pivot = pivot[pivot.sum().sort_values(ascending=False).index]\n\n    # Set up the plot\n    plt.figure(figsize=(14, 10))\n\n    # Create heatmap - using a mask for zero values to make them white\n    mask = pivot == 0\n    sns.heatmap(\n        pivot,\n        cmap='YlOrRd',\n        linewidths=0.5,\n        cbar_kws={'label': 'Combined Score'},\n        mask=mask\n    )\n\n    plt.title('Metal Involvement by Genus', fontsize=16)\n    plt.xticks(rotation=45, ha='right')\n    plt.tight_layout()\n\n    # Print a simple dictionary of the metals for reference\n    metal_list = sorted(metal_df['metal'].unique())\n    print(\"Metals involved in analysis:\")\n    for metal in metal_list:\n        print(f\"- {metal}\")\n\n    return plt.gcf()\n\n# To use this function:\nmetal_involvement = plot_metal_involvement(prioritized_markers)","metadata":{"id":"GQ9poSUTQ0j9","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.181Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\n# Define category colors using your mapping\ncategory_colors = {\n    1: '#008800',  # Dark green - Normal Operation\n    2: '#FF8C00',  # Dark orange - Early Warning\n    3: '#FF0000'   # Red - System Failure\n}\n\ncategory_labels = {\n    1: 'Normal Operation',\n    2: 'Early Warning',\n    3: 'System Failure'\n}\n\ndef plot_metal_involvement(prioritized_markers, top_n=50):\n    \"\"\"\n    Creates a visualization of metal involvement in corrosion mechanisms with a\n    dictionary-like display of metal abbreviations and full names.\n\n    Args:\n        prioritized_markers (DataFrame): DataFrame containing marker data with metals_involved column\n        top_n (int): Number of top markers to include\n\n    Returns:\n        plotly figure showing metal involvement across risk categories with metal name dictionary\n    \"\"\"\n    # Close any previous plots\n    plt.close('all')\n\n    # Select top markers by score\n    top_markers = prioritized_markers.sort_values('combined_score', ascending=False).head(top_n)\n\n    # Extract metals data\n    metals_data = []\n\n    # Define metal abbreviations and full names\n    metal_dict = {\n        'Fe': 'Iron',\n        'Cu': 'Copper',\n        'Zn': 'Zinc',\n        'Ni': 'Nickel',\n        'Mn': 'Manganese',\n        'Al': 'Aluminum',\n        'Cr': 'Chromium',\n        'Co': 'Cobalt',\n        'Mo': 'Molybdenum',\n        'W': 'Tungsten',\n        'V': 'Vanadium',\n        'Ti': 'Titanium',\n        'Mg': 'Magnesium',\n        'Ca': 'Calcium',\n        'Na': 'Sodium',\n        'K': 'Potassium',\n        'S': 'Sulfur',\n        'P': 'Phosphorus',\n        'Cl': 'Chlorine',\n        'Si': 'Silicon'\n    }\n\n    # Extract metals from the metals_involved column\n    for _, row in top_markers.iterrows():\n        if isinstance(row['metals_involved'], str) and row['metals_involved']:\n            metals = [m.strip() for m in row['metals_involved'].split(';')]\n\n            # For each metal, create a data point\n            for metal in metals:\n                # Try to identify if the metal is in our dictionary\n                metal_abbr = None\n                for abbr, full_name in metal_dict.items():\n                    if abbr.lower() in metal.lower() or full_name.lower() in metal.lower():\n                        metal_abbr = abbr\n                        break\n\n                # If not found, just use the first 2 chars or the whole name if it's short\n                if metal_abbr is None:\n                    metal_abbr = metal[:2].upper() if len(metal) > 2 else metal.upper()\n                    # Add to dictionary if not there\n                    if metal_abbr not in metal_dict:\n                        metal_dict[metal_abbr] = metal\n\n                # Add data point\n                metals_data.append({\n                    'Metal': metal_abbr,\n                    'Full Name': metal_dict.get(metal_abbr, metal),\n                    'Genus': row['Genus'],\n                    'Protein': row['protein_name'][:30] + '...' if len(row['protein_name']) > 30 else row['protein_name'],\n                    'Category 1': row['mean_1'] if pd.notna(row['mean_1']) else 0,\n                    'Category 2': row['mean_2'] if pd.notna(row['mean_2']) else 0,\n                    'Category 3': row['mean_3'] if pd.notna(row['mean_3']) else 0,\n                    'Score': row['combined_score'] if pd.notna(row['combined_score']) else 0\n                })\n\n    # If no metal data found, return an error message\n    if not metals_data:\n        print(\"No metal involvement data found in the prioritized markers.\")\n        return None\n\n    # Convert to DataFrame\n    metals_df = pd.DataFrame(metals_data)\n\n    # Calculate correlation with risk progression\n    metals_df['Correlation'] = metals_df.apply(\n        lambda x: np.corrcoef([1, 2, 3], [x['Category 1'], x['Category 2'], x['Category 3']])[0, 1]\n        if not (pd.isna(x['Category 1']) or pd.isna(x['Category 2']) or pd.isna(x['Category 3']))\n        else 0,\n        axis=1\n    )\n\n    # Aggregate data by metal\n    metal_agg = metals_df.groupby('Metal').agg({\n        'Full Name': 'first',  # Take the first full name\n        'Category 1': 'mean',\n        'Category 2': 'mean',\n        'Category 3': 'mean',\n        'Score': 'mean',\n        'Correlation': 'mean',\n        'Genus': lambda x: ', '.join(sorted(set(x)))[:50] + '...' if len(', '.join(sorted(set(x)))) > 50 else ', '.join(sorted(set(x))),\n        'Protein': 'count'  # Count how many proteins involve this metal\n    }).reset_index()\n\n    # Sort by correlation and count\n    metal_agg['Importance'] = metal_agg['Protein'] * metal_agg['Score']\n    metal_agg = metal_agg.sort_values('Importance', ascending=False)\n\n    # Prepare data for plotting\n    metals_pivot = pd.melt(\n        metal_agg,\n        id_vars=['Metal', 'Full Name', 'Correlation', 'Importance', 'Protein', 'Genus'],\n        value_vars=['Category 1', 'Category 2', 'Category 3'],\n        var_name='Category',\n        value_name='Abundance'\n    )\n\n    # Map category to category label\n    metals_pivot['Category Label'] = metals_pivot['Category'].map({\n        'Category 1': category_labels[1],\n        'Category 2': category_labels[2],\n        'Category 3': category_labels[3]\n    })\n\n    # Create a two-panel plot using plotly\n    fig = make_subplots(\n        rows=1, cols=2,\n        column_widths=[0.7, 0.3],\n        specs=[[{\"type\": \"bar\"}, {\"type\": \"table\"}]],\n        subplot_titles=(\"Metal Involvement Across Risk Categories\", \"Metal Reference\")\n    )\n\n    # Add bar chart for metal involvement\n    for i, category in enumerate(['Category 1', 'Category 2', 'Category 3']):\n        cat_data = metals_pivot[metals_pivot['Category'] == category]\n        fig.add_trace(\n            go.Bar(\n                name=category_labels[i+1],\n                x=cat_data['Metal'],\n                y=cat_data['Abundance'],\n                text=cat_data['Abundance'].round(3),\n                marker_color=category_colors[i+1],\n                hovertemplate=\"<b>%{x}</b><br>\" +\n                             \"Abundance: %{y:.3f}<br>\" +\n                             \"Correlation: %{customdata[0]:.2f}<br>\" +\n                             \"Proteins: %{customdata[1]}<br>\" +\n                             \"Genera: %{customdata[2]}<extra></extra>\",\n                customdata=np.column_stack((cat_data['Correlation'],\n                                          cat_data['Protein'],\n                                          cat_data['Genus']))\n            ),\n            row=1, col=1\n        )\n\n    # Add metal reference table\n    fig.add_trace(\n        go.Table(\n            header=dict(\n                values=[\"Abbr.\", \"Full Name\", \"Proteins\", \"Correlation\"],\n                font=dict(size=12, color=\"white\"),\n                fill_color=\"#4472C4\",\n                align=\"left\"\n            ),\n            cells=dict(\n                values=[\n                    metal_agg['Metal'],\n                    metal_agg['Full Name'],\n                    metal_agg['Protein'],\n                    metal_agg['Correlation'].round(2)\n                ],\n                font=dict(size=11),\n                fill_color=[[\"#E6F1FF\", \"white\"]*len(metal_agg)],\n                align=\"left\",\n                height=25\n            )\n        ),\n        row=1, col=2\n    )\n\n    # Update layout\n    fig.update_layout(\n        title={\n            'text': \"Metal Involvement in Microbial Corrosion Mechanisms\",\n            'y':0.98,\n            'x':0.5,\n            'xanchor': 'center',\n            'yanchor': 'top'\n        },\n        barmode='group',\n        height=600,\n        width=1200,\n        legend_title=\"Risk Category\",\n        font=dict(\n            family=\"Arial, sans-serif\",\n            size=12\n        )\n    )\n\n    # Update x and y axis properties\n    fig.update_xaxes(title_text=\"Metal\", row=1, col=1)\n    fig.update_yaxes(title_text=\"Mean Abundance\", row=1, col=1)\n\n    return fig\n\n# To use this function:\nfig = plot_metal_involvement(prioritized_markers, top_n=50)\nfig.show()  # In a notebook or dashboard environment","metadata":{"id":"pCwdCV5OPZcF","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.181Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport plotly.graph_objects as go\n\ndef debug_and_plot_temporal_transition(prioritized_markers):\n    \"\"\"\n    Debugs issues with the temporal transition analysis and creates a Sankey diagram\n    showing the flow of proteins between risk categories.\n    \"\"\"\n    # First, let's check if we have corrosion_mechanisms data\n    print(f\"Total rows in dataset: {len(prioritized_markers)}\")\n\n    # Check for non-empty corrosion_mechanisms\n    has_mechanisms = prioritized_markers['corrosion_mechanisms'].notna() & (prioritized_markers['corrosion_mechanisms'] != '')\n    print(f\"Rows with non-empty corrosion_mechanisms: {has_mechanisms.sum()}\")\n\n    # Check the first few mechanism entries to understand format\n    print(\"\\nSample corrosion_mechanisms entries:\")\n    for mech in prioritized_markers['corrosion_mechanisms'].dropna().head(3).values:\n        print(f\"  - {mech}\")\n\n    # The issue might be with the 'significant' column - check its values\n    if 'significant' in prioritized_markers.columns:\n        print(f\"\\nRows marked as significant: {prioritized_markers['significant'].sum()}\")\n        # Try using prioritized_markers instead of classified_results\n        sig_results = prioritized_markers.copy()\n    else:\n        print(\"\\n'significant' column not found - using all data\")\n        sig_results = prioritized_markers.copy()\n        sig_results['significant'] = True\n\n    # Create category dominance for each protein-genus pair\n    sig_results['dominant_category'] = sig_results[['mean_1', 'mean_2', 'mean_3']].idxmax(axis=1)\n    sig_results['dominant_category'] = sig_results['dominant_category'].map({\n        'mean_1': 'Category 1',\n        'mean_2': 'Category 2',\n        'mean_3': 'Category 3'\n    })\n\n    print(f\"Distribution by dominant category: {sig_results['dominant_category'].value_counts().to_dict()}\")\n\n    # Group by genus and corrosion mechanism\n    flow_data = []\n    mechanisms_set = set()\n\n    # Try different separator characters\n    for separator in [';', ',', '|']:\n        print(f\"\\nTrying separator: '{separator}'\")\n        mechanism_count = 0\n\n        for _, row in sig_results.iterrows():\n            if isinstance(row['corrosion_mechanisms'], str) and row['corrosion_mechanisms']:\n                mechanisms = [m.strip() for m in row['corrosion_mechanisms'].split(separator) if m.strip()]\n                mechanism_count += len(mechanisms)\n\n                if len(mechanisms) > 0:\n                    # If we found mechanisms, break out of the separator loop\n                    break\n\n        print(f\"Found {mechanism_count} mechanisms with separator '{separator}'\")\n        if mechanism_count > 0:\n            # We found our separator\n            break\n\n    # Use the best separator found\n    for _, row in sig_results.iterrows():\n        if isinstance(row['corrosion_mechanisms'], str) and row['corrosion_mechanisms']:\n            mechanisms = [m.strip() for m in row['corrosion_mechanisms'].split(separator) if m.strip()]\n\n            for m in mechanisms:\n                mechanisms_set.add(m)  # Track all unique mechanisms\n\n                # Determine the dominant category for this protein\n                dom_cat = row['dominant_category']\n\n                # Add a flow from mechanism to dominant category\n                flow_data.append({\n                    'source': m,\n                    'target': dom_cat,\n                    'value': 1,  # Count each protein once\n                    'genus': row['Genus'],\n                    'protein': row['protein_name']\n                })\n\n    # Convert to DataFrame\n    flow_df = pd.DataFrame(flow_data)\n\n    # Print some diagnostic info\n    print(f\"\\nNumber of unique mechanisms: {len(mechanisms_set)}\")\n    print(f\"Number of flows: {len(flow_df)}\")\n\n    # Check if we have any data before proceeding\n    if len(flow_df) == 0:\n        print(\"No flows found. Check your corrosion_mechanisms data format.\")\n        return None\n\n    # Create node lists\n    mechanisms = sorted(list(mechanisms_set))\n    categories = ['Category 1', 'Category 2', 'Category 3']\n    all_nodes = mechanisms + categories\n\n    # Map source and target to indices\n    node_indices = {node: i for i, node in enumerate(all_nodes)}\n\n    flow_df['source_idx'] = flow_df['source'].map(node_indices)\n    flow_df['target_idx'] = flow_df['target'].map(node_indices)\n\n    # Aggregate flows by source and target\n    agg_flows = flow_df.groupby(['source_idx', 'target_idx']).size().reset_index(name='value')\n\n    # Define node colors - use specified category colors\n    category_colors = {\n        'Category 1': '#008800',  # Dark green\n        'Category 2': '#FF8C00',  # Dark orange\n        'Category 3': '#FF0000'   # Red\n    }\n\n    # Generate mechanism colors - use a gradient\n    import matplotlib.cm as cm\n    from matplotlib.colors import rgb2hex\n\n    cmap = cm.get_cmap('viridis', len(mechanisms))\n    mechanism_colors = {m: rgb2hex(cmap(i)[:3]) for i, m in enumerate(mechanisms)}\n\n    # Combine all colors\n    node_colors = [mechanism_colors.get(node, '#9467bd') if node in mechanisms\n                  else category_colors.get(node, '#1f77b4') for node in all_nodes]\n\n    # Create labels with proper formatting\n    labels = []\n    for node in all_nodes:\n        if node in mechanisms:\n            # Truncate long mechanism names\n            if len(node) > 25:\n                labels.append(node[:22] + '...')\n            else:\n                labels.append(node)\n        else:\n            # Format category names\n            labels.append(node)\n\n    # Create Sankey diagram\n    fig = go.Figure(data=[go.Sankey(\n        arrangement=\"snap\",  # Help prevent overlapping\n        node=dict(\n            pad=15,\n            thickness=20,\n            line=dict(color=\"black\", width=0.5),\n            label=labels,\n            color=node_colors,\n            hoverinfo=\"all\",\n            hovertemplate=\"<b>%{label}</b><br>Count: %{value}<extra></extra>\"\n        ),\n        link=dict(\n            source=agg_flows['source_idx'],\n            target=agg_flows['target_idx'],\n            value=agg_flows['value'],\n            hovertemplate=\"<b>%{source.label}</b> → <b>%{target.label}</b><br>Count: %{value}<extra></extra>\"\n        )\n    )])\n\n    fig.update_layout(\n        title_text=\"Metabolic Mechanisms Across Risk Categories\",\n        font_size=12,\n        height=800,\n        width=1200,\n        # Add more margin to accommodate labels\n        margin=dict(l=50, r=50, t=50, b=50)\n    )\n\n    return fig\n\n# To use this function:\ndebug_fig = debug_and_plot_temporal_transition(prioritized_markers)","metadata":{"id":"B4ZH63MNTAQa","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.181Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 5. TEMPORAL TRANSITION ANALYSIS\ndef plot_temporal_transition(classified_results):\n    \"\"\"\n    Creates a Sankey diagram showing the flow of proteins between risk categories.\n    \"\"\"\n    from plotly.graph_objects import Sankey\n\n    # Filter for significant proteins\n    sig_results = classified_results[classified_results['significant'] == True].copy()\n\n    # Create category dominance for each protein-genus pair\n    sig_results['dominant_category'] = sig_results[['mean_cat1', 'mean_cat2', 'mean_cat3']].idxmax(axis=1)\n    sig_results['dominant_category'] = sig_results['dominant_category'].map({\n        'mean_cat1': 'Category 1',\n        'mean_cat2': 'Category 2',\n        'mean_cat3': 'Category 3'\n    })\n\n    # Group by genus and corrosion mechanism\n    flow_data = []\n    for genus, group in sig_results.groupby('Genus'):\n        for mech in group['corrosion_mechanisms'].dropna().unique():\n            if isinstance(mech, str) and mech:\n                mechanisms = [m.strip() for m in mech.split(';')]\n                for m in mechanisms:\n                    # Count proteins in each category\n                    cat1_count = ((group['dominant_category'] == 'Category 1') &\n                                 (group['corrosion_mechanisms'].str.contains(m, na=False))).sum()\n                    cat2_count = ((group['dominant_category'] == 'Category 2') &\n                                 (group['corrosion_mechanisms'].str.contains(m, na=False))).sum()\n                    cat3_count = ((group['dominant_category'] == 'Category 3') &\n                                 (group['corrosion_mechanisms'].str.contains(m, na=False))).sum()\n\n                    # Add flows between each category\n                    if cat1_count > 0 and cat2_count > 0:\n                        flow_data.append({\n                            'source': 'Category 1',\n                            'target': 'Category 2',\n                            'value': min(cat1_count, cat2_count),\n                            'mechanism': m\n                        })\n                    if cat2_count > 0 and cat3_count > 0:\n                        flow_data.append({\n                            'source': 'Category 2',\n                            'target': 'Category 3',\n                            'value': min(cat2_count, cat3_count),\n                            'mechanism': m\n                        })\n\n    # Convert to DataFrame\n    flow_df = pd.DataFrame(flow_data)\n\n    # Create node lists\n    nodes = ['Category 1', 'Category 2', 'Category 3']\n    mechanisms = flow_df['mechanism'].unique()\n    all_nodes = nodes + list(mechanisms)\n\n    # Map source and target to indices\n    node_indices = {node: i for i, node in enumerate(all_nodes)}\n    flow_df['source_idx'] = flow_df['source'].map(node_indices)\n    flow_df['target_idx'] = flow_df['target'].map(node_indices)\n\n    # Create Sankey diagram\n    fig = go.Figure(data=[go.Sankey(\n        node=dict(\n            pad=15,\n            thickness=20,\n            line=dict(color=\"black\", width=0.5),\n            label=all_nodes,\n            color=['rgba(31, 119, 180, 0.8)', 'rgba(255, 127, 14, 0.8)', 'rgba(214, 39, 40, 0.8)'] +\n                  ['rgba(148, 103, 189, 0.8)'] * len(mechanisms)\n        ),\n        link=dict(\n            source=flow_df['source_idx'],\n            target=flow_df['target_idx'],\n            value=flow_df['value'],\n            label=flow_df['mechanism'],\n        )\n    )])\n\n    fig.update_layout(\n        title_text=\"Metabolic Transitions Between Risk Categories\",\n        font_size=12,\n        height=800,\n        width=1200\n    )\n\n    return fig\n\ntemporal_transition = plot_temporal_transition(classified_results)","metadata":{"id":"jnelNG2Hl9EC","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.181Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prioritized_markers.columns","metadata":{"id":"dR8PIpnFUCca","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.181Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 6. MECHANISM DISTRIBUTION BY RISK CATEGORY\ndef plot_mechanism_distribution(prioritized_markers):\n    \"\"\"\n    Creates stacked bar charts showing mechanism distribution across risk categories.\n    \"\"\"\n    # Process mechanism data\n    mech_data = []\n\n    for _, row in prioritized_markers.iterrows():\n        if isinstance(row['corrosion_mechanisms'], str) and row['corrosion_mechanisms']:\n            mechanisms = [m.strip() for m in row['corrosion_mechanisms'].split(';')]\n            for mech in mechanisms:\n                mech_data.append({\n                    'mechanism': mech,\n                    'Category 1': row['mean_cat1'],\n                    'Category 2': row['mean_cat2'],\n                    'Category 3': row['mean_cat3']\n                })\n\n    # Convert to DataFrame\n    mech_df = pd.DataFrame(mech_data)\n\n    # Group by mechanism\n    grouped = mech_df.groupby('mechanism').agg({\n        'Category 1': 'sum',\n        'Category 2': 'sum',\n        'Category 3': 'sum'\n    }).reset_index()\n\n    # Calculate total abundance and sort\n    grouped['total'] = grouped['Category 1'] + grouped['Category 2'] + grouped['Category 3']\n    grouped = grouped.sort_values('total', ascending=False).head(10)\n\n    # Prepare data for stacked bar chart\n    data = []\n\n    for category in ['Category 1', 'Category 2', 'Category 3']:\n        data.append(\n            go.Bar(\n                name=category,\n                x=grouped['mechanism'],\n                y=grouped[category],\n                text=grouped[category].round(2),\n                textposition='auto'\n            )\n        )\n\n    # Create figure\n    fig = go.Figure(data=data)\n\n    # Update layout\n    fig.update_layout(\n        barmode='stack',\n        title='Top 10 Corrosion Mechanisms by Risk Category',\n        xaxis_title='Mechanism',\n        yaxis_title='Total Abundance',\n        legend_title='Risk Category',\n        height=600,\n        width=1000\n    )\n\n    return fig\n\nmechanism_distribution = plot_mechanism_distribution(prioritized_markers)","metadata":{"id":"suz2oJNvj7QZ","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.181Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.colors import LinearSegmentedColormap\nimport matplotlib.patches as mpatches\nfrom scipy.stats import gaussian_kde\nfrom sklearn.preprocessing import StandardScaler\nimport re\n\n# Use your custom category dictionary and color mapping\ncategory_colors = {\n    1: '#008800',  # Dark green - Normal Operation\n    2: '#FF8C00',  # Dark orange - Early Warning\n    3: '#FF0000'   # Red - System Failure\n}\n\ncategories_labels = {\n    1: 'Normal Operation',\n    2: 'Early Warning',\n    3: 'System Failure'\n}\n\ndef plot_mechanism_analysis(prioritized_markers, mechanisms_df=None, top_n=30):\n    \"\"\"\n    Creates comprehensive visualizations of corrosion mechanisms across risk categories.\n\n    Args:\n        prioritized_markers (DataFrame): DataFrame containing marker data\n        mechanisms_df (DataFrame, optional): DataFrame with mechanism focus data\n        top_n (int): Number of top markers to include\n\n    Returns:\n        matplotlib figure with multiple subplots showing different aspects of mechanisms\n    \"\"\"\n    # Select top markers by score\n    top_markers = prioritized_markers.sort_values('combined_score', ascending=False).head(top_n)\n\n    # Create figure with multiple subplots\n    fig = plt.figure(figsize=(22, 18))\n\n    # 1. Mechanism matrix across risk categories (top left)\n    ax1 = plt.subplot2grid((2, 2), (0, 0))\n\n    # Extract and process mechanisms\n    mechanisms_data = []\n\n    for _, row in top_markers.iterrows():\n        if isinstance(row['corrosion_mechanisms'], str) and row['corrosion_mechanisms']:\n            mechanisms = [m.strip() for m in row['corrosion_mechanisms'].split(';')]\n            for mech in mechanisms:\n                if mech:  # Ensure mechanism is not empty\n                    # Simplify mechanism name if too long\n                    mech_display = mech if len(mech) <= 30 else mech[:27] + '...'\n\n                    mechanisms_data.append({\n                        'Mechanism': mech_display,\n                        'Original': mech,\n                        'Category 1': row['mean_1'] if pd.notna(row['mean_1']) else 0,\n                        'Category 2': row['mean_2'] if pd.notna(row['mean_2']) else 0,\n                        'Category 3': row['mean_3'] if pd.notna(row['mean_3']) else 0,\n                        'Score': row['combined_score'] if pd.notna(row['combined_score']) else 0,\n                        'Genus': row['Genus'],\n                        'Protein': row['protein_name']\n                    })\n\n    # Convert to DataFrame\n    mech_df = pd.DataFrame(mechanisms_data)\n\n    # Calculate frequencies of mechanisms\n    mech_counts = mech_df['Mechanism'].value_counts()\n\n    # Filter to mechanisms that appear at least twice\n    common_mechs = mech_counts[mech_counts >= 2].index.tolist()\n    filtered_mech_df = mech_df[mech_df['Mechanism'].isin(common_mechs)]\n\n    # Aggregate by mechanism\n    agg_mech = filtered_mech_df.groupby('Mechanism').agg({\n        'Category 1': 'mean',\n        'Category 2': 'mean',\n        'Category 3': 'mean',\n        'Score': 'mean'\n    }).reset_index()\n\n    # Calculate correlation with risk progression\n    agg_mech['Correlation'] = agg_mech.apply(\n        lambda x: np.corrcoef([1, 2, 3], [x['Category 1'], x['Category 2'], x['Category 3']])[0, 1],\n        axis=1\n    )\n\n    # Sort by correlation\n    agg_mech = agg_mech.sort_values('Correlation', ascending=False)\n\n    # Take top 10 mechanisms\n    plot_mechs = pd.concat([\n        agg_mech.head(5),  # Top 5 positively correlated\n        agg_mech.tail(5)   # Top 5 negatively correlated\n    ])\n\n    # Create heatmap data\n    heatmap_data = plot_mechs.set_index('Mechanism')[['Category 1', 'Category 2', 'Category 3']]\n\n    # Calculate z-scores within each row\n    heat_data_zscore = pd.DataFrame(index=heatmap_data.index, columns=heatmap_data.columns)\n    for idx, row in heatmap_data.iterrows():\n        if row.std() > 0:  # Avoid division by zero\n            heat_data_zscore.loc[idx] = (row - row.mean()) / row.std()\n        else:\n            heat_data_zscore.loc[idx] = row - row.mean()\n\n    # Create heatmap\n    sns.heatmap(\n        heat_data_zscore,\n        cmap='coolwarm',\n        center=0,\n        annot=True,\n        fmt=\".2f\",\n        linewidths=0.5,\n        xticklabels=[categories_labels[1], categories_labels[2], categories_labels[3]],\n        ax=ax1\n    )\n\n    # Add correlation values as text\n    for i, idx in enumerate(heat_data_zscore.index):\n        corr = plot_mechs.loc[plot_mechs['Mechanism'] == idx, 'Correlation'].values[0]\n        ax1.text(3.1, i + 0.5, f\"r = {corr:.2f}\", va='center')\n\n    ax1.set_title('Key Corrosion Mechanisms Across Risk Categories', fontsize=14)\n\n    # 2. Genus-Mechanism relationship (top right)\n    ax2 = plt.subplot2grid((2, 2), (0, 1))\n\n    # Create a mapping of genera to mechanisms\n    genus_mech_map = {}\n\n    for _, row in mech_df.iterrows():\n        genus = row['Genus']\n        mech = row['Mechanism']\n\n        if genus not in genus_mech_map:\n            genus_mech_map[genus] = {}\n\n        if mech not in genus_mech_map[genus]:\n            genus_mech_map[genus][mech] = 0\n\n        genus_mech_","metadata":{"id":"NrychGg0ugE-","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.182Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# 7. PROTEIN ABUNDANCE TRAJECTORIES\ndef plot_protein_trajectories(prioritized_markers, top_n=20):\n    \"\"\"\n    Creates a line chart showing protein abundance trajectories across risk categories.\n    \"\"\"\n    # Select top proteins\n    top_proteins = prioritized_markers.sort_values('combined_score', ascending=False).head(top_n)\n\n    # Prepare data for trajectories\n    trajectories = []\n\n    for i, row in top_proteins.iterrows():\n        trajectories.append({\n            'id': f\"{row['Genus']} - {row['protein_name']}\",\n            'Category 1': row['mean_cat1'],\n            'Category 2': row['mean_cat2'],\n            'Category 3': row['mean_cat3'],\n            'pattern': row['pattern'],\n            'score': row['combined_score']\n        })\n\n    # Convert to DataFrame\n    traj_df = pd.DataFrame(trajectories)\n\n    # Melt for plotting\n    melted = pd.melt(\n        traj_df,\n        id_vars=['id', 'pattern', 'score'],\n        value_vars=['Category 1', 'Category 2', 'Category 3'],\n        var_name='category',\n        value_name='abundance'\n    )\n\n    # Map categories to numeric values for plotting\n    melted['category_num'] = melted['category'].map({\n        'Category 1': 1,\n        'Category 2': 2,\n        'Category 3': 3\n    })\n\n    # Create line chart\n    fig = px.line(\n        melted,\n        x='category_num',\n        y='abundance',\n        color='id',\n        line_group='id',\n        hover_data=['pattern', 'score'],\n        markers=True,\n        title='Protein Abundance Trajectories Across Risk Categories',\n        labels={'category_num': 'Risk Category', 'abundance': 'Mean Abundance'},\n        height=600,\n        width=1000\n    )\n\n    # Update x-axis\n    fig.update_xaxes(\n        tickvals=[1, 2, 3],\n        ticktext=['Category 1', 'Category 2', 'Category 3']\n    )\n\n    return fig\n\nprotein_trajectories = plot_protein_trajectories(prioritized_markers, top_n=20)","metadata":{"id":"8qk6uBgsmKry","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.182Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 8. UMAP WITH PROTEIN METADATA\ndef plot_umap_with_metadata(classified_results):\n    \"\"\"\n    Creates a UMAP visualization colored by protein metadata.\n    \"\"\"\n    # Prepare data for UMAP\n    features = classified_results[['mean_cat1', 'mean_cat2', 'mean_cat3']].copy()\n\n    # Standardize features\n    scaler = StandardScaler()\n    scaled_features = scaler.fit_transform(features)\n\n    # Compute UMAP embedding\n    reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)\n    embedding = reducer.fit_transform(scaled_features)\n\n    # Add embedding to dataframe\n    umap_df = classified_results.copy()\n    umap_df['umap_x'] = embedding[:, 0]\n    umap_df['umap_y'] = embedding[:, 1]\n\n    # Add dominant category\n    umap_df['dominant_category'] = umap_df[['mean_cat1', 'mean_cat2', 'mean_cat3']].idxmax(axis=1)\n    umap_df['dominant_category'] = umap_df['dominant_category'].map({\n        'mean_cat1': 'Category 1',\n        'mean_cat2': 'Category 2',\n        'mean_cat3': 'Category 3'\n    })\n\n    # Create UMAP visualization with four facets\n    fig = make_subplots(\n        rows=2,\n        cols=2,\n        subplot_titles=(\n            'Colored by Dominant Category',\n            'Colored by Pathway Classification',\n            'Colored by Has Metal',\n            'Colored by Corrosion Relevance'\n        )\n    )\n\n    # Facet 1: Colored by dominant category\n    fig.add_trace(\n        go.Scatter(\n            x=umap_df['umap_x'],\n            y=umap_df['umap_y'],\n            mode='markers',\n            marker=dict(\n                size=5,\n                color=umap_df['dominant_category'].map({\n                    'Category 1': 'blue',\n                    'Category 2': 'orange',\n                    'Category 3': 'red'\n                }),\n                opacity=0.7\n            ),\n            text=umap_df['Genus'] + ' - ' + umap_df['protein_name'],\n            hoverinfo='text',\n            showlegend=False\n        ),\n        row=1, col=1\n    )\n\n    # Facet 2: Colored by pathway classification\n    fig.add_trace(\n        go.Scatter(\n            x=umap_df['umap_x'],\n            y=umap_df['umap_y'],\n            mode='markers',\n            marker=dict(\n                size=5,\n                color=umap_df['pathway_classification'].map({\n                    'niche-specific': 'green',\n                    'mixed': 'purple',\n                    'universal': 'gray'\n                }),\n                opacity=0.7\n            ),\n            text=umap_df['Genus'] + ' - ' + umap_df['protein_name'],\n            hoverinfo='text',\n            showlegend=False\n        ),\n        row=1, col=2\n    )\n\n    # Facet 3: Colored by has_metal\n    fig.add_trace(\n        go.Scatter(\n            x=umap_df['umap_x'],\n            y=umap_df['umap_y'],\n            mode='markers',\n            marker=dict(\n                size=5,\n                color=umap_df['has_metal'].map({\n                    True: 'gold',\n                    False: 'lightgray'\n                }),\n                opacity=0.7\n            ),\n            text=umap_df['Genus'] + ' - ' + umap_df['protein_name'],\n            hoverinfo='text',\n            showlegend=False\n        ),\n        row=2, col=1\n    )\n\n    # Facet 4: Colored by corrosion_relevance\n    fig.add_trace(\n        go.Scatter(\n            x=umap_df['umap_x'],\n            y=umap_df['umap_y'],\n            mode='markers',\n            marker=dict(\n                size=5,\n                color=umap_df['corrosion_relevance'].map({\n                    'high': 'red',\n                    'medium': 'orange',\n                    'low': 'yellow'\n                }),\n                opacity=0.7\n            ),\n            text=umap_df['Genus'] + ' - ' + umap_df['protein_name'],\n            hoverinfo='text',\n            showlegend=False\n        ),\n        row=2, col=2\n    )\n\n    # Update layout\n    fig.update_layout(\n        title='UMAP Visualization of Protein-Genus Pairs with Metadata',\n        height=800,\n        width=1200\n    )\n\n    return fig\numap_metadata = plot_umap_with_metadata(classified_results)","metadata":{"id":"fLSIyEkblNp3","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.182Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# @title Genus vs Mechanism\nplt.subplots(figsize=(8, 8))\n\n# Ensure 'Genus' and 'Mechanism' are treated as categorical\nmechanisms['Genus'] = pd.Categorical(mechanisms['Genus'])\nmechanisms['Mechanism'] = pd.Categorical(mechanisms['Mechanism'])\n\ndf_2dhist = pd.DataFrame({\n    x_label: grp['Mechanism'].value_counts()\n    for x_label, grp in mechanisms.groupby('Genus')\n})\n\n# Convert index and columns to lists to avoid MultiIndex issue\ndf_2dhist = df_2dhist.reindex(index=df_2dhist.index.tolist(), columns=df_2dhist.columns.tolist())\n\nsns.heatmap(df_2dhist, cmap='viridis')\nplt.xlabel('Genus')\n_ = plt.ylabel('Mechanism')","metadata":{"id":"U7ljhBfNa3tB","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.182Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"inverse_markers = complete_results['inverse_markers']","metadata":{"id":"Owo7FeqpaRwJ","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.182Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define category colors\ncategory_colors = {\n    1: '#008800',  # Dark green\n    2: '#FF8C00',  # Dark orange\n    3: '#FF0000'   # Red\n}\n\ncategories_labels = {\n    1: 'Normal Operation',\n    2: 'Early Warning',\n    3: 'System Failure'\n}\n\ndef plot_top_protein_heatmap(df, n_top=10, sort_by='corrosion_final_score'):\n    \"\"\"\n    Create a heatmap of top protein-genera combinations across categories.\n\n    Parameters:\n    - df: DataFrame with processed results (prioritized_markers)\n    - n_top: Number of top entries to display per category\n    - sort_by: Column to sort by for selecting top entries\n    \"\"\"\n    # Filter for significant results\n    if 'significant' in df.columns:\n        significant_df = df[df['significant'] == True].copy(deep=False)\n    else:\n        significant_df = df.copy(deep=False)\n\n    # Get top entries for each category\n    try:\n        # Try to filter by abundance pattern\n        top_cat1 = significant_df[significant_df['pattern'].str.contains('higher in cat 1', na=False)].nlargest(n_top, sort_by)\n        top_cat2 = significant_df[significant_df['pattern'].str.contains('higher in cat 2', na=False)].nlargest(n_top, sort_by)\n        top_cat3 = significant_df[significant_df['pattern'].str.contains('higher in cat 3', na=False)].nlargest(n_top, sort_by)\n    except:\n        # Alternative approach - determine dominant category by mean values\n        significant_df['dominant_category'] = significant_df[['mean_cat1', 'mean_cat2', 'mean_cat3']].idxmax(axis=1)\n        significant_df['dominant_category'] = significant_df['dominant_category'].str.replace('mean_cat', '')\n\n        top_cat1 = significant_df[significant_df['dominant_category'] == '1'].nlargest(n_top, sort_by)\n        top_cat2 = significant_df[significant_df['dominant_category'] == '2'].nlargest(n_top, sort_by)\n        top_cat3 = significant_df[significant_df['dominant_category'] == '3'].nlargest(n_top, sort_by)\n\n    # Combine top entries\n    top_combined = pd.concat([top_cat1, top_cat2, top_cat3])\n\n    # Create a unique identifier for each protein-genera combo\n    top_combined['protein_genera'] = top_combined['Genus'] + ' - ' + top_combined['protein_name'].str[:20]\n\n    # Create a pivot table for the heatmap\n    pivot_data = top_combined.pivot_table(\n        index='protein_genera',\n        values=['mean_cat1', 'mean_cat2', 'mean_cat3']\n    )\n\n    # Normalize for better visualization\n    # This helps see patterns even when absolute values differ greatly\n    norm_data = pivot_data.div(pivot_data.max(axis=1), axis=0)\n\n    # Set up the plot with a good height based on number of entries\n    plt.figure(figsize=(10, max(8, 0.3 * len(norm_data))))\n\n    # Create a heatmap with custom color gradient\n    cmap = sns.color_palette(\"YlOrRd\", as_cmap=True)\n\n    # Create the heatmap\n    ax = sns.heatmap(\n        norm_data,\n        cmap=cmap,\n        annot=pivot_data.round(2),  # Show original values but display normalized colors\n        fmt=\".2f\",\n        linewidths=0.5,\n        cbar_kws={'label': 'Normalized Abundance'}\n    )\n\n    # Style the plot\n    ax.set_title('Top Protein-Genera Combinations by Category', fontsize=16)\n\n    # Rename columns for better readability\n    new_labels = [categories_labels[1], categories_labels[2], categories_labels[3]]\n    ax.set_xticklabels(new_labels, rotation=45, ha='right')\n\n    plt.tight_layout()\n\n    return plt.gcf()\n\ndef plot_category_specific_heatmaps(df, n_top=10, sort_by='corrosion_final_score'):\n    \"\"\"\n    Create three separate heatmaps highlighting the top protein-genera combinations\n    that are dominant in each category.\n\n    Parameters:\n    - df: DataFrame with processed results (prioritized_markers)\n    - n_top: Number of top entries to display per category\n    - sort_by: Column to sort by for selecting top entries\n    \"\"\"\n    # Filter for significant results\n    if 'significant' in df.columns:\n        significant_df = df[df['significant'] == True].copy(deep=False)\n    else:\n        significant_df = df.copy(deep=False)\n\n    # Create figure with three subplots\n    fig, axes = plt.subplots(1, 3, figsize=(18, 10), sharey=False)\n\n    # Process each category\n    for i, cat in enumerate([1, 2, 3]):\n        # Try to filter by abundance pattern first\n        try:\n            cat_data = significant_df[significant_df['pattern'].str.contains(f'higher in cat {cat}', na=False)]\n            if cat_data.empty:\n                raise ValueError(\"No data found with pattern\")\n        except:\n            # Alternative approach - determine dominant category by mean values\n            cat_col = f'mean_cat{cat}'\n            other_cols = [f'mean_cat{j}' for j in [1, 2, 3] if j != cat]\n            cat_data = significant_df[significant_df[cat_col] > significant_df[other_cols].max(axis=1)]\n\n        # Get top entries\n        top_data = cat_data.nlargest(n_top, sort_by).copy()\n\n        # Create a unique identifier\n        top_data['protein_genera'] = top_data['Genus'] + ' - ' + top_data['protein_name'].str[:20]\n\n        # Extract values for this category and set as dataframe\n        values_df = top_data.set_index('protein_genera')[[f'mean_cat{cat}']]\n        values_df.columns = [f'Category {cat}']\n\n        # Create a custom colormap\n        cat_cmap = LinearSegmentedColormap.from_list(\n            f'cat{cat}_cmap',\n            ['white', category_colors[cat]]\n        )\n\n        # Plot the heatmap\n        ax = sns.heatmap(\n            values_df,\n            annot=True,\n            fmt=\".2f\",\n            cmap=cat_cmap,\n            ax=axes[i],\n            cbar_kws={'label': 'Abundance'}\n        )\n\n        # Style the plot\n        ax.set_title(f'{categories_labels[cat]}', fontsize=14)\n        ax.set_xlabel('')\n\n        # Only show y-axis labels on the first subplot\n        if i > 0:\n            ax.set_ylabel('')\n\n    # Add overall title\n    plt.suptitle('Top Protein-Genera Combinations by Category', fontsize=16, y=1.05)\n    plt.tight_layout()\n\n    return fig\n\n# Running on prioritized markers\nfig1 = plot_top_protein_heatmap(top_markers)\n\nfig2 = plot_category_specific_heatmaps(prioritized_markers)\n","metadata":{"id":"uCzmUKAB0O1X","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.182Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_top_protein_heatmap(prioritized_markers, n_top=10)","metadata":{"id":"OILrTfIJ2F0u","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.182Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Select top pathways based on combined_score or corrosion_final_score\ndef create_forest_plot(markers_df, n_top=15, sort_by='combined_score'):\n    # Sort and select top pathways\n    top_markers = markers_df.sort_values(by=sort_by, ascending=False).head(n_top)\n\n    # Calculate difference between categories (similar to log2fc values)\n    # Using mean values across categories\n    top_markers = top_markers.assign(\n        diff_2_vs_1=top_markers['mean_2'] - top_markers['mean_1'],\n        diff_3_vs_2=top_markers['mean_3'] - top_markers['mean_2']\n    )\n\n    # Create figure\n    fig, ax = plt.subplots(figsize=(10, 12))\n\n    # Create a custom label combining genus and protein\n    labels = [f\"{g} - {p[:30]}...\" if len(p) > 30 else f\"{g} - {p}\"\n             for g, p in zip(top_markers['Genus'], top_markers['protein_name'])]\n\n    # Position on y-axis\n    y_pos = np.arange(len(labels))\n\n    # Create a categorical color palette based on hierarchies (from hierarchy column)\n    # Extract the top level of hierarchy for coloring\n    hierarchies = top_markers['hierarchy'].apply(lambda x: x.split('/')[0] if isinstance(x, str) and '/' in x else 'Other')\n    unique_hierarchies = hierarchies.unique()\n\n    color_palette = sns.color_palette(\"husl\", len(unique_hierarchies))\n    color_map = dict(zip(unique_hierarchies, color_palette))\n    point_colors = [color_map[h] for h in hierarchies]\n\n    # Plot the difference between category 3 and 1 (overall change)\n    diff_3_vs_1 = top_markers['mean_3'] - top_markers['mean_1']\n\n    # Plot points\n    scatter = ax.scatter(\n        diff_3_vs_1,\n        y_pos,\n        c=point_colors,\n        s=80,\n        zorder=3\n    )\n\n    # Add confidence intervals/error bars using standard deviation\n    for i, (idx, row) in enumerate(top_markers.iterrows()):\n        # Calculate combined standard error for the difference\n        se = np.sqrt((row['std_1']**2 / row['count_1']) + (row['std_3']**2 / row['count_3']))\n        # 95% confidence interval\n        ci = 1.96 * se\n\n        # Plot error bars\n        ax.plot(\n            [diff_3_vs_1.iloc[i] - ci, diff_3_vs_1.iloc[i] + ci],\n            [y_pos[i], y_pos[i]],\n            'k-',\n            alpha=0.7,\n            zorder=2\n        )\n\n    # Add vertical line at x=0\n    ax.axvline(x=0, color='gray', linestyle='-', alpha=0.7, zorder=1)\n\n    # Customize the plot\n    ax.set_yticks(y_pos)\n    ax.set_yticklabels(labels)\n    ax.set_xlabel('Difference in mean abundance (Category 3 - Category 1)')\n    ax.set_title('Differential Abundance of Key Pathways Between Risk Categories')\n    ax.grid(axis='x', linestyle='--', alpha=0.3)\n\n    # Add significance markers\n    for i, p_val in enumerate(top_markers['p_value']):\n        if p_val <= 0.05:\n            marker = '*' if p_val <= 0.05 else ''\n            marker = '**' if p_val <= 0.01 else marker\n            marker = '***' if p_val <= 0.001 else marker\n\n            ax.text(\n                diff_3_vs_1.iloc[i] + (0.05 * max(abs(diff_3_vs_1))),\n                y_pos[i],\n                marker,\n                fontsize=12,\n                va='center'\n            )\n\n    # Add a legend for hierarchies\n    handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color, markersize=8)\n              for color in color_map.values()]\n    ax.legend(handles, color_map.keys(), title='Pathway Hierarchy',\n             loc='lower right', bbox_to_anchor=(1.15, 0))\n\n    plt.tight_layout()\n    #plt.savefig('pathway_differential_abundance.png', dpi=300, bbox_inches='tight')\n    #plt.show()\n\n    return fig\n\n# Example usage:\ncreate_forest_plot(top_markers, n_top=15, sort_by='combined_score')","metadata":{"id":"S-Y3uidSKXkW","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.183Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_functional_group_heatmap(classified_results, mechanism_focus=True, top_n=100):\n    \"\"\"\n    Create a heatmap grouped by functional categories (mechanisms or pathway classification)\n\n    Parameters:\n        classified_results: DataFrame with classified pathways\n        mechanism_focus: Whether to focus on mechanisms (True) or pathway classification (False)\n        top_n: Number of top pairs to use (default: 100)\n\n    Returns:\n        matplotlib figure\n    \"\"\"\n    # Get top markers\n    top_markers = classified_results.head(top_n)\n\n    if mechanism_focus and 'corrosion_mechanisms' in top_markers.columns:\n        # Extract all mechanisms\n        all_mechanisms = set()\n        for mechs in top_markers['corrosion_mechanisms'].dropna():\n            if isinstance(mechs, str):\n                all_mechanisms.update([m.strip() for m in mechs.split(';')])\n\n        # Create a matrix: rows=genera, columns=mechanisms\n        genera = top_markers['Genus'].unique()\n        mechanisms = sorted(list(all_mechanisms))\n\n        matrix = np.zeros((len(genera), len(mechanisms)))\n\n        # Fill matrix with counts\n        for i, genus in enumerate(genera):\n            genus_markers = top_markers[top_markers['Genus'] == genus]\n\n            for _, row in genus_markers.iterrows():\n                if pd.notna(row['corrosion_mechanisms']) and isinstance(row['corrosion_mechanisms'], str):\n                    marker_mechanisms = [m.strip() for m in row['corrosion_mechanisms'].split(';')]\n                    for mechanism in marker_mechanisms:\n                        if mechanism in mechanisms:\n                            j = mechanisms.index(mechanism)\n                            matrix[i, j] += 1\n\n        # Plot\n        plt.figure(figsize=(4, 5))\n\n        # Use a better colormap for mechanisms\n        cmap = LinearSegmentedColormap.from_list('corrosion', ['#FFFFFF', '#FF9966', '#CC3300'])\n\n        ax = sns.heatmap(matrix, cmap=cmap, linewidths=0.5, linecolor='gray',\n                      xticklabels=mechanisms, yticklabels=genera)\n\n        plt.title('Genus-Mechanism Relationship (Count of Proteins)')\n        plt.xlabel('Corrosion Mechanism')\n        plt.ylabel('Genus')\n        plt.xticks(rotation=45, ha='right')\n\n    else:\n        # Use pathway classification\n        if 'pathway_classification' not in top_markers.columns:\n            return None\n\n        # Create a matrix: rows=genera, columns=classifications\n        genera = top_markers['Genus'].unique()\n        classifications = ['universal', 'mixed', 'niche-specific']\n\n        matrix = np.zeros((len(genera), len(classifications)))\n\n        # Fill matrix with counts\n        for i, genus in enumerate(genera):\n            genus_markers = top_markers[top_markers['Genus'] == genus]\n\n            for classification in classifications:\n                j = classifications.index(classification)\n                count = sum(genus_markers['pathway_classification'] == classification)\n                matrix[i, j] = count\n\n        # Plot\n        plt.figure(figsize=(12, 10))\n        cmap = LinearSegmentedColormap.from_list('pathways', ['#FFFFFF', '#66CCFF', '#003366'])\n\n        ax = sns.heatmap(matrix, cmap=cmap, linewidths=0.5, linecolor='gray',\n                      xticklabels=classifications, yticklabels=genera)\n\n        plt.title('Genus-Pathway Classification Relationship (Count of Proteins)')\n        plt.xlabel('Pathway Classification')\n        plt.ylabel('Genus')\n\n    plt.tight_layout()\n    return plt.gcf()\n\nplot_functional_group_heatmap(classified_results, mechanism_focus=True, top_n=100)","metadata":{"id":"iVidoKkJ-gIk","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.183Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_pathway_site_heatmap(data_frame, n_pathways=20, group_by='corrosion_mechanisms'):\n    \"\"\"\n    Create a heatmap showing pathway abundance across sites, grouped by corrosion mechanism\n\n    Parameters: pandas DataFrame prioritized_markers\n    n_pathways : int Number of top pathways to include\n    group_by : str column to use for grouping (e.g., 'corrosion_mechanisms', 'metals_involved')\n    \"\"\"\n    # Use a different sorting metric if combined_score isn't available\n    sort_columns = ['corrosion_final_score', 'combined_score', 'corr', 'h_statistic']\n    sort_col = next((col for col in sort_columns if col in data_frame.columns), None)\n\n    if sort_col:\n        top_pathways = data_frame.sort_values(by=sort_col, ascending=False).head(n_pathways)\n    else:\n        # If none of the sort columns exist, use the first n_pathways rows\n        top_pathways = data_frame.head(n_pathways)\n\n    # Create a unique identifier for each pathway\n    top_pathways['pathway_id'] = top_pathways['Genus'] + ' - ' + top_pathways['protein_name'].apply(\n        lambda x: x[:20] + '...' if len(x) > 20 else x)\n\n    # Create a long-format DataFrame for the heatmap\n    heatmap_data = []\n\n    for _, row in top_pathways.iterrows():\n        # Parse the Sites column - handle different formats\n        if 'Sites' in row:\n            sites_str = row['Sites']\n            # Handle different formats of the Sites column\n            if isinstance(sites_str, str):\n                if ';' in sites_str:\n                    # Handle semicolon-separated format\n                    sites = [s.strip() for s in sites_str.split(';')]\n                elif ',' in sites_str:\n                    # Handle comma-separated format\n                    sites = [s.strip() for s in sites_str.split(',')]\n                elif '[' in sites_str:\n                    # Try to safely evaluate as a list\n                    try:\n                        sites = eval(sites_str)\n                    except:\n                        sites = [sites_str]\n                else:\n                    # Single site\n                    sites = [sites_str]\n            elif isinstance(sites_str, list):\n                sites = sites_str\n            else:\n                sites = []\n        else:\n            # If Sites column doesn't exist, use an empty list\n            sites = []\n\n        # Get abundance for each category\n        for category in [1, 2, 3]:\n            # Check if the mean columns exist\n            mean_col = f'mean_{category}'\n            if mean_col in row:\n                mean_value = row[mean_col]\n            else:\n                # Try alternative column names\n                alt_mean_col = f'mean_cat{category}'\n                if alt_mean_col in row:\n                    mean_value = row[alt_mean_col]\n                else:\n                    # If no abundance data for this category, skip\n                    continue\n\n            # Process the group_by column\n            if group_by in row:\n                mechanisms = row[group_by]\n                if isinstance(mechanisms, str):\n                    # Handle different potential formats\n                    if '[' in mechanisms:\n                        try:\n                            mechanisms = eval(mechanisms)\n                        except:\n                            mechanisms = [m.strip() for m in mechanisms.split(',') if m.strip()]\n                    elif ',' in mechanisms:\n                        mechanisms = [m.strip() for m in mechanisms.split(',') if m.strip()]\n                    else:\n                        mechanisms = [mechanisms]\n                elif isinstance(mechanisms, list):\n                    pass  # Already a list\n                else:\n                    mechanisms = ['Unknown']\n            else:\n                mechanisms = ['Unknown']\n\n            primary_mechanism = mechanisms[0] if mechanisms else 'Unknown'\n\n            heatmap_data.append({\n                'pathway_id': row['pathway_id'],\n                'category': f'Category {category}',\n                'abundance': mean_value,\n                'mechanism': primary_mechanism,\n                'genus': row['Genus']\n            })\n\n    # If no data could be processed, return with a message\n    if not heatmap_data:\n        print(\"No suitable data found for heatmap visualization\")\n        return None\n\n    heatmap_df = pd.DataFrame(heatmap_data)\n\n    # Create a pivot table for the heatmap\n    pivot_data = heatmap_df.pivot_table(\n        index=['mechanism', 'pathway_id'],\n        columns='category',\n        values='abundance',\n        aggfunc='mean'\n    )\n\n    # Sort by mechanism and then by abundance in Category 3\n    if 'Category 3' in pivot_data.columns:\n        pivot_data = pivot_data.sort_values(by=['mechanism', 'Category 3'], ascending=[True, False])\n\n    # Create the figure\n    plt.figure(figsize=(12, n_pathways * 0.4 + 4))\n\n    # Set diverging color palette centered at the median\n    cmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n    # Create the heatmap\n    ax = sns.heatmap(\n        pivot_data,\n        cmap=cmap,\n        center=pivot_data.values.mean(),\n        annot=True,\n        fmt=\".2f\",\n        linewidths=.5,\n        cbar_kws={\"label\": \"Abundance\"},\n        yticklabels=True\n    )\n\n    # Customize\n    plt.title(f'Pathway Abundance Across Risk Categories Grouped by {group_by.replace(\"_\", \" \").title()}')\n    plt.tight_layout()\n\n    return ax\n\n# Example usage:\ncreate_pathway_site_heatmap(prioritized_markers, n_pathways=10, group_by='corrosion_mechanisms')","metadata":{"id":"AyamylmAK3Ve","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.183Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_bubble_chart(data_frame, n_top=20):\n    \"\"\"\n    Create a bubble chart showing relationships between pathways, risk categories, and metals.\n\n    Parameters:ata_frame : prioritized_markers DataFrame\n    n_top : int Number of top pathways to include\n    \"\"\"\n    # Select top pathways based on combined_score\n    top_paths = data_frame.sort_values('combined_score', ascending=False).head(n_top)\n\n    # Create a long-form DataFrame for the bubble chart\n    bubble_data = []\n\n    for _, row in top_paths.iterrows():\n        # Create a simplified pathway name (first 20 chars of protein name + genus)\n        pathway_name = f\"{row['Genus']}: {row['protein_name'][:20]}...\"\n\n        # Process metals involved\n        metals = row['metals_involved']\n        if isinstance(metals, str):\n            if '[' in metals:\n                metals = eval(metals)\n            elif ',' in metals:\n                metals = [m.strip() for m in metals.split(',')]\n            else:\n                metals = [metals]\n\n        # If no metals or empty list, use \"None\"\n        primary_metal = metals[0] if isinstance(metals, list) and metals else \"None\"\n\n        # Add data points for each category\n        for category in [1, 2, 3]:\n            bubble_data.append({\n                'pathway': pathway_name,\n                'category': f'Category {category}',\n                'abundance': row[f'mean_{category}'],\n                'primary_metal': primary_metal,\n                'corrosion_score': row['corrosion_final_score'],\n                'log2fc': row[f'log2fc_1_to_2'] if category == 2 else row[f'log2fc_2_to_3'] if category == 3 else 0\n            })\n\n    bubble_df = pd.DataFrame(bubble_data)\n\n    # Create a categorical order for the pathways based on overall abundance\n    pathway_order = bubble_df.groupby('pathway')['abundance'].mean().sort_values(ascending=False).index\n\n    # Create a categorical order for metals based on their correlation with abundance\n    metal_order = bubble_df.groupby('primary_metal')['abundance'].mean().sort_values(ascending=False).index\n\n    # Set up the figure\n    plt.figure(figsize=(16, 12))\n\n    # Create bubble chart\n    ax = sns.scatterplot(\n        data=bubble_df,\n        x='category',\n        y='pathway',\n        size='abundance',\n        hue='primary_metal',\n        palette='viridis',\n        sizes=(20, 1000),  # Min and max bubble size\n        alpha=0.7,\n        legend='brief',\n        order=['Category 1', 'Category 2', 'Category 3'],\n        hue_order=metal_order,\n    )\n\n    # Add pathway labels\n    for label in pathway_order:\n        subset = bubble_df[bubble_df['pathway'] == label]\n        if not subset.empty:\n            max_abundance_idx = subset['abundance'].idxmax()\n            max_row = subset.loc[max_abundance_idx]\n\n            # Add text annotations for significant log2fc values\n            if abs(max_row['log2fc']) > 1:  # Only add for meaningful fold changes\n                cat_index = ['Category 1', 'Category 2', 'Category 3'].index(max_row['category'])\n                plt.text(\n                    cat_index,\n                    list(pathway_order).index(label),\n                    f\" FC:{max_row['log2fc']:.1f}\",\n                    fontsize=8,\n                    verticalalignment='center'\n                )\n\n    # Adjust legend\n    plt.legend(title='Primary Metal', bbox_to_anchor=(1.05, 1), loc='upper left')\n\n    # Set title and labels\n    plt.title('Pathway Abundance Across Risk Categories by Metal Involvement', fontsize=14)\n    plt.xlabel('Risk Category', fontsize=12)\n    plt.ylabel('Pathway', fontsize=12)\n\n    # Adjust layout\n    plt.tight_layout()\n    plt.savefig('pathway_metal_bubble_chart.png', dpi=300, bbox_inches='tight')\n    plt.show()\n\n    return ax\n\n# Example usage:\ncreate_bubble_chart(prioritized_markers, n_top=20)","metadata":{"id":"wQf6YwTFLEyn","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.183Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prior","metadata":{"id":"itKFXFy8ue71","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.183Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_sunburst_chart(data_frame, category=3, output_file='pathway_sunburst.html'):\n    \"\"\"\n    Create a sunburst chart showing hierarchical representation of pathways\n    for a specific risk category.\n\n    Parameters: data_frame classified_results\n    category : int Risk category to visualize (1, 2, or 3)\n    output_file : str file name to save the interactive HTML plot\n    \"\"\"\n    # Filter data to significant pathways\n    sig_df = data_frame[data_frame['significant'] == True].copy(deep=False)\n\n    # Prepare data for sunburst chart\n    sunburst_data = []\n\n    for _, row in sig_df.iterrows():\n        # Extract hierarchy levels - assuming hierarchy is in path format\n        hierarchy = row['hierarchy']\n        if not isinstance(hierarchy, str) or not hierarchy:\n            hierarchy = \"Unclassified\"\n\n        # Split hierarchy path\n        hierarchy_levels = hierarchy.split('/')\n        levels = []\n\n        # Build all path components\n        for i in range(len(hierarchy_levels)):\n            levels.append('/'.join(hierarchy_levels[:i+1]))\n\n        # Calculate abundance value\n        abundance = row[f'mean_{category}']\n\n        # Add the full path with the value\n        sunburst_data.append({\n            'id': '/'.join(hierarchy_levels),\n            'parent': '/'.join(hierarchy_levels[:-1]) if len(hierarchy_levels) > 1 else '',\n            'labels': hierarchy_levels[-1],\n            'values': abundance,\n            'genus': row['Genus'],\n            'protein': row['protein_name'],\n            'corrosion_relevance': row['corrosion_relevance'],\n            'pathway_classification': row['pathway_classification'] if 'pathway_classification' in row else 'Unknown'\n        })\n\n        # Add all parent paths\n        for i in range(len(hierarchy_levels)-1, 0, -1):\n            parent_path = '/'.join(hierarchy_levels[:i])\n            path = '/'.join(hierarchy_levels[:i+1])\n\n            # Check if this parent path is already in data\n            if not any(d['id'] == path for d in sunburst_data):\n                sunburst_data.append({\n                    'id': path,\n                    'parent': parent_path if i > 1 else '',\n                    'labels': hierarchy_levels[i],\n                    'values': abundance,  # This will get summed up in plotly\n                    'genus': '',\n                    'protein': '',\n                    'corrosion_relevance': '',\n                    'pathway_classification': ''\n                })\n\n        # Add the root node if needed\n        if len(hierarchy_levels) > 0 and not any(d['id'] == hierarchy_levels[0] for d in sunburst_data):\n            sunburst_data.append({\n                'id': hierarchy_levels[0],\n                'parent': '',\n                'labels': hierarchy_levels[0],\n                'values': abundance,\n                'genus': '',\n                'protein': '',\n                'corrosion_relevance': '',\n                'pathway_classification': ''\n            })\n\n    # Convert to DataFrame\n    sunburst_df = pd.DataFrame(sunburst_data)\n\n    # Create the sunburst chart with Plotly\n    fig = px.sunburst(\n        sunburst_df,\n        ids='id',\n        parents='parent',\n        names='labels',\n        values='values',\n        color='values',\n        hover_data=['genus', 'protein', 'corrosion_relevance', 'pathway_classification'],\n        color_continuous_scale='Viridis',\n        title=f'Hierarchical Pathway Distribution for Category {category}'\n    )\n\n    # Update layout\n    fig.update_layout(\n        width=900,\n        height=900,\n        margin=dict(t=50, l=0, r=0, b=10)\n    )\n\n    # Save to HTML file\n    pio.write_html(fig, file=output_file, auto_open=True)\n\n    return fig\n\n# Example usage:\ncreate_sunburst_chart(classified_results, category=3, output_file='pathway_sunburst_cat3.html')","metadata":{"id":"LSHPyikRLP-j","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.184Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_network_graph(data_frame, n_top=15, threshold=0.5, category=3):\n    \"\"\"\n    Create a network graph showing relationships between genera, pathways, and corrosion mechanisms.\n    Parameters: data_frame : pandas DataFrame prioritized_markers DataFrame\n    n_top : int   Number of top genera-pathway combinations to include\n    threshold : float  Minimum combined_score to include (0-1)\n    category : int  Risk category to visualize (1, 2, or 3)\n    \"\"\"\n    # Filter data\n    filtered_df = data_frame[\n        (data_frame['combined_score'] >= threshold) &\n        (data_frame[f'mean_{category}'] > 0)\n    ].sort_values('combined_score', ascending=False).head(n_top)\n\n    # Create a graph\n    G = nx.Graph()\n\n    # Add nodes and edges\n    for _, row in filtered_df.iterrows():\n        genus = row['Genus']\n        protein = row['protein_name'][:20] + '...' if len(row['protein_name']) > 20 else row['protein_name']\n\n        # Process corrosion mechanisms\n        mechanisms = row['corrosion_mechanisms']\n        if isinstance(mechanisms, str):\n            if '[' in mechanisms:\n                mechanisms = eval(mechanisms)\n            elif ',' in mechanisms:\n                mechanisms = [m.strip() for m in mechanisms.split(',')]\n            else:\n                mechanisms = [mechanisms]\n\n        # Add nodes if they don't exist\n        if not G.has_node(genus):\n            G.add_node(genus, type='genus',\n                      abundance=row[f'mean_{category}'],\n                      corr_score=row['corr'])\n\n        if not G.has_node(protein):\n            G.add_node(protein, type='protein',\n                      score=row['combined_score'],\n                      hierarchy=row['hierarchy'].split('/')[0] if isinstance(row['hierarchy'], str) else 'Unknown')\n\n        # Add edge between genus and protein\n        G.add_edge(genus, protein, weight=row[f'mean_{category}'])\n\n        # Add mechanism nodes and edges\n        for mech in mechanisms:\n            if mech and isinstance(mech, str):\n                if not G.has_node(mech):\n                    G.add_node(mech, type='mechanism')\n\n                # Connect mechanism to protein\n                G.add_edge(protein, mech, weight=1)\n\n    # Set up the plot\n    plt.figure(figsize=(16, 12))\n\n    # Set positions using spring layout\n    pos = nx.spring_layout(G, k=0.5, iterations=50, seed=42)\n\n    # Prepare node colors by type\n    node_colors = []\n    node_sizes = []\n    node_alphas = []\n\n    for node in G.nodes():\n        if G.nodes[node]['type'] == 'genus':\n            # Color genera by correlation score (red for positive, blue for negative)\n            corr = G.nodes[node]['corr_score']\n            if corr > 0:\n                color = (1, 0, 0, min(1, abs(corr)))  # Red with alpha based on correlation strength\n            else:\n                color = (0, 0, 1, min(1, abs(corr)))  # Blue with alpha based on correlation strength\n            node_colors.append(color)\n            node_sizes.append(300 + G.nodes[node]['abundance'] * 100)\n            node_alphas.append(min(1, abs(corr)))\n\n        elif G.nodes[node]['type'] == 'protein':\n            # Color proteins by hierarchy\n            hierarchy = G.nodes[node]['hierarchy']\n            # Create a deterministic color based on hierarchy string\n            h_hash = sum([ord(c) for c in hierarchy]) % 100 / 100.0\n            color = plt.cm.tab20(h_hash)\n            node_colors.append(color)\n            node_sizes.append(200 + G.nodes[node]['score'] * 300)\n            node_alphas.append(G.nodes[node]['score'])\n\n        elif G.nodes[node]['type'] == 'mechanism':\n            # Use a distinct color for mechanisms\n            node_colors.append('green')\n            node_sizes.append(150)\n            node_alphas.append(0.8)\n\n    # Get edge weights for width\n    edge_weights = [G[u][v]['weight'] for u, v in G.edges()]\n    max_weight = max(edge_weights)\n    normalized_weights = [w/max_weight * 3 for w in edge_weights]\n\n    # Draw the network\n    nodes = nx.draw_networkx_nodes(\n        G, pos,\n        node_color=node_colors,\n        node_size=node_sizes,\n        alpha=0.8\n    )\n\n    edges = nx.draw_networkx_edges(\n        G, pos,\n        width=normalized_weights,\n        alpha=0.6,\n        edge_color='gray'\n    )\n\n    labels = nx.draw_networkx_labels(\n        G, pos,\n        font_size=10,\n        font_weight='bold'\n    )\n\n    # Add legend\n    genus_patch = mpatches.Patch(color='lightcoral', label='Genus (red=positive corr, blue=negative)')\n    protein_patch = mpatches.Patch(color='lightblue', label='Protein (color=hierarchy)')\n    mech_patch = mpatches.Patch(color='lightgreen', label='Corrosion Mechanism')\n    plt.legend(handles=[genus_patch, protein_patch, mech_patch], loc='upper left', bbox_to_anchor=(1, 1))\n\n    # Add title\n    plt.title(f'Network of Genera, Proteins, and Corrosion Mechanisms for Category {category}', fontsize=14)\n\n    # Remove axis\n    plt.axis('off')\n\n    # Adjust layout\n    plt.tight_layout()\n    plt.savefig(f'network_graph_cat{category}.png', dpi=300, bbox_inches='tight')\n    plt.show()\n\n    return G\nclassified_results= complete_results['classified_results']\nprioritized_markers = complete_results['prioritized_markers']\n# Example usage:\ncreate_network_graph(prioritized_markers, n_top=15, threshold=0.5, category=3)","metadata":{"id":"FG7RGsVBIN-A","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.184Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def improved_genus_enzyme_plot(increasing_results, eccontri_df, genus_name, top_n=5):\n    \"\"\"\n    Create an improved plot showing genus and enzyme class abundance across ordered sites.\n    \"\"\"\n    import matplotlib.pyplot as plt\n    import numpy as np\n    import pandas as pd\n\n    # Filter data for the specified genus\n    genus_markers = increasing_results[increasing_results['Genus'] == genus_name]\n    genus_data = eccontri_df[eccontri_df['Genus'] == genus_name]\n\n    if len(genus_data) == 0:\n        print(f\"No data found for genus {genus_name}\")\n        return\n\n    # Get categories and sort sites within each category\n    categories = sorted(genus_data['Category'].unique())\n    sites_by_category = {}\n\n    for cat in categories:\n        # Extract sites for this category and sort them numerically\n        cat_sites = genus_data[genus_data['Category'] == cat]['Sites'].unique()\n        # Extract numeric part of site name for sorting\n        cat_sites_sorted = sorted(cat_sites, key=lambda x: int(x.split('_')[1]))\n        sites_by_category[cat] = cat_sites_sorted\n\n    # Flatten the ordered sites list\n    ordered_sites = []\n    for cat in categories:\n        ordered_sites.extend(sites_by_category[cat])\n\n    # Calculate genus abundance by site\n    genus_abundance = {}\n    for site in ordered_sites:\n        site_data = genus_data[genus_data['Sites'] == site]\n        genus_abundance[site] = site_data['norm_abund_contri'].sum()\n\n    # Get top enzyme classes\n    if 'enzyme_class' in genus_markers.columns and genus_markers['enzyme_class'].notna().sum() > 0:\n        valid_classes = genus_markers[genus_markers['enzyme_class'].notna() &\n                                     (genus_markers['enzyme_class'] != '')]\n\n        if len(valid_classes) > 0:\n            enzyme_counts = valid_classes['enzyme_class'].value_counts()\n            top_classes = enzyme_counts.head(top_n).index\n            using_proteins = False\n        else:\n            protein_counts = genus_markers['protein_name'].value_counts()\n            top_classes = protein_counts.head(top_n).index\n            using_proteins = True\n    else:\n        protein_counts = genus_markers['protein_name'].value_counts()\n        top_classes = protein_counts.head(top_n).index\n        using_proteins = True\n\n    # Calculate abundance by site for each class/protein\n    class_abundance = {}\n    for cls in top_classes:\n        class_abundance[cls] = {}\n\n        for site in ordered_sites:\n            if using_proteins:\n                site_data = genus_data[(genus_data['Sites'] == site) &\n                                     (genus_data['protein_name'] == cls)]\n            else:\n                class_proteins = genus_markers[genus_markers['enzyme_class'] == cls]['protein_name'].unique()\n                site_data = genus_data[(genus_data['Sites'] == site) &\n                                     (genus_data['protein_name'].isin(class_proteins))]\n\n            class_abundance[cls][site] = site_data['norm_abund_contri'].sum()\n\n    # Create figure with two y-axes\n    fig, ax1 = plt.subplots(figsize=(16, 8))\n    ax2 = ax1.twinx()\n\n    # Prepare x-axis\n    x = np.arange(len(ordered_sites))\n\n    # Plot genus abundance\n    genus_values = [genus_abundance.get(site, 0) for site in ordered_sites]\n    ax1.plot(x, genus_values, 'b-', linewidth=2.5, marker='o', markersize=6,\n             label=f\"{genus_name} (Genus)\")\n\n    # Plot enzyme classes with offsets for better visibility\n    colors = plt.cm.tab10(np.linspace(0, 1, len(top_classes)))\n    max_class_value = 0\n\n    for cls_values in class_abundance.values():\n        for site in ordered_sites:\n            max_class_value = max(max_class_value, cls_values.get(site, 0))\n\n    offset_step = max_class_value * 0.1  # 10% offset between classes\n\n    for i, cls in enumerate(top_classes):\n        offset = i * offset_step\n        class_values = [class_abundance[cls].get(site, 0) + offset for site in ordered_sites]\n        label = f\"{cls}\" if len(cls) < 30 else f\"{cls[:27]}...\"\n        ax2.plot(x, class_values, '--', color=colors[i], linewidth=1.5, marker='o',\n                markersize=4, label=f\"{label} (offset: +{offset:.2f})\")\n\n    # Set axis labels\n    ax1.set_ylabel('Genus Abundance', color='b', fontsize=12)\n    ax1.tick_params(axis='y', labelcolor='b')\n\n    ax2.set_ylabel('Enzyme Class Abundance', color='r', fontsize=12)\n    ax2.tick_params(axis='y', labelcolor='r')\n\n    # Add category dividers\n    site_indices = [0]  # Start with the first site\n    category_labels = []\n\n    for i, cat in enumerate(categories):\n        if i > 0:\n            idx = site_indices[-1] + len(sites_by_category[categories[i-1]])\n            site_indices.append(idx)\n            # Add vertical line\n            plt.axvline(x=idx-0.5, color='black', linestyle='-', linewidth=2)\n\n    # Add category labels at the top\n    for i, cat in enumerate(categories):\n        start_idx = site_indices[i]\n        end_idx = start_idx + len(sites_by_category[cat]) - 1\n        mid_point = (start_idx + end_idx) / 2\n        plt.text(mid_point, ax1.get_ylim()[1] * 1.05, f\"Category {cat}\",\n                horizontalalignment='center', fontsize=14, fontweight='bold')\n\n    # Set x-ticks for sites\n    plt.xticks(x, [site.split('_')[1] for site in ordered_sites], rotation=90)\n    plt.xlabel('Site Number', fontsize=12)\n\n    # Add grid\n    ax1.grid(True, axis='y', linestyle='--', alpha=0.3)\n\n    # Add title and legend\n    plt.title(f\"Genus vs Enzyme Class Abundance for {genus_name}\", fontsize=16)\n    ax1.legend(loc='upper left', fontsize=10)\n    ax2.legend(loc='upper right', fontsize=9)\n\n    # Adjust layout\n    plt.tight_layout()\n    plt.subplots_adjust(top=0.9)  # Make room for category labels\n\n    # Print category information\n    for cat in categories:\n        print(f\"Category {cat} Sites: {sites_by_category[cat]}\")\n\n    plt.show()\n#['Pseudomonas', 'Simplicispira', 'Acidovorax', 'Dechloromonas', 'Desulfobulbus', 'Silanimonas', 'Propionivibrio', 'Ruminiclostridium_1', 'Achromobacter', 'Blastomonas', 'Bacillus', 'Bradyrhizobium', 'Sphingomonas', 'Brevundimonas', 'Paracoccus', 'Variovorax', 'Nitrospira', 'Desulfovibrio', 'Phenylobacterium', 'Pseudoxanthomonas', 'Erysipelothrix', 'Geothrix', 'Acetobacterium', 'Shewanella', 'Gelria', 'Micrococcus', 'Desulfotomaculum', 'Streptococcus', 'Pseudorhodoferax', 'Propionibacterium', 'Methylocystis', 'Azospira', 'Smithella', 'Sphingopyxis', 'Caulobacter', 'Novosphingobium', 'Staphylococcus', 'Afipia', 'Porphyrobacter', 'Sphingobium', 'Tepidimonas', 'Halomonas', 'Desulfomicrobium', 'Thiobacillus', 'Phreatobacter', 'Chryseobacterium', 'Oxalobacteraceae_unclassified', 'Anoxybacillus', 'Tessaracoccus', 'Hydrogenophaga', 'Legionella', 'Corynebacterium', 'Mycobacterium', 'Desulfosporosinus', 'Enhydrobacter', 'Opitutus', 'Sediminibacterium', 'Thermincola', 'Clostridium_sensu_stricto_12', 'Acidisoma', 'Gallionella', 'Enterococcus', 'Clostridium', 'Mycoplana', 'Syntrophus', 'Bulleidia', 'Neisseria', 'Treponema', 'Psb-m-3', 'Wchb1-05', 'Pseudoalteromonas', 'Flavisolibacter', 'Prevotella', 'Brevibacterium', 'Ralstonia', 'Brachybacterium', 'Oxobacter', 'Oerskovia', 'Cutibacterium', 'Aestuariimicrobium', 'Herbaspirillum', 'Beta_proteobacterium', 'Candidatus_desulforudis', 'Pseudarthrobacter', 'Desulfobacterium']\nimproved_genus_enzyme_plot(prioritized_markers, ECcontri_Uniprot_enriched, 'Pseudomonas', top_n=10)","metadata":{"id":"1B1NzKQt7U2q","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.184Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Top Markers by Score -\nBar chart showing the highest scoring markers\n\n","metadata":{"id":"kC6ERUGAreDR"}},{"cell_type":"code","source":"def plot_top_markers_by_score(prioritized_markers, top_n=20, figsize=(8, 6)):\n    \"\"\"\n    Create a horizontal bar chart of top markers by score\n    \"\"\"\n    # Get top markers\n    top_markers = prioritized_markers.head(top_n)\n\n    # Create labels\n    labels = [f\"{row['Genus']} - {row['protein_name'][:30]}\"\n              for _, row in top_markers.iterrows()]\n\n    # Create figure\n    plt.figure(figsize=figsize)\n\n    # Plot horizontal bars\n    plt.barh(range(len(labels)), top_markers['combined_score'], color='steelblue')\n\n    # Add labels and title\n    plt.yticks(range(len(labels)), labels)\n    plt.xlabel('Combined Score')\n    plt.title(f'Top {top_n} Corrosion Markers by Score')\n    plt.grid(axis='x', linestyle='--', alpha=0.7)\n\n    plt.tight_layout()\n    return plt.gcf()\n\nplot_top_markers_by_score(prioritized_markers, top_n=20, figsize=(8, 6))","metadata":{"id":"VSDtFpVLqEZh","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.184Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Genus-Mechanism Heatmap\nShows relationships between top genera and corrosion mechanisms","metadata":{"id":"gsj41VblqH0R"}},{"cell_type":"code","source":"def plot_genus_mechanism_heatmap(prioritized_markers, top_genera=10, top_markers_per_genus=5, figsize=None):\n    \"\"\"\n    Create a heatmap showing top genera and their corrosion mechanisms\n    \"\"\"\n    # Ensure corrosion_mechanisms column exists\n    if 'corrosion_mechanisms' not in prioritized_markers.columns:\n        raise ValueError(\"DataFrame must contain 'corrosion_mechanisms' column\")\n\n    # Get top genera\n    top_genera_list = prioritized_markers['Genus'].value_counts().head(top_genera).index.tolist()\n\n    # Filter for top genera\n    genus_df = prioritized_markers[prioritized_markers['Genus'].isin(top_genera_list)].copy(deep=False)\n\n    # Get top markers per genus\n    top_markers = []\n    for genus in top_genera_list:\n        genus_markers = genus_df[genus_df['Genus'] == genus].head(top_markers_per_genus)\n        top_markers.append(genus_markers)\n\n    top_markers_df = pd.concat(top_markers)\n\n    # Extract all unique mechanisms\n    all_mechanisms = set()\n    for mechs in top_markers_df['corrosion_mechanisms'].dropna():\n        if isinstance(mechs, str):\n            all_mechanisms.update([m.strip() for m in mechs.split(';')])\n\n    all_mechanisms = sorted(list(all_mechanisms))\n\n    # Create matrix for heatmap (genus x mechanism)\n    heatmap_data = np.zeros((len(top_genera_list), len(all_mechanisms)))\n\n    # Fill matrix with scores\n    for i, genus in enumerate(top_genera_list):\n        genus_markers = top_markers_df[top_markers_df['Genus'] == genus]\n\n        for _, row in genus_markers.iterrows():\n            if pd.notna(row['corrosion_mechanisms']):\n                mechanisms = [m.strip() for m in row['corrosion_mechanisms'].split(';')]\n                score = row['combined_score']\n\n                for mechanism in mechanisms:\n                    if mechanism in all_mechanisms:\n                        j = all_mechanisms.index(mechanism)\n                        heatmap_data[i, j] += score\n\n    # Create heatmap\n    plt.figure(figsize=figsize)\n    sns.heatmap(heatmap_data, annot=True, fmt='.1f', cmap='YlGnBu',\n                xticklabels=all_mechanisms, yticklabels=top_genera_list)\n\n    plt.title('Genus-Mechanism Relationship (Combined Score)')\n    plt.ylabel('Genus')\n    plt.xlabel('Corrosion Mechanism')\n    plt.tight_layout()\n\n    return plt.gcf()\n\nplot_genus_mechanism_heatmap(prioritized_markers, top_genera=10, top_markers_per_genus=5, figsize=(8, 6))","metadata":{"id":"TTlyG6qMqMb3","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.184Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Abundance Across Categories\nLine plots showing how abundance changes across corrosion categories","metadata":{"id":"TWqnGHDmqPg4"}},{"cell_type":"code","source":"def plot_abundance_across_categories(prioritized_markers, top_n=5, figsize=None):\n    \"\"\"\n    Plot abundance patterns across corrosion categories for top markers\n    \"\"\"\n    # Get mean columns\n    mean_cols = [col for col in prioritized_markers.columns if col.startswith('mean_')]\n\n    if not mean_cols:\n        raise ValueError(\"No mean abundance columns found (should start with 'mean_')\")\n\n    # Get top markers\n    top_markers = prioritized_markers.head(top_n)\n\n    # Extract category numbers from column names\n    categories = [col.replace('mean_', '') for col in mean_cols]\n\n    # Create figure\n    plt.figure(figsize=figsize)\n\n    # Plot abundance patterns for each marker\n    for i, (_, row) in enumerate(top_markers.iterrows()):\n        label = f\"{row['Genus']} - {row['protein_name'][:20]}\"\n        values = [row[col] for col in mean_cols]\n        plt.plot(categories, values, marker='o', linewidth=2, label=label)\n\n    # Add labels and title\n    plt.xlabel('Corrosion Category')\n    plt.ylabel('Mean Abundance')\n    plt.title(f'Abundance Patterns Across Corrosion Categories (Top {top_n} Markers)')\n    plt.grid(linestyle='--', alpha=0.7)\n    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n\n    plt.tight_layout()\n    return plt.gcf()\n\nplot_abundance_across_categories(prioritized_markers, top_n=5, figsize=(8, 6))","metadata":{"id":"sepU-uzRhOY2","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.184Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Pathway Classification Breakdown\nShows distribution of universal vs niche-specific pathways\n\n","metadata":{"id":"o2xnVZOYqYGP"}},{"cell_type":"code","source":"def plot_pathway_classification_breakdown(prioritized_markers, figsize=(8, 6)):\n    \"\"\"\n    Create pie and bar charts showing pathway classification breakdown\n    \"\"\"\n    if 'pathway_classification' not in prioritized_markers.columns:\n        raise ValueError(\"DataFrame must contain 'pathway_classification' column\")\n\n    # Count pathway classifications\n    class_counts = prioritized_markers['pathway_classification'].value_counts()\n\n    # Create figure with subplots\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)\n\n    # Pie chart\n    ax1.pie(class_counts, labels=class_counts.index, autopct='%1.1f%%',\n           shadow=True, startangle=90, colors=['#ff9999','#66b3ff','#99ff99'])\n    ax1.set_title('Pathway Classification Distribution')\n\n    # Bar chart\n    ax2.bar(class_counts.index, class_counts.values, color=['#ff9999','#66b3ff','#99ff99'])\n    ax2.set_title('Pathway Classification Counts')\n    ax2.set_ylabel('Count')\n\n    #plt.tight_layout()\n    return fig\n\nclassified_results= complete_results['classified_results']\nplot_pathway_classification_breakdown(classified_results, figsize=(12, 8))","metadata":{"id":"sXTXJ0gQm2es","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.184Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def prepare_genera_pca(base_matrix, category_mapping=None):\n    \"\"\"\n    Prepare genera data for PCA with handling of multi-index categories\n\n    Parameters: base_matrix : pandas.DataFrame, Matrix with multi-index (Sites, Category)\n                category_mapping : pandas.Series,  Category mapping\n\n    Returns:    X_pca : numpy.ndarray, PCA transformed data\n                explained_variance_ratio : numpy.ndarray  Explained variance ratios\n                loadings : pandas.DataFrame, PCA loadings\n                categories : pandas.Series,  Categories for each site\n    \"\"\"\n    # Extract categories if they're in the multi-index\n    if isinstance(base_matrix.index, pd.MultiIndex):\n        categories = base_matrix.index.get_level_values('Category')\n        # No need to drop category as it's in the index\n        X = base_matrix\n    else:\n        # Use provided category mapping or None\n        categories = category_mapping\n        X = base_matrix\n\n    # No need for iloc[1:] as we don't have enzyme names as first row anymore\n    X_for_scaling = X.astype(float)\n\n    # Standardize\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X_for_scaling)\n\n    # PCA\n    pca = PCA(n_components=2)\n    X_pca = pca.fit_transform(X_scaled)\n\n    # Create loadings DataFrame with proper multi-index columns\n    loadings = pd.DataFrame(\n        pca.components_.T,\n        index=X.columns,  # preserving multi-index columns (Genus, protein_name)\n        columns=['PC1', 'PC2']\n    )\n\n    return X_pca, pca.explained_variance_ratio_, loadings, categories\ndef plot_pca_results(X_pca, explained_variance, Category, title, category_colors, categories_labels,\n                     pc1_idx=0, pc2_idx=1):  # Add parameters for component indices\n    \"\"\"\n    Plot PCA with risk categories\n\n    Parameters:\n    -----------\n    X_pca : numpy array   PCA transformed data\n    explained_variance : numpy array        Explained variance ratios\n    Category : array-like  Category labels for each sample\n    title : str   Plot title\n    category_colors : dict  Mapping of categories to colors\n    categories_labels : dict  Mapping of categories to display labels\n    pc1_idx : int        Index of the first PC to plot (default 0 for PC1)\n    pc2_idx : int        Index of the second PC to plot (default 1 for PC2)\n    \"\"\"\n    plt.figure(figsize=(10, 8))\n\n    # Plot using specified components\n    for category in sorted(set(Category)):\n        mask = Category == category\n        plt.scatter(\n            X_pca[mask, pc1_idx],  # Specified PC for x-axis\n            X_pca[mask, pc2_idx],  # Specified PC for y-axis\n            c=category_colors[category],\n            label=categories_labels[category],\n            alpha=0.7,\n            s=100\n        )\n\n    plt.xlabel(f'PC{pc1_idx+1} ({explained_variance[pc1_idx]:.1%} variance explained)')\n    plt.ylabel(f'PC{pc2_idx+1} ({explained_variance[pc2_idx]:.1%} variance explained)')\n    plt.title(title)\n    plt.legend(title='Risk Category')\n    plt.tight_layout()\n    plt.show()\n\n\ndef prepare_flexible_pca(data_matrix, categories=None, n_components=None):\n    \"\"\"\n    Prepare PCA with flexible number of components\n\n    Parameters:\n    -----------\n    data_matrix : pandas DataFrame   Input data with samples as rows and features as columns\n    categories : array-like,   Category labels for each sample\n    n_components : int,   Number of components to calculate (None for all possible)\n    n_plot : int   Number of components to return for plotting\n\n    Returns:\n    --------\n    X_pca : numpy array   PCA transformed data (first n_plot components)\n    explained_variance : numpy array  Explained variance ratios for all components\n    loadings : pandas DataFrame  PCA loadings with feature names as index\n    \"\"\"\n    # Standardize\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(data_matrix)\n\n    # PCA\n    pca = PCA(n_components=n_components)\n    X_pca_full = pca.fit_transform(X_scaled)\n\n    # Get loadings for all components\n    loadings = pd.DataFrame(\n        pca.components_.T,\n        index=data_matrix.columns,\n        columns=[f'PC{i+1}' for i in range(pca.n_components_)]\n    )\n\n    # Return only requested components for plotting\n    X_pca = X_pca_full\n\n    # Return all calculated components\n    return X_pca_full, pca.explained_variance_ratio_, loadings\n\n# Calculate PCA with all components\nX_pca_all, var_ratio_all, loadings_all = prepare_flexible_pca(base_matrix)\n\n# Plot different component combinations\n# PC1 vs PC2 (default)\nplot_pca_results(X_pca_all, var_ratio_all, categories.values, \"PC1 vs PC2\",\n                 category_colors, categories_labels)\n\n# PC2 vs PC3\nplot_pca_results(X_pca_all, var_ratio_all, categories.values, \"PC2 vs PC3\",\n                 category_colors, categories_labels,\n                 pc1_idx=1, pc2_idx=2)\n\n# PC3 vs PC4\nplot_pca_results(X_pca_all, var_ratio_all, categories.values, \"PC3 vs PC4\",\n                 category_colors, categories_labels,\n                 pc1_idx=2, pc2_idx=3)","metadata":{"id":"n6cJpbQ5jmh0","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.185Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Adapting for Source Groups","metadata":{"id":"_830Bc_bGY3w"}},{"cell_type":"markdown","source":"## 9.5 Retrieving Statistically Significant Groups\n\nFrom notebook 3_Feature_selection the file finalist.xlsx contain the groups worked and that were statistically significant in relation to the risk label. This groups posses interest since the relationship to the label could show better understanding in contrast with the different groups of known bacteria, core taxa, checked bacteria and the mixed groups.\nThe idea is to understand if the core taxa which make up a large influence on the comunities on the water and cooling systems are also influencing corrosion.\n","metadata":{"id":"DJiXA5rnlid7"}},{"cell_type":"code","source":"source_groups = {\n    \"known_bacteria\": known_bacteria_list,\n    \"pure_checked\": pure_checked_list,\n    \"pure_core\": pure_core_list,\n    \"checked_core\": checked_core_list\n}\n\nInfluencers_uniques_path = base_dir / \"finalist_dfs.xlsx\"\n# Integrated taxa from origin genus as headers with levels 6 for the genera, 7 for the GID, muss be cleaned\nInfluencers_uniques = pd.read_excel(Influencers_uniques_path, sheet_name='Influencers_uniques', header=[0,1,2,3,4,5,6,7], engine ='openpyxl')\n# Drop first row (index 0) and first column in one chain\nInfluencers_uniques = Influencers_uniques.drop(index=0)\nInfluencers_uniques = Influencers_uniques.drop(Influencers_uniques.columns[0], axis=1)\nInfluencers_uniques = Influencers_uniques.astype({'Sites': str})\n# Remove 'Unnamed' level names\nInfluencers_uniques.columns = Influencers_uniques.columns.map(lambda x: tuple('' if \"Unnamed\" in str(level) else level for level in x))\nInfluencers_uniques_list= Influencers_uniques.columns.get_level_values(6)\nInfluencers_uniques_list= Influencers_uniques_list[Influencers_uniques_list !='']\n\n### Updating the groups to visualise\nsource_groups = {\n    \"known_bacteria\": known_bacteria_list,\n    \"pure_checked\": pure_checked_list,\n    \"pure_core\": pure_core_list,\n    \"checked_core\": checked_core_list,\n    \"Influencers_uniques\": Influencers_uniques_list,\n}","metadata":{"id":"jvd8IvkvlcPL","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.185Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Suppose known_bacteria_list is our list of genera\ngroup_cols_known = [col for col in base_matrix.columns if col[0] in known_bacteria_list]\nbase_matrix_known = base_matrix.loc[:, group_cols_known]\n\nplot_top_proteins_across_categories(\n    base_matrix_known,\n    categories=[1,2,3],\n    n_top=5,\n    n_genera=10,\n    category_level=1\n)","metadata":{"id":"JaOwWSY1DFP9","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.185Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 10. Pathways Analysis\n\n## 10.1. Pathways distribution by Risk Category","metadata":{"id":"6tLvr9eYYel7"}},{"cell_type":"code","source":"def prepare_pathway_pca(metabolic_info, use_col='Pathways'):\n    \"\"\"\n    Convert pathway strings to numeric features for PCA\n\n    Parameters:\n    -----------\n    metabolic_info : pandas.DataFrame,   DataFrame with 'Pathways' column containing comma-separated pathway strings\n\n    Returns:\n    --------\n    X_pca : numpy.ndarray,    PCA transformed data\n    explained_variance_ratio : numpy.ndarray,     Explained variance ratios\n    loadings : pandas.DataFrame,   PCA loadings with pathway names as index\n    pathway_matrix : pandas.Dataframe,  Binary matrix of pathway presence/absence (useful for further analysis)\n    \"\"\"\n    # Handle NaN values first\n    valid_data = metabolic_info[metabolic_info[use_col].notna()]\n\n    # Create set of unique items with explicit string handling\n    all_items = set()\n    for item_str in valid_data[use_col]:\n        if isinstance(item_str, str):  # Ensure it's a string\n            items = [i.strip() for i in item_str.strip('[]').split(',') if i.strip()]\n            all_items.update(items)\n\n    # Create binary matrix with explicit index preservation\n    data_dict = {}\n    original_index = metabolic_info.index\n\n    for item in all_items:\n        if item:  # Skip empty strings\n            item_escaped = re.escape(item)\n            data_dict[item] = metabolic_info[use_col].str.contains(\n                item_escaped,\n                regex=True,\n                na=False\n            ).astype(int)\n\n    data_matrix = pd.DataFrame(data_dict, index=original_index)\n\n    # Print debug info\n    print(f\"Created matrix with {data_matrix.shape[1]} features\")\n    print(f\"Non-zero entries: {data_matrix.astype(bool).sum().sum()}\")\n\n    # Run PCA with explicit scaling\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(data_matrix)\n    pca = PCA(n_components=2)\n    X_pca = pca.fit_transform(X_scaled)\n\n    loadings = pd.DataFrame(\n        pca.components_.T,\n        index=data_matrix.columns,\n        columns=['PC1', 'PC2']\n    )\n\n    return X_pca, pca.explained_variance_ratio_, loadings, data_matrix\n\ndef plot_metabolic_pca_results(X_pca, explained_variance, metabolic_sites_info, category_dict, title, category_colors, categories_labels):\n    \"\"\"\n    Plot PCA results for pathways with risk categories\n\n    Parameters:     X_pca : numpy array  PCA transformed coordinates\n                    explained_variance : numpy array   Explained variance ratios\n                    metabolic_info : pandas DataFrame   The metabolic info DataFrame with Sites index\n                    category_dict : dict     Mapping of sites to categories\n    \"\"\"\n    plt.figure(figsize=(10, 8))\n\n    # Get categories for each site in metabolic_info\n    if isinstance(metabolic_sites_info.index, pd.MultiIndex):\n        Sites = metabolic_sites_info.index.get_level_values('Sites')\n    else:\n        Sites = metabolic_sites_info.index\n\n    plot_categories = pd.Series(Sites).map(category_dict)\n\n    # Plot each category\n    for category in sorted(set(plot_categories)):\n        mask = plot_categories == category\n        plt.scatter( X_pca[mask, 0], X_pca[mask, 1], c=category_colors[category],\n            label=categories_labels[category], alpha=0.7, s=100)\n\n    plt.xlabel(f'PC1 ({explained_variance[0]:.1%} variance explained)')\n    plt.ylabel(f'PC2 ({explained_variance[1]:.1%} variance explained)')\n    plt.title(title)\n    plt.legend(title='Risk Category')\n    plt.tight_layout()\n    plt.show()\n# For pathway PCA\nX_pca_path, var_ratio_path, loadings_path, pathway_matrix = prepare_pathway_pca(metabolic_sites_info, use_col='Pathways')\n\nplot_metabolic_pca_results( X_pca_path, var_ratio_path,  metabolic_sites_info, category_dict, \"Pathways PCA by Risk Category\", category_colors, categories_labels)","metadata":{"id":"dakf67H0Yel7","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.185Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 10.2. Top Pathways Loadings by Category","metadata":{"id":"kqmkIJeaYel7"}},{"cell_type":"code","source":"def plot_pca_loadings_heatmap(loadings, top_n=20):\n    \"\"\"Plot a heatmap of pathway loadings for PC1 and PC2.\n       Parameters:     loadings: DataFrame with PCA loadings\n       top_n: Number of top pathways to display     \"\"\"\n    plt.figure(figsize=(10, 8))\n    # Select top pathways based on absolute contribution to PC1 and PC2\n    top_pathways = (loadings[['PC1', 'PC2']].abs().sum(axis=1).nlargest(top_n).index)\n    # Filter the loadings dataframe\n    heatmap_data = loadings.loc[top_pathways, ['PC1', 'PC2']]\n    sns.heatmap(heatmap_data, annot=True, cmap='coolwarm', center=0)\n    plt.title('Top Pathway Contributions to PC1 and PC2')\n    plt.xlabel('Principal Components')\n    plt.ylabel('Pathways')\n    plt.tight_layout()\n    plt.show()\n\nplot_pca_loadings_heatmap(loadings_genera)","metadata":{"id":"gY-sQA13Yel7","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.185Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 10.3. Pathways patterns by source groups","metadata":{"id":"UXRurjWFuvgW"}},{"cell_type":"markdown","source":"|Sites|---|site_1|site_1|site_1|site_2|site_2|site_2|site_2|\n|---|---|---|---|---|---|---|---|---|\n|Genus|---|genus_1|genus_2|genus3|genus_2|genus_70|genus_154|genus_520|\n|Pathways|---|---|---|---|---|---|---|---|\n|pathway_1|---|---|---|---|---|---|---|---|\n|pathway_2|---|---|---|---|---|---|---|---|\n","metadata":{"id":"GH3EfeFrYel8"}},{"cell_type":"code","source":"def analyze_bacterial_groups(base_matrix, metabolic_sites_info, source_groups):\n    \"\"\"\n    Analyze relationships between bacterial groups and functional patterns.\n\n    Parameters:\n    - base_matrix: DataFrame with sites and functional data.\n      (Columns are multi-indexed (Site, Genus) or similar structure.)\n    - metabolic_sites_info: DataFrame with site-genus level information.\n    - source_groups: dict with group names as keys and list of genera as values.\n\n    Returns:\n    - results: dict with analysis results for each group.\n    \"\"\"\n    results = {}\n\n    for source_name, genus_list in source_groups.items():\n\n        # Filter columns where the first level (e.g., site or genus) is in the group list.\n        group_cols = [col for col in base_matrix.columns if col[0] in genus_list]\n        group_data = base_matrix.loc[:, group_cols]\n\n        # Standardize the data\n        scaler = MinMaxScaler() # Changing from standard scaler to robustscaler\n        scaled_data = scaler.fit_transform(group_data)\n\n        # PCA analysis\n        pca = PCA(n_components=5)\n        pca_result = pca.fit_transform(scaled_data)\n\n        print(\"\\nPCA Variance Explained:\")\n        for i, var in enumerate(pca.explained_variance_ratio_):\n            print(f\"PC{i+1}: {var:.2%}\")\n        print(f\"Total variance explained: {sum(pca.explained_variance_ratio_):.2%}\")\n\n        # UMAP analysis\n        reducer = umap.UMAP(random_state=42)\n        umap_result = reducer.fit_transform(scaled_data)\n\n        # Save results for current group\n        results[source_name] = {\n            'pca': pca_result,\n            'umap': umap_result,\n            'pca_explained': pca.explained_variance_ratio_,\n            'data': group_data\n        }\n\n        # Plottinextract categories from base_matrix index if available.\n        try:\n            categories = base_matrix.index.get_level_values('Category')\n        except (KeyError, AttributeError):\n            # If no 'Category' level, assign a default category (e.g., all 1)\n            categories = pd.Series(np.ones(group_data.shape[0]), index=group_data.index)\n\n        # PCA plot\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n        for cat in sorted(set(categories)):\n            mask = categories == cat\n            ax1.scatter(pca_result[mask, 0],\n                        pca_result[mask, 1],\n                        c=category_colors.get(cat, '#000000'),\n                        label=categories_labels.get(cat, f'Cat {cat}'),\n                        alpha=0.7)\n        ax1.set_title(f'PCA - {source_name}')\n        ax1.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n        ax1.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n        ax1.legend()\n\n        # UMAP plot\n        for cat in sorted(set(categories)):\n            mask = categories == cat\n            ax2.scatter(umap_result[mask, 0],\n                        umap_result[mask, 1],\n                        c=category_colors.get(cat, '#000000'),\n                        label=categories_labels.get(cat, f'Cat {cat}'),\n                        alpha=0.7)\n        ax2.set_title(f'UMAP - {source_name}')\n        ax2.set_xlabel('UMAP 1')\n        ax2.set_ylabel('UMAP 2')\n        ax2.legend()\n\n        plt.tight_layout()\n        plt.show()\n\n        # PCA Explained Variance plot\n        plt.figure(figsize=(10, 6))\n        plt.plot(range(1, len(pca.explained_variance_ratio_) + 1),\n                 np.cumsum(pca.explained_variance_ratio_), 'bo-')\n        plt.xlabel('Number of Components')\n        plt.ylabel('Cumulative Explained Variance Ratio')\n        plt.title(f'PCA Explained Variance - {source_name}')\n        plt.grid(True)\n        plt.tight_layout()\n        plt.show()\n\n    return results\n","metadata":{"id":"G0i9EqyPYel8","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.185Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"{known_bacteria}: group_data.shape = {group_data.shape}\")\n","metadata":{"id":"BjwgGTo22fJB","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.185Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"known_bacteria_list = ['Clostridium', 'Corynebacterium', 'Novosphingobium', 'Streptococcus', 'Thiobacillus', 'Acetobacterium', 'Bacillus', 'Desulfotomaculum', 'Desulfovibrio', 'Micrococcus', 'Propionibacterium',\n 'Pseudomonas', 'Staphylococcus', 'Desulfobacterium', 'Desulfobulbus', 'Gallionella', 'Shewanella']\n\ngroup_cols_known = [col for col in base_matrix.columns if col[0] in known_bacteria_list]\ngroup_data_known = base_matrix.loc[:, group_cols_known]\n\ngroup_data_known.head()","metadata":{"id":"aNhqT9G92y19","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.186Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(group_data_known.head(), group_data_known.shape)","metadata":{"id":"KJteNaoZ46f7","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.186Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for gname, glist in source_groups.items():\n    group_cols = [col for col in base_matrix.columns if col[0] in glist]\n    tmp_data = base_matrix.loc[:, group_cols]\n    print(gname, tmp_data.shape)\n","metadata":{"id":"FWvbkrMc6_Yu","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.186Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"corr_matrix = group_data_known.corr()\ncorr_matrix","metadata":{"id":"-ad6mjcN7MhN","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.186Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"corr_matrix","metadata":{"id":"z55hs_rh7ULF","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.187Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Bacterial Groups Analysis Component","metadata":{"id":"eH_ifJ0wnnLo"}},{"cell_type":"code","source":"def analyze_combined_groups(base_matrix, source_groups, group_names=['checked_core', 'Influencers_uniques']):\n    \"\"\"\n    Analyze combined bacterial groups while preserving their individual contributions.\n\n    Parameters:\n    - base_matrix: DataFrame with sites and functional data\n    - source_groups: dict with group names as keys and list of genera as values\n    - group_names: list of group names to combine\n\n    Returns:\n    - Combined analysis results including PCA, UMAP and variance explained\n    \"\"\"\n    # Filter for selected groups\n    selected_genera = []\n    for group in group_names:\n        selected_genera.extend(source_groups[group])\n\n    # Remove duplicates while preserving order\n    selected_genera = list(dict.fromkeys(selected_genera))\n\n    # Filter columns for selected genera\n    group_cols = [col for col in base_matrix.columns if col[0] in selected_genera]\n    combined_data = base_matrix.loc[:, group_cols]\n\n    # Remove zero columns\n    combined_data = combined_data.loc[:, (combined_data != 0).any(axis=0)]\n\n    # Standardize\n    scaler = StandardScaler()\n    scaled_data = scaler.fit_transform(combined_data)\n\n    # PCA\n    pca = PCA(n_components=3)\n    pca_result = pca.fit_transform(scaled_data)\n\n    # UMAP\n    reducer = umap.UMAP(random_state=42)\n    umap_result = reducer.fit_transform(scaled_data)\n\n    results = {\n        'pca': pca_result,\n        'umap': umap_result,\n        'pca_explained': pca.explained_variance_ratio_,\n        'data': combined_data,\n        'genera': selected_genera\n    }\n\n    # Plottinextract categories from base_matrix index if available.\n    try:\n        categories = base_matrix.index.get_level_values('Category')\n    except (KeyError, AttributeError):\n        # If no 'Category' level, assign a default category (e.g., all 1)\n        categories = pd.Series(np.ones(combined_data.shape[0]), index=combined_data.index)\n\n    # PCA plot\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n    for cat in sorted(set(categories)):\n        mask = categories == cat\n        ax1.scatter(pca_result[mask, 0],\n                    pca_result[mask, 1],\n                    c=category_colors.get(cat, '#000000'),\n                    label=categories_labels.get(cat, f'Cat {cat}'),\n                    alpha=0.7)\n    ax1.set_title(f'PCA - {combined_data}')\n    ax1.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n    ax1.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n    ax1.legend()\n\n    # UMAP plot\n    for cat in sorted(set(categories)):\n        mask = categories == cat\n        ax2.scatter(umap_result[mask, 0],\n                    umap_result[mask, 1],\n                    c=category_colors.get(cat, '#000000'),\n                    label=categories_labels.get(cat, f'Cat {cat}'),\n                    alpha=0.7)\n    ax2.set_title(f'UMAP - {group_names}')\n    ax2.set_xlabel('UMAP 1')\n    ax2.set_ylabel('UMAP 2')\n    ax2.legend()\n\n    plt.tight_layout()\n    plt.show()\n\n    # PCA Explained Variance plot\n    plt.figure(figsize=(10, 6))\n    plt.plot(range(1, len(pca.explained_variance_ratio_) + 1),\n              np.cumsum(pca.explained_variance_ratio_), 'bo-')\n    plt.xlabel('Number of Components')\n    plt.ylabel('Cumulative Explained Variance Ratio')\n    plt.title(f'PCA Explained Variance - {group_names}')\n    plt.grid(True)\n    plt.tight_layout()\n    plt.show()\n\n    return results","metadata":{"id":"IsAIvUBYntEu","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.187Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results = analyze_combined_groups(base_matrix, source_groups, group_names=['checked_core', 'Influencers_uniques'])","metadata":{"id":"Voj9pVqlAR3E","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.187Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{"id":"uuOHFlIQJ1YD"}},{"cell_type":"code","source":"from scipy import stats\nfrom statsmodels.stats.multitest import multipletests","metadata":{"id":"iJhbme4LoEgY","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.187Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def analyze_category_enrichment(base_matrix, category_dict, source_groups):\n    \"\"\"\n    Analyze pathway enrichment within each risk category.\n    Uses relative abundance and statistical testing to identify\n    significantly enriched proteins in each category.\n    \"\"\"\n    # Get the sites and categories from the MultiIndex\n    sites_categories = pd.Series(\n        base_matrix.index.get_level_values('Category'),\n        index=base_matrix.index.get_level_values('Sites')\n    )\n\n    def get_enrichment_for_group(group_data, category):\n        # Get data for this category\n        cat_mask = group_data.index.get_level_values(\"Category\")  == category\n        cat_data = group_data[cat_mask]\n        other_data= group_data[~cat_mask]\n\n        # Calculate mean abundances\n        cat_means = cat_data.mean()\n        other_means = other_data.mean()\n\n        # Calculate fold change\n        fold_change = np.log2(cat_means / other_means)\n\n        # Perform statistical test (Mann-Whitney U)\n        pvalues = []\n        for col in group_data.columns:\n            stat, pval = stats.mannwhitneyu(\n                cat_data[col],\n                other_data[col],\n                alternative='greater'\n            )\n            pvalues.append(pval)\n\n        # Create results DataFrame\n        results = pd.DataFrame({\n            'fold_change': fold_change,\n            'pvalue': pvalues,\n            'mean_abundance': cat_means\n        })\n\n        # Add multiple testing correction\n        results['padj'] = multipletests(results['pvalue'], method='fdr_bh')[1]\n\n        return results\n\n    enrichment_results = {}\n\n    # Analyze each source group\n    for group_name, genera in source_groups.items():\n        print(f\"\\nAnalyzing {group_name}...\")\n\n        # Filter for genera in this group\n        group_cols = [col for col in base_matrix.columns if col[0] in genera]\n        if not group_cols:\n            continue\n\n        group_data = base_matrix[group_cols]\n\n        # Get enrichment for each category\n        group_results = {}\n        for cat in [1, 2, 3]:\n            results = get_enrichment_for_group(group_data, cat)\n            group_results[cat] = results\n\n        enrichment_results[group_name] = group_results\n\n        # Plot volcano plots for each category\n        plt.figure(figsize=(15, 5))\n        plt.suptitle(f\"Protein Enrichment Analysis - {group_name}\", y=1.05)\n\n        for i, cat in enumerate([1, 2, 3], 1):\n            results = group_results[cat]\n\n            plt.subplot(1, 3, i)\n\n            # Create volcano plot\n            plt.scatter(\n                results['fold_change'],\n                -np.log10(results['padj']),\n                alpha=0.6,\n                c=category_colors[cat],\n                s= results['mean_abundance']*1000\n            )\n\n            # Add significance lines\n            plt.axhline(-np.log10(0.05), color='red', linestyle='--', alpha=0.3)\n            plt.axvline(0, color='black', linestyle='--', alpha=0.3)\n\n            plt.title(f\"{categories_labels[cat]}\")\n            plt.xlabel(\"Log2 Fold Change\")\n            plt.ylabel(\"-log10(adjusted p-value)\")\n\n        plt.tight_layout()\n        plt.show()\n\n        # Print top enriched proteins\n        for cat in [1, 2, 3]:\n            results = group_results[cat]\n            significant = results[results['padj'] < 0.05].sort_values('fold_change', ascending=False)\n\n            print(f\"\\nTop enriched proteins in {categories_labels[cat]} for {group_name}:\")\n            if len(significant) > 0:\n                print(significant.head(10))\n            else:\n                print(\"No significantly enriched proteins found\")\n\n    return enrichment_results\n\n# Run the analysis\nenrichment_results = analyze_category_enrichment(base_matrix, category_dict, source_groups)","metadata":{"id":"ytCoE1icYel8","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.187Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_comparison_table(enrichment_results):\n    \"\"\"\n    Create a structured comparison table from enrichment results\n    \"\"\"\n    # Create empty list to store rows\n    comparison_rows = []\n\n    for group_name, group_results in enrichment_results.items():\n        for category in [1, 2, 3]:\n            if category in group_results:\n                results = group_results[category]\n                significant = results[results['padj'] < 0.05]\n\n                if len(significant) > 0:\n                    for idx, row in significant.head(10).iterrows():\n                        comparison_rows.append({\n                            'Group': group_name,\n                            'Category': categories_labels[category],\n                            'Genus': idx[0],\n                            'Protein': idx[1],\n                            'Fold_Change': row['fold_change'],\n                            'Padj': row['padj'],\n                            'Mean_Abundance': row['mean_abundance']\n                        })\n\n    # Create DataFrame\n    comparison_df = pd.DataFrame(comparison_rows)\n\n    # Save to CSV with proper formatting\n    comparison_df.to_csv('enrichment_comparison.csv', index=False)\n\n    return comparison_df\n\n# Create comparison table\ncomparison_table = create_comparison_table(enrichment_results)\n\n# Display formatted table\nprint(\"\\nComparison Table Preview:\")\nprint(comparison_table.to_string())","metadata":{"id":"tpDMQg0fZVGn","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.187Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"__________________________________","metadata":{"id":"4zoPuDB4Yel8"}},{"cell_type":"markdown","source":"https://www.youtube.com/watch?v=jQVNsyAnDMo\n\nhttps://microreact.org/\n\n","metadata":{"id":"lxMZgylFpBX4"}},{"cell_type":"code","source":"def create_integrated_visualization(df, results, metadata=None):\n    \"\"\"\n    Create an integrated visualization combining PCA, clustering, and metadata\n\n    Parameters:\n    df: Original pathway data\n    results: Results from explore_pathway_patterns\n    metadata: DataFrame with risk labels, materials, etc.\n    \"\"\"\n    fig = plt.figure(figsize=(15, 10))\n\n    # 1. PCA with clustering\n    pca_data = results['pca']['components']\n    clusters = results['clustering'][5]['kmeans']  # Using k=5 clusters\n\n    plt.subplot(2, 2, 1)\n    scatter = plt.scatter(pca_data[:, 0], pca_data[:, 1],\n                         c=clusters, cmap='Set2', alpha=0.6)\n    plt.title('PCA Components with Clusters')\n    plt.xlabel('PC1')\n    plt.ylabel('PC2')\n    plt.colorbar(scatter, label='Cluster')\n\n    # 2. Top pathway contributions\n    plt.subplot(2, 2, 2)\n    top_loadings = abs(results['pca']['loadings']['PC1']).nlargest(10)\n    sns.barplot(x=top_loadings.values, y=top_loadings.index)\n    plt.title('Top 10 Pathways Contributing to PC1')\n    plt.xlabel('Absolute Loading')\n\n    # 3. Correlation structure summary\n    plt.subplot(2, 2, 3)\n    corr_summary = results['correlation'].abs().mean()\n    sns.histplot(corr_summary, bins=50)\n    plt.title('Distribution of Mean Correlation Strengths')\n    plt.xlabel('Mean |Correlation|')\n\n    plt.tight_layout()\n    return fig","metadata":{"id":"RWCQOr4KuvgW","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.188Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"create_integrated_visualization(base_matrix, results_patterns, metadata=None)","metadata":{"id":"dZvr6vjWuvgW","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.188Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 9.3 Analysing Pathways Organic Fate\n\nNow the task is to identify the most abundant pathways in the samples, focusing specifically on organic matter-related metabolism. Ultimately creating visualizations to understand pathway distributions and analyze correlations between pathways.","metadata":{"id":"bk9_ZGhfuvgW"}},{"cell_type":"code","source":"def analyze_metabolic_pathways(df):\n    \"\"\"\n    Analyze metabolic pathways from PICRUSt output\n\n    Parameters:\n    df: pandas DataFrame with pathways as index and samples as columns\n    \"\"\"\n    # Calculate mean abundance across samples for each pathway\n    mean_abundance = df.mean(axis=1).sort_values(ascending=False)\n\n    # Get top 20 most abundant pathways\n    top_pathways = mean_abundance.head(20)\n\n    # Create heatmap of top pathways across samples\n    plt.figure(figsize=(15, 8))\n    sns.heatmap(df.loc[top_pathways.index],\n                cmap='YlOrRd',\n                center=0,\n                robust=True,\n                xticklabels=True,\n                yticklabels=True)\n    plt.title('Top 20 Most Abundant Pathways Across Samples')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.show()\n\n    # Filter for organic matter metabolism related pathways\n    organic_terms = ['carbon', 'carbohydrate', 'lipid', 'fatty acid',\n                    'organic acid', 'amino acid', 'degradation']\n\n    organic_pathways = df.index[df.index.str.lower().str.contains('|'.join(organic_terms))]\n    organic_data = df.loc[organic_pathways]\n\n    # Calculate summary statistics for organic matter pathways\n    pathway_stats = pd.DataFrame({\n        'mean_abundance': organic_data.mean(axis=1),\n        'std_abundance': organic_data.std(axis=1),\n        'cv': organic_data.std(axis=1) / organic_data.mean(axis=1) * 100\n    }).sort_values('mean_abundance', ascending=False)\n\n    return pathway_stats, organic_data\n\ndef plot_pathway_distribution(pathway_stats):\n    \"\"\"Plot distribution of pathway abundances\"\"\"\n    plt.figure(figsize=(12, 6))\n    sns.boxplot(data=pathway_stats.reset_index(),\n                x='mean_abundance',\n                y='index',\n                order=pathway_stats.index[:15])\n    plt.title('Top 15 Organic Matter Related Pathways')\n    plt.xlabel('Mean Abundance')\n    plt.tight_layout()\n    plt.show()\n\n# Calling the function\nstats, organic_data = analyze_metabolic_pathways(Picrust_Result)\nplot_pathway_distribution(stats)","metadata":{"id":"11HYL0iNuvgW","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.188Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def analyze_pathway_patterns(df):\n    \"\"\"\n    Analyze pathway patterns using sites vs pathways abundances\n    \"\"\"\n    # Create the correct matrix: sites vs pathways with abundances\n    pathway_matrix = df.pivot_table(\n        values='norm_abund_contri',\n        index='Sites',          # Sites as rows\n        columns='Pathways',     # Pathways as columns\n        aggfunc='sum',          # Sum abundances\n        fill_value=0\n    )\n\n    print(\"Matrix shape:\", pathway_matrix.shape)\n\n    # Standardize data\n    scaler = StandardScaler()\n    scaled_data = scaler.fit_transform(pathway_matrix)\n\n    # PCA\n    pca = PCA(n_components=5)\n    X_pca = pca.fit_transform(scaled_data)\n\n    # UMAP\n    reducer = umap.UMAP(random_state=42)\n    umap_result = reducer.fit_transform(scaled_data)\n\n    # Get categories for sites\n    categories = pd.Series(pathway_matrix.index).map(lambda x: category_dict[x])\n\n    # Plot both PCA and UMAP\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n\n    # PCA explained variance\n    ax1.plot(range(1, 6), pca.explained_variance_ratio_, 'bo-')\n    print(\"\\nPCA Variance Explained:\")\n    print(f\"Total (5 components): {sum(pca.explained_variance_ratio_):.2%}\")\n    ax1.set_title('PCA Explained Variance')\n    ax1.set_xlabel('Component')\n    ax1.set_ylabel('Explained Variance Ratio')\n\n    # PCA scatter\n    for cat in category_colors.keys():\n        mask = categories == cat\n        ax2.scatter(X_pca[mask, 0],\n                   X_pca[mask, 1],\n                   c=category_colors[cat],\n                   label=categories_labels[cat],\n                   alpha=0.7)\n\n    ax2.set_title('PCA First Two Components')\n    ax2.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n    ax2.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n    ax2.legend()\n    plt.tight_layout()\n    plt.show()\n\n    # UMAP plot\n    plt.figure(figsize=(12, 8))\n    for cat in category_colors.keys():\n        mask = categories == cat\n        plt.scatter(umap_result[mask, 0],\n                   umap_result[mask, 1],\n                   c=category_colors[cat],\n                   label=categories_labels[cat],\n                   alpha=0.7)\n\n    plt.title('UMAP Projection of Pathways')\n    plt.xlabel('UMAP1')\n    plt.ylabel('UMAP2')\n    plt.legend()\n    plt.show()\n\n# Run the analysis\nresults = analyze_pathway_patterns(metabolic_sites_info)","metadata":{"id":"FZkQ1j5X5fHL","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.188Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# To analyze specific pathways of interest:\ndef analyze_specific_pathways(df, pathway_list):\n    \"\"\"\n    Analyze specific pathways of interest\n\n    Parameters:\n    df: DataFrame with pathway data\n    pathway_list: list of pathway names to analyze\n    \"\"\"\n    specific_data = df.loc[df.index.str.contains('|'.join(pathway_list), case=False)]\n\n    # Create correlation matrix for these pathways\n    corr = specific_data.T.corr()\n\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(corr, annot=True, cmap='coolwarm', center=0)\n    plt.title('Correlation between Selected Pathways')\n    plt.tight_layout()\n    plt.show()\n\n    return specific_data.describe()\n\n# Calling the funtion\nDescription = analyze_specific_pathways(Picrust_Result, Picrust_Result.index.tolist())","metadata":{"id":"G9GJgu3yuvgW","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.188Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 9.4. Pathways Relevant to Corrosion\nThis code witll categorise pathways into key groups: sulfur metabolism (critical for sulfate-reducing bacteria), Metal-related pathways (iron, manganese, etc.); organic acid production (which can influence local pH); biofilm formation (important for corrosion processes) and electron transfer mechanisms. Then it would analyse correlations between these different categories to understand potential synergistic effects, identifying the most abundant pathways in each category","metadata":{"id":"_HEIwON2uvgW"}},{"cell_type":"code","source":"def analyze_corrosion_pathways(df):\n    \"\"\"\n    Analyze pathways relevant to microbially influenced corrosion (MIC)\n\n    Parameters:\n    df: pandas DataFrame with pathways as index and samples as columns\n    \"\"\"\n    # Define relevant pathway terms for different corrosion mechanisms\n    pathway_categories = {\n        'sulfur': ['sulfur', 'sulfate', 'sulfide', 'thiosulfate', 'sulfite', 'sulfonate'],\n        'metal': ['iron', 'metal', 'Fe', 'manganese', 'chromium', 'nickel'],\n        'organic_acid': ['organic acid', 'acetate', 'formate', 'lactate', 'pyruvate'],\n        'biofilm': ['biofilm', 'exopolysaccharide', 'EPS', 'adhesion'],\n        'electron_transfer': ['cytochrome', 'electron transport', 'oxidoreductase']\n    }\n\n    # Function to filter pathways by category\n    def get_category_pathways(terms):\n        return df.index[df.index.str.lower().str.contains('|'.join(terms), regex=True)]\n\n    # Analyze each category\n    category_data = {}\n    category_stats = {}\n\n    for category, terms in pathway_categories.items():\n        pathways = get_category_pathways(terms)\n        if len(pathways) > 0:\n            category_data[category] = df.loc[pathways]\n            category_stats[category] = pd.DataFrame({\n                'mean_abundance': category_data[category].mean(axis=1),\n                'std_abundance': category_data[category].std(axis=1),\n                'cv': category_data[category].std(axis=1) / category_data[category].mean(axis=1) * 100\n            }).sort_values('mean_abundance', ascending=False)\n\n    return category_data, category_stats\n\ndef plot_corrosion_pathways(category_data, category_stats):\n    \"\"\"\n    Create visualizations for corrosion-related pathways\n    \"\"\"\n    # Plot top pathways for each category\n    for category, data in category_stats.items():\n        if len(data) > 0:\n            plt.figure(figsize=(12, min(6, max(3, len(data)*0.3))))\n            sns.barplot(data=data.head(10).reset_index(),\n                       x='mean_abundance',\n                       y='index',\n                       palette='YlOrRd')\n            plt.title(f'Top {min(10, len(data))} {category.replace(\"_\", \" \").title()} Related Pathways')\n            plt.xlabel('Mean Abundance')\n            plt.ylabel('Pathway')\n            plt.tight_layout()\n            plt.show()\n\n    # Create correlation heatmap between categories\n    category_means = pd.DataFrame({\n        cat: data.mean(axis=1) for cat, data in category_data.items()\n    })\n\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(category_means.corr(),\n                annot=True,\n                cmap='coolwarm',\n                center=0,\n                vmin=-1,\n                vmax=1)\n    plt.title('Correlation between Pathway Categories')\n    plt.tight_layout()\n    plt.show()\n\ndef analyze_pathway_interactions(df, category_data):\n    \"\"\"\n    Analyze interactions between different pathway categories\n    \"\"\"\n    # Calculate mean abundance for each category\n    category_abundances = pd.DataFrame({\n        category: data.mean(axis=0)\n        for category, data in category_data.items()\n    })\n\n    # Calculate correlations between categories\n    correlations = category_abundances.corr()\n\n    # Identify potential synergistic relationships\n    high_correlations = correlations.unstack()\n    high_correlations = high_correlations[high_correlations != 1.0]\n    high_correlations = high_correlations[abs(high_correlations) > 0.5]\n\n    return category_abundances, correlations, high_correlations.sort_values(ascending=False)\n\n# Analysing Corrosion Pathways\ncategory_data, category_stats = analyze_corrosion_pathways(Picrust_Result)\nplot_corrosion_pathways(category_data, category_stats)\nabundances, correlations, high_corr = analyze_pathway_interactions(Picrust_Result, category_data)","metadata":{"id":"l8FN-ChvuvgW","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.188Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 9.5. Heating and Cooling Systems Pathway Analysis\nCreating independent analyses:\n\nFailure analysis (based on human assessment/estimation)\nMicrobiological analysis (16S rRNA)\nPhysicochemical parameters\n\n\nUsing physicochemical parameters as labels/indicators of corrosion state - this is quite clever because it gives us an objective measure without directly mixing in the biological data\nThen planning to correlate the microbial communities with these states through machine learning\n\nAnd now to use PICRUSt's functional predictions to validate our assumptions about organic matter metabolism. It can help confirm if the bacteria identified through correlations actually have the metabolic capacity to influence corrosion\nIt might reveal unexpected metabolic pathways that could explain the correlations. The following script will Validate our organic matter assumptions by:\n\nBreaking down different types of organic matter processing\nLooking at both degradation and synthesis pathways\nIdentifying transport mechanisms\n\nConnect with our physicochemical parameters by analyzing pathways that could influence:\n\npH modulation\nTemperature response\nMetal interactions","metadata":{"id":"bi4wuyN7uvgW"}},{"cell_type":"code","source":"def analyze_system_pathways(df):\n    \"\"\"\n    Analyze pathways relevant to heating/cooling system corrosion\n\n    Parameters:\n    df: pandas DataFrame with pathways as index and samples as columns\n    \"\"\"\n    # Define pathway categories relevant to system conditions\n    pathway_categories = {\n        # Water chemistry influence\n        'ph_modulation': ['acid', 'alkaline', 'proton pump', 'pH homeostasis'],\n\n        # Temperature adaptation\n        'temp_response': ['heat shock', 'cold shock', 'temperature response'],\n\n        # Organic matter processing\n        'carbon_metabolism': [\n            'carbon fixation', 'carbon utilization',\n            'organic acid', 'fatty acid',\n            'carbohydrate metabolism'\n        ],\n\n        # Corrosion-related\n        'metal_interaction': [\n            'iron', 'metal', 'oxidation-reduction',\n            'electron transport', 'metal binding'\n        ],\n\n        # Biofilm formation\n        'surface_attachment': [\n            'biofilm', 'adhesion', 'exopolysaccharide',\n            'extracellular matrix'\n        ]\n    }\n\n    # Filter and analyze pathways\n    def get_category_pathways(terms):\n        return df.index[df.index.str.lower().str.contains('|'.join(terms), regex=True)]\n\n    category_data = {}\n    category_stats = {}\n\n    for category, terms in pathway_categories.items():\n        pathways = get_category_pathways(terms)\n        if len(pathways) > 0:\n            category_data[category] = df.loc[pathways]\n\n            # Calculate basic statistics\n            category_stats[category] = pd.DataFrame({\n                'mean_abundance': category_data[category].mean(axis=1),\n                'std_abundance': category_data[category].std(axis=1),\n                'cv': category_data[category].std(axis=1) / category_data[category].mean(axis=1) * 100,\n                'presence': (category_data[category] > 0).mean(axis=1) * 100  # % of samples with pathway\n            }).sort_values('mean_abundance', ascending=False)\n\n    return category_data, category_stats\n\ndef analyze_organic_matter_pathways(df):\n    \"\"\"\n    Detailed analysis of organic matter related pathways\n    \"\"\"\n    # Specific organic matter categories\n    organic_categories = {\n        'degradation': ['degradation', 'breakdown', 'catabolism'],\n        'synthesis': ['biosynthesis', 'anabolism', 'synthesis'],\n        'transport': ['transport', 'uptake', 'export'],\n        'modification': ['modification', 'conversion', 'transformation']\n    }\n\n    organic_data = {}\n\n    for category, terms in organic_categories.items():\n        pathways = df.index[df.index.str.lower().str.contains(\n            '|'.join(terms), regex=True\n        ) & df.index.str.lower().str.contains(\n            'organic|carbon|fatty acid|lipid|protein|amino acid'\n        )]\n        if len(pathways) > 0:\n            organic_data[category] = df.loc[pathways]\n\n    return organic_data\n\ndef plot_pathway_distributions(category_stats, category_data):\n    \"\"\"\n    Create visualizations for pathway distributions\n    \"\"\"\n    for category, stats in category_stats.items():\n        if len(stats) > 0:\n            # Create subplot with dual axis\n            fig, ax1 = plt.subplots(figsize=(12, min(8, max(4, len(stats)*0.3))))\n            ax2 = ax1.twinx()\n\n            # Plot mean abundance\n            sns.barplot(data=stats.head(10).reset_index(),\n                       x='mean_abundance',\n                       y='index',\n                       color='skyblue',\n                       ax=ax1)\n\n            # Plot presence percentage\n            stats.head(10)['presence'].plot(\n                marker='o',\n                color='red',\n                ax=ax2\n            )\n\n            ax1.set_title(f'{category.replace(\"_\", \" \").title()} Pathways')\n            ax1.set_xlabel('Mean Abundance')\n            ax2.set_xlabel('Presence (%)')\n\n            plt.tight_layout()\n            plt.show()\n\n# Calling the analysis\ncategory_data, category_stats = analyze_system_pathways(Picrust_Result)\norganic_data = analyze_organic_matter_pathways(Picrust_Result)\nplot_pathway_distributions(category_stats, category_data)","metadata":{"id":"IWBttrFbuvgW","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.188Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"I have a big gap on the cation anion account and then used mackensy, 2012 method from the usgs to check ec measured Vs calculated and cation Vs ions. It is a big gap still, but I have a lot of OM so I could no assume as normally that OM is CH4 so I attribute it to small organic acids and put acetate and oxalate as OM representatives, I have a small study of small acids form on failure analysis and also report of a mass that has a magnetic consistency, so I infere that those muss be some organic metalic compound but only accounted for AC- and Ox-2, I thought better to chose this other compounds Fe rich but I don't know how to do it actually. So in my bacteria I actually found lots of them with Ac- metabolism whiles I was looking at the families I realise no only oxobacter accendants, but others similar, also got important biofilm formers, there is also halogen related and should be, big deal of difference make the material and location cause water treatment, unfortunately the annotations are no to be taken as parameters but can serve as annotations","metadata":{"id":"uFrF4oTRuvgX"}},{"cell_type":"markdown","source":"validate assumptions about:\n\nOrganic acid presence (by showing metabolic capability)\nMetal-organic complex formation (through siderophore and metal-binding pathways)\nBiofilm formation potential (which can influence local chemistry)\n\nValidate acetate/oxalate assumptions by showing if these metabolic pathways are actually present\nLook for other potential organic acid pathways might want to consider\nIdentify metal-organic interaction pathways that could explain magnetic mass observation","metadata":{"id":"GXMFmbSruvgX"}},{"cell_type":"code","source":"def analyze_organic_metal_pathways(df):\n    \"\"\"\n    Analyze pathways related to organic acid metabolism and metal interactions\n\n    Parameters:\n    df: pandas DataFrame with pathways as index and samples as columns\n    \"\"\"\n    # Define specific pathway categories\n    pathway_categories = {\n        'organic_acid_metabolism': [\n            'acetate', 'acetic acid', 'acetyl',\n            'oxalate', 'oxalic acid',\n            'organic acid', 'fatty acid',\n            'carboxylic acid'\n        ],\n\n        'metal_organic_interaction': [\n            'siderophore', 'metal binding',\n            'iron complex', 'metal transport',\n            'metallophore', 'metal organic'\n        ],\n\n        'biofilm_formation': [\n            'biofilm', 'exopolysaccharide',\n            'extracellular matrix', 'adhesion'\n        ],\n\n        'halogen_related': [\n            'halogen', 'chloride', 'bromide',\n            'halide', 'dehalogenation'\n        ]\n    }\n\n    # Analyze each category\n    def get_category_pathways(terms):\n        return df.index[df.index.str.lower().str.contains('|'.join(terms), regex=True)]\n\n    pathway_data = {}\n    pathway_stats = {}\n\n    for category, terms in pathway_categories.items():\n        pathways = get_category_pathways(terms)\n        if len(pathways) > 0:\n            pathway_data[category] = df.loc[pathways]\n\n            # Calculate comprehensive statistics\n            pathway_stats[category] = pd.DataFrame({\n                'mean_abundance': pathway_data[category].mean(axis=1),\n                'std_abundance': pathway_data[category].std(axis=1),\n                'cv': pathway_data[category].std(axis=1) / pathway_data[category].mean(axis=1) * 100,\n                'presence': (pathway_data[category] > 0).mean(axis=1) * 100,  # % of samples with pathway\n                'relative_abundance': pathway_data[category].mean(axis=1) / df.mean(axis=1).mean() * 100\n            }).sort_values('mean_abundance', ascending=False)\n\n    return pathway_data, pathway_stats\n\ndef analyze_pathway_relationships(pathway_data):\n    \"\"\"\n    Analyze relationships between different pathway categories\n    \"\"\"\n    # Calculate mean abundance for each category across samples\n    category_means = pd.DataFrame({\n        category: data.mean(axis=0)\n        for category, data in pathway_data.items()\n    })\n\n    # Calculate correlations\n    correlations = category_means.corr()\n\n    # Identify potential functional relationships\n    high_correlations = correlations.unstack()\n    high_correlations = high_correlations[high_correlations != 1.0]\n    high_correlations = high_correlations[abs(high_correlations) > 0.5]\n\n    return category_means, correlations, high_correlations.sort_values(ascending=False)\n\ndef plot_pathway_analysis(pathway_stats, pathway_data):\n    \"\"\"\n    Create visualizations for pathway analysis\n    \"\"\"\n    for category, stats in pathway_stats.items():\n        if len(stats) > 0:\n            # Create subplot with dual axis\n            fig, ax1 = plt.subplots(figsize=(12, min(8, max(4, len(stats)*0.3))))\n            ax2 = ax1.twinx()\n\n            # Plot abundance and relative abundance\n            sns.barplot(data=stats.head(10).reset_index(),\n                       x='relative_abundance',\n                       y='index',\n                       color='skyblue',\n                       ax=ax1)\n\n            stats.head(10)['presence'].plot(\n                marker='o',\n                color='red',\n                ax=ax2\n            )\n\n            ax1.set_title(f'{category.replace(\"_\", \" \").title()} Pathways')\n            ax1.set_xlabel('Relative Abundance (%)')\n            ax2.set_xlabel('Presence (%)')\n\n            plt.tight_layout()\n            plt.show()\n\n# calling the function\npathway_data, pathway_stats = analyze_organic_metal_pathways(Picrust_Result)category_means, correlations, high_corr = analyze_pathway_relationships(pathway_data)\nplot_pathway_analysis(pathway_stats, pathway_data)","metadata":{"id":"d3sujtH1uvgX","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.189Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 9.6. Corrosion Relevant Pathways\n\nFocus on corrosion-relevant pathways by categorizing them into:\n\nOrganic acid metabolism (relevant to our acetate/oxalate observations)\nSulfur metabolism\nMetal interactions\nBiofilm formation\n\n\nHandle the high-dimensional data by:\n\nUsing dimensionality reduction (PCA)\nCalculating summary statistics\nVisualizing key patterns\n\n\nAddress our specific interests:\n\nOrganic matter metabolism pathways\nMetal-organic interactions\nCorrelations with physicochemical parameters","metadata":{"id":"IvV8Sor3uvgX"}},{"cell_type":"code","source":"def analyze_corrosion_pathways(df):\n    \"\"\"\n    Analyze pathways relevant to microbially influenced corrosion\n    \"\"\"\n    # Define pathway categories relevant to corrosion\n    pathway_categories = {\n        'organic_acid': [\n            'CENTFERM-PWY',  # Central fermentation pathways\n            'FERMENTATION-PWY',  # Mixed acid fermentation\n            'GLYCOLYSIS',  # Glucose fermentation\n            'PWY-5100',  # Pyruvate fermentation\n            'GALACTUROCAT-PWY'  # Galacturonate degradation\n        ],\n        'sulfur': [\n            'PWY-6932',  # Sulfate reduction\n            'SO4ASSIM-PWY',  # Sulfate assimilation\n            'SULFATE-CYS-PWY'  # Sulfate to cysteine\n        ],\n        'metal_interaction': [\n            'PWY-7219',  # Iron oxidation\n            'PWY-7221',  # Iron reduction\n            'HEME-BIOSYNTHESIS-II',  # Iron-containing compounds\n            'P125-PWY'  # Metal resistance\n        ],\n        'biofilm': [\n            'COLANSYN-PWY',  # Colanic acid (biofilm)\n            'EXOPOLYSACC-PWY',  # Exopolysaccharide\n            'GLUCOSE1PMETAB-PWY'  # UDP-glucose synthesis\n        ]\n    }\n\n    # Extract relevant pathways and their abundances\n    relevant_pathways = {}\n    for category, pathways in pathway_categories.items():\n        category_data = df[df.index.isin(pathways)]\n        if not category_data.empty:\n            relevant_pathways[category] = category_data\n\n    # Calculate summary statistics\n    summary_stats = {}\n    for category, data in relevant_pathways.items():\n        summary_stats[category] = {\n            'mean_abundance': data.mean().mean(),\n            'std_abundance': data.mean().std(),\n            'present_in_samples': (data > 0).mean().mean() * 100,\n            'pathways_found': len(data)\n        }\n\n    # Dimension reduction for visualization\n    if df.shape[0] > 0:\n        # Standardize the data\n        scaler = StandardScaler()\n        scaled_data = scaler.fit_transform(df.T)\n\n        # PCA\n        pca = PCA(n_components=2)\n        pca_result = pca.fit_transform(scaled_data)\n\n        return relevant_pathways, summary_stats, pca_result, pca.explained_variance_ratio_\n\n    return relevant_pathways, summary_stats, None, None\n\ndef plot_pathway_analysis(relevant_pathways, summary_stats, pca_result=None, explained_variance=None):\n    \"\"\"\n    Create visualizations for pathway analysis\n    \"\"\"\n    # Plot mean abundances by category\n    plt.figure(figsize=(12, 6))\n    categories = list(summary_stats.keys())\n    means = [stats['mean_abundance'] for stats in summary_stats.values()]\n    presence = [stats['present_in_samples'] for stats in summary_stats.values()]\n\n    ax1 = plt.gca()\n    ax2 = ax1.twinx()\n\n    bars = ax1.bar(categories, means, alpha=0.6, color='skyblue')\n    ax1.set_ylabel('Mean Abundance')\n\n    line = ax2.plot(categories, presence, 'ro-', label='Presence %')\n    ax2.set_ylabel('Presence in Samples (%)')\n\n    plt.title('Pathway Categories Overview')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.show()\n\n    # If PCA results available, plot them\n    if pca_result is not None:\n        plt.figure(figsize=(10, 8))\n        plt.scatter(pca_result[:, 0], pca_result[:, 1], alpha=0.6)\n        plt.xlabel(f'PC1 ({explained_variance[0]*100:.1f}%)')\n        plt.ylabel(f'PC2 ({explained_variance[1]*100:.1f}%)')\n        plt.title('PCA of Pathway Abundances')\n        plt.tight_layout()\n        plt.show()\n\n# Calling the functions\nrelevant_pathways, summary_stats, pca_result, explained_variance = analyze_corrosion_pathways(Picrust_Result)\nplot_pathway_analysis(relevant_pathways, summary_stats, pca_result, explained_variance)","metadata":{"id":"EThn4vkFuvgX","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.189Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 9.7. Functional Pathway Clustering Analysis\nHierarchical Clustering:\n\nGroups pathways based on their abundance patterns\nCreates a dendrogram to visualize relationships\nAutomatically determines optimal number of clusters\n\n\nCorrelation-based Analysis:\n\nIdentifies pathways that behave similarly across samples\nCreates correlation heatmap to visualize relationships\nHelps identify functional modules\n\n\nFeature Creation:\n\nGenerates new features based on cluster statistics:\n\nMean abundance per cluster\nTotal abundance per cluster\nPathway diversity within clusters\n\nReduce dimensionality while maintaining biological meaning\nIdentify functional modules that might be working together\nCreate more robust features for our ML analysis","metadata":{"id":"8ZgXC50euvgX"}},{"cell_type":"code","source":"def cluster_pathways(df, n_clusters=None, corr_threshold=0.7):\n    \"\"\"\n    Cluster pathways based on their functional similarity\n\n    Parameters:\n    df: DataFrame with pathways as rows and samples as columns\n    n_clusters: Number of clusters (if None, determined automatically)\n    corr_threshold: Correlation threshold for considering pathways related\n    \"\"\"\n    # Standardize the data\n    scaler = StandardScaler()\n    scaled_data = scaler.fit_transform(df.T).T\n\n    # Calculate correlation matrix\n    corr_matrix = np.corrcoef(scaled_data)\n\n    # Create linkage matrix for hierarchical clustering\n    linkage_matrix = hierarchy.linkage(pdist(scaled_data), method='ward')\n\n    if n_clusters is None:\n        # Automatically determine number of clusters using elbow method\n        last = linkage_matrix[-10:, 2]\n        acceleration = np.diff(last, 2)\n        n_clusters = len(last) - np.argmax(acceleration) + 1\n\n    # Perform clustering\n    clustering = AgglomerativeClustering(n_clusters=n_clusters)\n    cluster_labels = clustering.fit_predict(scaled_data)\n\n    # Create cluster summary\n    cluster_summary = pd.DataFrame({\n        'pathway': df.index,\n        'cluster': cluster_labels\n    })\n\n    return cluster_labels, linkage_matrix, corr_matrix, cluster_summary\n\ndef analyze_pathway_clusters(df, cluster_labels):\n    \"\"\"\n    Analyze the characteristics of each pathway cluster\n    \"\"\"\n    cluster_stats = {}\n\n    for cluster in np.unique(cluster_labels):\n        # Get pathways in this cluster\n        cluster_paths = df.index[cluster_labels == cluster]\n        cluster_data = df.loc[cluster_paths]\n\n        # Calculate statistics\n        cluster_stats[cluster] = {\n            'size': len(cluster_paths),\n            'mean_abundance': cluster_data.mean().mean(),\n            'std_abundance': cluster_data.mean().std(),\n            'pathways': list(cluster_paths),\n            'correlation': np.corrcoef(cluster_data),\n            'total_abundance': cluster_data.sum().mean()\n        }\n\n    return cluster_stats\n\ndef plot_pathway_clusters(df, linkage_matrix, corr_matrix, cluster_labels, cluster_stats):\n    \"\"\"\n    Create visualizations for pathway clusters\n    \"\"\"\n    # Plot dendrogram\n    plt.figure(figsize=(15, 10))\n    plt.title('Pathway Clustering Dendrogram')\n    hierarchy.dendrogram(linkage_matrix, labels=df.index, leaf_rotation=90)\n    plt.tight_layout()\n    plt.show()\n\n    # Plot correlation heatmap\n    plt.figure(figsize=(12, 12))\n    sns.heatmap(pd.DataFrame(corr_matrix, index=df.index, columns=df.index),\n                cmap='coolwarm', center=0, vmin=-1, vmax=1)\n    plt.title('Pathway Correlation Heatmap')\n    plt.tight_layout()\n    plt.show()\n\n    # Plot cluster sizes and abundances\n    plt.figure(figsize=(12, 6))\n    clusters = list(cluster_stats.keys())\n    sizes = [stats['size'] for stats in cluster_stats.values()]\n    abundances = [stats['mean_abundance'] for stats in cluster_stats.values()]\n\n    ax1 = plt.gca()\n    ax2 = ax1.twinx()\n\n    ax1.bar(clusters, sizes, alpha=0.6, color='skyblue')\n    ax1.set_ylabel('Number of Pathways')\n\n    ax2.plot(clusters, abundances, 'ro-')\n    ax2.set_ylabel('Mean Abundance')\n\n    plt.title('Cluster Sizes and Abundances')\n    plt.tight_layout()\n    plt.show()\n\ndef create_cluster_features(df, cluster_labels):\n    \"\"\"\n    Create new features based on pathway clusters\n    \"\"\"\n    n_clusters = len(np.unique(cluster_labels))\n    cluster_features = pd.DataFrame(index=df.columns)\n\n    for cluster in range(n_clusters):\n        # Get pathways in this cluster\n        cluster_paths = df.index[cluster_labels == cluster]\n\n        # Calculate mean abundance for cluster\n        cluster_features[f'cluster_{cluster}'] = df.loc[cluster_paths].mean()\n\n        # Calculate total abundance for cluster\n        cluster_features[f'cluster_{cluster}_total'] = df.loc[cluster_paths].sum()\n\n        # Calculate diversity within cluster\n        cluster_features[f'cluster_{cluster}_diversity'] = (df.loc[cluster_paths] > 0).sum()\n\n    return cluster_features\n\n# Calling the fUNCTION\ncluster_labels, linkage_matrix, corr_matrix, cluster_summary = cluster_pathways(Picrust_Result)\ncluster_stats = analyze_pathway_clusters(Picrust_Result, cluster_labels)\nplot_pathway_clusters(Picrust_Result, linkage_matrix, corr_matrix, cluster_labels, cluster_stats)\ncluster_features = create_cluster_features(Picrust_Result, cluster_labels)","metadata":{"id":"l6NsHQCquvgX","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.189Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 13 Organic Metal Pathways","metadata":{"id":"Os65js9EuvgX"}},{"cell_type":"code","source":"\ndef analyze_organic_metal_pathways(df):\n    \"\"\"\n    Analyze pathways related to organic acid metabolism and metal interactions\n\n    Parameters:\n    df: pandas DataFrame with pathways as index and samples as columns\n    \"\"\"\n    # Define specific pathway categories\n    pathway_categories = {\n        'organic_acid_metabolism': [\n            'acetate', 'acetic acid', 'acetyl',\n            'oxalate', 'oxalic acid',\n            'organic acid', 'fatty acid',\n            'carboxylic acid'\n        ],\n\n        'metal_organic_interaction': [\n            'siderophore', 'metal binding',\n            'iron complex', 'metal transport',\n            'metallophore', 'metal organic'\n        ],\n\n        'biofilm_formation': [\n            'biofilm', 'exopolysaccharide',\n            'extracellular matrix', 'adhesion'\n        ],\n\n        'halogen_related': [\n            'halogen', 'chloride', 'bromide',\n            'halide', 'dehalogenation'\n        ]\n    }\n\n    # Analyze each category\n    def get_category_pathways(terms):\n        return df.index[df.index.str.lower().str.contains('|'.join(terms), regex=True)]\n\n    pathway_data = {}\n    pathway_stats = {}\n\n    for category, terms in pathway_categories.items():\n        pathways = get_category_pathways(terms)\n        if len(pathways) > 0:\n            pathway_data[category] = df.loc[pathways]\n\n            # Calculate comprehensive statistics\n            pathway_stats[category] = pd.DataFrame({\n                'mean_abundance': pathway_data[category].mean(axis=1),\n                'std_abundance': pathway_data[category].std(axis=1),\n                'cv': pathway_data[category].std(axis=1) / pathway_data[category].mean(axis=1) * 100,\n                'presence': (pathway_data[category] > 0).mean(axis=1) * 100,  # % of samples with pathway\n                'relative_abundance': pathway_data[category].mean(axis=1) / df.mean(axis=1).mean() * 100\n            }).sort_values('mean_abundance', ascending=False)\n\n    return pathway_data, pathway_stats\n\ndef analyze_pathway_relationships(pathway_data):\n    \"\"\"\n    Analyze relationships between different pathway categories\n    \"\"\"\n    # Calculate mean abundance for each category across samples\n    category_means = pd.DataFrame({\n        category: data.mean(axis=0)\n        for category, data in pathway_data.items()\n    })\n\n    # Calculate correlations\n    correlations = category_means.corr()\n\n    # Identify potential functional relationships\n    high_correlations = correlations.unstack()\n    high_correlations = high_correlations[high_correlations != 1.0]\n    high_correlations = high_correlations[abs(high_correlations) > 0.5]\n\n    return category_means, correlations, high_correlations.sort_values(ascending=False)\n\ndef plot_pathway_analysis(pathway_stats, pathway_data):\n    \"\"\"\n    Create visualizations for pathway analysis\n    \"\"\"\n    for category, stats in pathway_stats.items():\n        if len(stats) > 0:\n            # Create subplot with dual axis\n            fig, ax1 = plt.subplots(figsize=(12, min(8, max(4, len(stats)*0.3))))\n            ax2 = ax1.twinx()\n\n            # Plot abundance and relative abundance\n            sns.barplot(data=stats.head(10).reset_index(),\n                       x='relative_abundance',\n                       y='index',\n                       color='skyblue',\n                       ax=ax1)\n\n            stats.head(10)['presence'].plot(\n                marker='o',\n                color='red',\n                ax=ax2\n            )\n\n            ax1.set_title(f'{category.replace(\"_\", \" \").title()} Pathways')\n            ax1.set_xlabel('Relative Abundance (%)')\n            ax2.set_xlabel('Presence (%)')\n\n            plt.tight_layout()\n            plt.show()","metadata":{"id":"Vgw9Pwp8uvgX","trusted":true,"execution":{"execution_failed":"2025-03-30T18:38:54.189Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{"id":"Wf76SqphuvgX"}}]}